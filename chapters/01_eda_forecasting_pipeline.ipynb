{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e107b9e",
   "metadata": {},
   "source": [
    "# Complete Learning Guide — ATSAF Course Materials\n",
    "\n",
    "## Overview\n",
    "\n",
    "This directory contains comprehensive learning documentation for the ATSAF (Automated Time Series Forecasting) project. All materials are aligned with the actual codebase implementation, verified against source files, and include runnable examples, checkpoints, and exercises.\n",
    "\n",
    "## Structure\n",
    "\n",
    "Each chapter markdown follows a consistent template:\n",
    "\n",
    "1. **Outcomes** — What you can do after completing the chapter\n",
    "2. **Concepts** — Plain English explanations of key terms\n",
    "3. **Architecture** — What inputs/outputs/invariants define the system\n",
    "4. **Files Touched** — Which source files to read and understand\n",
    "5. **Step-by-Step Walkthrough** — Runnable examples with expected outputs\n",
    "6. **Metrics & Success Criteria** — How to know you've succeeded\n",
    "7. **Pitfalls** — Common mistakes and how to avoid them\n",
    "8. **Mini-Checkpoint** — Self-test questions\n",
    "9. **Exercises** — Optional hands-on practice (Easy/Medium/Hard)\n",
    "\n",
    "---\n",
    "\n",
    "## Chapters\n",
    "\n",
    "### [Chapter 0: Time Series Objects & Contracts (Python)](chapter0_time_series_objects.md)\n",
    "\n",
    "**Timeframe**: ~30-45 minutes to read and run\n",
    "\n",
    "**What you'll learn:**\n",
    "- How Python represents time-series objects (Series, DatetimeIndex)\n",
    "- The `unique_id, ds, y` data contract and why StatsForecast depends on it\n",
    "- How to normalize timestamps to UTC and detect DST edge cases\n",
    "- How to validate time-series integrity before modeling\n",
    "\n",
    "**Key files:**\n",
    "- `src/chapter0/objects.py` (Chapter 0 helpers)\n",
    "- `src/chapter1/validate.py` (validate_time_index)\n",
    "- `src/chapter1/eia_data_simple.py` (prepare_for_forecasting, validate_time_series_integrity)\n",
    "- `src/chapter3/tasks.py` (compute_time_series_integrity)\n",
    "\n",
    "**Success criteria:**\n",
    "- Can explain ts vs tsibble vs timetk in Python terms\n",
    "- Can build and validate a forecasting-ready table\n",
    "- Can spot timezone/DST risks before modeling\n",
    "\n",
    "---\n",
    "\n",
    "### [Chapter 1: Data Ingestion & Preparation](chapter1_ingestion.md)\n",
    "\n",
    "**Timeframe**: ~2-4 hours to read, understand, and run\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to fetch data from the EIA API with pagination and error handling\n",
    "- How to normalize data to UTC and validate time-series integrity\n",
    "- Why DST transitions break naive assumptions and how to detect them\n",
    "- How to transform raw data into forecasting-ready format (unique_id, ds, y)\n",
    "\n",
    "**Key files:**\n",
    "- `src/chapter1/eia_data_simple.py` (main orchestrator)\n",
    "- `src/chapter1/ingest.py`, `prepare.py`, `validate.py` (helper modules)\n",
    "\n",
    "**Success criteria:**\n",
    "- Can pull raw data, validate it, and format for forecasting\n",
    "- Understand time-series integrity checks (duplicates, missing hours, DST)\n",
    "- Can explain why UTC normalization is non-negotiable\n",
    "\n",
    "---\n",
    "\n",
    "### [Chapter 2: Experimentation & Backtesting](chapter2_experimentation.md)\n",
    "\n",
    "**Timeframe**: ~3-5 hours to read, understand, and experiment\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to design rolling-origin cross-validation for time-series\n",
    "- How to build a leaderboard comparing multiple models\n",
    "- When metrics like MAPE fail and what to use instead (RMSE, MASE)\n",
    "- How to calibrate prediction intervals and interpret coverage\n",
    "\n",
    "**Key files:**\n",
    "- `src/chapter2/backtesting.py` (rolling/expanding window strategies)\n",
    "- `src/chapter2/models.py` (model implementations: ARIMA, Prophet, XGBoost)\n",
    "- `src/chapter2/training.py` (orchestrate CV across splits)\n",
    "- `src/chapter2/evaluation.py` (compute metrics, rank models)\n",
    "\n",
    "**Success criteria:**\n",
    "- Can run CV and reproduce a leaderboard\n",
    "- Understand temporal leakage and why it's prevented\n",
    "- Can explain MAPE pitfalls and recommend alternatives\n",
    "\n",
    "---\n",
    "\n",
    "### [Chapter 3: Orchestration & Pipeline DAG](chapter3_orchestration.md)\n",
    "\n",
    "**Timeframe**: ~2-3 hours to read, understand, and deploy\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to decompose a forecasting workflow into independent, rerunnable tasks\n",
    "- Why idempotency matters and how atomic writes prevent corruption\n",
    "- How to build a DAG (Directed Acyclic Graph) of dependencies\n",
    "- How to deploy to Airflow for automated scheduling\n",
    "\n",
    "**Key files:**\n",
    "- `src/chapter3/tasks.py` (6 tasks: ingest, prepare, validate, train, register, forecast)\n",
    "- `src/chapter3/dag_builder.py` (Airflow DAG definition)\n",
    "- `src/chapter3/cli.py` (Typer-based CLI for manual runs)\n",
    "\n",
    "**Success criteria:**\n",
    "- Can run pipeline end-to-end from CLI\n",
    "- Understand why tasks are linear (no branching, yet)\n",
    "- Can verify idempotency: re-running produces same outputs\n",
    "\n",
    "---\n",
    "\n",
    "### [Chapter 4: Monitoring, Drift Detection & Alerts](chapter4_monitoring.md)\n",
    "\n",
    "**Timeframe**: ~4-6 hours to read, understand, and implement\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to persist forecasts in a queryable database\n",
    "- How to score forecasts against actuals (as they arrive)\n",
    "- How to detect model drift using statistical thresholds\n",
    "- How to monitor data freshness, completeness, and forecast staleness\n",
    "- When and how to trigger alerts\n",
    "\n",
    "**Key files:**\n",
    "- `src/chapter4/db.py` (SQLite schema and CRUD)\n",
    "- `src/chapter4/forecast_store.py` (persist wide → long format)\n",
    "- `src/chapter4/scoring.py` (join forecasts with actuals)\n",
    "- `src/chapter4/drift.py` (threshold-based drift detection)\n",
    "- `src/chapter4/alerts.py` (alert configuration and checking)\n",
    "- `src/chapter4/health.py` (data freshness/completeness checks)\n",
    "\n",
    "**Success criteria:**\n",
    "- Can persist forecasts and score them\n",
    "- Understand drift thresholds (mean ± k*std from backtest)\n",
    "- Can interpret alerts and decide when to retrain\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1) Run the full pipeline\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24\n",
    "```\n",
    "\n",
    "### 2) Read the relevant chapter\n",
    "- New to the objects and contracts? → Read [Chapter 0](chapter0_time_series_objects.md)\n",
    "- Just ingested data? → Read [Chapter 1](chapter1_ingestion.md)\n",
    "- Training models? → Read [Chapter 2](chapter2_experimentation.md)\n",
    "- Running pipelines? → Read [Chapter 3](chapter3_orchestration.md)\n",
    "- Monitoring forecasts? → Read [Chapter 4](chapter4_monitoring.md)\n",
    "\n",
    "### 3) Run the chapter's walkthrough\n",
    "Each chapter has a \"Step-by-Step Walkthrough\" section with copy-paste examples.\n",
    "\n",
    "### 4) Attempt the mini-checkpoint\n",
    "Self-test at the end of each chapter to verify understanding.\n",
    "\n",
    "### 5) (Optional) Attempt exercises\n",
    "Exercises range from easy (data exploration) to hard (refactoring for new requirements).\n",
    "\n",
    "---\n",
    "\n",
    "## Alignment with Actual Code\n",
    "\n",
    "### What Changed From Initial Plan\n",
    "\n",
    "The plan initially suggested implementing MLForecast + conformal prediction intervals, but **the actual codebase uses StatsForecast with model-based intervals**. All learning materials have been rewritten to match the real implementation.\n",
    "\n",
    "Similarly, Chapter 3 was described as having \"branching logic\" for no-new-data / validation failures, but the **actual pipeline is linear** (strict sequence). All examples reflect this reality.\n",
    "\n",
    "### Code Fixes Applied\n",
    "\n",
    "To ensure learning materials match code exactly, the following issues were corrected:\n",
    "\n",
    "1. **validate_clean() key mismatch** (Chapter 3, tasks.py)\n",
    "   - Error message was referencing non-existent keys (`duplicate_count`, `missing_hours_total`)\n",
    "   - **Fixed**: Changed to use actual keys from `compute_time_series_integrity()` (`duplicate_pairs`, `missing_hours`)\n",
    "\n",
    "2. **Hardcoded confidence level \"95\"** (Chapter 1, eia_data_simple.py)\n",
    "   - `evaluate_forecast()` accepted `confidence_level` parameter but then hardcoded 95 when looking up `{model}-lo-95` / `{model}-hi-95` columns\n",
    "   - **Fixed**: Parameterized to use `confidence_level` variable (lines 913-914, 933-934)\n",
    "\n",
    "3. **MAPE with zeros warning** (Chapter 1, eia_data_simple.py)\n",
    "   - MAPE metric can explode or become infinite when y ≈ 0\n",
    "   - **Added**: Warning in docstring recommending RMSE/MAE/MASE instead\n",
    "   - Also documented in Chapter 2 pitfalls section\n",
    "\n",
    "---\n",
    "\n",
    "## Design Principles Reflected in Materials\n",
    "\n",
    "### 1) Verifiability\n",
    "- Every claim about the code is traceable to actual source files\n",
    "- Every example is runnable (not pseudo-code)\n",
    "- Success criteria are observable (not vague)\n",
    "\n",
    "### 2) Fail-Loud Pattern\n",
    "- Code raises clear errors when assumptions are violated (e.g., validate_clean() raises on integrity failure)\n",
    "- Learning materials emphasize *why* this matters (prevents bad data from reaching training)\n",
    "\n",
    "### 3) Idempotency & Atomicity\n",
    "- Tasks can be re-run and produce same outputs\n",
    "- Writes are atomic: all-or-nothing (not partial files left behind)\n",
    "- Learning materials emphasize *when* to exploit this and *how* to verify it works\n",
    "\n",
    "### 4) Metrics Over Promises\n",
    "- Primary metrics clearly identified (RMSE > MAPE, MASE for drift detection)\n",
    "- Secondary metrics explained with context (why MAPE fails, why coverage matters)\n",
    "- Thresholds are data-driven (from backtest, not magic numbers)\n",
    "\n",
    "---\n",
    "\n",
    "## Common Learning Paths\n",
    "\n",
    "### Path 1: \"I just want to run forecasts\"\n",
    "1. Read Chapter 0 (10 min) — understand objects + contract\n",
    "2. Read Chapter 1 (15 min) — understand data\n",
    "3. Skim Chapter 2 (10 min) — know the model exists\n",
    "4. Read Chapter 3 (20 min) — run the pipeline\n",
    "5. **Done!**\n",
    "\n",
    "### Path 2: \"I want to understand the full system\"\n",
    "1. Read Chapter 0 (30 min) + run walkthrough (15 min)\n",
    "2. Read Chapter 1 (1 hour) + run walkthrough (30 min)\n",
    "3. Read Chapter 2 (1.5 hours) + run walkthrough (1 hour) + attempt exercises\n",
    "4. Read Chapter 3 (1 hour) + run walkthrough (30 min)\n",
    "5. Read Chapter 4 (1.5 hours) + run walkthrough (1 hour)\n",
    "6. **Total: ~9 hours**\n",
    "\n",
    "### Path 3: \"I want to modify the system\"\n",
    "1. Do Path 2 above\n",
    "2. For each chapter you want to modify, read \"Pitfalls\" section carefully\n",
    "3. Read the actual source code (not just examples)\n",
    "4. Make changes incrementally, verify idempotency at each step\n",
    "5. Update this learning guide if you change architecture\n",
    "\n",
    "---\n",
    "\n",
    "## FAQ\n",
    "\n",
    "### Q: Why is Chapter X different from the course plan?\n",
    "**A:** The course plan described one implementation (MLForecast + branching DAG + conformal intervals), but your actual codebase uses StatsForecast + linear DAG + model-based intervals. Learning materials reflect what you *actually* built, not the original plan.\n",
    "\n",
    "### Q: How do I know if I've mastered a chapter?\n",
    "**A:** Complete the \"Mini-Checkpoint\" questions without looking at answers. If you can explain all 4 questions clearly, you've got it.\n",
    "\n",
    "### Q: Can I just read the code without the learning guides?\n",
    "**A:** Yes, but you'll spend more time and miss important context. The guides are condensed versions of \"what matters\" and \"why.\" The code has all the details; the guides have all the insight.\n",
    "\n",
    "### Q: Why does the MAPE warning say it's undefined at zero?\n",
    "**A:** MAPE = sum(|error| / |actual|). When actual ≈ 0, the denominator is tiny → ratio explodes. This happens with solar (night = 0), wind (calm = 0), or any fuel with downtime. RMSE/MAE don't have this problem.\n",
    "\n",
    "### Q: What should I do if my forecasts drift?\n",
    "**A:** (1) Re-run Chapter 4 \"detect_drift()\" to confirm threshold breached. (2) Re-ingest recent data (Chapter 1) to check for data quality issues. (3) Re-train (Chapter 2, 3) with recent data. (4) Check for external events (holidays, policy changes, weather patterns).\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate\n",
    "- [ ] Run full pipeline from Chapter 3 CLI\n",
    "- [ ] Read Chapter 0 (objects + data contract)\n",
    "- [ ] Read Chapter 1 (data understanding)\n",
    "- [ ] Read Chapter 3 (pipeline orchestration)\n",
    "\n",
    "### Short-term (1-2 weeks)\n",
    "- [ ] Complete Chapter 2 walkthrough + exercises\n",
    "- [ ] Deploy to Airflow (Chapter 3, optional)\n",
    "- [ ] Set up monitoring alerts (Chapter 4, optional)\n",
    "\n",
    "### Medium-term (1-3 months)\n",
    "- [ ] Modify pipeline for different respondent/fuel types\n",
    "- [ ] Implement email/Slack alerts (Chapter 4)\n",
    "- [ ] Build dashboard (query Chapter 4 database)\n",
    "- [ ] Retrain on drift (orchestrate Chapter 3 → 4 feedback loop)\n",
    "\n",
    "### Long-term\n",
    "- [ ] Implement branching logic (no-new-data → skip training)\n",
    "- [ ] Add conformal prediction intervals (Chapter 2 extension)\n",
    "- [ ] Multi-model ensemble (Chapter 2 extension)\n",
    "- [ ] Automated hyperparameter tuning (Chapter 2 extension)\n",
    "\n",
    "---\n",
    "\n",
    "## Feedback & Improvements\n",
    "\n",
    "These learning materials are **living documentation**. If you find:\n",
    "- An example that doesn't run\n",
    "- A concept that's unclear\n",
    "- A pitfall not mentioned\n",
    "- A code issue not listed above\n",
    "\n",
    "Please update the relevant chapter markdown. **Keep materials honest**: if it's not in the code, don't claim it in the guide.\n",
    "\n",
    "---\n",
    "\n",
    "**Last updated:** January 11, 2026\n",
    "**Alignment verified against:** src/chapter[1-4]/, latest commits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5311a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"Current Working Directory:\", cwd)\n",
    "\n",
    "# change to c:\\docker_projects\\atsaf if not in currently\n",
    "target_dir = r'c:\\docker_projects\\atsaf'\n",
    "if cwd.lower() != target_dir.lower():\n",
    "    os.chdir(target_dir)\n",
    "    print(\"Changed Directory to:\", target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8f973",
   "metadata": {},
   "source": [
    "# Chapter 0 - Time Series Objects & Contracts (Python)\n",
    "\n",
    "## Outcomes (what I can do after this)\n",
    "\n",
    "- [ ] I can explain the Python equivalents of R ts / tsibble / timetk\n",
    "- [ ] I can create a forecasting-ready DataFrame with columns unique_id, ds, y\n",
    "- [ ] I can normalize timestamps to UTC and reason about DST edge cases\n",
    "- [ ] I can validate time-series integrity before modeling\n",
    "- [ ] I can add basic time-based features without leakage\n",
    "\n",
    "## Concepts (plain English)\n",
    "\n",
    "- **Datetime vs Timestamp**: Python `datetime` is the standard type; pandas wraps it as `Timestamp`\n",
    "- **Timezone-aware vs naive**: naive timestamps have no timezone; always normalize to UTC\n",
    "- **ts (R)**: a single series with a time index -> `pd.Series` with a `DatetimeIndex`\n",
    "- **tsibble (R)**: tidy time-series table -> DataFrame with `unique_id, ds, y`\n",
    "- **timetk (R)**: time-based feature helpers -> pandas `.dt`, `shift`, `rolling`, `expanding`\n",
    "- **Contract**: invariants that every downstream stage assumes (sorted, unique, regular)\n",
    "\n",
    "## R to Python mapping (where it fits)\n",
    "\n",
    "- **ts** -> `pd.Series` with `DatetimeIndex` (single series; good for quick checks)\n",
    "- **tsibble** -> long DataFrame `unique_id, ds, y` (StatsForecast and pipeline contract)\n",
    "- **timetk** -> pandas feature engineering helpers + optional `statsmodels` diagnostics\n",
    "- **Validation layer** -> `validate_time_series_integrity()` / `validate_time_index()`\n",
    "\n",
    "## Architecture (what we're building)\n",
    "\n",
    "### Inputs\n",
    "- Single series or tidy table from any source (API, CSV, Parquet)\n",
    "- Minimum fields: timestamps + values; plus series id for multi-series\n",
    "\n",
    "### Outputs\n",
    "- Forecasting-ready DataFrame with columns `[unique_id, ds, y]` (UTC, hourly)\n",
    "\n",
    "### Invariants (must always hold)\n",
    "- `ds` is normalized to UTC (timezone-naive in StatsForecast pipeline)\n",
    "- No duplicate `(unique_id, ds)` pairs\n",
    "- Regular hourly frequency with no gaps (unless explicitly allowed)\n",
    "- Data sorted by `unique_id, ds`\n",
    "- No leakage: features are built from past data only\n",
    "\n",
    "### Failure modes\n",
    "- Naive local timestamps (no timezone) -> DST ambiguity\n",
    "- DST fall-back duplicates (same hour twice) -> duplicate pairs\n",
    "- Missing hours -> gaps break backtesting\n",
    "- Mixed frequencies across series -> invalid model input\n",
    "\n",
    "## Files touched\n",
    "\n",
    "- **`src/chapter0/objects.py`** - Chapter 0 helpers (ts, tsibble, timetk-style)\n",
    "- **`src/chapter1/prepare.py`** - timezone parsing and normalization\n",
    "- **`src/chapter1/eia_data_simple.py`** - `prepare_for_forecasting()`, `validate_time_series_integrity()`\n",
    "- **`src/chapter1/validate.py`** - `validate_time_index()` and report printer\n",
    "- **`src/chapter3/tasks.py`** - `compute_time_series_integrity()` gate in pipeline\n",
    "\n",
    "## Step-by-step walkthrough\n",
    "\n",
    "### 1) Create a single-series \"ts\" object\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "idx = pd.date_range(\"2024-01-01\", periods=6, freq=\"h\", tz=\"UTC\")\n",
    "y = pd.Series([100, 102, 98, 101, 103, 99], index=idx)\n",
    "print(type(y), y.index.tz)\n",
    "```\n",
    "- **Expect**: pandas Series with UTC-aware index\n",
    "\n",
    "### 2) Convert to a \"tsibble\" style table\n",
    "```python\n",
    "df = y.reset_index()\n",
    "df.columns = [\"ds\", \"y\"]\n",
    "df[\"ds\"] = pd.to_datetime(df[\"ds\"], utc=True).dt.tz_localize(None)\n",
    "df[\"unique_id\"] = \"NG_US48\"\n",
    "df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "print(df.head())\n",
    "```\n",
    "- **Expect**: columns `[unique_id, ds, y]` with timezone-naive UTC timestamps\n",
    "\n",
    "### 3) Validate the time-series contract\n",
    "```python\n",
    "from src.chapter1.validate import validate_time_index, print_validation_report\n",
    "\n",
    "report = validate_time_index(df)\n",
    "print_validation_report(report)\n",
    "```\n",
    "- **Expect**: PASS, no duplicates, no missing hours\n",
    "\n",
    "### 4) Add timetk-style features (safe, leakage-free)\n",
    "```python\n",
    "df_features = df.assign(\n",
    "    hour=df[\"ds\"].dt.hour,\n",
    "    dayofweek=df[\"ds\"].dt.dayofweek,\n",
    "    y_lag1=df[\"y\"].shift(1),\n",
    "    y_roll24=df[\"y\"].rolling(24, min_periods=1).mean()\n",
    ")\n",
    "print(df_features.head())\n",
    "```\n",
    "- **Expect**: new columns; note the first row has NaNs for lagged values\n",
    "\n",
    "## Metrics & success criteria\n",
    "\n",
    "- `validate_time_index(df).is_valid` is True\n",
    "- `df[\"ds\"].dt.tz` is None (timezone-naive UTC)\n",
    "- No duplicate pairs or missing hours\n",
    "\n",
    "## Pitfalls (things that commonly break)\n",
    "\n",
    "1. **Naive timestamps**: missing timezone info hides DST issues\n",
    "2. **DST transitions**: fall-back creates duplicates, spring-forward creates gaps\n",
    "3. **Wide format**: multiple value columns; StatsForecast expects long format\n",
    "4. **Sorting**: unsorted `ds` breaks backtesting and feature alignment\n",
    "5. **Leakage**: rolling/lag features must use past values only\n",
    "\n",
    "## Mini-checkpoint (prove you learned it)\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "1. **What is the Python equivalent of an R `ts` object?**\n",
    "2. **Why does the pipeline require `unique_id, ds, y` even for a single series?**\n",
    "3. **What two DST problems does the integrity check catch?**\n",
    "4. **Which functions enforce the data contract in this repo?**\n",
    "\n",
    "**Answers:**\n",
    "1. A `pd.Series` with a `DatetimeIndex`.\n",
    "2. StatsForecast is multi-series-first; the contract stays consistent across one or many series.\n",
    "3. Fall-back duplicates and spring-forward missing hours.\n",
    "4. `validate_time_index()` and `validate_time_series_integrity()` / `compute_time_series_integrity()`.\n",
    "\n",
    "## Exercises (optional, but recommended)\n",
    "\n",
    "### Easy\n",
    "1. Build a 48-hour series and verify `validate_time_index()` passes.\n",
    "2. Introduce one duplicate timestamp and confirm the report fails.\n",
    "\n",
    "### Medium\n",
    "1. Create a series in `US/Eastern`, convert to UTC, and confirm `ds` is UTC.\n",
    "2. Introduce a 3-hour gap and compute how many missing hours are reported.\n",
    "\n",
    "### Hard\n",
    "1. Write a helper that enforces the `[unique_id, ds, y]` contract and call it before backtesting.\n",
    "2. Add a safe holiday or calendar feature using only `ds` (no future leakage).\n",
    "3. Extend the validation to allow missing hours within a known maintenance window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/chapter0/__init__.py\n",
    "# file: src/chapter0/__init__.py\n",
    "\"\"\"\n",
    "Chapter 0: Time Series Objects and Contracts (Python)\n",
    "\"\"\"\n",
    "\n",
    "from .objects import (\n",
    "    add_time_features,\n",
    "    assert_tsibble_contract,\n",
    "    normalize_to_utc,\n",
    "    to_ts_series,\n",
    "    to_tsibble,\n",
    "    validate_tsibble,\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    \"add_time_features\",\n",
    "    \"assert_tsibble_contract\",\n",
    "    \"normalize_to_utc\",\n",
    "    \"to_ts_series\",\n",
    "    \"to_tsibble\",\n",
    "    \"validate_tsibble\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62261bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/chapter0/objects.py\n",
    "# file: src/chapter0/objects.py\n",
    "\"\"\"\n",
    "Chapter 0: Python equivalents for ts / tsibble / timetk concepts.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.chapter1.validate import validate_time_index\n",
    "\n",
    "\n",
    "def normalize_to_utc(df: pd.DataFrame, ds_col: str = \"ds\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize a datetime column to timezone-naive UTC.\n",
    "\n",
    "    StatsForecast expects timezone-naive UTC timestamps.\n",
    "    \"\"\"\n",
    "    if ds_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required datetime column: {ds_col}\")\n",
    "\n",
    "    normalized = df.copy()\n",
    "    normalized[ds_col] = (\n",
    "        pd.to_datetime(normalized[ds_col], errors=\"raise\", utc=True)\n",
    "        .dt.tz_localize(None)\n",
    "    )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def to_ts_series(\n",
    "    df: pd.DataFrame,\n",
    "    ds_col: str = \"ds\",\n",
    "    y_col: str = \"y\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Create a single-series ts object: pd.Series with a DatetimeIndex (UTC-aware).\n",
    "    \"\"\"\n",
    "    if ds_col not in df.columns or y_col not in df.columns:\n",
    "        raise ValueError(f\"Expected columns: {ds_col}, {y_col}\")\n",
    "\n",
    "    ds = pd.to_datetime(df[ds_col], errors=\"raise\", utc=True)\n",
    "    series = pd.Series(df[y_col].to_numpy(), index=ds)\n",
    "    series.index = series.index.tz_convert(\"UTC\")\n",
    "    return series\n",
    "\n",
    "\n",
    "def to_tsibble(\n",
    "    df: pd.DataFrame,\n",
    "    unique_id_col: str = \"unique_id\",\n",
    "    ds_col: str = \"ds\",\n",
    "    y_col: str = \"y\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a tidy time-series table with columns [unique_id, ds, y].\n",
    "    \"\"\"\n",
    "    missing = [col for col in (unique_id_col, ds_col, y_col) if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    tidy = df[[unique_id_col, ds_col, y_col]].copy()\n",
    "    tidy.columns = [\"unique_id\", \"ds\", \"y\"]\n",
    "    tidy = normalize_to_utc(tidy, ds_col=\"ds\")\n",
    "    return tidy\n",
    "\n",
    "\n",
    "def validate_tsibble(df: pd.DataFrame) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate the tsibble contract using the Chapter 1 integrity check.\n",
    "    \"\"\"\n",
    "    result = validate_time_index(df)\n",
    "    if result.is_valid:\n",
    "        return True, \"valid\"\n",
    "    return False, \"invalid\"\n",
    "\n",
    "\n",
    "def assert_tsibble_contract(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Raise a ValueError if the tsibble contract is violated.\n",
    "    \"\"\"\n",
    "    result = validate_time_index(df)\n",
    "    if not result.is_valid:\n",
    "        raise ValueError(\n",
    "            f\"Invalid tsibble: duplicates={result.n_duplicates}, \"\n",
    "            f\"missing_hours={result.n_missing_hours}, \"\n",
    "            f\"monotonic={result.is_monotonic}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def add_time_features(\n",
    "    df: pd.DataFrame,\n",
    "    ds_col: str = \"ds\",\n",
    "    y_col: str = \"y\",\n",
    "    lags: Iterable[int] = (1, 24, 168),\n",
    "    rolling_windows: Iterable[int] = (24, 168),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Basic timetk-style features using pandas (safe, past-only).\n",
    "    \"\"\"\n",
    "    features = df.copy()\n",
    "    features = normalize_to_utc(features, ds_col=ds_col)\n",
    "    features = features.sort_values(ds_col).reset_index(drop=True)\n",
    "\n",
    "    features[\"hour\"] = features[ds_col].dt.hour\n",
    "    features[\"dayofweek\"] = features[ds_col].dt.dayofweek\n",
    "    features[\"month\"] = features[ds_col].dt.month\n",
    "\n",
    "    for lag in lags:\n",
    "        features[f\"y_lag_{lag}\"] = features[y_col].shift(lag)\n",
    "\n",
    "    shifted = features[y_col].shift(1)\n",
    "    for window in rolling_windows:\n",
    "        features[f\"y_roll_mean_{window}\"] = shifted.rolling(window, min_periods=1).mean()\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3917a",
   "metadata": {},
   "source": [
    "# Chapter 1 — Data Ingestion & Preparation\n",
    "\n",
    "## Outcomes (what I can do after this)\n",
    "\n",
    "- [ ] I can pull raw EIA electricity data via REST API with proper pagination and error handling\n",
    "- [ ] I can validate time-series data for duplicates, missing hours, and DST edge cases\n",
    "- [ ] I can transform raw data into forecasting-ready format (unique_id, ds, y)\n",
    "- [ ] I can explain why UTC normalization and data sorting matter for backtesting\n",
    "\n",
    "## Prerequisite (read first)\n",
    "\n",
    "- Chapter 0 for the time-series contract (unique_id, ds, y) and UTC rules\n",
    "\n",
    "## Concepts (plain English)\n",
    "\n",
    "- **API Pagination**: Splitting large datasets into fixed-size chunks to avoid timeouts\n",
    "- **Datetime Normalization**: Converting all timestamps to UTC and handling DST transitions\n",
    "- **Time-series Integrity**: Detecting duplicates, missing hours, and repeated timestamps (DST fall-back)\n",
    "- **Monotonicity**: Ensuring timestamps are sorted chronologically (required for backtesting)\n",
    "- **Data Schema**: unique_id (series identifier), ds (datetime), y (numeric value); tsibble analog in Python\n",
    "\n",
    "## Architecture (what we're building)\n",
    "\n",
    "### Inputs\n",
    "- EIA API credentials (via environment variable `EIA_API_KEY`)\n",
    "- Data range (start_date, end_date as YYYY-MM-DD strings)\n",
    "- Series identifier (respondent, fueltype)\n",
    "\n",
    "### Outputs\n",
    "- **raw.parquet**: Unmodified API response (columns: time, value, respondent, fueltype)\n",
    "- **clean.parquet**: Normalized data (columns: unique_id, ds, y, with UTC timestamps)\n",
    "- **metadata.json**: Data snapshot (row count, date range, integrity report)\n",
    "\n",
    "### Invariants (must always hold)\n",
    "- Timestamps must be monotonically increasing (no gaps, no duplicates, no reversals)\n",
    "- All values must be numeric and non-negative (electricity generation)\n",
    "- All timestamps must be in UTC (no local timezones)\n",
    "- No data loss: raw.parquet row count ≥ clean.parquet row count\n",
    "\n",
    "### Failure modes\n",
    "- API unavailable → retries up to 3 times, then raises RuntimeError\n",
    "- DST transition creates duplicate hour → integrity check catches it, raises ValueError\n",
    "- Missing hours in sequence → detected and reported, task fails if threshold exceeded\n",
    "- Non-numeric values → converted during prepare, logged as NaN if conversion fails\n",
    "\n",
    "## Files touched\n",
    "\n",
    "- **`src/chapter1/eia_data_simple.py`** (1,773 lines) — Main orchestrator (see Methods section)\n",
    "  - **`EIADataFetcher`** class: Pull, prepare, validate, format for forecasting\n",
    "  - **`ExperimentConfig`**: Defines horizon, windows, models for downstream chapters\n",
    "- **`src/chapter1/ingest.py`** — Paginated API calls with stable sort\n",
    "- **`src/chapter1/prepare.py`** — Datetime parsing and numeric conversion\n",
    "- **`src/chapter1/validate.py`** — Time-series integrity checks (duplicates, missing hours, DST)\n",
    "- **`src/chapter1/config.py`** — Settings (API key, date ranges, respondent/fuel types)\n",
    "\n",
    "## Step-by-step walkthrough\n",
    "\n",
    "### 1) Initialize the fetcher\n",
    "```python\n",
    "from src.chapter1.eia_data_simple import EIADataFetcher\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"EIA_API_KEY\")\n",
    "fetcher = EIADataFetcher(api_key)\n",
    "```\n",
    "- **Expect**: No errors. Fetcher is ready to pull data.\n",
    "- **If it fails**: Check that `.env` file exists and contains `EIA_API_KEY=<your_key>`\n",
    "\n",
    "### 2) Pull raw data\n",
    "```python\n",
    "df_raw = fetcher.pull_data(\n",
    "    start_date=\"2023-06-01\",\n",
    "    end_date=\"2023-06-30\",\n",
    "    respondent=\"NG_US48\",\n",
    "    fueltype=\"NG\"\n",
    ")\n",
    "print(f\"Raw rows: {len(df_raw)}, Columns: {df_raw.columns.tolist()}\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - ~720 rows (30 days × 24 hours)\n",
    "  - Columns: `['time', 'value', 'respondent', 'fueltype']`\n",
    "  - time is string (e.g., \"2023-06-01T00:00:00-04:00\")\n",
    "  - value is string (e.g., \"1234.56\")\n",
    "- **If it fails**:\n",
    "  - \"API Error\": Check API credentials and network connectivity\n",
    "  - \"Empty response\": Check date range and respondent/fueltype exist in EIA\n",
    "\n",
    "### 3) Prepare (normalize) data\n",
    "```python\n",
    "df_prepared = fetcher.prepare_data(df_raw)\n",
    "print(df_prepared.head())\n",
    "print(f\"Data types: {df_prepared.dtypes.to_dict()}\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - Columns: `['time', 'value', 'respondent', 'fueltype']`\n",
    "  - `time` is `datetime64[ns, UTC]` (pandas Timestamp)\n",
    "  - `value` is `float64`\n",
    "  - No NaN values\n",
    "- **If it fails**:\n",
    "  - Conversion error: Check for non-numeric values in API response (may indicate API schema change)\n",
    "\n",
    "### 4) Validate time-series integrity\n",
    "```python\n",
    "is_valid = fetcher.validate_data(df_prepared)\n",
    "print(f\"Basic validation: {is_valid}\")\n",
    "\n",
    "integrity = fetcher.validate_time_series_integrity(df_prepared, unique_id=\"respondent\")\n",
    "print(f\"Integrity status: {integrity['status']}\")\n",
    "print(f\"Duplicate pairs: {integrity.get('duplicate_count', 0)}\")\n",
    "print(f\"Missing hours: {integrity.get('missing_hours_total', 0)}\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - `is_valid` = True\n",
    "  - `integrity['status']` = \"valid\"\n",
    "  - Duplicate and missing hour counts = 0 (or acceptable based on thresholds)\n",
    "- **If it fails**:\n",
    "  - \"Duplicates detected\": DST fall-back (1 hour repeats). Check integrity['duplicate_pairs']\n",
    "  - \"Missing hours\": Gap in data (e.g., server downtime). Check `integrity['missing_gaps_count']` and `longest_gap_hours`\n",
    "\n",
    "### 5) Format for forecasting\n",
    "```python\n",
    "df_forecast = fetcher.prepare_for_forecasting(df_prepared, unique_id=\"respondent\")\n",
    "print(df_forecast.columns.tolist())\n",
    "print(df_forecast.head())\n",
    "```\n",
    "- **Expect**:\n",
    "  - Columns: `['unique_id', 'ds', 'y']`\n",
    "  - `unique_id` = \"NG_US48\" (constant)\n",
    "  - `ds` = datetime (UTC, hourly, no gaps)\n",
    "  - `y` = numeric value (energy generation)\n",
    "- **If it fails**: Time-series integrity failed (duplicates, gaps). Re-run step 4 to diagnose.\n",
    "\n",
    "### 6) End-to-end test\n",
    "```python\n",
    "df_full = fetcher.full_pipeline(\n",
    "    start_date=\"2023-06-01\",\n",
    "    end_date=\"2023-06-30\",\n",
    "    unique_id_col=\"respondent\"\n",
    ")\n",
    "print(f\"Output rows: {len(df_full)}, Columns: {df_full.columns.tolist()}\")\n",
    "```\n",
    "- **Expect**: Same as step 5 (columns: unique_id, ds, y)\n",
    "- **If it fails**: Check logs for which step failed (pull, prepare, validate, or format)\n",
    "\n",
    "## Metrics & success criteria\n",
    "\n",
    "### Primary metric\n",
    "- **Integrity status**: All records pass validation (no duplicates, no missing hours)\n",
    "\n",
    "### Secondary metrics\n",
    "- **Data freshness**: Last record within 24 hours of run time\n",
    "- **Row count consistency**: Raw ≥ Clean (loss should be minimal, <5%)\n",
    "- **Timezone correctness**: All timestamps are UTC\n",
    "\n",
    "### \"Good enough\" threshold\n",
    "- No duplicates (count = 0)\n",
    "- No missing hours within the date range\n",
    "- All values numeric and ≥ 0\n",
    "\n",
    "### What would make me re-ingest / re-validate\n",
    "- API returns unexpected schema (missing columns, wrong data type)\n",
    "- >5% row loss during prepare (indicates conversion failures)\n",
    "- New duplicates appear (suggests API bug or DST change)\n",
    "\n",
    "## Pitfalls (things that commonly break)\n",
    "\n",
    "1. **Timezone confusion**:\n",
    "   - API returns local time with offset (e.g., \"-04:00\"), but we convert to UTC\n",
    "   - If you skip UTC normalization, backtesting will be incorrect\n",
    "\n",
    "2. **DST transitions** (US Eastern Time, US Central Time):\n",
    "   - Fall-back (Nov, 1-2 AM repeats) → creates duplicate rows → integrity check catches it\n",
    "   - Spring-forward (Mar, 2-3 AM is skipped) → missing row → detected as gap\n",
    "   - Always run integrity check; don't assume API data is DST-clean\n",
    "\n",
    "3. **Empty or sparse data**:\n",
    "   - Some respondent/fueltype combos may have gaps (e.g., solar only during day)\n",
    "   - Prepare handles NaN → fill_value; validate reports gaps\n",
    "   - Don't force forecasting on sparse series without understanding why\n",
    "\n",
    "4. **API pagination edge case**:\n",
    "   - If dataset is exactly a multiple of page size, last page may appear empty\n",
    "   - Code has stable sort to avoid this, but monitor for off-by-one errors\n",
    "\n",
    "5. **Numeric conversion failing silently**:\n",
    "   - If a few rows have non-numeric values, `pd.to_numeric(..., errors='coerce')` sets them to NaN\n",
    "   - Validate always checks for NaN; if any appear, investigate API response\n",
    "\n",
    "## Mini-checkpoint (prove you learned it)\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "1. **Explain the purpose of UTC normalization**. Why can't we just use the API's local timezone?\n",
    "2. **What is a DST edge case** and how does our code detect it?\n",
    "3. **What are the three checks in `validate_time_series_integrity()`**? (duplicates, missing hours, …)\n",
    "4. **Why do we create a separate `prepare_for_forecasting()` step** instead of just using the raw API columns?\n",
    "\n",
    "**Answers:**\n",
    "1. Backtesting assumes monotonic, non-overlapping timestamps. Local timezones have repeated/missing hours during DST transitions. UTC eliminates this ambiguity.\n",
    "2. DST fall-back repeats 1 hour (e.g., 1:30 AM occurs twice). Our integrity check detects duplicate (unique_id, ds) pairs and reports them.\n",
    "3. Duplicates (same timestamp twice), missing hours (gaps in sequence), DST repeated hours (detected by duplicate).\n",
    "4. StatsForecast expects schema `(unique_id, ds, y)` with no extra columns. Transforming here makes the interface explicit and validates assumptions.\n",
    "\n",
    "## Exercises (optional, but recommended)\n",
    "\n",
    "### Easy\n",
    "1. Pull data for a different respondent (e.g., \"NG_CA1\") and verify the row count matches your date range.\n",
    "2. Change start_date to a DST transition date (e.g., \"2023-11-05\") and inspect the integrity report.\n",
    "\n",
    "### Medium\n",
    "1. Intentionally corrupt one row (e.g., set a timestamp to 12:30 instead of 12:00) and run integrity check. What gets reported?\n",
    "2. Pull a 1-year dataset and plot y vs ds. Look for gaps. Explain what you see (summer maintenance? solar seasonal?).\n",
    "\n",
    "### Hard\n",
    "1. Modify `validate_time_series_integrity()` to allow *up to* 2 duplicate pairs and *up to* 3 missing hours. Test that the threshold works correctly.\n",
    "2. Write a function that estimates \"optimal fill strategy\" for missing hours: forward-fill vs interpolation vs skip. Test on a 3-month dataset with known gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff281169",
   "metadata": {},
   "source": [
    "# Chapter 2 — Experimentation & Backtesting\n",
    "\n",
    "## Outcomes (what I can do after this)\n",
    "\n",
    "- [ ] I can set up and run rolling-origin cross-validation on time-series data\n",
    "- [ ] I can build a leaderboard comparing multiple models and pick a champion\n",
    "- [ ] I can explain why RMSE is primary and when metrics like MAPE become unreliable\n",
    "- [ ] I can interpret prediction interval coverage and assess whether intervals are well-calibrated\n",
    "- [ ] I can explain the difference between rolling-window and expanding-window backtesting strategies\n",
    "\n",
    "## Prerequisite (read first)\n",
    "\n",
    "- Chapter 0 for the time-series contract (unique_id, ds, y) and UTC rules\n",
    "\n",
    "## Concepts (plain English)\n",
    "\n",
    "- **Backtesting (rolling-origin CV)**: Simulating real-world forecasting by repeating train→test splits forward through time. Prevents temporal leakage.\n",
    "- **Horizon (h)**: How many time steps ahead we forecast (e.g., 24 hours)\n",
    "- **Step size**: How far forward we move the training/test boundary each iteration (e.g., 168 hours = 1 week)\n",
    "- **Expanding window**: Training set grows over time; test set is always fixed size\n",
    "- **Rolling window**: Both train and test windows slide forward together; train size stays constant\n",
    "- **Leakage**: Using future data to train models that predict the past (always a bug in backtesting)\n",
    "- **Coverage**: Percentage of actual values that fall inside the model's [lo, hi] prediction interval (should ≈ confidence level)\n",
    "- **MASE**: Mean Absolute Scaled Error; normalized by seasonal naive baseline; doesn't blow up at zero like MAPE\n",
    "- **Feature engineering**: Lagged values and time-based features (used by XGBoostModel)\n",
    "\n",
    "## Where this fits in the learning map (R → Python)\n",
    "\n",
    "- **Seasonal analysis** → seasonal naive baselines and MSTL models in `src/chapter1/eia_data_simple.py`\n",
    "- **Correlation analysis** → optional ACF/PACF diagnostics (use statsmodels if needed; not in pipeline by default)\n",
    "- **Smoothing methods** → `ExponentialSmoothingModel` in `src/chapter2/models.py` and Holt-Winters in the StatsForecast pipeline\n",
    "- **Decomposition** → MSTL models in `src/chapter1/eia_data_simple.py`\n",
    "- **Forecasting strategies** → rolling-origin CV in `src/chapter2/backtesting.py` and `EIADataFetcher.cross_validate()`\n",
    "\n",
    "## Feature engineering (optional ML track)\n",
    "\n",
    "- The StatsForecast pipeline only uses `unique_id`, `ds`, `y` (no X features).\n",
    "- `src/chapter2/feature_engineering.py` is for EDA or ML models that consume features.\n",
    "- For multi-step horizons, avoid leakage by using a forecasting-aware tool (e.g., MLForecast) or recursive feature generation.\n",
    "\n",
    "## Architecture (what we're building)\n",
    "\n",
    "### Inputs\n",
    "- **Forecasting-ready DataFrame**: columns `[unique_id, ds, y]`, UTC timestamps, no gaps/duplicates\n",
    "- **Backtest config**: horizon (h), n_windows, step_size, confidence level\n",
    "- **Model registry**: list of model classes to train and evaluate\n",
    "\n",
    "### Outputs\n",
    "- **cv_results.parquet**: Wide format from StatsForecast.cross_validation()\n",
    "  - Columns: unique_id, ds, [model names], [model-lo-95], [model-hi-95]\n",
    "- **leaderboard.parquet**: Model rankings (see below)\n",
    "- **Metrics for each model**: RMSE, MAE, MAPE, MASE, Coverage (%)\n",
    "\n",
    "### Invariants (must always hold)\n",
    "- No temporal leakage: training set ends strictly before test period begins\n",
    "- No data shuffling: timestamps always sorted ascending\n",
    "- Test periods do not overlap (no double-counting of actuals)\n",
    "- All models trained on identical train/test splits (fair comparison)\n",
    "\n",
    "### Failure modes\n",
    "- Sparse data in train window → model.fit() returns all NaN → metrics become NaN → leaderboard shows NaN rank\n",
    "- Sparse or zero values in test → MAPE undefined or infinite → use MASE/RMSE instead\n",
    "- Horizon too large → forecasting beyond data → poor coverage and high error\n",
    "- n_windows too large → leftover data at end not evaluated → incomplete picture\n",
    "\n",
    "## Files touched\n",
    "\n",
    "- **`src/chapter1/eia_data_simple.py`**\n",
    "  - `cross_validate()` method: Runs rolling-origin CV with StatsForecast or traditional models\n",
    "  - `evaluate_forecast()`: Computes RMSE/MAPE/MASE/coverage on holdout split\n",
    "  - `register_best_model()`: Logs champion model to MLflow\n",
    "  - `ExperimentConfig`: Dataclass defining horizon, n_windows, step_size, model list\n",
    "\n",
    "- **`src/chapter2/backtesting.py`**\n",
    "  - `RollingWindowBacktest` class: Fixed-size training window slides forward\n",
    "  - `ExpandingWindowBacktest` class: Training window grows; test set fixed size\n",
    "  - `BacktestingStrategy` interface: Routes to above based on config\n",
    "\n",
    "- **`src/chapter2/models.py`**\n",
    "  - `ForecastModel` (abstract base): fit(), predict(), get_name()\n",
    "  - Concrete implementations:\n",
    "    - `ExponentialSmoothingModel`: Simple trend-based smoothing (baseline)\n",
    "    - `ARIMAModel`: StatModels ARIMA with automatic parameter tuning\n",
    "    - `ProphetModel`: Facebook Prophet with seasonal decomposition\n",
    "    - `XGBoostModel`: Lagged features → tree ensemble\n",
    "  - `ModelFactory`: Create models by name string\n",
    "\n",
    "- **`src/chapter2/feature_engineering.py`**\n",
    "  - `build_timetk_features()`: Calendar + lag + rolling features (pandas-only)\n",
    "\n",
    "- **`src/chapter2/training.py`**\n",
    "  - `TrainingPipeline`: Orchestrates training across backtesting splits\n",
    "  - For each split: fit all models, generate forecasts, compute metrics\n",
    "  - Returns: results DataFrame with all metrics\n",
    "\n",
    "- **`src/chapter2/evaluation.py`**\n",
    "  - `ModelSelector` class: select_best_model(), generate_leaderboard()\n",
    "  - Metrics computation (NaN-aware): RMSE, MAE, MAPE, MASE, Coverage\n",
    "  - Ranking by primary metric (default: RMSE)\n",
    "\n",
    "## Step-by-step walkthrough\n",
    "\n",
    "### 1) Prepare forecasting-ready data\n",
    "```python\n",
    "from src.chapter1.eia_data_simple import EIADataFetcher\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"EIA_API_KEY\")\n",
    "fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "df_clean = fetcher.pull_data(\n",
    "    start_date=\"2023-01-01\",\n",
    "    end_date=\"2023-12-31\"\n",
    ").pipe(fetcher.prepare_data)\n",
    "\n",
    "df_forecast = fetcher.prepare_for_forecasting(\n",
    "    df_clean, unique_id=\"NG_US48\"\n",
    ")\n",
    "print(f\"Rows: {len(df_forecast)}, Columns: {df_forecast.columns.tolist()}\")\n",
    "```\n",
    "- **Expect**: 8,760 rows (365 days × 24 hours), columns: [unique_id, ds, y]\n",
    "- **If it fails**: Check Chapter 1 prerequisites (API key, date range, data integrity)\n",
    "\n",
    "### 2) Define backtest configuration\n",
    "```python\n",
    "from src.chapter1.eia_data_simple import ExperimentConfig\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    name=\"baseline_experiment\",\n",
    "    horizon=24,          # Forecast next 24 hours\n",
    "    n_windows=5,         # 5 train/test splits\n",
    "    step_size=168,       # Move forward 1 week each time\n",
    "    confidence_level=95,\n",
    "    models=[\"AutoARIMA\", \"SeasonalNaive\", \"HoltWinters\"],\n",
    "    metrics=[\"rmse\", \"mape\", \"mase\", \"coverage\"]\n",
    ")\n",
    "print(f\"Config: {config}\")\n",
    "```\n",
    "- **Expect**: Config object with all parameters set\n",
    "- **If it fails**: Check that model names exist in chapter2.models.ModelFactory\n",
    "\n",
    "### 3) Run cross-validation\n",
    "```python\n",
    "cv_results, leaderboard = fetcher.cross_validate(\n",
    "    df_forecast,\n",
    "    horizon=config.horizon,\n",
    "    n_windows=config.n_windows,\n",
    "    step_size=config.step_size,\n",
    "    level=[config.confidence_level],\n",
    "    models_to_train=config.models\n",
    ")\n",
    "\n",
    "print(\"CV Results shape:\", cv_results.shape)\n",
    "print(\"\\nLeaderboard (top 3):\")\n",
    "print(leaderboard.head(3))\n",
    "```\n",
    "- **Expect**:\n",
    "  - `cv_results`: Wide DataFrame with columns like `[unique_id, ds, AutoARIMA, AutoARIMA-lo-95, AutoARIMA-hi-95, ...]`\n",
    "  - `leaderboard`: Ranked by RMSE (ascending)\n",
    "    - Columns: model, rmse_mean, rmse_std, mape_mean, mase_mean, coverage_pct, rank\n",
    "    - Top row = best model (lowest RMSE)\n",
    "- **If it fails**:\n",
    "  - NaN in leaderboard: Insufficient data in train window or model failed to converge\n",
    "  - Memory error: Too many windows or horizon; reduce n_windows or horizon\n",
    "\n",
    "### 4) Interpret the leaderboard\n",
    "```python\n",
    "print(f\"Champion model: {leaderboard.iloc[0]['model']}\")\n",
    "print(f\"RMSE: {leaderboard.iloc[0]['rmse_mean']:.2f} ± {leaderboard.iloc[0]['rmse_std']:.2f}\")\n",
    "print(f\"Coverage: {leaderboard.iloc[0]['coverage_pct']:.1f}%\")\n",
    "\n",
    "# Check if coverage is reasonable (should be near confidence_level)\n",
    "expected_coverage = 95\n",
    "actual_coverage = leaderboard.iloc[0]['coverage_pct']\n",
    "if abs(actual_coverage - expected_coverage) > 5:\n",
    "    print(f\"⚠️  Coverage is {actual_coverage:.1f}%, expected ≈{expected_coverage}%\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - Champion RMSE is lowest among all models\n",
    "  - Coverage ≈ 95% (within ±5% is acceptable)\n",
    "  - MASE < 1 means better than seasonal naive baseline\n",
    "- **If it fails**:\n",
    "  - Coverage >> 95%: Intervals too wide; model is overconfident\n",
    "  - Coverage << 95%: Intervals too tight; model underestimates uncertainty\n",
    "  - MASE > 1: Model worse than naive seasonal; may need more tuning or longer training window\n",
    "\n",
    "### 5) Evaluate on holdout split\n",
    "```python\n",
    "# Train on first 80%, test on last 20%\n",
    "n_test = int(len(df_forecast) * 0.2)\n",
    "train = df_forecast.iloc[:-n_test]\n",
    "test = df_forecast.iloc[-n_test:]\n",
    "\n",
    "# Get champion from leaderboard\n",
    "champion_model_name = leaderboard.iloc[0]['model']\n",
    "\n",
    "# Forecast and evaluate\n",
    "forecast = fetcher.evaluate_forecast(\n",
    "    train, test,\n",
    "    model=champion_model_name,\n",
    "    horizon=config.horizon,\n",
    "    confidence_level=config.confidence_level\n",
    ")\n",
    "\n",
    "print(f\"Holdout RMSE: {forecast['rmse']:.2f}\")\n",
    "print(f\"Holdout Coverage: {forecast['coverage_pct']:.1f}%\")\n",
    "```\n",
    "- **Expect**: Metrics on holdout set consistent with CV results (±10% variance is normal)\n",
    "- **If it fails**: Holdout metrics much worse than CV → possible distribution shift or model overfitting to CV splits\n",
    "\n",
    "### 6) Register champion in MLflow\n",
    "```python\n",
    "model_uri = fetcher.register_best_model(\n",
    "    leaderboard=leaderboard,\n",
    "    config=config,\n",
    "    df_train=df_forecast,\n",
    "    run_name=\"experiment_v1\"\n",
    ")\n",
    "\n",
    "print(f\"Model registered: {model_uri}\")\n",
    "```\n",
    "- **Expect**: MLflow run created with model artifact, metrics logged\n",
    "- **If it fails**: MLflow server unavailable or config missing; check MLflow setup\n",
    "\n",
    "## Metrics & success criteria\n",
    "\n",
    "### Primary metric\n",
    "- **RMSE** (Root Mean Squared Error): Lower is better. Penalizes large errors heavily. Use as main ranking criterion.\n",
    "\n",
    "### Secondary metrics\n",
    "- **MASE** (Mean Absolute Scaled Error): Normalized by seasonal naive; MASE < 1 = better than baseline\n",
    "- **Coverage %**: Should be ≈ confidence_level (95%); ±5% acceptable\n",
    "- **MAE**: Absolute error in original units; easier to interpret than RMSE\n",
    "\n",
    "### \"Good enough\" threshold\n",
    "- RMSE < [domain-specific baseline] (depends on magnitude of y; compare to naive)\n",
    "- MASE < 1 (beating seasonal naive)\n",
    "- Coverage 90% to 98% (if using 95% confidence level)\n",
    "\n",
    "### What would make me retrain / change strategy\n",
    "- RMSE increases >20% vs previous experiment → model degradation; check for data drift\n",
    "- Coverage < 85% or > 99% → intervals miscalibrated; recalibrate or use different method\n",
    "- MASE > 1.5 → worse than naive; horizon may be too long or train window too short\n",
    "- Multiple models have RMSE = NaN → insufficient training data; increase train window or reduce horizon\n",
    "\n",
    "## Pitfalls (things that commonly break)\n",
    "\n",
    "1. **MAPE with zero or near-zero values**:\n",
    "   - MAPE = sum(|y_true - y_pred| / |y_true|) → explodes when y_true ≈ 0\n",
    "   - Example: Solar generation at night = 0 → MAPE = ∞\n",
    "   - **Fix**: Always use RMSE/MAE/MASE as primary metrics; report MAPE with caution or only on non-zero subset\n",
    "\n",
    "2. **Temporal leakage (common mistake)**:\n",
    "   - If train and test overlap or test comes before train, metrics are meaningless\n",
    "   - Our code validates no leakage, but if you modify it, this will silently break results\n",
    "   - **Fix**: Always verify: `cutoff_date < test_start_date`\n",
    "\n",
    "3. **Confidence level hard-coded**:\n",
    "   - Currently we hard-code \"95\" in several places; if you want different confidence level, it may not work\n",
    "   - **Fix**: Pass confidence_level through all functions consistently\n",
    "\n",
    "4. **Too many windows with sparse data**:\n",
    "   - If n_windows is large and data is sparse, training windows may be nearly empty\n",
    "   - Model fails to fit → returns NaN → leaderboard shows NaN ranks\n",
    "   - **Fix**: Check that min_train_size ≥ 100 and each window has data; reduce n_windows if needed\n",
    "\n",
    "5. **Horizon too large**:\n",
    "   - If h=168 (1 week ahead), but your model only sees 30-day history, uncertainty is huge\n",
    "   - Coverage will be poor; consider shortening horizon or lengthening training window\n",
    "   - **Fix**: Plot y vs ds to understand seasonality; set horizon ≤ 1 season\n",
    "\n",
    "## Mini-checkpoint (prove you learned it)\n",
    "\n",
    "Answer these:\n",
    "\n",
    "1. **Explain rolling-origin vs expanding-window backtesting**. When would you use each?\n",
    "2. **Why does MAPE fail when y=0**? What metric is more robust?\n",
    "3. **What does \"coverage\" mean in the context of prediction intervals?**\n",
    "4. **If leaderboard shows coverage=85% (below 95%), what does it mean and how would you fix it?**\n",
    "\n",
    "**Answers:**\n",
    "1. Rolling-origin: train window is fixed size, slides forward (mimics real forecast deployment). Expanding: train grows over time (useful to detect model degradation as data volume increases). Use rolling-origin to match production; use expanding to detect drift.\n",
    "2. MAPE = |error| / |y|; when y≈0, ratio explodes. Use RMSE (absolute scale) or MASE (normalized by baseline) instead.\n",
    "3. Coverage is the % of test actuals that fall within [lower, upper] prediction interval. Should ≈ confidence level (95% for 95% interval).\n",
    "4. Coverage=85% means the interval is too tight (underestimating uncertainty). Model may be overconfident. Expand intervals or recalibrate using conformal prediction.\n",
    "\n",
    "## Exercises (optional, but recommended)\n",
    "\n",
    "### Easy\n",
    "1. Run cross-validation with horizon=12 (instead of 24) and compare RMSE vs horizon=24. Explain the difference.\n",
    "2. Extract leaderboard for one model and plot its RMSE across the 5 CV windows. Is it stable or does it spike?\n",
    "\n",
    "### Medium\n",
    "1. Run cross-validation with step_size=24 (daily) instead of 168 (weekly). How many windows are created? How do metrics change?\n",
    "2. Identify the bottom-ranked model in leaderboard. Calculate MASE manually for one test window and verify it's > 1.\n",
    "\n",
    "### Hard\n",
    "1. Implement a custom metric (e.g., \"% of forecasts within 10% of actual\") and add it to leaderboard.\n",
    "2. Run cross-validation for 3 different series (e.g., different respondents or fueltypes) and compare their leaderboards. Which series is easier/harder to forecast? Why?\n",
    "3. Modify the confidence level to 80% (instead of 95%) and re-run CV. Compare coverage between 80% and 95% intervals. Verify they differ as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/chapter2/feature_engineering.py\n",
    "# file: src/chapter2/feature_engineering.py\n",
    "\"\"\"\n",
    "Chapter 2: Simple timetk-style feature helpers (pandas only).\n",
    "\n",
    "These helpers are not used by the default StatsForecast pipeline. Use them\n",
    "for EDA or when you add an ML-based forecasting track that explicitly consumes\n",
    "X features (e.g., MLForecast or sklearn models).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_calendar_features(df: pd.DataFrame, ds_col: str = \"ds\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add basic calendar features from a datetime column.\n",
    "    \"\"\"\n",
    "    features = df.copy()\n",
    "    if ds_col not in features.columns:\n",
    "        raise ValueError(f\"Missing datetime column: {ds_col}\")\n",
    "\n",
    "    ds = pd.to_datetime(features[ds_col], errors=\"raise\")\n",
    "    features[\"hour\"] = ds.dt.hour\n",
    "    features[\"dayofweek\"] = ds.dt.dayofweek\n",
    "    features[\"dayofyear\"] = ds.dt.dayofyear\n",
    "    features[\"month\"] = ds.dt.month\n",
    "    features[\"is_weekend\"] = ds.dt.dayofweek >= 5\n",
    "    return features\n",
    "\n",
    "\n",
    "def add_lag_features(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str = \"y\",\n",
    "    lags: Iterable[int] = (1, 24, 168),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add lag features using past values only.\n",
    "    \"\"\"\n",
    "    features = df.copy()\n",
    "    if y_col not in features.columns:\n",
    "        raise ValueError(f\"Missing value column: {y_col}\")\n",
    "\n",
    "    for lag in lags:\n",
    "        features[f\"{y_col}_lag_{lag}\"] = features[y_col].shift(lag)\n",
    "    return features\n",
    "\n",
    "\n",
    "def add_rolling_features(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str = \"y\",\n",
    "    windows: Iterable[int] = (24, 168),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add rolling-window mean features with a 1-step shift to avoid leakage.\n",
    "    \"\"\"\n",
    "    features = df.copy()\n",
    "    if y_col not in features.columns:\n",
    "        raise ValueError(f\"Missing value column: {y_col}\")\n",
    "\n",
    "    shifted = features[y_col].shift(1)\n",
    "    for window in windows:\n",
    "        features[f\"{y_col}_roll_mean_{window}\"] = shifted.rolling(window, min_periods=1).mean()\n",
    "    return features\n",
    "\n",
    "\n",
    "def build_timetk_features(\n",
    "    df: pd.DataFrame,\n",
    "    ds_col: str = \"ds\",\n",
    "    y_col: str = \"y\",\n",
    "    lags: Iterable[int] = (1, 24, 168),\n",
    "    windows: Iterable[int] = (24, 168),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: calendar + lag + rolling features.\n",
    "    \"\"\"\n",
    "    features = add_calendar_features(df, ds_col=ds_col)\n",
    "    features = add_lag_features(features, y_col=y_col, lags=lags)\n",
    "    features = add_rolling_features(features, y_col=y_col, windows=windows)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/chapter1/eia_data_simple.py\n",
    "# file: src/chapter1/eia_data_simple.py\n",
    "\"\"\"\n",
    "EIA Data Fetcher - Step by Step\n",
    "=================================\n",
    "\n",
    "A simplified module for pulling EIA electricity generation data\n",
    "following the same step-by-step approach as the R script.\n",
    "\n",
    "This module makes it easy to understand each step and test along the way.\n",
    "\n",
    "Usage:\n",
    "    from eia_data_simple import EIADataFetcher\n",
    "    import os\n",
    "\n",
    "    # Step 1: Initialize\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    # Step 2: Pull raw data\n",
    "    df_raw = fetcher.pull_data(\n",
    "        start_date=\"2023-01-01\",\n",
    "        end_date=\"2024-12-31\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Inspect data\n",
    "    print(f\"Rows: {len(df_raw)}\")\n",
    "    print(df_raw.head())\n",
    "\n",
    "    # Step 4: Prepare (convert types, sort, etc.)\n",
    "    df_prepared = fetcher.prepare_data(df_raw)\n",
    "\n",
    "    # Step 5: Validate\n",
    "    is_valid = fetcher.validate_data(df_prepared)\n",
    "\n",
    "    # Step 6: Get statistics\n",
    "    stats = fetcher.get_stats(df_prepared)\n",
    "    print(stats)\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Optional: pydantic-settings for type-safe config\n",
    "try:\n",
    "    from pydantic_settings import BaseSettings\n",
    "    PYDANTIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYDANTIC_AVAILABLE = False\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Optional: MLflow for experiment tracking\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow.models import infer_signature\n",
    "    MLFLOW_AVAILABLE = True\n",
    "    logger.info(\"MLflow is available for experiment tracking\")\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    logger.warning(\"MLflow not available - experiment tracking disabled\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"\n",
    "    Configuration for backtesting experiments.\n",
    "\n",
    "    This defines the \"contract\" for reproducible experiments:\n",
    "    - What data range to use\n",
    "    - How to split for cross-validation\n",
    "    - Which models and metrics to evaluate\n",
    "\n",
    "    Example:\n",
    "        >>> config = ExperimentConfig(\n",
    "        ...     name=\"baseline_experiment\",\n",
    "        ...     horizon=24,\n",
    "        ...     n_windows=5,\n",
    "        ...     step_size=168,  # Weekly steps\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    name: str = \"default_experiment\"\n",
    "    horizon: int = 24              # Forecast horizon in hours\n",
    "    n_windows: int = 5             # Number of CV windows\n",
    "    step_size: int = 168           # Hours between windows (168 = 1 week)\n",
    "    confidence_level: int = 95     # Prediction interval level\n",
    "    models: List[str] = field(default_factory=lambda: [\n",
    "        \"SeasonalNaive\", \"AutoARIMA\", \"MSTL\"\n",
    "    ])\n",
    "    metrics: List[str] = field(default_factory=lambda: [\n",
    "        \"rmse\", \"mape\", \"mase\", \"coverage\"\n",
    "    ])\n",
    "\n",
    "\n",
    "# Pydantic-settings config (if available)\n",
    "if PYDANTIC_AVAILABLE:\n",
    "    class Settings(BaseSettings):\n",
    "        \"\"\"\n",
    "        Type-safe configuration using pydantic-settings.\n",
    "\n",
    "        Automatically reads from environment variables or .env file.\n",
    "        Validates types and provides defaults.\n",
    "\n",
    "        Example:\n",
    "            >>> settings = Settings()\n",
    "            >>> print(settings.eia_api_key[:8])\n",
    "        \"\"\"\n",
    "        eia_api_key: str\n",
    "        respondent: str = \"US48\"\n",
    "        fueltype: str = \"NG\"\n",
    "        start_date: str = \"2024-01-01\"\n",
    "        end_date: str = \"2024-12-31\"\n",
    "\n",
    "        class Config:\n",
    "            env_file = \".env\"\n",
    "            env_file_encoding = \"utf-8\"\n",
    "\n",
    "\n",
    "class EIADataFetcher:\n",
    "    \"\"\"\n",
    "    Step-by-step EIA data fetcher matching R script workflow.\n",
    "\n",
    "    Each method represents a step in the data pipeline:\n",
    "    1. pull_data() - Fetch from API\n",
    "    2. inspect_data() - View structure\n",
    "    3. prepare_data() - Clean and convert\n",
    "    4. validate_data() - Check quality\n",
    "    5. get_stats() - Calculate summary stats\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the fetcher with API credentials.\n",
    "\n",
    "        Args:\n",
    "            api_key: EIA API key from https://www.eia.gov/opendata/\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "        logger.info(f\"Fetcher initialized (API key length: {len(api_key)})\")\n",
    "        print(f\"Step 1: Fetcher initialized (API key length: {len(api_key)})\")\n",
    "\n",
    "    def pull_data(\n",
    "        self,\n",
    "        start_date: str = \"2023-01-01\",\n",
    "        end_date: str = \"2024-12-31\",\n",
    "        respondent: str = \"US48\",\n",
    "        fueltype: str = \"NG\",\n",
    "        length: int = 5000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        STEP 2: Pull raw data from EIA API with pagination.\n",
    "\n",
    "        Features:\n",
    "        - Handles >5000 row datasets with pagination loop\n",
    "        - Stable sort order (period ascending) for reproducibility\n",
    "        - Logs pagination details for transparency\n",
    "\n",
    "        Args:\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            respondent: Region code (default: US48 = Lower 48 states)\n",
    "            fueltype: Fuel type code (default: NG = Natural Gas)\n",
    "            length: Records per request (default: 5000, max allowed)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with raw API response\n",
    "\n",
    "        Example:\n",
    "            >>> df_raw = fetcher.pull_data()\n",
    "            >>> print(f\"Retrieved {len(df_raw)} rows\")\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 2: Pulling data from EIA API...\")\n",
    "        print(f\"  Date range: {start_date} to {end_date}\")\n",
    "        print(f\"  Respondent: {respondent}\")\n",
    "        print(f\"  Fuel type: {fueltype}\")\n",
    "\n",
    "        all_records = []\n",
    "        offset = 0\n",
    "        request_count = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                # Build API parameters with pagination and STABLE SORT\n",
    "                # Using sort params ensures pages don't shuffle during pagination\n",
    "                params = {\n",
    "                    \"api_key\": self.api_key,\n",
    "                    \"data[]\": \"value\",\n",
    "                    \"facets[respondent][]\": respondent,\n",
    "                    \"facets[fueltype][]\": fueltype,\n",
    "                    \"frequency\": \"hourly\",\n",
    "                    \"start\": f\"{start_date}T00\",\n",
    "                    \"end\": f\"{end_date}T23\",\n",
    "                    \"length\": length,\n",
    "                    \"offset\": offset,\n",
    "                    # STABLE SORT: Request data in ascending order from the API\n",
    "                    # This ensures consistent ordering across paginated requests\n",
    "                    \"sort[0][column]\": \"period\",\n",
    "                    \"sort[0][direction]\": \"asc\",\n",
    "                }\n",
    "\n",
    "                # Make API request\n",
    "                logger.info(f\"API request: offset={offset}, length={length}\")\n",
    "                response = requests.get(self.api_url, params=params)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Parse response\n",
    "                data = response.json()\n",
    "                records = data[\"response\"][\"data\"]\n",
    "                request_count += 1\n",
    "\n",
    "                logger.debug(f\"Request {request_count}: received {len(records)} rows\")\n",
    "\n",
    "                if not records:\n",
    "                    break  # No more data\n",
    "\n",
    "                all_records.extend(records)\n",
    "                offset += length\n",
    "\n",
    "            if not all_records:\n",
    "                raise ValueError(\"No data returned from API\")\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            # Data is already sorted ascending by period from the API (stable sort params)\n",
    "            # We still verify sort order here as a safety check\n",
    "            df = pd.DataFrame(all_records)\n",
    "            df = df.sort_values(\"period\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "            print(f\"  Sending requests...\")\n",
    "            print(f\"  [OK] Retrieved {len(df)} total rows across {request_count} request(s)\")\n",
    "            print(f\"  Columns: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "            logger.info(f\"Data pull complete: {len(df)} rows in {request_count} API requests\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data pull failed: {e}\", exc_info=True)\n",
    "            print(f\"  [ERROR] {e}\")\n",
    "            raise\n",
    "\n",
    "    def inspect_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        STEP 3: Inspect raw data structure.\n",
    "\n",
    "        Displays:\n",
    "        - Data shape\n",
    "        - Column info\n",
    "        - First few rows\n",
    "        - Data types\n",
    "\n",
    "        Example:\n",
    "            >>> fetcher.inspect_data(df_raw)\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 3: Inspecting data structure...\")\n",
    "        print(f\"  Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "        print(f\"\\n  Column info:\")\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            non_null = df[col].notna().sum()\n",
    "            print(f\"    - {col}: {dtype} ({non_null} non-null)\")\n",
    "\n",
    "        print(f\"\\n  First 3 rows:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"    Row {i}: {dict(row)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame, timezone_policy: str = \"UTC\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        STEP 4: Prepare data (clean, convert, sort).\n",
    "\n",
    "        Performs:\n",
    "        - Parse datetime from 'period' field\n",
    "        - Apply timezone policy (UTC normalization for consistency)\n",
    "        - Convert 'value' to numeric\n",
    "        - Sort by datetime\n",
    "        - Standardize column names\n",
    "\n",
    "        Args:\n",
    "            df: Raw DataFrame from pull_data()\n",
    "            timezone_policy: \"UTC\" (recommended) - normalize all times to UTC\n",
    "\n",
    "        Returns:\n",
    "            Cleaned and prepared DataFrame\n",
    "\n",
    "        Example:\n",
    "            >>> df_clean = fetcher.prepare_data(df_raw, timezone_policy=\"UTC\")\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 4: Preparing data...\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        # Parse period to datetime (fail-loud on parse errors)\n",
    "        print(f\"  - Parsing period field...\")\n",
    "        try:\n",
    "            df[\"period\"] = pd.to_datetime(df[\"period\"], errors=\"raise\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            logger.error(f\"DateTime parsing failed: {e}\")\n",
    "            logger.error(f\"Sample period values: {df['period'].head(10).tolist()}\")\n",
    "            raise ValueError(f\"Cannot parse period field as datetime: {e}\") from e\n",
    "\n",
    "        # Apply timezone policy (UTC normalization)\n",
    "        print(f\"  - Applying timezone policy: {timezone_policy}\")\n",
    "        if timezone_policy == \"UTC\":\n",
    "            # Assume period is in UTC if no timezone info\n",
    "            if df[\"period\"].dt.tz is None:\n",
    "                df[\"period\"] = df[\"period\"].dt.tz_localize(\"UTC\")\n",
    "            else:\n",
    "                df[\"period\"] = df[\"period\"].dt.tz_convert(\"UTC\")\n",
    "            logger.info(\"Timezone policy: UTC normalization applied\")\n",
    "\n",
    "        # Extract date\n",
    "        print(f\"  - Extracting date...\")\n",
    "        df[\"date\"] = df[\"period\"].dt.date\n",
    "\n",
    "        # Convert value to numeric (fail-loud on coercion)\n",
    "        print(f\"  - Converting value to numeric...\")\n",
    "        df[\"value_before_coercion\"] = df[\"value\"].copy()  # Keep original for audit\n",
    "        try:\n",
    "            df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"raise\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            # Count non-numeric values and provide detailed error\n",
    "            non_numeric_rows = df[pd.to_numeric(df[\"value\"], errors=\"coerce\").isna()]\n",
    "            logger.error(f\"Numeric conversion failed: {len(non_numeric_rows)} non-numeric rows\")\n",
    "            logger.error(f\"Sample non-numeric values: {non_numeric_rows['value'].head(5).tolist()}\")\n",
    "            raise ValueError(\n",
    "                f\"Cannot convert value column to numeric: {len(non_numeric_rows)} unparseable rows. \"\n",
    "                f\"This typically indicates upstream schema changes or data quality issues. \"\n",
    "                f\"Sample values: {non_numeric_rows['value'].head(3).tolist()}\"\n",
    "            ) from e\n",
    "\n",
    "        # Verify no coercions occurred\n",
    "        if df[\"value\"].isna().sum() > 0:\n",
    "            coercion_count = df[\"value\"].isna().sum()\n",
    "            logger.error(f\"Coercion produced {coercion_count} NaN values during numeric conversion\")\n",
    "            raise ValueError(\n",
    "                f\"Numeric conversion coerced {coercion_count} values to NaN. \"\n",
    "                f\"Review original values: {df[df['value'].isna()]['value_before_coercion'].head(5).tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Sort by datetime\n",
    "        print(f\"  - Sorting by datetime...\")\n",
    "        df = df.sort_values(\"period\").reset_index(drop=True)\n",
    "\n",
    "        # Standardize column names\n",
    "        df.columns = [col.lower().replace(\"-\", \"_\") for col in df.columns]\n",
    "\n",
    "        # Remove temporary audit column if present\n",
    "        df = df.drop(columns=[\"value_before_coercion\"], errors=\"ignore\")\n",
    "\n",
    "        # Select key columns\n",
    "        key_cols = [\"date\", \"period\", \"value\", \"respondent\", \"fueltype\"]\n",
    "        df = df[[col for col in key_cols if col in df.columns]]\n",
    "\n",
    "        print(f\"  [OK] Data prepared: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        logger.info(f\"Data preparation complete: {df.shape[0]} rows\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def validate_time_series_integrity(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        STEP 4B: Comprehensive time series integrity validation.\n",
    "\n",
    "        Critical checks for production:\n",
    "        - No duplicates on (unique_id, ds)\n",
    "        - Regular hourly frequency\n",
    "        - Missing hours detection\n",
    "        - DST repeated hours (freq = 0)\n",
    "        - Complete final hours for backtesting\n",
    "\n",
    "        Args:\n",
    "            df: Prepared DataFrame from prepare_for_forecasting()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with integrity report:\n",
    "            - duplicate_count: Number of duplicate (unique_id, ds) pairs\n",
    "            - missing_hours_total: Total count of missing hours across all gaps\n",
    "            - missing_gaps_count: Number of gaps detected\n",
    "            - longest_gap_hours: Duration of longest gap\n",
    "            - dst_repeated_hours: Count of repeated hours (DST backward)\n",
    "            - gaps_detail: List of gap locations with hour counts\n",
    "            - status: \"valid\" or \"invalid\"\n",
    "\n",
    "        Example:\n",
    "            >>> df_forecast = fetcher.prepare_for_forecasting(df)\n",
    "            >>> integrity = fetcher.validate_time_series_integrity(df_forecast)\n",
    "            >>> print(integrity['status'])\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 4B: Validating time series integrity...\")\n",
    "\n",
    "        report = {\n",
    "            \"duplicate_count\": 0,\n",
    "            \"missing_hours_total\": 0,\n",
    "            \"missing_gaps_count\": 0,\n",
    "            \"longest_gap_hours\": 0,\n",
    "            \"dst_repeated_hours\": 0,\n",
    "            \"gaps_detail\": [],\n",
    "            \"status\": \"valid\"\n",
    "        }\n",
    "\n",
    "        df_sorted = df.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        # Check 1: Duplicates on (unique_id, ds)\n",
    "        dups = df_sorted.groupby([\"unique_id\", \"ds\"]).size()\n",
    "        duplicate_rows = (dups > 1).sum()\n",
    "        report[\"duplicate_count\"] = int(duplicate_rows)\n",
    "\n",
    "        if duplicate_rows > 0:\n",
    "            print(f\"  [FAIL] Found {duplicate_rows} duplicate (unique_id, ds) pairs\")\n",
    "            logger.error(f\"Time series integrity: {duplicate_rows} duplicates\")\n",
    "            report[\"status\"] = \"invalid\"\n",
    "            return report\n",
    "        else:\n",
    "            print(f\"  [OK] No duplicates on (unique_id, ds)\")\n",
    "\n",
    "        # Check 2-4: Frequency, gaps, DST for each series\n",
    "        for uid in df_sorted[\"unique_id\"].unique():\n",
    "            sub = df_sorted[df_sorted[\"unique_id\"] == uid].copy()\n",
    "            sub = sub.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "            # Calculate time differences\n",
    "            time_diffs = sub[\"ds\"].diff()\n",
    "            expected_freq = pd.Timedelta(hours=1)\n",
    "\n",
    "            # Missing hours: count actual missing hours per gap (not gap occurrences)\n",
    "            # For a gap of 5 hours (e.g., 10:00 to 15:00), there are 4 missing hours\n",
    "            missing_mask = time_diffs > expected_freq\n",
    "            if missing_mask.any():\n",
    "                gap_indices = sub.index[missing_mask].tolist()\n",
    "                report[\"missing_gaps_count\"] += len(gap_indices)\n",
    "\n",
    "                for idx in gap_indices:\n",
    "                    if idx > 0:\n",
    "                        gap_duration = time_diffs[idx]\n",
    "                        gap_hours = gap_duration.total_seconds() / 3600\n",
    "                        # Missing hours = gap duration - 1 (e.g., 5 hour gap = 4 missing)\n",
    "                        missing_hours_in_gap = int(gap_hours - 1)\n",
    "                        report[\"missing_hours_total\"] += missing_hours_in_gap\n",
    "\n",
    "                        report[\"gaps_detail\"].append({\n",
    "                            \"unique_id\": uid,\n",
    "                            \"before_ds\": sub.loc[idx-1, \"ds\"],\n",
    "                            \"after_ds\": sub.loc[idx, \"ds\"],\n",
    "                            \"gap_hours\": gap_hours,\n",
    "                            \"missing_hours\": missing_hours_in_gap\n",
    "                        })\n",
    "\n",
    "            # DST repeated hours (gap = 0, clocks go back)\n",
    "            repeated_mask = time_diffs == pd.Timedelta(0)\n",
    "            repeated_in_series = repeated_mask.sum()\n",
    "            report[\"dst_repeated_hours\"] += int(repeated_in_series)\n",
    "\n",
    "            # Longest gap\n",
    "            if len(time_diffs) > 0:\n",
    "                max_gap = time_diffs.max()\n",
    "                if pd.notna(max_gap):\n",
    "                    gap_hours = max_gap.total_seconds() / 3600\n",
    "                    report[\"longest_gap_hours\"] = max(\n",
    "                        report[\"longest_gap_hours\"],\n",
    "                        gap_hours\n",
    "                    )\n",
    "\n",
    "        # Report findings\n",
    "        if report[\"duplicate_count\"] > 0:\n",
    "            print(f\"  [FAIL] Found {report['duplicate_count']} duplicates\")\n",
    "            logger.error(f\"Time series integrity FAILED: {report['duplicate_count']} duplicate (unique_id, ds) pairs detected\")\n",
    "            report[\"status\"] = \"invalid\"\n",
    "            return report\n",
    "        else:\n",
    "            print(f\"  [OK] No duplicates\")\n",
    "\n",
    "        if report[\"missing_hours_total\"] > 0:\n",
    "            print(f\"  [FAIL] {report['missing_hours_total']} missing hours detected ({report['missing_gaps_count']} gaps)\")\n",
    "            logger.error(\n",
    "                f\"Time series integrity FAILED: {report['missing_hours_total']} missing hours, \"\n",
    "                f\"{report['missing_gaps_count']} gaps, longest gap {report['longest_gap_hours']:.1f} hours\"\n",
    "            )\n",
    "            gap_summary = \"\\n\".join([\n",
    "                f\"  {g['unique_id']}: {g['before_ds']} → {g['after_ds']} ({g['gap_hours']:.1f} hours, {g['missing_hours']} missing)\"\n",
    "                for g in report[\"gaps_detail\"][:5]  # Show first 5 gaps\n",
    "            ])\n",
    "            report[\"status\"] = \"invalid\"\n",
    "            return report\n",
    "        else:\n",
    "            print(f\"  [OK] No missing hours (complete frequency)\")\n",
    "\n",
    "        if report[\"dst_repeated_hours\"] > 0:\n",
    "            print(f\"  [INFO] {report['dst_repeated_hours']} DST repeated hours (clocks back)\")\n",
    "            logger.info(f\"Time series has {report['dst_repeated_hours']} DST repeated hours, which is expected\")\n",
    "\n",
    "        print(f\"  [OK] Time series integrity validated\")\n",
    "        logger.info(f\"Time series integrity report: {report}\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def validate_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        STEP 5: Validate data quality.\n",
    "\n",
    "        Checks:\n",
    "        - No empty DataFrame\n",
    "        - Value column is numeric\n",
    "        - No missing values\n",
    "        - Dates are in order\n",
    "\n",
    "        Returns:\n",
    "            bool: True if all validations pass\n",
    "\n",
    "        Example:\n",
    "            >>> is_valid = fetcher.validate_data(df_clean)\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 5: Validating data...\")\n",
    "\n",
    "        checks_passed = 0\n",
    "        checks_total = 5\n",
    "\n",
    "        # Check 1: Not empty\n",
    "        if len(df) > 0:\n",
    "            print(f\"  [OK] Data is not empty: {len(df)} rows\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  [FAIL] Data is empty\")\n",
    "            return False\n",
    "\n",
    "        # Check 2: Value column exists and is numeric\n",
    "        if \"value\" in df.columns and pd.api.types.is_numeric_dtype(df[\"value\"]):\n",
    "            print(f\"  [OK] Value column is numeric\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  [FAIL] Value column missing or not numeric\")\n",
    "            return False\n",
    "\n",
    "        # Check 3: No missing values in value column\n",
    "        missing = df[\"value\"].isna().sum()\n",
    "        if missing == 0:\n",
    "            print(f\"  [OK] No missing values in 'value' column\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  [WARN] {missing} missing values in 'value' column\")\n",
    "            checks_passed += 1  # Warning, not failure\n",
    "\n",
    "        # Check 4: Dates are in order\n",
    "        if df[\"period\"].is_monotonic_increasing:\n",
    "            print(f\"  [OK] Dates are in chronological order\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  [FAIL] Dates are not in order\")\n",
    "            return False\n",
    "\n",
    "        # Check 5: Value range is reasonable (electricity in MWh)\n",
    "        if df[\"value\"].min() > 0 and df[\"value\"].max() < 1_000_000:\n",
    "            print(f\"  [OK] Value range is reasonable\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  [WARN] Value range seems unusual: {df['value'].min():.0f} to {df['value'].max():.0f}\")\n",
    "            checks_passed += 1  # Warning\n",
    "\n",
    "        print(f\"\\n  Validation: {checks_passed}/{checks_total} checks passed\")\n",
    "        return True\n",
    "\n",
    "    def get_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        STEP 6: Calculate summary statistics.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - date_range: (start_date, end_date)\n",
    "            - record_count: Total records\n",
    "            - value_stats: min, max, mean, std\n",
    "            - missing_count: Count of NaN values\n",
    "\n",
    "        Example:\n",
    "            >>> stats = fetcher.get_stats(df_clean)\n",
    "            >>> print(f\"Date range: {stats['date_range']}\")\n",
    "        \"\"\"\n",
    "        print(f\"\\nStep 6: Calculating statistics...\")\n",
    "\n",
    "        stats = {\n",
    "            \"date_range\": (df[\"date\"].min(), df[\"date\"].max()),\n",
    "            \"record_count\": len(df),\n",
    "            \"value_stats\": {\n",
    "                \"min\": df[\"value\"].min(),\n",
    "                \"max\": df[\"value\"].max(),\n",
    "                \"mean\": df[\"value\"].mean(),\n",
    "                \"std\": df[\"value\"].std(),\n",
    "            },\n",
    "            \"missing_count\": df[\"value\"].isna().sum(),\n",
    "        }\n",
    "\n",
    "        print(f\"  Date range: {stats['date_range'][0]} to {stats['date_range'][1]}\")\n",
    "        print(f\"  Record count: {stats['record_count']}\")\n",
    "        print(f\"  Value range: {stats['value_stats']['min']:.0f} to {stats['value_stats']['max']:.0f} MWh\")\n",
    "        print(f\"  Value mean: {stats['value_stats']['mean']:.0f} MWh\")\n",
    "        print(f\"  Value std: {stats['value_stats']['std']:.0f} MWh\")\n",
    "        print(f\"  Missing values: {stats['missing_count']}\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def prepare_for_forecasting(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        unique_id: str = \"1\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare data for statsforecast/mlforecast by reformatting columns.\n",
    "\n",
    "        Statsforecast requires: unique_id, ds (timestamp), y (values)\n",
    "\n",
    "        Args:\n",
    "            df: Cleaned DataFrame from prepare_data()\n",
    "            unique_id: Series identifier (default: fuel type like \"NG\")\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with statsforecast format (unique_id, ds, y)\n",
    "\n",
    "        Example:\n",
    "            >>> df_clean = fetcher.prepare_data(df_raw)\n",
    "            >>> df_forecast = fetcher.prepare_for_forecasting(df_clean)\n",
    "        \"\"\"\n",
    "        # Validation: check required columns\n",
    "        required_cols = ['period', 'value']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            logger.error(f\"Missing required columns: {missing_cols}\")\n",
    "            raise ValueError(\n",
    "                f\"prepare_for_forecasting requires columns {required_cols}, \"\n",
    "                f\"but found: {df.columns.tolist()}. \"\n",
    "                f\"Ensure prepare_data() completed successfully.\"\n",
    "            )\n",
    "\n",
    "        # Validation: check for NaN values (should not exist after prepare_data)\n",
    "        nan_count_period = df['period'].isna().sum()\n",
    "        nan_count_value = df['value'].isna().sum()\n",
    "\n",
    "        if nan_count_period > 0:\n",
    "            logger.error(f\"Found {nan_count_period} NaN values in period column\")\n",
    "            raise ValueError(\n",
    "                f\"Cannot prepare forecasting data with {nan_count_period} NaN values in period. \"\n",
    "                f\"This indicates incomplete datetime parsing. \"\n",
    "                f\"Run validate_data() and check prepare_data() error logs.\"\n",
    "            )\n",
    "\n",
    "        if nan_count_value > 0:\n",
    "            logger.error(f\"Found {nan_count_value} NaN values in value column\")\n",
    "            raise ValueError(\n",
    "                f\"Cannot prepare forecasting data with {nan_count_value} NaN values in value. \"\n",
    "                f\"This indicates failed numeric conversion. \"\n",
    "                f\"Run validate_data() and check prepare_data() error logs.\"\n",
    "            )\n",
    "\n",
    "        df_forecast = df.copy()\n",
    "        df_forecast.columns = [col.lower() for col in df_forecast.columns]\n",
    "\n",
    "        # Rename columns for statsforecast\n",
    "        df_forecast['ds'] = df_forecast['period']\n",
    "        df_forecast['y'] = df_forecast['value']\n",
    "        df_forecast['unique_id'] = unique_id\n",
    "\n",
    "        # CRITICAL: StatsForecast requires timezone-naive UTC datetimes\n",
    "        # Convert from tz-aware UTC to timezone-naive\n",
    "        df_forecast['ds'] = pd.to_datetime(df_forecast['ds'], errors='raise', utc=True).dt.tz_localize(None)\n",
    "\n",
    "        # Keep only required columns\n",
    "        df_forecast = df_forecast[['unique_id', 'ds', 'y']].reset_index(drop=True)\n",
    "\n",
    "        logger.info(f\"Prepared {len(df_forecast)} records for forecasting with unique_id={unique_id}\")\n",
    "        logger.debug(f\"ds dtype: {df_forecast['ds'].dtype}, tz={df_forecast['ds'].dt.tz if hasattr(df_forecast['ds'], 'dt') else 'N/A'}\")\n",
    "\n",
    "        return df_forecast\n",
    "\n",
    "    def train_test_split(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        test_hours: int = 72\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split data into training and testing sets.\n",
    "\n",
    "        Leaves last N hours for testing, rest for training.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with 'ds' (datetime) column\n",
    "            test_hours: Number of hours to reserve for testing (default: 72)\n",
    "\n",
    "        Returns:\n",
    "            (train_df, test_df)\n",
    "\n",
    "        Example:\n",
    "            >>> train_df, test_df = fetcher.train_test_split(df_forecast, test_hours=168)\n",
    "            >>> print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "        \"\"\"\n",
    "        df = df.sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "        # Calculate split point\n",
    "        split_point = df['ds'].max() - timedelta(hours=test_hours)\n",
    "\n",
    "        train_df = df[df['ds'] <= split_point].reset_index(drop=True)\n",
    "        test_df = df[df['ds'] > split_point].reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\nTrain/Test Split:\")\n",
    "        print(f\"  Training set: {len(train_df)} records\")\n",
    "        print(f\"  Testing set: {len(test_df)} records\")\n",
    "        print(f\"  Split point: {split_point}\")\n",
    "\n",
    "        return train_df, test_df\n",
    "\n",
    "    def build_models(self) -> List:\n",
    "        \"\"\"\n",
    "        Build a list of forecasting models for statsforecast.\n",
    "\n",
    "        Models included:\n",
    "        - AutoARIMA: Auto-tuned ARIMA model\n",
    "        - SeasonalNaive: Baseline seasonal model (same value from year ago)\n",
    "        - DynamicOptimizedTheta: Theta model with automatic optimization\n",
    "        - HoltWinters: Exponential smoothing model\n",
    "        - MSTL_ARIMA: Multi-seasonal trend with ARIMA trend forecaster\n",
    "        - MSTL_HoltWinters: Multi-seasonal trend with HoltWinters trend forecaster\n",
    "\n",
    "        Returns:\n",
    "            List of statsforecast model objects\n",
    "\n",
    "        Example:\n",
    "            >>> from statsforecast import StatsForecast\n",
    "            >>> models = fetcher.build_models()\n",
    "            >>> sf = StatsForecast(models=models, freq='h')\n",
    "        \"\"\"\n",
    "        from statsforecast.models import (MSTL, AutoARIMA,\n",
    "                                          DynamicOptimizedTheta, HoltWinters,\n",
    "                                          SeasonalNaive)\n",
    "\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            DynamicOptimizedTheta(season_length=24),\n",
    "            HoltWinters(season_length=24),\n",
    "            MSTL(season_length=[24, 24 * 7], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "            MSTL(season_length=[24, 24 * 7], trend_forecaster=HoltWinters(), alias=\"MSTL_HoltWinters\"),\n",
    "        ]\n",
    "\n",
    "        return models\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        horizon: int = 72,\n",
    "        confidence_level: int = 95\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create forecasts using StatsForecast.\n",
    "\n",
    "        Args:\n",
    "            train_df: Training DataFrame with unique_id, ds, y columns\n",
    "            horizon: Number of steps to forecast (default: 72 hours)\n",
    "            confidence_level: Prediction interval level (default: 95%)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with forecasts and prediction intervals\n",
    "\n",
    "        Example:\n",
    "            >>> forecast_df = fetcher.forecast(train_df, horizon=168)\n",
    "            >>> print(forecast_df.head())\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA\n",
    "\n",
    "        models = self.build_models()\n",
    "\n",
    "        # Initialize StatsForecast object\n",
    "        sf = StatsForecast(\n",
    "            models=models,\n",
    "            freq='h',  # Hourly data (lowercase for pandas compatibility)\n",
    "            fallback_model=AutoARIMA(),\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTraining {len(models)} models...\")\n",
    "        print(f\"  Models: {', '.join([type(m).__name__ for m in models])}\")\n",
    "        print(f\"  Horizon: {horizon} hours\")\n",
    "        print(f\"  Confidence level: {confidence_level}%\")\n",
    "\n",
    "        # Generate forecast (note: h comes first in signature)\n",
    "        forecast_df = sf.forecast(h=horizon, df=train_df, level=[confidence_level])\n",
    "\n",
    "        print(f\"  [OK] Forecast generated: {len(forecast_df)} predictions\")\n",
    "\n",
    "        return forecast_df\n",
    "\n",
    "    def evaluate_forecast(\n",
    "        self,\n",
    "        forecast_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        train_df: Optional[pd.DataFrame] = None,\n",
    "        season_length: int = 24,\n",
    "        confidence_level: int = 95\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate forecast performance against test data.\n",
    "\n",
    "        Calculates: MAPE, RMSE, MASE, and prediction interval coverage\n",
    "\n",
    "        Critical features:\n",
    "        - Merge on (unique_id, ds) for multi-series correctness\n",
    "        - All metrics computed on valid rows only (NaN-aware)\n",
    "        - Coverage denominator = valid rows (not total rows)\n",
    "        - MASE scales error relative to seasonal naive baseline\n",
    "\n",
    "        Args:\n",
    "            forecast_df: DataFrame from forecast() method\n",
    "            test_df: Test partition from train_test_split()\n",
    "            train_df: Training data (required for MASE calculation)\n",
    "            season_length: Seasonal period for MASE (default: 24 hours)\n",
    "            confidence_level: Prediction interval level (default: 95%)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with performance metrics for each model\n",
    "\n",
    "        Example:\n",
    "            >>> metrics = fetcher.evaluate_forecast(\n",
    "            ...     forecast_df, test_df, train_df, confidence_level=95\n",
    "            ... )\n",
    "            >>> print(metrics.sort_values('rmse'))\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import (mean_absolute_percentage_error,\n",
    "                                     mean_squared_error)\n",
    "\n",
    "        # Merge forecast with test data on BOTH unique_id and ds (critical for multi-series)\n",
    "        fc = forecast_df.merge(\n",
    "            test_df,\n",
    "            how=\"left\",\n",
    "            on=[\"unique_id\", \"ds\"]  # ← BOTH keys for correctness\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Evaluation merge: {len(forecast_df)} forecast rows, {len(test_df)} test rows, {len(fc)} merged rows\")\n",
    "\n",
    "        # Helper functions for metrics (NaN-aware)\n",
    "        def mape(y, yhat):\n",
    "            \"\"\"Mean Absolute Percentage Error (ignoring NaNs).\n",
    "\n",
    "            ⚠️ WARNING: MAPE is undefined when y ≈ 0 (e.g., solar at night).\n",
    "            For series with zeros, prefer RMSE, MAE, or MASE instead.\n",
    "            \"\"\"\n",
    "            mask = y.notna() & yhat.notna()\n",
    "            if mask.sum() == 0:\n",
    "                return np.nan\n",
    "            return mean_absolute_percentage_error(y[mask], yhat[mask])\n",
    "\n",
    "        def rmse(y, yhat):\n",
    "            \"\"\"Root Mean Squared Error (ignoring NaNs)\"\"\"\n",
    "            mask = y.notna() & yhat.notna()\n",
    "            if mask.sum() == 0:\n",
    "                return np.nan\n",
    "            return np.sqrt(mean_squared_error(y[mask], yhat[mask]))\n",
    "\n",
    "        def mase(y, yhat, y_train, season_length=24):\n",
    "            \"\"\"\n",
    "            Mean Absolute Scaled Error.\n",
    "\n",
    "            Scales the MAE by the MAE of a seasonal naive forecast on training data.\n",
    "            MASE < 1 means the model beats seasonal naive.\n",
    "            MASE > 1 means seasonal naive is better.\n",
    "\n",
    "            Args:\n",
    "                y: Actual test values\n",
    "                yhat: Predicted values\n",
    "                y_train: Training data for computing naive baseline\n",
    "                season_length: Seasonal period (default: 24 for hourly data)\n",
    "            \"\"\"\n",
    "            if y_train is None or len(y_train) < season_length + 1:\n",
    "                return np.nan\n",
    "\n",
    "            mask = y.notna() & yhat.notna()\n",
    "            if mask.sum() == 0:\n",
    "                return np.nan\n",
    "\n",
    "            # MAE of the forecast\n",
    "            mae_forecast = np.mean(np.abs(y[mask].values - yhat[mask].values))\n",
    "\n",
    "            # MAE of seasonal naive on training data\n",
    "            # Seasonal naive: y_t = y_{t - season_length}\n",
    "            y_train_arr = y_train[\"y\"].values\n",
    "            naive_errors = np.abs(y_train_arr[season_length:] - y_train_arr[:-season_length])\n",
    "            mae_naive = np.mean(naive_errors)\n",
    "\n",
    "            if mae_naive < 1e-10:  # Avoid division by zero\n",
    "                return np.nan\n",
    "\n",
    "            return mae_forecast / mae_naive\n",
    "\n",
    "        def coverage(y, lower, upper):\n",
    "            \"\"\"Prediction interval coverage (ignoring NaNs, denominator = valid rows)\"\"\"\n",
    "            mask = y.notna() & lower.notna() & upper.notna()\n",
    "            if mask.sum() == 0:\n",
    "                return np.nan\n",
    "            within = ((y[mask] >= lower[mask]) & (y[mask] <= upper[mask])).sum()\n",
    "            return (within / mask.sum()) * 100  # denominator = valid rows only\n",
    "\n",
    "        # Get model names (exclude metadata columns and interval bounds)\n",
    "        model_cols = [col for col in forecast_df.columns\n",
    "                     if col not in ['unique_id', 'ds'] and\n",
    "                     not col.endswith(f'-lo-{confidence_level}') and\n",
    "                     not col.endswith(f'-hi-{confidence_level}')]\n",
    "\n",
    "        # Calculate metrics for each model\n",
    "        rows = []\n",
    "        for model in model_cols:\n",
    "            y = fc[\"y\"]\n",
    "            yhat = fc[model]\n",
    "\n",
    "            # Count valid rows for this model\n",
    "            mask = y.notna() & yhat.notna()\n",
    "            valid_count = mask.sum()\n",
    "\n",
    "            lo_col = f\"{model}-lo-{confidence_level}\"\n",
    "            hi_col = f\"{model}-hi-{confidence_level}\"\n",
    "            if lo_col in fc.columns and hi_col in fc.columns:\n",
    "                coverage_value = coverage(\n",
    "                    y=y,\n",
    "                    lower=fc[lo_col],\n",
    "                    upper=fc[hi_col],\n",
    "                )\n",
    "            else:\n",
    "                coverage_value = np.nan\n",
    "\n",
    "            rows.append({\n",
    "                \"model\": model,\n",
    "                \"mape\": mape(y=y, yhat=yhat),\n",
    "                \"rmse\": rmse(y=y, yhat=yhat),\n",
    "                \"mase\": mase(y=y, yhat=yhat, y_train=train_df, season_length=season_length),\n",
    "                \"coverage\": coverage_value,\n",
    "                \"valid_rows\": valid_count,\n",
    "            })\n",
    "\n",
    "        fc_performance = pd.DataFrame(rows).sort_values('rmse')\n",
    "\n",
    "        # Report merge quality\n",
    "        valid_total = fc[\"y\"].notna().sum()\n",
    "        print(f\"\\nEvaluation Metrics (on {valid_total} valid rows out of {len(fc)} total):\")\n",
    "        print(f\"{'Model':<20} {'MAPE':<8} {'RMSE':<8} {'MASE':<8} {'Coverage':<10} {'Valid':<8}\")\n",
    "        print(\"-\" * 62)\n",
    "        for _, row in fc_performance.iterrows():\n",
    "            coverage_str = f\"{row['coverage']:.1f}%\" if pd.notna(row['coverage']) else \"N/A\"\n",
    "            mase_str = f\"{row['mase']:.3f}\" if pd.notna(row['mase']) else \"N/A\"\n",
    "            print(f\"{row['model']:<20} {row['mape']:.4f}  {row['rmse']:<8.0f} {mase_str:<8} {coverage_str:<10} {int(row['valid_rows']):<8}\")\n",
    "\n",
    "        logger.info(f\"Evaluation complete: {valid_total} valid rows, {len(fc_performance)} models evaluated\")\n",
    "\n",
    "        return fc_performance\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        config: Optional[ExperimentConfig] = None,\n",
    "        horizon: int = 24,\n",
    "        n_windows: int = 5,\n",
    "        step_size: int = 168,\n",
    "        confidence_level: int = 95\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Run rolling origin cross-validation for robust model evaluation.\n",
    "\n",
    "        Instead of a single train/test split, this creates multiple windows:\n",
    "        - Window 1: Train on data up to t1, test on t1 to t1+horizon\n",
    "        - Window 2: Train on data up to t2, test on t2 to t2+horizon\n",
    "        - ... and so on\n",
    "\n",
    "        This gives you a better estimate of how your model will perform\n",
    "        on future unseen data.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with unique_id, ds, y columns\n",
    "            config: ExperimentConfig (overrides other params if provided)\n",
    "            horizon: Forecast horizon in hours (default: 24)\n",
    "            n_windows: Number of CV windows (default: 5)\n",
    "            step_size: Hours between windows (default: 168 = 1 week)\n",
    "            confidence_level: Prediction interval level (default: 95)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (cv_results_df, leaderboard_df)\n",
    "            - cv_results_df: Raw predictions for each cutoff\n",
    "            - leaderboard_df: Aggregated metrics per model\n",
    "\n",
    "        Example:\n",
    "            >>> cv_results, leaderboard = fetcher.cross_validate(\n",
    "            ...     df_forecast,\n",
    "            ...     horizon=24,\n",
    "            ...     n_windows=5,\n",
    "            ...     step_size=168\n",
    "            ... )\n",
    "            >>> print(leaderboard)\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "\n",
    "        # Use config if provided\n",
    "        if config is not None:\n",
    "            horizon = config.horizon\n",
    "            n_windows = config.n_windows\n",
    "            step_size = config.step_size\n",
    "            confidence_level = config.confidence_level\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CROSS-VALIDATION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Horizon: {horizon} hours\")\n",
    "        print(f\"  Windows: {n_windows}\")\n",
    "        print(f\"  Step size: {step_size} hours\")\n",
    "\n",
    "        # Build models\n",
    "        models = self.build_models()\n",
    "\n",
    "        # Create StatsForecast object\n",
    "        sf = StatsForecast(\n",
    "            models=models,\n",
    "            freq='h',\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Run cross-validation\n",
    "        # This is the key statsforecast method for backtesting\n",
    "        print(f\"\\n  Running {n_windows} CV windows...\")\n",
    "        cv_df = sf.cross_validation(\n",
    "            df=df,\n",
    "            h=horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=[confidence_level],\n",
    "        )\n",
    "\n",
    "        print(f\"  [OK] CV complete: {len(cv_df)} total predictions\")\n",
    "        print(f\"  Cutoff dates: {cv_df['cutoff'].nunique()} unique\")\n",
    "\n",
    "        # Compute metrics per cutoff per model\n",
    "        cv_metrics = []\n",
    "        model_names = [type(m).__name__ if not hasattr(m, 'alias') else m.alias\n",
    "                       for m in models]\n",
    "\n",
    "        for cutoff in cv_df[\"cutoff\"].unique():\n",
    "            window_df = cv_df[cv_df[\"cutoff\"] == cutoff]\n",
    "            y_true = window_df[\"y\"].values\n",
    "\n",
    "            for model in model_names:\n",
    "                if model not in window_df.columns:\n",
    "                    continue\n",
    "\n",
    "                y_pred = window_df[model].values\n",
    "                mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # RMSE\n",
    "                rmse_val = np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2))\n",
    "\n",
    "                # MAPE\n",
    "                mape_val = 100 * np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "                # Coverage\n",
    "                lo_col = f\"{model}-lo-{confidence_level}\"\n",
    "                hi_col = f\"{model}-hi-{confidence_level}\"\n",
    "                coverage_val = np.nan\n",
    "                if lo_col in window_df.columns and hi_col in window_df.columns:\n",
    "                    lo = window_df[lo_col].values\n",
    "                    hi = window_df[hi_col].values\n",
    "                    within = ((y_true >= lo) & (y_true <= hi)).sum()\n",
    "                    coverage_val = 100 * within / len(y_true)\n",
    "\n",
    "                cv_metrics.append({\n",
    "                    \"cutoff\": cutoff,\n",
    "                    \"model\": model,\n",
    "                    \"rmse\": rmse_val,\n",
    "                    \"mape\": mape_val,\n",
    "                    \"coverage\": coverage_val,\n",
    "                })\n",
    "\n",
    "        cv_metrics_df = pd.DataFrame(cv_metrics)\n",
    "\n",
    "        # Create leaderboard by aggregating across windows\n",
    "        leaderboard = cv_metrics_df.groupby(\"model\").agg({\n",
    "            \"rmse\": [\"mean\", \"std\"],\n",
    "            \"mape\": [\"mean\", \"std\"],\n",
    "            \"coverage\": \"mean\",\n",
    "        }).round(2)\n",
    "\n",
    "        # Flatten column names\n",
    "        leaderboard.columns = [\"_\".join(col).strip() for col in leaderboard.columns.values]\n",
    "        leaderboard = leaderboard.sort_values(\"rmse_mean\").reset_index()\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"LEADERBOARD (aggregated across {n_windows} windows)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"{'Model':<20} {'RMSE':<12} {'MAPE':<12} {'Coverage':<10}\")\n",
    "        print(\"-\" * 54)\n",
    "        for _, row in leaderboard.iterrows():\n",
    "            rmse_str = f\"{row['rmse_mean']:.0f} ± {row['rmse_std']:.0f}\"\n",
    "            mape_str = f\"{row['mape_mean']:.2f} ± {row['mape_std']:.2f}\"\n",
    "            cov_str = f\"{row['coverage_mean']:.1f}%\"\n",
    "            print(f\"{row['model']:<20} {rmse_str:<12} {mape_str:<12} {cov_str:<10}\")\n",
    "\n",
    "        return cv_df, leaderboard\n",
    "\n",
    "    def register_best_model(\n",
    "        self,\n",
    "        leaderboard: pd.DataFrame,\n",
    "        model_name: str = None,\n",
    "        experiment_name: str = \"eia_forecasting\",\n",
    "        alias: str = \"champion\",\n",
    "        train_df: Optional[pd.DataFrame] = None,\n",
    "        default_horizon: int = 24,\n",
    "        freq: str = \"h\",\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Register the best model from cross-validation to MLflow Model Registry.\n",
    "        FIXED: Actually logs an MLflow Model at artifact_path=\"model\" (pyfunc),\n",
    "        so mlflow.register_model(runs:/.../model, ...) succeeds.\n",
    "        \"\"\"\n",
    "        if not MLFLOW_AVAILABLE:\n",
    "            print(\"  [SKIP] MLflow not available - cannot register model\")\n",
    "            return None\n",
    "\n",
    "        if train_df is None:\n",
    "            raise ValueError(\n",
    "                \"register_best_model requires train_df (unique_id/ds/y) so we can log a real MLflow model.\"\n",
    "            )\n",
    "        required = {\"unique_id\", \"ds\", \"y\"}\n",
    "        if not required.issubset(train_df.columns):\n",
    "            raise ValueError(f\"train_df must have {sorted(required)}, got {train_df.columns.tolist()}\")\n",
    "\n",
    "        # Select best model\n",
    "        if model_name is None:\n",
    "            model_name = leaderboard.iloc[0][\"model\"]\n",
    "\n",
    "        best_metrics = leaderboard[leaderboard[\"model\"] == model_name].iloc[0]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MODEL REGISTRY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Best model: {model_name}\")\n",
    "        print(f\"  RMSE: {best_metrics['rmse_mean']:.0f}\")\n",
    "        print(f\"  Registering with alias: {alias}\")\n",
    "\n",
    "        import mlflow\n",
    "        import mlflow.pyfunc\n",
    "        from mlflow.models import infer_signature\n",
    "\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        # --- pyfunc wrapper ---\n",
    "        class _StatsForecastPyFunc(mlflow.pyfunc.PythonModel):\n",
    "            def __init__(self, chosen_model: str, confidence_level: int, freq: str, default_h: int):\n",
    "                self.chosen_model = chosen_model\n",
    "                self.confidence_level = int(confidence_level)\n",
    "                self.freq = freq\n",
    "                self.default_h = int(default_h)\n",
    "\n",
    "            @staticmethod\n",
    "            def _build_models():\n",
    "                from statsforecast.models import (\n",
    "                    MSTL, AutoARIMA, DynamicOptimizedTheta, HoltWinters, SeasonalNaive\n",
    "                )\n",
    "                return [\n",
    "                    AutoARIMA(season_length=24),\n",
    "                    SeasonalNaive(season_length=24),\n",
    "                    DynamicOptimizedTheta(season_length=24),\n",
    "                    HoltWinters(season_length=24),\n",
    "                    MSTL(season_length=[24, 24 * 7], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "                    MSTL(season_length=[24, 24 * 7], trend_forecaster=HoltWinters(), alias=\"MSTL_HoltWinters\"),\n",
    "                ]\n",
    "\n",
    "            def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "                import pandas as pd\n",
    "                from statsforecast import StatsForecast\n",
    "\n",
    "                df = model_input.copy()\n",
    "\n",
    "                # Allow passing a horizon in the input (single value repeated is fine)\n",
    "                if \"horizon\" in df.columns:\n",
    "                    h = int(df[\"horizon\"].iloc[0])\n",
    "                    df = df.drop(columns=[\"horizon\"])\n",
    "                else:\n",
    "                    h = self.default_h\n",
    "\n",
    "                # Expect unique_id/ds/y\n",
    "                if not {\"unique_id\", \"ds\", \"y\"}.issubset(df.columns):\n",
    "                    raise ValueError(\n",
    "                        f\"pyfunc input must contain unique_id/ds/y (+ optional horizon). Got: {df.columns.tolist()}\"\n",
    "                    )\n",
    "\n",
    "                df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"raise\")\n",
    "                # Optional safety: strip tz if present\n",
    "                if getattr(df[\"ds\"].dt, \"tz\", None) is not None:\n",
    "                    df[\"ds\"] = df[\"ds\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "                # Pick the chosen model by alias or class name\n",
    "                chosen = None\n",
    "                for m in self._build_models():\n",
    "                    name = getattr(m, \"alias\", type(m).__name__)\n",
    "                    if name == self.chosen_model or type(m).__name__ == self.chosen_model:\n",
    "                        chosen = m\n",
    "                        break\n",
    "                if chosen is None:\n",
    "                    available = [getattr(m, \"alias\", type(m).__name__) for m in self._build_models()]\n",
    "                    raise ValueError(f\"Unknown chosen_model={self.chosen_model}. Available: {available}\")\n",
    "\n",
    "                sf = StatsForecast(models=[chosen], freq=self.freq, n_jobs=1)\n",
    "                out = sf.forecast(df=df, h=h, level=[self.confidence_level]).reset_index()\n",
    "                return out\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"register_{model_name}\"):\n",
    "            # Log metrics/params (same as you had)\n",
    "            mlflow.log_metric(\"rmse_mean\", float(best_metrics[\"rmse_mean\"]))\n",
    "            mlflow.log_metric(\"rmse_std\", float(best_metrics[\"rmse_std\"]))\n",
    "            mlflow.log_metric(\"mape_mean\", float(best_metrics[\"mape_mean\"]))\n",
    "            mlflow.log_metric(\"coverage_mean\", float(best_metrics[\"coverage_mean\"]))\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "            # Input example needs enough history for MSTL weekly season length\n",
    "            # Use last 30 days (720 hours) to be safe\n",
    "            example_in = train_df.sort_values(\"ds\").tail(24 * 30)[[\"unique_id\", \"ds\", \"y\"]].copy()\n",
    "            example_in[\"horizon\"] = int(default_horizon)\n",
    "            # ✅ MLflow requires timezone-naive datetimes\n",
    "            example_in[\"ds\"] = pd.to_datetime(example_in[\"ds\"], errors=\"raise\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "\n",
    "            # Run once to infer output signature (can be a bit slow, but deterministic)\n",
    "            pyfunc_model = _StatsForecastPyFunc(\n",
    "                chosen_model=str(model_name),\n",
    "                confidence_level=int(best_metrics.get(\"coverage_mean\", 95) and 95),  # keep your pipeline default\n",
    "                freq=freq,\n",
    "                default_h=int(default_horizon),\n",
    "            )\n",
    "            example_out = pyfunc_model.predict(None, example_in)\n",
    "\n",
    "            signature = infer_signature(example_in, example_out)\n",
    "\n",
    "            # THIS is the key fix: log a real MLflow Model at artifact_path=\"model\"\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=\"model\",\n",
    "                python_model=pyfunc_model,\n",
    "                signature=signature,\n",
    "                input_example=example_in,\n",
    "                pip_requirements=[\n",
    "                    f\"mlflow=={mlflow.__version__}\",\n",
    "                    \"pandas\",\n",
    "                    \"numpy\",\n",
    "                    \"statsforecast\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            registered_model_name = f\"{experiment_name}_{model_name}\"\n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "            try:\n",
    "                result = mlflow.register_model(model_uri, registered_model_name)\n",
    "                version = result.version\n",
    "\n",
    "                # Alias (works on newer MLflow); safe fallback if not supported\n",
    "                client = mlflow.tracking.MlflowClient()\n",
    "                try:\n",
    "                    client.set_registered_model_alias(registered_model_name, alias, version)\n",
    "                except Exception:\n",
    "                    client.set_model_version_tag(registered_model_name, version, \"alias\", alias)\n",
    "\n",
    "                print(f\"  [OK] Registered: {registered_model_name} v{version}\")\n",
    "                print(f\"  [OK] Alias '{alias}' assigned to v{version}\")\n",
    "                logger.info(f\"Model registered: {registered_model_name} v{version} with alias {alias}\")\n",
    "                return f\"{registered_model_name}@{alias}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model registration failed: {e}\")\n",
    "                print(f\"  [WARN] Registration failed: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "    def _create_plot(self, test_df: pd.DataFrame, forecast_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Create an interactive plotly visualization of forecast vs actuals.\n",
    "\n",
    "        Args:\n",
    "            test_df: Test partition with actual values\n",
    "            forecast_df: Forecast predictions\n",
    "\n",
    "        Returns:\n",
    "            Plotly Figure object\n",
    "        \"\"\"\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        # Merge data\n",
    "        merged = test_df.merge(forecast_df, on=['unique_id', 'ds'])\n",
    "        merged = merged.sort_values('ds')\n",
    "\n",
    "        # Create figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add actual values\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=merged['ds'],\n",
    "            y=merged['y'],\n",
    "            mode='lines',\n",
    "            name='Actual',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ))\n",
    "\n",
    "        # Add forecast from best model (MSTL_ARIMA)\n",
    "        if 'MSTL_ARIMA' in merged.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=merged['ds'],\n",
    "                y=merged['MSTL_ARIMA'],\n",
    "                mode='lines',\n",
    "                name='MSTL_ARIMA (Best)',\n",
    "                line=dict(color='red', width=2, dash='dash')\n",
    "            ))\n",
    "\n",
    "            # Add 95% confidence interval if available\n",
    "            if 'MSTL_ARIMA-hi-95' in merged.columns and 'MSTL_ARIMA-lo-95' in merged.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=merged['ds'].tolist() + merged['ds'].tolist()[::-1],\n",
    "                    y=merged['MSTL_ARIMA-hi-95'].tolist() + merged['MSTL_ARIMA-lo-95'].tolist()[::-1],\n",
    "                    fill='toself',\n",
    "                    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                    line=dict(color='rgba(255, 0, 0, 0)'),\n",
    "                    name='95% Confidence Interval'\n",
    "                ))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='EIA Electricity Generation: Forecast vs Actual',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Generation (MWh)',\n",
    "            hovermode='x unified',\n",
    "            height=400,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def run_experiment(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        experiment_name: str,\n",
    "        test_hours: int = 72,\n",
    "        models_to_test: Optional[List[str]] = None,\n",
    "        track_with_mlflow: bool = False\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a complete forecasting experiment with model evaluation.\n",
    "\n",
    "        Args:\n",
    "            df: Cleaned DataFrame from full_pipeline()\n",
    "            experiment_name: Name for this experiment\n",
    "            test_hours: Hours to reserve for testing (default: 72)\n",
    "            models_to_test: List of model names to test (None = all)\n",
    "            track_with_mlflow: Whether to log to MLflow (default: False)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with experiment results including:\n",
    "            - experiment_name: Name of experiment\n",
    "            - timestamp: When experiment was run\n",
    "            - data_shape: (rows, columns) of data used\n",
    "            - train_size: Number of training records\n",
    "            - test_size: Number of testing records\n",
    "            - metrics: Performance metrics for each model\n",
    "            - best_model: Name of best performing model\n",
    "            - results: Full results DataFrame\n",
    "\n",
    "        Example:\n",
    "            >>> results = fetcher.run_experiment(df, \"exp1_baseline\")\n",
    "            >>> print(results['best_model'])\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting experiment: {experiment_name} with test_hours={test_hours}\")\n",
    "\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"EXPERIMENT: {experiment_name}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Start MLflow run if enabled\n",
    "        if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "            mlflow.start_run(run_name=experiment_name)\n",
    "            mlflow.log_param(\"experiment_name\", experiment_name)\n",
    "            mlflow.log_param(\"test_hours\", test_hours)\n",
    "            logger.info(\"MLflow run started for experiment\")\n",
    "\n",
    "        try:\n",
    "            # Prepare data\n",
    "            logger.info(\"Preparing data for experiment\")\n",
    "            print(f\"\\nPreparing data for experiment...\")\n",
    "            df_forecast = self.prepare_for_forecasting(df)\n",
    "            train_df, test_df = self.train_test_split(df_forecast, test_hours=test_hours)\n",
    "\n",
    "            # Log data info\n",
    "            logger.info(f\"Data shape: {df.shape}, Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "            print(f\"  Data shape: {df.shape}\")\n",
    "            print(f\"  Training records: {len(train_df)}\")\n",
    "            print(f\"  Testing records: {len(test_df)}\")\n",
    "\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.log_param(\"data_rows\", df.shape[0])\n",
    "                mlflow.log_param(\"train_size\", len(train_df))\n",
    "                mlflow.log_param(\"test_size\", len(test_df))\n",
    "\n",
    "            # Train and forecast\n",
    "            logger.info(\"Training models\")\n",
    "            print(f\"\\nTraining models...\")\n",
    "            forecast_df = self.forecast(train_df, horizon=len(test_df))\n",
    "\n",
    "            # Evaluate (pass train_df for MASE calculation)\n",
    "            logger.info(\"Evaluating model performance\")\n",
    "            print(f\"\\nEvaluating performance...\")\n",
    "            metrics_df = self.evaluate_forecast(forecast_df, test_df, train_df=train_df)\n",
    "\n",
    "            # Log metrics to MLflow\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                for _, row in metrics_df.iterrows():\n",
    "                    model_name = row['model']\n",
    "                    mlflow.log_metrics({\n",
    "                        f\"{model_name}_mape\": row['mape'],\n",
    "                        f\"{model_name}_rmse\": row['rmse'],\n",
    "                        f\"{model_name}_coverage\": row['coverage'],\n",
    "                    })\n",
    "\n",
    "            # Identify best model\n",
    "            best_model = metrics_df.iloc[0]['model']\n",
    "            best_rmse = metrics_df.iloc[0]['rmse']\n",
    "\n",
    "            logger.info(f\"Experiment {experiment_name} complete - Best model: {best_model}, RMSE: {best_rmse:.0f}\")\n",
    "\n",
    "            # Compile results\n",
    "            experiment_results = {\n",
    "                \"experiment_name\": experiment_name,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"data_shape\": df.shape,\n",
    "                \"train_size\": len(train_df),\n",
    "                \"test_size\": len(test_df),\n",
    "                \"metrics\": metrics_df,\n",
    "                \"best_model\": best_model,\n",
    "                \"best_rmse\": best_rmse,\n",
    "                \"results\": metrics_df\n",
    "            }\n",
    "\n",
    "            # Log summary\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(\"best_rmse\", best_rmse)\n",
    "                mlflow.log_param(\"best_model\", best_model)\n",
    "\n",
    "            print(f\"\\n[OK] Experiment complete!\")\n",
    "            print(f\"  Best model: {best_model}\")\n",
    "            print(f\"  Best RMSE: {best_rmse:.0f}\")\n",
    "\n",
    "            return experiment_results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Experiment {experiment_name} failed: {str(e)}\", exc_info=True)\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.log_param(\"error\", str(e))\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.end_run()\n",
    "                logger.info(f\"MLflow run ended for experiment {experiment_name}\")\n",
    "\n",
    "    def save_datasets(\n",
    "        self,\n",
    "        raw_df: pd.DataFrame,\n",
    "        clean_df: pd.DataFrame,\n",
    "        integrity_report: Dict,\n",
    "        output_dir: str = \"data\",\n",
    "        pull_params: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save datasets with versioning and metadata for reproducibility.\n",
    "\n",
    "        Creates:\n",
    "        - raw.parquet: Unmodified API response\n",
    "        - clean.parquet: After prepare + validate\n",
    "        - metadata.json: Pull parameters, timestamps, row counts, integrity report\n",
    "\n",
    "        This enables:\n",
    "        - Reproducibility across experiments\n",
    "        - Data lineage tracking\n",
    "        - Debugging with raw vs clean comparison\n",
    "        - Integrity validation history\n",
    "\n",
    "        Args:\n",
    "            raw_df: Raw DataFrame from pull_data()\n",
    "            clean_df: Cleaned DataFrame from prepare_data()\n",
    "            integrity_report: Report from validate_time_series_integrity()\n",
    "            output_dir: Directory to save datasets (default: \"data\")\n",
    "            pull_params: Dictionary of pull_data() parameters for metadata\n",
    "\n",
    "        Returns:\n",
    "            Path to saved metadata file\n",
    "\n",
    "        Example:\n",
    "            >>> metadata_path = fetcher.save_datasets(\n",
    "            ...     raw_df, clean_df, integrity_report,\n",
    "            ...     pull_params={\"start_date\": \"2024-12-01\", \"respondent\": \"US48\"}\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Generate timestamp for this version\n",
    "        timestamp = datetime.now(tz=pytz.UTC).isoformat()\n",
    "\n",
    "        # Build metadata\n",
    "        metadata = {\n",
    "            \"version\": \"1.0\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"pull_parameters\": pull_params or {},\n",
    "            \"raw_row_count\": len(raw_df),\n",
    "            \"clean_row_count\": len(clean_df),\n",
    "            \"validation_status\": \"passed\" if integrity_report.get(\"status\") == \"valid\" else \"failed\",\n",
    "            \"integrity_report\": {\n",
    "                \"duplicate_count\": integrity_report.get(\"duplicate_count\", 0),\n",
    "                \"missing_hours\": integrity_report.get(\"missing_hours\", 0),\n",
    "                \"longest_gap_hours\": integrity_report.get(\"longest_gap_hours\", 0),\n",
    "                \"dst_repeated_hours\": integrity_report.get(\"dst_repeated_hours\", 0),\n",
    "            },\n",
    "            \"columns\": {\n",
    "                \"raw\": list(raw_df.columns),\n",
    "                \"clean\": list(clean_df.columns),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save raw data\n",
    "        raw_path = os.path.join(output_dir, \"raw.parquet\")\n",
    "        raw_df.to_parquet(raw_path, index=False)\n",
    "        print(f\"  [OK] Raw data saved: {raw_path}\")\n",
    "        logger.info(f\"Raw data saved: {raw_path} ({len(raw_df)} rows)\")\n",
    "\n",
    "        # Save clean data\n",
    "        clean_path = os.path.join(output_dir, \"clean.parquet\")\n",
    "        clean_df.to_parquet(clean_path, index=False)\n",
    "        print(f\"  [OK] Clean data saved: {clean_path}\")\n",
    "        logger.info(f\"Clean data saved: {clean_path} ({len(clean_df)} rows)\")\n",
    "\n",
    "        # Save metadata\n",
    "        metadata_path = os.path.join(output_dir, \"metadata.json\")\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        print(f\"  [OK] Metadata saved: {metadata_path}\")\n",
    "        logger.info(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "        return metadata_path\n",
    "\n",
    "    def compare_experiments(self, experiments: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare results from multiple experiments.\n",
    "\n",
    "        Args:\n",
    "            experiments: List of experiment result dictionaries\n",
    "\n",
    "        Returns:\n",
    "            DataFrame comparing best models from each experiment\n",
    "\n",
    "        Example:\n",
    "            >>> exp1 = fetcher.run_experiment(df, \"exp1\")\n",
    "            >>> exp2 = fetcher.run_experiment(df, \"exp2\")\n",
    "            >>> comparison = fetcher.compare_experiments([exp1, exp2])\n",
    "        \"\"\"\n",
    "        logger.info(f\"Comparing {len(experiments)} experiments\")\n",
    "\n",
    "        comparison_rows = []\n",
    "\n",
    "        for exp in experiments:\n",
    "            best_row = exp['results'].iloc[0]\n",
    "            comparison_rows.append({\n",
    "                \"experiment\": exp[\"experiment_name\"],\n",
    "                \"best_model\": exp[\"best_model\"],\n",
    "                \"mape\": best_row['mape'],\n",
    "                \"rmse\": best_row['rmse'],\n",
    "                \"coverage\": best_row['coverage'],\n",
    "                \"timestamp\": exp[\"timestamp\"]\n",
    "            })\n",
    "\n",
    "        comparison_df = pd.DataFrame(comparison_rows).sort_values('rmse')\n",
    "\n",
    "        # Log comparison results\n",
    "        logger.info(\"Experiment comparison results:\")\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            logger.info(f\"  {row['experiment']}: {row['best_model']} (RMSE: {row['rmse']:.0f}, MAPE: {row['mape']:.4f})\")\n",
    "\n",
    "        best_exp = comparison_df.iloc[0]\n",
    "        logger.info(f\"Best overall: {best_exp['experiment']} with {best_exp['best_model']} (RMSE: {best_exp['rmse']:.0f})\")\n",
    "\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n{'Experiment':<25} {'Best Model':<20} {'RMSE':<10} {'MAPE':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            print(f\"{row['experiment']:<25} {row['best_model']:<20} {row['rmse']:.0f}      {row['mape']:.4f}\")\n",
    "\n",
    "        return comparison_df\n",
    "\n",
    "    def full_pipeline(\n",
    "        self,\n",
    "        start_date: str = \"2023-01-01\",\n",
    "        end_date: str = \"2024-12-31\",\n",
    "        respondent: str = \"US48\",\n",
    "        fueltype: str = \"NG\",\n",
    "        track_with_mlflow: bool = False\n",
    "    ) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: pull -> prepare -> validate -> stats.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start date for data pull (YYYY-MM-DD)\n",
    "            end_date: End date for data pull (YYYY-MM-DD)\n",
    "            respondent: Region code (default: US48)\n",
    "            fueltype: Fuel type code (default: NG)\n",
    "            track_with_mlflow: Whether to log to MLflow (default: False)\n",
    "\n",
    "        Returns:\n",
    "            (cleaned_dataframe, statistics_dict)\n",
    "\n",
    "        Example:\n",
    "            >>> df, stats = fetcher.full_pipeline()\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting full pipeline: {start_date} to {end_date}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FULL PIPELINE: Pull -> Prepare -> Validate -> Stats\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Start MLflow run if enabled\n",
    "        if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "            mlflow.start_run(run_name=f\"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "            mlflow.log_param(\"start_date\", start_date)\n",
    "            mlflow.log_param(\"end_date\", end_date)\n",
    "            mlflow.log_param(\"respondent\", respondent)\n",
    "            mlflow.log_param(\"fueltype\", fueltype)\n",
    "            logger.info(\"MLflow run started\")\n",
    "\n",
    "        try:\n",
    "            # Step 2: Pull\n",
    "            logger.info(\"Pulling raw data from EIA API\")\n",
    "            df_raw = self.pull_data(start_date, end_date, respondent, fueltype)\n",
    "\n",
    "            # Step 3: Inspect\n",
    "            logger.info(f\"Raw data shape: {df_raw.shape}\")\n",
    "            self.inspect_data(df_raw)\n",
    "\n",
    "            # Step 4: Prepare\n",
    "            logger.info(\"Preparing data\")\n",
    "            df_clean = self.prepare_data(df_raw)\n",
    "\n",
    "            # Step 5: Validate\n",
    "            logger.info(\"Validating data\")\n",
    "            is_valid = self.validate_data(df_clean)\n",
    "\n",
    "            if not is_valid:\n",
    "                logger.warning(\"Data validation failed\")\n",
    "                if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                    mlflow.log_param(\"validation_status\", \"failed\")\n",
    "            else:\n",
    "                logger.info(\"Data validation passed\")\n",
    "                if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                    mlflow.log_param(\"validation_status\", \"passed\")\n",
    "\n",
    "            # Step 6: Stats\n",
    "            logger.info(\"Computing statistics\")\n",
    "            stats = self.get_stats(df_clean)\n",
    "\n",
    "            # Log stats to MLflow\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.log_metric(\"record_count\", stats['record_count'])\n",
    "                mlflow.log_metric(\"value_min\", stats['value_stats']['min'])\n",
    "                mlflow.log_metric(\"value_max\", stats['value_stats']['max'])\n",
    "                mlflow.log_metric(\"value_mean\", stats['value_stats']['mean'])\n",
    "                mlflow.log_metric(\"missing_count\", stats['missing_count'])\n",
    "                logger.info(\"Statistics logged to MLflow\")\n",
    "\n",
    "            logger.info(\"Full pipeline completed successfully\")\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PIPELINE COMPLETE\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "            return df_clean, stats\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed with error: {str(e)}\", exc_info=True)\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.log_param(\"error\", str(e))\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            if track_with_mlflow and MLFLOW_AVAILABLE:\n",
    "                mlflow.end_run()\n",
    "                logger.info(\"MLflow run ended\")\n",
    "\n",
    "\n",
    "# Allow testing individual steps\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: EIA_API_KEY not found in environment variables or .env file\")\n",
    "        print(\"Please create a .env file with: EIA_API_KEY=your_api_key_here\")\n",
    "        exit(1)\n",
    "\n",
    "    # Initialize\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    # Run full pipeline\n",
    "    df, stats = fetcher.full_pipeline(\n",
    "        start_date=\"2024-12-01\",\n",
    "        end_date=\"2024-12-31\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nLast 5 rows:\")\n",
    "    print(df.tail())\n",
    "\n",
    "    print(\"\\n[OK] Data successfully loaded from .env file!\")\n",
    "    print(\"[OK] Ready for time series analysis and forecasting\")\n",
    "\n",
    "    # FORECASTING WORKFLOW\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FORECASTING WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Step 1: Prepare data for forecasting\n",
    "    df_forecast = fetcher.prepare_for_forecasting(df)\n",
    "    print(f\"\\nData reformatted for forecasting:\")\n",
    "    print(f\"  Columns: {list(df_forecast.columns)}\")\n",
    "    print(f\"  Shape: {df_forecast.shape}\")\n",
    "\n",
    "    # Step 2: Train/test split (72 hours test set)\n",
    "    train_df, test_df = fetcher.train_test_split(df_forecast, test_hours=72)\n",
    "\n",
    "    # Step 3: Train models and create forecast\n",
    "    forecast_df = fetcher.forecast(train_df, horizon=len(test_df))\n",
    "\n",
    "    # Step 4: Evaluate performance (pass train_df for MASE)\n",
    "    metrics = fetcher.evaluate_forecast(forecast_df, test_df, train_df=train_df)\n",
    "\n",
    "    # Step 5: Visualize results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        print(\"\\nGenerating forecast visualization...\")\n",
    "\n",
    "        # Create the plot using StatsForecast's plot method\n",
    "        p = fetcher._create_plot(test_df, forecast_df)\n",
    "\n",
    "        # Display the plot inline in Jupyter\n",
    "        p.show()\n",
    "\n",
    "        print(f\"  [OK] Forecast plot displayed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [INFO] Plotly visualization setup: {str(e)[:60]}...\")\n",
    "        print(\"  (This is optional - forecast metrics are available above)\")\n",
    "\n",
    "    print(\"\\n[OK] Forecasting workflow complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61461706",
   "metadata": {},
   "source": [
    "# Chapter 3 — Orchestration & Pipeline DAG\n",
    "\n",
    "## Outcomes (what I can do after this)\n",
    "\n",
    "- [ ] I can run an end-to-end forecasting pipeline from the CLI\n",
    "- [ ] I can understand how tasks decompose a workflow and why idempotency matters\n",
    "- [ ] I can deploy the pipeline as an Airflow DAG (if Airflow is installed)\n",
    "- [ ] I can visualize the pipeline dependency graph and explain task ordering\n",
    "- [ ] I can modify task configurations without breaking downstream dependencies\n",
    "\n",
    "## Concepts (plain English)\n",
    "\n",
    "- **Task**: An atomic unit of work that can be re-run independently (pull data, validate, train, forecast)\n",
    "- **DAG** (Directed Acyclic Graph): A visual representation of task dependencies (X → Y means X must finish before Y starts)\n",
    "- **Idempotency**: A task can be re-run multiple times with the same inputs and produce the same outputs (no side effects like duplicate records)\n",
    "- **Atomic writes**: File writes that either fully complete or fail (no partial files left behind)\n",
    "- **Linear pipeline**: Tasks execute sequentially in a fixed order (no branching or conditional execution)\n",
    "- **CLI**: Command-line interface to trigger the pipeline (vs. scheduled in Airflow)\n",
    "- **Run ID**: A unique identifier for one pipeline execution (timestamps, UUIDs, etc.) used to group artifacts\n",
    "\n",
    "## Architecture (what we're building)\n",
    "\n",
    "### Inputs\n",
    "- **CLI arguments**:\n",
    "  - `--start-date` YYYY-MM-DD\n",
    "  - `--end-date` YYYY-MM-DD\n",
    "  - `--horizon` integer (hours ahead to forecast)\n",
    "  - `--overwrite` boolean (re-run all tasks if True)\n",
    "  - `--output-dir` path (default: \"artifacts/\")\n",
    "\n",
    "- **Environment**: `.env` file with `EIA_API_KEY`\n",
    "\n",
    "### Outputs\n",
    "- **data/raw.parquet**: Raw API response\n",
    "- **data/clean.parquet**: Normalized data (UTC, valid schema)\n",
    "- **data/metadata.json**: Ingestion snapshot (date range, row count, integrity report)\n",
    "- **artifacts/cv_results.parquet**: Cross-validation forecast table\n",
    "- **artifacts/leaderboard.parquet**: Model rankings\n",
    "- **artifacts/predictions.parquet**: Final forecast (trained on all clean data)\n",
    "- **mlflow/**: MLflow experiment and model artifact\n",
    "\n",
    "### Pipeline Flow\n",
    "```\n",
    "ingest_eia()\n",
    "    ↓ (produces data/raw.parquet)\n",
    "prepare_clean()\n",
    "    ↓ (produces data/clean.parquet)\n",
    "validate_clean()\n",
    "    ↓ (validates, produces report)\n",
    "train_backtest_select()\n",
    "    ↓ (produces cv_results, leaderboard)\n",
    "register_champion()\n",
    "    ↓ (registers model in MLflow)\n",
    "forecast_publish()\n",
    "    ↓ (produces predictions)\n",
    "(optional) Chapter 4 integration\n",
    "```\n",
    "\n",
    "### Invariants (must always hold)\n",
    "- Each task's output directory exists before writing\n",
    "- Outputs are written atomically (all or nothing)\n",
    "- If `--overwrite=False`, skip task if output already exists (idempotent)\n",
    "- Task ordering is strict: no task runs until all predecessors finish\n",
    "- All intermediate files include run_id for traceability\n",
    "\n",
    "### Failure modes\n",
    "- API unavailable during ingest_eia() → task fails, pipeline stops, no partial files\n",
    "- Validation fails (duplicates, missing hours) → pipeline stops before training (prevents bad model)\n",
    "- Training runs out of memory → no model artifact written (MLflow stays clean)\n",
    "- Forecast publish fails → no predictions written, but leaderboard is preserved (can retry)\n",
    "\n",
    "## Files touched\n",
    "\n",
    "- **`src/chapter3/tasks.py`** (433 lines)\n",
    "  - `ingest_eia(config, run_id)` → raw.parquet\n",
    "  - `prepare_clean(raw_path, config, run_id)` → clean.parquet, metadata.json\n",
    "  - `validate_clean(clean_path, run_id)` → raises ValueError if not valid\n",
    "  - `train_backtest_select(clean_path, config, run_id)` → cv_results.parquet, leaderboard.parquet\n",
    "  - `register_champion(leaderboard, config, clean_path, run_id)` → registers in MLflow\n",
    "  - `forecast_publish(clean_path, config, run_id)` → predictions.parquet\n",
    "\n",
    "- **`src/chapter3/dag_builder.py`** (121 lines)\n",
    "  - `build_daily_dag()`: Returns Airflow DAG (if airflow is installed)\n",
    "  - `build_dag_dot()`: Returns DOT graph string for CLI visualization\n",
    "  - DAG has schedule_interval = \"0 6 * * *\" (6 AM UTC daily)\n",
    "\n",
    "- **`src/chapter3/cli.py`** (69 lines)\n",
    "  - `run()`: Typer command to execute pipeline from CLI\n",
    "  - Parses arguments, generates run_id, calls tasks in sequence\n",
    "\n",
    "- **`src/chapter3/config.py`** (67 lines)\n",
    "  - `PipelineConfig` dataclass: all configuration parameters\n",
    "\n",
    "- **`src/chapter3/io_utils.py`** (33 lines)\n",
    "  - Helpers for atomic parquet/JSON writes\n",
    "\n",
    "## Step-by-step walkthrough\n",
    "\n",
    "### 1) Verify setup and prerequisites\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -m pytest tests/ -v  # Run tests to verify environment\n",
    "echo $EIA_API_KEY  # Verify API key is set in .env\n",
    "```\n",
    "- **Expect**: Tests pass; API key is set\n",
    "- **If it fails**: Check .env file and install dependencies with `pip install -e .` or `uv sync`\n",
    "\n",
    "### 2) View pipeline DAG (without running)\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -c \"\n",
    "from src.chapter3.dag_builder import build_dag_dot\n",
    "dot_string = build_dag_dot()\n",
    "print(dot_string)\n",
    "\"\n",
    "# Copy output to https://dreampuf.github.io/GraphvizOnline/ to visualize\n",
    "```\n",
    "- **Expect**: DOT graph showing 6 tasks and dependencies: ingest → prepare → validate → train → register → forecast\n",
    "- **If it fails**: Check that dag_builder.py is present and imports are correct\n",
    "\n",
    "### 3) Run pipeline end-to-end (CLI)\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24 \\\n",
    "  --output-dir artifacts/\n",
    "```\n",
    "- **Expect**:\n",
    "  - Logs: \"Task: ingest_eia started\", \"Task: prepare_clean started\", etc.\n",
    "  - Files created: data/raw.parquet, data/clean.parquet, artifacts/cv_results.parquet, etc.\n",
    "  - Run finishes in 2-10 minutes depending on data size and model training\n",
    "- **If it fails**:\n",
    "  - \"API Error\": Check EIA_API_KEY in .env\n",
    "  - \"Validation failed\": Re-run Chapter 1 to diagnose integrity issues\n",
    "  - \"Out of memory\": Reduce date range or horizon; reduce n_windows\n",
    "\n",
    "### 4) Inspect task outputs\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Check raw data\n",
    "df_raw = pd.read_parquet(\"c:\\docker_projects\\atsaf\\data\\raw.parquet\")\n",
    "print(f\"Raw shape: {df_raw.shape}, Columns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Check clean data\n",
    "df_clean = pd.read_parquet(\"c:\\docker_projects\\atsaf\\data\\clean.parquet\")\n",
    "print(f\"Clean shape: {df_clean.shape}, Columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "# Check leaderboard\n",
    "leaderboard = pd.read_parquet(\"c:\\docker_projects\\atsaf\\artifacts\\leaderboard.parquet\")\n",
    "print(f\"\\nLeaderboard:\\n{leaderboard.head()}\")\n",
    "\n",
    "# Check metadata\n",
    "import json\n",
    "with open(\"c:\\docker_projects\\atsaf\\data\\metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "    print(f\"\\nMetadata: {metadata}\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - raw: columns [time, value, respondent, fueltype]\n",
    "  - clean: columns [unique_id, ds, y] with UTC timestamps\n",
    "  - leaderboard: columns [model, rmse_mean, rmse_std, rank]\n",
    "  - metadata: status='valid', row_count, date_range\n",
    "- **If it fails**: Check pipeline logs to see which task failed\n",
    "\n",
    "### 5) Re-run with `--overwrite` flag\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24 \\\n",
    "  --overwrite  # Force re-run all tasks\n",
    "```\n",
    "- **Expect**: Same outputs as step 3 (deterministic)\n",
    "- **If it fails**: Something is non-deterministic (e.g., random seed not set in model)\n",
    "\n",
    "### 6) Re-run without `--overwrite` (test idempotency)\n",
    "```bash\n",
    "cd c:\\docker_projects\\atsaf\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24\n",
    "  # No --overwrite flag\n",
    "```\n",
    "- **Expect**: Pipeline finishes immediately (skips all tasks because files exist)\n",
    "- **If it fails**: Tasks are not checking for existing outputs; check `if output_path.exists(): return` in each task\n",
    "\n",
    "### 7) Deploy to Airflow (optional, if Airflow is installed)\n",
    "```bash\n",
    "# Set Airflow home\n",
    "export AIRFLOW_HOME=~/airflow_atsaf\n",
    "\n",
    "# Initialize DB\n",
    "airflow db init\n",
    "\n",
    "# Create Airflow user\n",
    "airflow users create \\\n",
    "  --username admin \\\n",
    "  --firstname Admin \\\n",
    "  --lastname User \\\n",
    "  --role Admin \\\n",
    "  --email admin@example.com\n",
    "\n",
    "# Copy DAG to Airflow DAGs folder\n",
    "cp src/chapter3/dag_builder.py ~/airflow_atsaf/dags/atsaf_daily_pipeline.py\n",
    "\n",
    "# Start scheduler and webserver\n",
    "airflow scheduler &\n",
    "airflow webserver --port 8080 &\n",
    "\n",
    "# Visit http://localhost:8080 to see DAG\n",
    "```\n",
    "- **Expect**: DAG appears in Airflow UI; can manually trigger runs\n",
    "- **If it fails**: Check Airflow logs at ~/airflow_atsaf/logs/\n",
    "\n",
    "## Metrics & success criteria\n",
    "\n",
    "### Primary metric\n",
    "- **Pipeline success rate**: 100% of runs complete without errors\n",
    "\n",
    "### Secondary metrics\n",
    "- **Task execution time**: Each task < 5 minutes (ingest < 1min, train < 3min)\n",
    "- **Idempotency**: Re-running with same inputs produces same outputs\n",
    "- **File integrity**: All output files valid (non-empty, correct schema)\n",
    "\n",
    "### \"Good enough\" threshold\n",
    "- Pipeline completes in < 10 minutes for 12-month dataset\n",
    "- All 6 tasks execute in order with no skipped tasks (on first run)\n",
    "- Artifacts folder contains all expected files (raw, clean, cv_results, leaderboard, predictions)\n",
    "\n",
    "### What would make me redesign\n",
    "- Task execution time > 30 minutes → parallelize or reduce data\n",
    "- Output files corrupt (can't read as parquet) → implement better error handling\n",
    "- Idempotency broken (re-run produces different results) → add random seed or determinism\n",
    "\n",
    "## Pitfalls (things that commonly break)\n",
    "\n",
    "1. **File path confusion (Windows vs Linux)**:\n",
    "   - Windows uses `\\`, Linux uses `/`\n",
    "   - Our code uses `pathlib.Path` which handles both, but command-line args may have issues\n",
    "   - **Fix**: Always use forward slashes in CLI args or use raw strings (r\"path\\to\\file\")\n",
    "\n",
    "2. **Task ordering assumptions**:\n",
    "   - If you refactor and skip validate_clean(), bad data gets trained\n",
    "   - Our linear order is strict: ingest → prepare → validate → train → register → forecast\n",
    "   - **Fix**: Never skip or reorder tasks without understanding downstream impact\n",
    "\n",
    "3. **Output directory doesn't exist**:\n",
    "   - If `artifacts/` or `data/` directory doesn't exist, writes fail silently\n",
    "   - **Fix**: Run `mkdir -p artifacts/ data/` first, or let tasks create them\n",
    "\n",
    "4. **API rate limiting**:\n",
    "   - EIA API limits to ~50 requests/second; if pulling large date ranges, may timeout\n",
    "   - **Fix**: Add delay between requests or use batch API endpoint\n",
    "\n",
    "5. **MLflow registration fails silently**:\n",
    "   - If MLflow is not initialized or artifact store is misconfigured, registration may skip\n",
    "   - **Fix**: Check `mlflow.get_tracking_uri()` and `mlflow.get_artifact_uri()` before deploying\n",
    "\n",
    "6. **run_id collision**:\n",
    "   - If two pipelines run simultaneously with same run_id, they may overwrite each other\n",
    "   - **Fix**: Use UUID or timestamp + hostname in run_id\n",
    "\n",
    "## Mini-checkpoint (prove you learned it)\n",
    "\n",
    "Answer these:\n",
    "\n",
    "1. **Draw the DAG by hand**: 6 boxes for tasks, arrows for dependencies. Explain why no arrows go backward.\n",
    "2. **What is idempotency and why does prepare_clean() check if output exists before writing?**\n",
    "3. **If validate_clean() is skipped, what could go wrong?** Give a concrete example.\n",
    "4. **If horizon=24 is changed to horizon=48 in CLI, which tasks re-run and which are cached?**\n",
    "\n",
    "**Answers:**\n",
    "1. Boxes: [ingest] → [prepare] → [validate] → [train] → [register] → [forecast]. No backward arrows because time flows forward; we can't validate before ingesting.\n",
    "2. Idempotency means re-running with same inputs yields same outputs (no hidden state). prepare_clean() checks if output exists; if so, skips work (saves time). This requires no side effects (pure function style).\n",
    "3. Without validation, bad data (duplicates, missing hours) enters training. Model learns on leaky/corrupt series → poor forecasts. In production, invalid data gets a bad model deployed.\n",
    "4. Only train_backtest_select() and forecast_publish() re-run (they depend on horizon). ingest, prepare, validate use same data regardless of horizon, so cached files are reused.\n",
    "\n",
    "## Exercises (optional, but recommended)\n",
    "\n",
    "### Easy\n",
    "1. Run the pipeline with a 1-month date range and measure total execution time. Then run with 3-month range. Does time scale linearly with data size?\n",
    "2. Manually delete `artifacts/leaderboard.parquet` and re-run with `--overwrite=false`. Does the pipeline detect missing file and re-train?\n",
    "\n",
    "### Medium\n",
    "1. Modify the CLI to accept a `--model-list` argument (e.g., `--model-list AutoARIMA,HoltWinters`) and pass it to train_backtest_select(). Test with different model combos.\n",
    "2. Add a new task `save_metrics_csv()` that converts artifacts/leaderboard.parquet to CSV. Update the DAG to place it after forecast_publish().\n",
    "\n",
    "### Hard\n",
    "1. Implement `--check-only` flag that runs through ingest/prepare/validate but skips training and forecasting. Useful for data validation without model cost.\n",
    "2. Modify the pipeline to run for 5 different respondents (e.g., NG_CA1, NG_TX, NG_US48, etc.) in parallel within one `run` call. Track run_id per respondent to avoid collisions.\n",
    "3. Deploy to Airflow and set up a task retry policy: if a task fails, retry 3 times with 5-minute delays. Test by injecting a temporary API error and verifying retry behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ce5994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/chapter3/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter3/config.py\n",
    "# file: src/chapter3/config.py\n",
    "\"\"\"\n",
    "Chapter 3: Pipeline Configuration\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PipelineConfig:\n",
    "    # Data parameters\n",
    "    start_date: str = \"2024-01-01\"\n",
    "    end_date: str = \"2024-12-31\"\n",
    "    respondent: str = \"US48\"\n",
    "    fueltype: str = \"NG\"\n",
    "\n",
    "    # IO\n",
    "    data_dir: str = \"data\"\n",
    "    artifacts_dir: str = \"artifacts\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    # Forecasting / backtest\n",
    "    horizon: int = 24\n",
    "    n_windows: int = 5\n",
    "    step_size: int = 168\n",
    "    confidence_level: int = 95\n",
    "\n",
    "    # Scheduling (Airflow)\n",
    "    schedule: str = \"0 6 * * *\"\n",
    "    retries: int = 3\n",
    "    retry_delay_minutes: int = 5\n",
    "\n",
    "    # MLflow\n",
    "    experiment_name: str = \"eia_forecasting\"\n",
    "    model_alias: str = \"champion\"\n",
    "\n",
    "    def run_id(self) -> str:\n",
    "        return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def data_path(self) -> Path:\n",
    "        return Path(self.data_dir)\n",
    "\n",
    "    def artifacts_path(self) -> Path:\n",
    "        return Path(self.artifacts_dir)\n",
    "\n",
    "    def raw_path(self) -> Path:\n",
    "        return self.data_path() / \"raw.parquet\"\n",
    "\n",
    "    def clean_path(self) -> Path:\n",
    "        return self.data_path() / \"clean.parquet\"\n",
    "\n",
    "    def metadata_path(self) -> Path:\n",
    "        return self.data_path() / \"metadata.json\"\n",
    "\n",
    "    def leaderboard_path(self) -> Path:\n",
    "        return self.artifacts_path() / \"leaderboard.parquet\"\n",
    "\n",
    "    def cv_results_path(self) -> Path:\n",
    "        return self.artifacts_path() / \"cv_results.parquet\"\n",
    "\n",
    "    def predictions_path(self) -> Path:\n",
    "        return self.artifacts_path() / \"predictions.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6323c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/chapter3/io_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter3/io_utils.py\n",
    "# file: src/chapter3/io_utils.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def atomic_write_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Atomic parquet write: write to temp in same directory, then replace.\n",
    "    \"\"\"\n",
    "    ensure_dir(path.parent)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n",
    "def atomic_write_json(payload: Dict[str, Any], path: Path) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, default=str)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/chapter3/tasks.py\n",
    "# file: src/chapter3/tasks.py\n",
    "\"\"\"\n",
    "Chapter 3: Idempotent Pipeline Tasks\n",
    "\n",
    "These tasks are designed to be:\n",
    "- deterministic for a given config (historical pulls)\n",
    "- atomic on write\n",
    "- safe to rerun (overwrite flag controls)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.chapter3.config import PipelineConfig\n",
    "from src.chapter3.io_utils import atomic_write_json, atomic_write_parquet, ensure_dir\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _import_fetcher():\n",
    "    \"\"\"\n",
    "    Supports either:\n",
    "      - eia_data_simple.py at repo root\n",
    "      - or src/chapter1/eia_data_simple.py (if you later move it)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from eia_data_simple import EIADataFetcher, ExperimentConfig  # type: ignore\n",
    "        return EIADataFetcher, ExperimentConfig\n",
    "    except Exception:\n",
    "        from ..chapter1.eia_data_simple import EIADataFetcher, ExperimentConfig  # type: ignore\n",
    "        return EIADataFetcher, ExperimentConfig\n",
    "\n",
    "\n",
    "def _require_api_key() -> str:\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise EnvironmentError(\n",
    "            \"EIA_API_KEY is missing. Add it to your environment or .env file.\"\n",
    "        )\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def compute_time_series_integrity(df_forecast: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Computes the same core invariants as your fetcher’s integrity method,\n",
    "    but returns a dict reliably (your current method prints but doesn’t return).\n",
    "    Requires columns: unique_id, ds, y\n",
    "    \"\"\"\n",
    "    if not {\"unique_id\", \"ds\", \"y\"}.issubset(df_forecast.columns):\n",
    "        raise ValueError(f\"Expected unique_id/ds/y, got {df_forecast.columns.tolist()}\")\n",
    "\n",
    "    df = df_forecast.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # duplicates on (unique_id, ds)\n",
    "    dup_counts = df.groupby([\"unique_id\", \"ds\"]).size()\n",
    "    duplicate_pairs = int((dup_counts > 1).sum())\n",
    "\n",
    "    missing_hours = 0\n",
    "    longest_gap_hours = 0.0\n",
    "    gaps_detail = []\n",
    "\n",
    "    for uid in df[\"unique_id\"].unique():\n",
    "        sub = df[df[\"unique_id\"] == uid].sort_values(\"ds\").reset_index(drop=True)\n",
    "        diffs = sub[\"ds\"].diff()\n",
    "        expected = pd.Timedelta(hours=1)\n",
    "\n",
    "        # gaps > 1 hour\n",
    "        miss_mask = diffs > expected\n",
    "        missing_hours += int(miss_mask.sum())\n",
    "\n",
    "        if len(diffs) > 0 and diffs.notna().any():\n",
    "            max_gap = diffs.max()\n",
    "            if pd.notna(max_gap):\n",
    "                gap_h = max_gap.total_seconds() / 3600\n",
    "                longest_gap_hours = max(longest_gap_hours, gap_h)\n",
    "\n",
    "        if miss_mask.any():\n",
    "            idxs = sub.index[miss_mask].tolist()\n",
    "            for idx in idxs[:10]:\n",
    "                gaps_detail.append(\n",
    "                    {\n",
    "                        \"unique_id\": uid,\n",
    "                        \"before_ds\": sub.loc[idx - 1, \"ds\"],\n",
    "                        \"after_ds\": sub.loc[idx, \"ds\"],\n",
    "                        \"gap_hours\": float((sub.loc[idx, \"ds\"] - sub.loc[idx - 1, \"ds\"]).total_seconds() / 3600),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    status = \"valid\" if (duplicate_pairs == 0 and missing_hours == 0) else \"invalid\"\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"duplicate_pairs\": duplicate_pairs,\n",
    "        \"missing_hours\": int(missing_hours),\n",
    "        \"longest_gap_hours\": float(longest_gap_hours),\n",
    "        \"gaps_detail\": gaps_detail,\n",
    "        \"n_rows\": int(len(df)),\n",
    "    }\n",
    "\n",
    "\n",
    "def ingest_eia(config: PipelineConfig, run_id: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Task 1: Pull raw data and save data/raw.parquet\n",
    "    \"\"\"\n",
    "    raw_path = config.raw_path()\n",
    "    ensure_dir(raw_path.parent)\n",
    "\n",
    "    if raw_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[ingest] raw exists, skipping: {raw_path}\")\n",
    "        return str(raw_path)\n",
    "\n",
    "    EIADataFetcher, _ = _import_fetcher()\n",
    "    api_key = _require_api_key()\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_raw = fetcher.pull_data(\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "        respondent=config.respondent,\n",
    "        fueltype=config.fueltype,\n",
    "    )\n",
    "\n",
    "    atomic_write_parquet(df_raw, raw_path)\n",
    "    logger.info(f\"[ingest] wrote raw: {raw_path} ({len(df_raw)} rows)\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"ingest\",\n",
    "                raw_rows=len(df_raw)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[ingest] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return str(raw_path)\n",
    "\n",
    "\n",
    "def prepare_clean(raw_path: str, config: PipelineConfig, run_id: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Task 2: Prepare clean dataset and save data/clean.parquet + data/metadata.json\n",
    "    \"\"\"\n",
    "    clean_path = config.clean_path()\n",
    "    ensure_dir(clean_path.parent)\n",
    "\n",
    "    if clean_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[prepare] clean exists, skipping: {clean_path}\")\n",
    "        return str(clean_path)\n",
    "\n",
    "    EIADataFetcher, _ = _import_fetcher()\n",
    "    api_key = os.getenv(\"EIA_API_KEY\", \"dummy\")  # no API call in this step\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_raw = pd.read_parquet(raw_path)\n",
    "    df_clean = fetcher.prepare_data(df_raw, timezone_policy=\"UTC\")\n",
    "\n",
    "    # Keep metadata for Chapter 4 health checks\n",
    "    metadata = {\n",
    "        \"pull_timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"respondent\": config.respondent,\n",
    "        \"fueltype\": config.fueltype,\n",
    "        \"start_date\": config.start_date,\n",
    "        \"end_date\": config.end_date,\n",
    "        \"raw_rows\": int(len(df_raw)),\n",
    "        \"clean_rows\": int(len(df_clean)),\n",
    "    }\n",
    "\n",
    "    atomic_write_parquet(df_clean, clean_path)\n",
    "    atomic_write_json(metadata, config.metadata_path())\n",
    "\n",
    "    logger.info(f\"[prepare] wrote clean: {clean_path} ({len(df_clean)} rows)\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"prepare\",\n",
    "                clean_rows=len(df_clean)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[prepare] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return str(clean_path)\n",
    "\n",
    "\n",
    "def validate_clean(clean_path: str, run_id: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Task 3: Validate time series integrity (duplicates + missing hours).\n",
    "    \"\"\"\n",
    "    EIADataFetcher, _ = _import_fetcher()\n",
    "    api_key = os.getenv(\"EIA_API_KEY\", \"dummy\")\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_clean = pd.read_parquet(clean_path)\n",
    "    df_forecast = fetcher.prepare_for_forecasting(df_clean, unique_id=\"NG_US48\")\n",
    "\n",
    "    report = compute_time_series_integrity(df_forecast)\n",
    "    if report[\"status\"] != \"valid\":\n",
    "        raise ValueError(\n",
    "            f\"Time-series integrity failed: \"\n",
    "            f\"{report['duplicate_pairs']} duplicate (unique_id, ds) pairs; \"\n",
    "            f\"{report['missing_hours']} missing hours; \"\n",
    "            f\"longest gap={report['longest_gap_hours']:.1f}h\"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[validate] OK: {report}\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"validate\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[validate] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def train_backtest_select(clean_path: str, config: PipelineConfig, run_id: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Task 4: Cross-validate and write artifacts/leaderboard.parquet + artifacts/cv_results.parquet\n",
    "    Returns leaderboard.\n",
    "    \"\"\"\n",
    "    leaderboard_path = config.leaderboard_path()\n",
    "    cv_path = config.cv_results_path()\n",
    "    ensure_dir(leaderboard_path.parent)\n",
    "\n",
    "    if leaderboard_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[train] leaderboard exists, skipping: {leaderboard_path}\")\n",
    "        return pd.read_parquet(leaderboard_path)\n",
    "\n",
    "    EIADataFetcher, ExperimentConfig = _import_fetcher()\n",
    "    api_key = os.getenv(\"EIA_API_KEY\", \"dummy\")\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_clean = pd.read_parquet(clean_path)\n",
    "    df_forecast = fetcher.prepare_for_forecasting(df_clean, unique_id=\"NG_US48\")\n",
    "\n",
    "    exp = ExperimentConfig(\n",
    "        name=config.experiment_name,\n",
    "        horizon=config.horizon,\n",
    "        n_windows=config.n_windows,\n",
    "        step_size=config.step_size,\n",
    "        confidence_level=config.confidence_level,\n",
    "    )\n",
    "\n",
    "    cv_results, leaderboard = fetcher.cross_validate(df_forecast, config=exp)\n",
    "\n",
    "    atomic_write_parquet(cv_results, cv_path)\n",
    "    atomic_write_parquet(leaderboard, leaderboard_path)\n",
    "\n",
    "    logger.info(f\"[train] wrote cv: {cv_path} ({len(cv_results)} rows)\")\n",
    "    logger.info(f\"[train] wrote leaderboard: {leaderboard_path} ({len(leaderboard)} rows)\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"train\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[train] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "def register_champion(leaderboard: pd.DataFrame, config: PipelineConfig, clean_path: str, run_id: str = \"\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Task 5: Register best model in MLflow (if available).\n",
    "    \"\"\"\n",
    "    EIADataFetcher, _ = _import_fetcher()\n",
    "    api_key = os.getenv(\"EIA_API_KEY\", \"dummy\")\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_clean = pd.read_parquet(clean_path)\n",
    "    df_forecast = fetcher.prepare_for_forecasting(df_clean, unique_id=\"NG_US48\")\n",
    "\n",
    "    model_uri = fetcher.register_best_model(\n",
    "        leaderboard=leaderboard,\n",
    "        experiment_name=config.experiment_name,\n",
    "        alias=config.model_alias,\n",
    "        train_df=df_forecast,\n",
    "        default_horizon=config.horizon,\n",
    "        freq=\"h\",\n",
    "    )\n",
    "    logger.info(f\"[register] model_uri={model_uri}\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"register\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[register] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return model_uri\n",
    "\n",
    "\n",
    "\n",
    "def forecast_publish(clean_path: str, config: PipelineConfig, run_id: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Task 6: Fit on all clean data and publish artifacts/predictions.parquet\n",
    "    \"\"\"\n",
    "    pred_path = config.predictions_path()\n",
    "    ensure_dir(pred_path.parent)\n",
    "\n",
    "    if pred_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[forecast] predictions exist, skipping: {pred_path}\")\n",
    "        return str(pred_path)\n",
    "\n",
    "    EIADataFetcher, _ = _import_fetcher()\n",
    "    api_key = os.getenv(\"EIA_API_KEY\", \"dummy\")\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "\n",
    "    df_clean = pd.read_parquet(clean_path)\n",
    "    df_train = fetcher.prepare_for_forecasting(df_clean, unique_id=\"NG_US48\")\n",
    "\n",
    "    forecast_df = fetcher.forecast(\n",
    "        train_df=df_train,\n",
    "        horizon=config.horizon,\n",
    "        confidence_level=config.confidence_level,\n",
    "    )\n",
    "\n",
    "    atomic_write_parquet(forecast_df, pred_path)\n",
    "    logger.info(f\"[forecast] wrote predictions: {pred_path} ({len(forecast_df)} rows)\")\n",
    "\n",
    "    # Optional: persist to Chapter 4 monitoring store\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.forecast_store import persist_forecasts\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "\n",
    "            persist_forecasts(\n",
    "                db_path=MonitoringConfig().db_path,\n",
    "                run_id=run_id,\n",
    "                forecast_df=forecast_df,\n",
    "                confidence_level=config.confidence_level,\n",
    "            )\n",
    "            logger.info(f\"[forecast] persisted to Chapter 4 store\")\n",
    "        except ImportError:\n",
    "            logger.debug(\"[forecast] Chapter 4 not available, skipping persist\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[forecast] Chapter 4 persist failed: {e}\")\n",
    "\n",
    "    # Optional: log to Chapter 4 monitoring\n",
    "    if run_id:\n",
    "        try:\n",
    "            from src.chapter4.run_log import log_run\n",
    "            from src.chapter4.config import MonitoringConfig\n",
    "            log_run(\n",
    "                MonitoringConfig().db_path,\n",
    "                run_id,\n",
    "                \"success\",\n",
    "                \"forecast\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"[forecast] Chapter 4 logging failed: {e}\")\n",
    "\n",
    "    return str(pred_path)\n",
    "\n",
    "\n",
    "def run_full_pipeline(config: PipelineConfig) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs tasks in order and returns a summary dict.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"START PIPELINE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    # Compute run_id once and pass through all tasks (for Chapter 4 monitoring)\n",
    "    run_id = config.run_id()\n",
    "    logger.info(f\"Pipeline run_id: {run_id}\")\n",
    "\n",
    "    raw = ingest_eia(config, run_id=run_id)\n",
    "    clean = prepare_clean(raw, config, run_id=run_id)\n",
    "    integrity = validate_clean(clean, run_id=run_id)\n",
    "    leaderboard = train_backtest_select(clean, config, run_id=run_id)\n",
    "    model_uri = register_champion(leaderboard, config, clean, run_id=run_id)\n",
    "    predictions = forecast_publish(clean, config, run_id=run_id)\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"] if len(leaderboard) else None\n",
    "    best_rmse = leaderboard.iloc[0][\"rmse_mean\"] if len(leaderboard) else None\n",
    "\n",
    "    out = {\n",
    "        \"raw_path\": raw,\n",
    "        \"clean_path\": clean,\n",
    "        \"integrity\": integrity,\n",
    "        \"leaderboard_path\": str(config.leaderboard_path()),\n",
    "        \"cv_results_path\": str(config.cv_results_path()),\n",
    "        \"predictions_path\": predictions,\n",
    "        \"best_model\": best_model,\n",
    "        \"best_rmse_mean\": best_rmse,\n",
    "        \"model_uri\": model_uri,\n",
    "        \"run_id\": run_id,\n",
    "    }\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"PIPELINE COMPLETE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2336fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/chapter3/dag_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter3/dag_builder.py\n",
    "# file: src/chapter3/dag_builder.py\n",
    "\"\"\"\n",
    "Chapter 3: DAG Builder\n",
    "\n",
    "- If Airflow is installed: returns a real Airflow DAG\n",
    "- If not: provides a DOT graph string for notebook visualization\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from src.chapter3.config import PipelineConfig\n",
    "\n",
    "try:\n",
    "    from airflow import DAG\n",
    "    from airflow.operators.python import PythonOperator\n",
    "    AIRFLOW_AVAILABLE = True\n",
    "except Exception:\n",
    "    AIRFLOW_AVAILABLE = False\n",
    "    DAG = None\n",
    "    PythonOperator = None\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 3,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "def build_daily_dag(\n",
    "    dag_id: str = \"eia_daily_pipeline\",\n",
    "    schedule: str = \"0 6 * * *\",\n",
    "    start_date: Optional[datetime] = None,\n",
    "    default_args: Optional[Dict[str, Any]] = None,\n",
    "    config: Optional[PipelineConfig] = None,\n",
    ") -> \"DAG\":\n",
    "    if not AIRFLOW_AVAILABLE:\n",
    "        raise ImportError(\"Airflow is not installed. Install apache-airflow to use build_daily_dag().\")\n",
    "\n",
    "    from src.chapter3.tasks import (\n",
    "        ingest_eia,\n",
    "        prepare_clean,\n",
    "        validate_clean,\n",
    "        train_backtest_select,\n",
    "        forecast_publish,\n",
    "        register_champion,\n",
    "    )\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = datetime.utcnow() - timedelta(days=1)\n",
    "    if default_args is None:\n",
    "        default_args = DEFAULT_ARGS.copy()\n",
    "    if config is None:\n",
    "        config = PipelineConfig()\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=\"EIA Forecasting Pipeline\",\n",
    "        schedule_interval=schedule,\n",
    "        start_date=start_date,\n",
    "        catchup=False,\n",
    "        tags=[\"eia\", \"forecasting\"],\n",
    "    ) as dag:\n",
    "\n",
    "        t_ingest = PythonOperator(\n",
    "            task_id=\"ingest\",\n",
    "            python_callable=lambda: ingest_eia(config),\n",
    "        )\n",
    "\n",
    "        t_prepare = PythonOperator(\n",
    "            task_id=\"prepare\",\n",
    "            python_callable=lambda: prepare_clean(str(config.raw_path()), config),\n",
    "        )\n",
    "\n",
    "        t_validate = PythonOperator(\n",
    "            task_id=\"validate\",\n",
    "            python_callable=lambda: validate_clean(str(config.clean_path())),\n",
    "        )\n",
    "\n",
    "        def _train():\n",
    "            lb = train_backtest_select(str(config.clean_path()), config)\n",
    "            # returning a DataFrame is not ideal for XCom; write path is the artifact\n",
    "            return str(config.leaderboard_path())\n",
    "\n",
    "        t_train = PythonOperator(task_id=\"train\", python_callable=_train)\n",
    "\n",
    "        def _register():\n",
    "            import pandas as pd\n",
    "            lb = pd.read_parquet(config.leaderboard_path())\n",
    "            return register_champion(lb, config, str(config.clean_path()))\n",
    "\n",
    "\n",
    "        t_register = PythonOperator(task_id=\"register\", python_callable=_register)\n",
    "\n",
    "        t_forecast = PythonOperator(\n",
    "            task_id=\"forecast\",\n",
    "            python_callable=lambda: forecast_publish(str(config.clean_path()), config),\n",
    "        )\n",
    "\n",
    "        t_ingest >> t_prepare >> t_validate >> t_train >> t_register >> t_forecast\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def build_dag_dot() -> str:\n",
    "    \"\"\"\n",
    "    DOT graph fallback (works without Airflow) for notebook visualization.\n",
    "    \"\"\"\n",
    "    return \"\"\"digraph EIA_PIPELINE {\n",
    "  rankdir=LR;\n",
    "  node [shape=box, style=\"rounded,filled\", fillcolor=\"#eef2ff\"];\n",
    "\n",
    "  ingest -> prepare -> validate -> train -> register -> forecast;\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dbb144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 15:38:37,512 | INFO | ============================================================\n",
      "2026-01-10 15:38:37,513 | INFO | START PIPELINE\n",
      "2026-01-10 15:38:37,513 | INFO | ============================================================\n",
      "2026-01-10 15:38:37,514 | INFO | [ingest] raw exists, skipping: data\\raw.parquet\n",
      "2026-01-10 15:38:37,515 | INFO | [prepare] clean exists, skipping: data\\clean.parquet\n",
      "2026-01-10 15:38:37,516 | INFO | Fetcher initialized (API key length: 40)\n",
      "2026-01-10 15:38:37,521 | INFO | Prepared 8761 records for forecasting with unique_id=NG_US48\n",
      "2026-01-10 15:38:37,525 | INFO | [validate] OK: {'status': 'valid', 'duplicate_pairs': 0, 'missing_hours': 0, 'longest_gap_hours': 1.0, 'gaps_detail': [], 'n_rows': 8761}\n",
      "2026-01-10 15:38:37,525 | INFO | [train] leaderboard exists, skipping: artifacts\\leaderboard.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetcher initialized (API key length: 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 15:38:37,528 | INFO | Fetcher initialized (API key length: 40)\n",
      "2026-01-10 15:38:37,532 | INFO | Prepared 8761 records for forecasting with unique_id=NG_US48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetcher initialized (API key length: 40)\n",
      "\n",
      "============================================================\n",
      "MODEL REGISTRY\n",
      "============================================================\n",
      "  Best model: MSTL\n",
      "  RMSE: 10311\n",
      "  Registering with alias: champion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\docker_projects\\atsaf\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning:\n",
      "\n",
      "Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "\n",
      "2026/01/10 15:38:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/10 15:38:38 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "2026/01/10 15:38:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae48a4eeb07441295b727ed504a1c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'eia_forecasting_MSTL' already exists. Creating a new version of this model...\n",
      "2026/01/10 15:38:38 WARNING mlflow.tracking._model_registry.fluent: Run with id cada044807444ef6b5a0b1ad5062e468 has no artifacts at artifact path 'model', registering model based on models:/m-b30590d542e342b5a9db4ba41aee4088 instead\n",
      "Created version '4' of model 'eia_forecasting_MSTL'.\n",
      "2026-01-10 15:38:38,666 | INFO | Model registered: eia_forecasting_MSTL v4 with alias champion\n",
      "2026-01-10 15:38:38,674 | INFO | [register] model_uri=eia_forecasting_MSTL@champion\n",
      "2026-01-10 15:38:38,675 | INFO | [forecast] predictions exist, skipping: artifacts\\predictions.parquet\n",
      "2026-01-10 15:38:38,676 | INFO | ============================================================\n",
      "2026-01-10 15:38:38,676 | INFO | PIPELINE COMPLETE\n",
      "2026-01-10 15:38:38,677 | INFO | ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Registered: eia_forecasting_MSTL v4\n",
      "  [OK] Alias 'champion' assigned to v4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                 Pipeline Results                                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key              </span>┃<span style=\"font-weight: bold\"> Value                                                                                        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> raw_path         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> data\\raw.parquet                                                                             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> clean_path       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> data\\clean.parquet                                                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> integrity        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> {'status': 'valid', 'duplicate_pairs': 0, 'missing_hours': 0, 'longest_gap_hours': 1.0,      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">                  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 'gaps_detail': [], 'n_rows': 8761}                                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> leaderboard_path </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> artifacts\\leaderboard.parquet                                                                </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> cv_results_path  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> artifacts\\cv_results.parquet                                                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> predictions_path </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> artifacts\\predictions.parquet                                                                </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> best_model       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> MSTL                                                                                         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> best_rmse_mean   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 10311.3                                                                                      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> model_uri        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> eia_forecasting_MSTL@champion                                                                </span>│\n",
       "└──────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                 Pipeline Results                                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                                                                       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mraw_path        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mdata\\raw.parquet                                                                            \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mclean_path      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mdata\\clean.parquet                                                                          \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mintegrity       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m{'status': 'valid', 'duplicate_pairs': 0, 'missing_hours': 0, 'longest_gap_hours': 1.0,     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m                  \u001b[0m│\u001b[32m \u001b[0m\u001b[32m'gaps_detail': [], 'n_rows': 8761}                                                          \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mleaderboard_path\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32martifacts\\leaderboard.parquet                                                               \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mcv_results_path \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32martifacts\\cv_results.parquet                                                                \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mpredictions_path\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32martifacts\\predictions.parquet                                                               \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mbest_model      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mMSTL                                                                                        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mbest_rmse_mean  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m10311.3                                                                                     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mmodel_uri       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32meia_forecasting_MSTL@champion                                                               \u001b[0m\u001b[32m \u001b[0m│\n",
       "└──────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%writefile src/chapter3/cli.py\n",
    "# file: src/chapter3/cli.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import typer\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "from src.chapter3.config import PipelineConfig\n",
    "from src.chapter3.tasks import run_full_pipeline\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "app = typer.Typer(add_completion=False)\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def _strip_ipykernel_args(argv: list[str]) -> list[str]:\n",
    "    \"\"\"Jupyter/ipykernel injects `-f <connection_file>` into sys.argv.\"\"\"\n",
    "    out = [argv[0]]\n",
    "    i = 1\n",
    "    while i < len(argv):\n",
    "        a = argv[i]\n",
    "        if a in (\"-f\", \"--f\"):\n",
    "            i += 2  # skip flag + value\n",
    "            continue\n",
    "        if a.startswith(\"--f=\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        out.append(a)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "@app.command()\n",
    "def run(\n",
    "    start_date: str = \"2024-01-01\",\n",
    "    end_date: str = \"2024-12-31\",\n",
    "    horizon: int = 24,\n",
    "    respondent: str = \"US48\",\n",
    "    fueltype: str = \"NG\",\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    cfg = PipelineConfig(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        horizon=horizon,\n",
    "        respondent=respondent,\n",
    "        fueltype=fueltype,\n",
    "        overwrite=overwrite,\n",
    "    )\n",
    "\n",
    "    results = run_full_pipeline(cfg)\n",
    "\n",
    "    table = Table(title=\"Pipeline Results\")\n",
    "    table.add_column(\"Key\", style=\"cyan\")\n",
    "    table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "    for k, v in results.items():\n",
    "        table.add_row(str(k), str(v))\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv = _strip_ipykernel_args(sys.argv)\n",
    "    app(standalone_mode=False)  # <-- prevents SystemExit in Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a39fd",
   "metadata": {},
   "source": [
    "# Chapter 4 — Monitoring, Drift Detection & Alerts\n",
    "\n",
    "## Outcomes (what I can do after this)\n",
    "\n",
    "- [ ] I can persist forecasts and actuals into a queryable database\n",
    "- [ ] I can compute rolling accuracy metrics and detect model drift\n",
    "- [ ] I can set alert thresholds based on backtest performance\n",
    "- [ ] I can run health checks (freshness, completeness, forecast staleness)\n",
    "- [ ] I can interpret drift reports and decide when to retrain\n",
    "\n",
    "## Concepts (plain English)\n",
    "\n",
    "- **Forecast persistence**: Store predictions in a time-series database for later scoring\n",
    "- **Actuals**: Real observed values that come in after the forecast horizon (e.g., 24 hours later)\n",
    "- **Scoring**: Comparing predictions vs actuals using metrics (RMSE, MAPE, coverage)\n",
    "- **Drift**: Model performance degrades over time (e.g., MAPE increases from 5% to 8%)\n",
    "- **Restatement**: Re-forecasting recent periods as new actuals arrive (e.g., rescore last 7 days hourly)\n",
    "- **Health check**: Monitoring data freshness, completeness, and forecast staleness\n",
    "- **Alert threshold**: Metric value that triggers an alert (e.g., MAPE > 10% is critical)\n",
    "- **Rolling window**: Computing metrics over a sliding time window (e.g., last 7 days)\n",
    "\n",
    "## Architecture (what we're building)\n",
    "\n",
    "### Inputs\n",
    "- **Forecasts table** (from Chapter 3):\n",
    "  - Columns: run_id, created_ts_utc, unique_id, ds, model, yhat, lo, hi\n",
    "  - One row per (model, unique_id, hour)\n",
    "\n",
    "- **Actuals** (append-only stream):\n",
    "  - Same schema as forecasts, but marked as observed\n",
    "  - Arrives continuously (e.g., hourly updates from Chapter 1 data fetch)\n",
    "\n",
    "- **Backtest metrics** (from Chapter 2):\n",
    "  - Historical RMSE, MAPE, coverage from cross-validation\n",
    "  - Used to compute alert thresholds (mean ± k*std)\n",
    "\n",
    "### Outputs\n",
    "- **Metrics database** (SQLite):\n",
    "  - `pipeline_runs`: Execution log (when pipeline ran, status, rows processed)\n",
    "  - `forecasts`: Stored predictions (queryable by model, date range, unique_id)\n",
    "  - `forecast_scores`: Rolling metrics (RMSE/MAPE/coverage per horizon)\n",
    "  - `alerts`: Alert events (triggered when threshold breached)\n",
    "\n",
    "- **Health report**:\n",
    "  - Data freshness: Hours since last ingest\n",
    "  - Data completeness: Missing hours in recent window\n",
    "  - Forecast freshness: Hours since predictions generated\n",
    "\n",
    "### Invariants (must always hold)\n",
    "- Forecast stored once, scored multiple times (as actuals arrive)\n",
    "- Alerts are immutable (never deleted, only logged)\n",
    "- Health checks don't require actuals (can run in real-time, before scoring)\n",
    "- No missing primary keys: (run_id, model, unique_id, ds) uniquely identifies a forecast\n",
    "\n",
    "### Failure modes\n",
    "- Forecast not yet scored: yhat exists but no metrics (actuals haven't arrived yet)\n",
    "- Actuals missing entirely: Can't score forecasts → alert STALE_FORECAST\n",
    "- Data drift (schema changed): New columns appear; scoring fails → alert PIPELINE_FAILURE\n",
    "- Threshold too sensitive: Generates too many alerts → tune k (std multiplier)\n",
    "\n",
    "## Files touched\n",
    "\n",
    "- **`src/chapter4/db.py`** (279 lines)\n",
    "  - `init_monitoring_db()`: Creates SQLite schema (pipeline_runs, forecasts, forecast_scores, alerts)\n",
    "  - `MonitoringDB` class: Read/write interface to monitoring database\n",
    "\n",
    "- **`src/chapter4/forecast_store.py`** (code present but not in line count)\n",
    "  - `persist_forecasts()`: Unpivots wide forecast table → long table → writes to forecasts table\n",
    "\n",
    "- **`src/chapter4/scoring.py`** (code present)\n",
    "  - `score_forecasts()`: Join forecasts with actuals, compute RMSE/MAPE/coverage per horizon\n",
    "\n",
    "- **`src/chapter4/drift.py`** (code present)\n",
    "  - `compute_drift_threshold_from_backtest()`: Threshold = best_metric + k*std\n",
    "  - `rolling_accuracy()`: Query historical metrics for a model/series/horizon\n",
    "  - `detect_drift()`: Compare latest metric to threshold, return drift/ok/no_data\n",
    "  - `write_alert()`: Persist alert to database\n",
    "\n",
    "- **`src/chapter4/health.py`** (code present)\n",
    "  - `check_freshness()`: Hours since last ingest\n",
    "  - `check_completeness()`: Missing hours in recent data\n",
    "  - `check_forecast_freshness()`: Hours since predictions generated\n",
    "  - `full_health_check()`: Combined report\n",
    "\n",
    "- **`src/chapter4/alerts.py`** (279 lines)\n",
    "  - `AlertSeverity` enum: INFO, WARNING, CRITICAL\n",
    "  - `AlertType` enum: STALE_DATA, MISSING_DATA, STALE_FORECAST, DATA_DRIFT, MODEL_DRIFT, PIPELINE_FAILURE\n",
    "  - `AlertConfig`: Thresholds for data freshness, completeness, forecast age, drift severity\n",
    "  - `check_alerts()`: Evaluate health + drift against thresholds\n",
    "  - `send_alert()`: Route to log/email/slack (currently log-only)\n",
    "\n",
    "- **`src/chapter4/run_log.py`** (code present)\n",
    "  - `log_run()`: Log pipeline execution (start time, status, row counts, duration)\n",
    "\n",
    "- **`src/chapter4/config.py`** (21 lines)\n",
    "  - Configuration for alert thresholds and monitoring\n",
    "\n",
    "## Step-by-step walkthrough\n",
    "\n",
    "### 1) Initialize monitoring database\n",
    "```python\n",
    "from src.chapter4.db import init_monitoring_db\n",
    "\n",
    "db_path = \"monitoring.db\"\n",
    "init_monitoring_db(db_path)\n",
    "\n",
    "# Verify schema created\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "tables = cursor.fetchall()\n",
    "print(f\"Tables: {[t[0] for t in tables]}\")\n",
    "```\n",
    "- **Expect**: 4 tables created: `[pipeline_runs, forecasts, forecast_scores, alerts]`\n",
    "- **If it fails**: Check that db_path is writable and SQLite is installed\n",
    "\n",
    "### 2) Persist forecasts from Chapter 3\n",
    "```python\n",
    "from src.chapter4.forecast_store import persist_forecasts\n",
    "import pandas as pd\n",
    "\n",
    "# Load forecast from Chapter 3 (wide format)\n",
    "forecasts_wide = pd.read_parquet(\"artifacts/predictions.parquet\")\n",
    "print(f\"Wide shape: {forecasts_wide.shape}, Columns: {forecasts_wide.columns.tolist()}\")\n",
    "\n",
    "# Persist to monitoring DB (converts to long format)\n",
    "persist_forecasts(\n",
    "    db_path=\"monitoring.db\",\n",
    "    forecasts_wide=forecasts_wide,\n",
    "    run_id=\"run_20240101_060000\",\n",
    "    model_name=\"AutoARIMA\"\n",
    ")\n",
    "\n",
    "# Verify in DB\n",
    "conn = sqlite3.connect(\"monitoring.db\")\n",
    "df_stored = pd.read_sql_query(\n",
    "    \"SELECT * FROM forecasts LIMIT 5\",\n",
    "    conn\n",
    ")\n",
    "print(f\"\\nStored forecasts:\\n{df_stored}\")\n",
    "```\n",
    "- **Expect**: 5 rows with columns [run_id, created_ts_utc, unique_id, ds, model, yhat, lo, hi]\n",
    "- **If it fails**: forecasts_wide may have wrong schema; check expected wide columns (model name, model-lo-95, model-hi-95)\n",
    "\n",
    "### 3) Check data freshness and completeness\n",
    "```python\n",
    "from src.chapter4.health import full_health_check\n",
    "\n",
    "report = full_health_check(\n",
    "    clean_data_path=\"data/clean.parquet\",\n",
    "    predictions_path=\"artifacts/predictions.parquet\",\n",
    "    now=pd.Timestamp.utcnow()\n",
    ")\n",
    "\n",
    "print(f\"Health Report:\")\n",
    "print(f\"  Data freshness: {report['data_freshness_hours']:.1f} hours old\")\n",
    "print(f\"  Data completeness: {report['missing_hours']:.0f} missing hours\")\n",
    "print(f\"  Forecast freshness: {report['forecast_freshness_hours']:.1f} hours old\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - data_freshness_hours ≈ 0 (just ingested)\n",
    "  - missing_hours = 0 (complete data)\n",
    "  - forecast_freshness_hours ≈ 0 (just generated)\n",
    "- **If it fails**: Check that clean_data_path and predictions_path exist\n",
    "\n",
    "### 4) Compute drift threshold from backtest\n",
    "```python\n",
    "from src.chapter4.drift import compute_drift_threshold_from_backtest\n",
    "import pandas as pd\n",
    "\n",
    "# Load backtest leaderboard from Chapter 2\n",
    "leaderboard = pd.read_parquet(\"artifacts/leaderboard.parquet\")\n",
    "champion_model = leaderboard.iloc[0]['model']\n",
    "backtest_rmse_mean = leaderboard.iloc[0]['rmse_mean']\n",
    "backtest_rmse_std = leaderboard.iloc[0]['rmse_std']\n",
    "\n",
    "# Threshold = mean + 2*std (configurable k)\n",
    "threshold = compute_drift_threshold_from_backtest(\n",
    "    metric_mean=backtest_rmse_mean,\n",
    "    metric_std=backtest_rmse_std,\n",
    "    k=2.0,  # Standard deviations above mean\n",
    "    metric_name=\"RMSE\"\n",
    ")\n",
    "\n",
    "print(f\"Backtest RMSE: {backtest_rmse_mean:.2f} ± {backtest_rmse_std:.2f}\")\n",
    "print(f\"Drift threshold (RMSE > {threshold:.2f}): Alert triggered\")\n",
    "```\n",
    "- **Expect**: threshold ≈ backtest_rmse_mean + 2*backtest_rmse_std\n",
    "- **If it fails**: leaderboard may not have expected columns (rmse_mean, rmse_std)\n",
    "\n",
    "### 5) Score forecasts vs actuals\n",
    "```python\n",
    "from src.chapter4.scoring import score_forecasts\n",
    "import pandas as pd\n",
    "\n",
    "# Load stored forecasts\n",
    "conn = sqlite3.connect(\"monitoring.db\")\n",
    "forecasts_df = pd.read_sql_query(\n",
    "    \"SELECT * FROM forecasts WHERE run_id='run_20240101_060000'\",\n",
    "    conn\n",
    ")\n",
    "\n",
    "# Load actuals (same schema as forecasts, but y_actual instead of yhat)\n",
    "# For now, use clean data as \"actuals\" (in real deployment, actuals come from live ingest)\n",
    "actuals_df = pd.read_parquet(\"data/clean.parquet\")\n",
    "actuals_df.columns = ['unique_id', 'ds', 'y_actual']\n",
    "\n",
    "# Score (compute metrics)\n",
    "scores = score_forecasts(\n",
    "    forecasts=forecasts_df,\n",
    "    actuals=actuals_df,\n",
    "    horizon_hours=24\n",
    ")\n",
    "\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"Columns: {scores.columns.tolist()}\")\n",
    "print(f\"Sample:\\n{scores.head()}\")\n",
    "```\n",
    "- **Expect**: Columns [run_id, model, unique_id, horizon_hours, rmse, mape, coverage_pct, valid_rows]\n",
    "- **If it fails**: actuals_df schema may not match; check column names\n",
    "\n",
    "### 6) Detect drift\n",
    "```python\n",
    "from src.chapter4.drift import detect_drift\n",
    "\n",
    "# Get rolling metrics for the model over last 7 days\n",
    "rolling_metrics = rolling_accuracy(\n",
    "    db_path=\"monitoring.db\",\n",
    "    model_name=\"AutoARIMA\",\n",
    "    unique_id=\"NG_US48\",\n",
    "    horizon_hours=24,\n",
    "    window_days=7\n",
    ")\n",
    "\n",
    "# Latest metric (most recent day)\n",
    "if len(rolling_metrics) > 0:\n",
    "    latest_rmse = rolling_metrics.iloc[-1]['rmse']\n",
    "    threshold = 12.5  # From step 4\n",
    "\n",
    "    status = detect_drift(\n",
    "        latest_metric=latest_rmse,\n",
    "        threshold=threshold,\n",
    "        metric_name=\"RMSE\"\n",
    "    )\n",
    "\n",
    "    print(f\"Latest RMSE: {latest_rmse:.2f}\")\n",
    "    print(f\"Threshold: {threshold:.2f}\")\n",
    "    print(f\"Status: {status}\")\n",
    "else:\n",
    "    print(\"No historical metrics; skipping drift check\")\n",
    "```\n",
    "- **Expect**: status = \"ok\" (if latest_rmse < threshold) or \"drift\" (if latest_rmse > threshold)\n",
    "- **If it fails**: rolling_metrics may be empty (no historical data yet); this is normal on first run\n",
    "\n",
    "### 7) Run health checks and trigger alerts\n",
    "```python\n",
    "from src.chapter4.alerts import check_alerts, AlertConfig\n",
    "from src.chapter4.db import MonitoringDB\n",
    "\n",
    "# Set alert config\n",
    "config = AlertConfig(\n",
    "    data_freshness_warning_hours=4,\n",
    "    data_freshness_critical_hours=6,\n",
    "    missing_data_warning_hours=1,\n",
    "    missing_data_critical_hours=3,\n",
    "    forecast_freshness_warning_hours=12,\n",
    "    forecast_freshness_critical_hours=24,\n",
    "    drift_warning_threshold=0.1,      # 10% increase\n",
    "    drift_critical_threshold=0.3,     # 30% increase\n",
    "    rmse_increase_warning_pct=15,\n",
    "    rmse_increase_critical_pct=25\n",
    ")\n",
    "\n",
    "# Run checks\n",
    "health = full_health_check(...)\n",
    "db = MonitoringDB(\"monitoring.db\")\n",
    "\n",
    "alerts = check_alerts(\n",
    "    health_report=health,\n",
    "    config=config,\n",
    "    db=db,\n",
    "    model_name=\"AutoARIMA\"\n",
    ")\n",
    "\n",
    "print(f\"Alerts triggered: {len(alerts)}\")\n",
    "for alert in alerts:\n",
    "    print(f\"  [{alert.severity}] {alert.alert_type}: {alert.message}\")\n",
    "```\n",
    "- **Expect**: No alerts if system is healthy\n",
    "- **If it fails**: Check health report values (freshness, completeness) against thresholds\n",
    "\n",
    "### 8) Query historical metrics\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"monitoring.db\")\n",
    "\n",
    "# Leaderboard: average metrics per model (across all dates/unique_ids)\n",
    "leaderboard = pd.read_sql_query(\"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        ROUND(AVG(rmse), 2) as avg_rmse,\n",
    "        ROUND(AVG(mape), 2) as avg_mape,\n",
    "        ROUND(AVG(coverage_pct), 1) as avg_coverage,\n",
    "        COUNT(*) as n_scores\n",
    "    FROM forecast_scores\n",
    "    GROUP BY model\n",
    "    ORDER BY avg_rmse\n",
    "\"\"\", conn)\n",
    "print(f\"Leaderboard:\\n{leaderboard}\")\n",
    "\n",
    "# Time-series of RMSE for champion model\n",
    "rmse_over_time = pd.read_sql_query(\"\"\"\n",
    "    SELECT\n",
    "        DATE(scored_ts_utc) as date,\n",
    "        ROUND(AVG(rmse), 2) as daily_rmse\n",
    "    FROM forecast_scores\n",
    "    WHERE model = 'AutoARIMA'\n",
    "    GROUP BY DATE(scored_ts_utc)\n",
    "    ORDER BY date DESC\n",
    "    LIMIT 14\n",
    "\"\"\", conn)\n",
    "print(f\"\\nRMSE over last 14 days:\\n{rmse_over_time}\")\n",
    "\n",
    "# Alerts log\n",
    "alerts = pd.read_sql_query(\"\"\"\n",
    "    SELECT alert_ts_utc, alert_type, severity, message\n",
    "    FROM alerts\n",
    "    ORDER BY alert_ts_utc DESC\n",
    "    LIMIT 10\n",
    "\"\"\", conn)\n",
    "print(f\"\\nRecent alerts:\\n{alerts}\")\n",
    "```\n",
    "- **Expect**:\n",
    "  - Leaderboard sorted by avg_rmse (champion first)\n",
    "  - RMSE values stable or gradually increasing (if drifting)\n",
    "  - Alerts only if thresholds breached\n",
    "- **If it fails**: No data in tables (need to run earlier steps first)\n",
    "\n",
    "## Metrics & success criteria\n",
    "\n",
    "### Primary metric\n",
    "- **Drift detection latency**: Alert triggered within 1 hour of drift occurring\n",
    "\n",
    "### Secondary metrics\n",
    "- **False positive rate**: <5% of alerts are spurious (threshold too sensitive)\n",
    "- **Coverage tracking**: Prediction intervals stay well-calibrated (coverage ≈ 95%)\n",
    "- **Data freshness**: Max age of ingested data < 6 hours\n",
    "\n",
    "### \"Good enough\" threshold\n",
    "- Alert thresholds based on backtest performance (mean ± 2*std)\n",
    "- Health checks run daily with no critical alerts\n",
    "- Forecasts are scored within 24 hours of horizon completion\n",
    "\n",
    "### What would make me retrain / change monitoring\n",
    "- MAPE increases > 50% vs backtest → drift detected → retrain\n",
    "- Coverage < 85% or > 99% → intervals miscalibrated → recalibrate or add conformal prediction\n",
    "- Data freshness > 24 hours → data pipeline broken → investigate ingestion\n",
    "- >10% of forecasts missing actuals → scoring broken → investigate data join\n",
    "\n",
    "## Pitfalls (things that commonly break)\n",
    "\n",
    "1. **No actuals data initially**:\n",
    "   - Forecasts arrive immediately, but actuals take 24+ hours to arrive\n",
    "   - Scoring fails if you try to score today's 24-hour forecast (actuals not yet observed)\n",
    "   - **Fix**: Implement \"future actuals\" step: daily ingest fetches latest actuals and back-scores forecasts\n",
    "\n",
    "2. **Threshold too aggressive**:\n",
    "   - If k=1 (mean + 1*std), threshold is too low → constant false alarms\n",
    "   - If k=3 (mean + 3*std), threshold too high → misses real drift\n",
    "   - **Recommendation**: Start with k=2, tune based on false positive rate after 2 weeks\n",
    "\n",
    "3. **Rolling window includes forecast horizon**:\n",
    "   - If scoring yesterday's forecast with today's data, that's correct (forecast completed)\n",
    "   - If trying to score today's forecast with today's data, that's wrong (horizon incomplete)\n",
    "   - **Fix**: Only score forecasts where horizon_date ≤ today - 1 day\n",
    "\n",
    "4. **Alerts not being sent**:\n",
    "   - Current implementation logs alerts to database; email/Slack are stubs\n",
    "   - If you expect email alerts, they won't come\n",
    "   - **Fix**: Implement send_alert() with real email/Slack integration\n",
    "\n",
    "5. **No baseline for drift detection**:\n",
    "   - First few days of scores are too sparse to compare rolling window\n",
    "   - Drift detection only works after backtest threshold is set\n",
    "   - **Fix**: Seed threshold from backtest leaderboard (not from live scores)\n",
    "\n",
    "6. **Duplicate actuals**:\n",
    "   - If data ingest is rerun for the same date range, actuals may duplicate\n",
    "   - Scoring will count same actual twice → metrics become meaningless\n",
    "   - **Fix**: Use `INSERT OR REPLACE` (upsert) when writing actuals; key on (unique_id, ds)\n",
    "\n",
    "## Mini-checkpoint (prove you learned it)\n",
    "\n",
    "Answer these:\n",
    "\n",
    "1. **Why can't we score a 24-hour forecast immediately after generating it?** When is it safe to score?\n",
    "2. **Explain the drift threshold formula**: threshold = mean + k*std. What does k control?\n",
    "3. **What's the difference between a forecast not being scored yet vs. a forecast drifting?** How does the system tell them apart?\n",
    "4. **If the threshold is threshold = 12.5 RMSE and latest RMSE = 12.3, should an alert trigger?**\n",
    "\n",
    "**Answers:**\n",
    "1. We can't score it immediately because the actual value isn't observed yet (it's 24 hours in the future). Safe to score once the forecast period is complete and actuals are available (next day).\n",
    "2. k=std multiplier. Higher k = wider band (fewer alerts, lower false positive rate). Lower k = tighter band (more alerts, higher false positive rate). k=2 is standard (95% of data under normal distribution).\n",
    "3. Not yet scored: yhat exists, no metric row in forecast_scores (actuals haven't arrived). Drifting: metric row exists, but rmse > threshold. System checks both: if forecast too old without score → STALE_FORECAST alert; if metric exists and rmse > threshold → DRIFT alert.\n",
    "4. No alert. Latest RMSE (12.3) < threshold (12.5), so status=\"ok\". Alert only triggers if latest RMSE > 12.5.\n",
    "\n",
    "## Exercises (optional, but recommended)\n",
    "\n",
    "### Easy\n",
    "1. Run full health check 3 times (at different times). How do freshness values change? When does data become \"critical\"?\n",
    "2. Query the alerts table and count alerts by type and severity. Which type is most common?\n",
    "\n",
    "### Medium\n",
    "1. Manually modify one forecast's yhat value (e.g., multiply by 2) and re-score. How much does RMSE change? Does it trigger a drift alert?\n",
    "2. Set drift threshold very low (k=0.5) and re-run drift detection. How many false alarms do you get?\n",
    "3. Write a SQL query to find the longest period without any forecast scores (data gap). Investigate why.\n",
    "\n",
    "### Hard\n",
    "1. Implement restatement logic: for each day, re-score all forecasts from last 7 days as new actuals arrive. Verify RMSE stabilizes over the 7-day window.\n",
    "2. Build a dashboard query that shows rolling 7-day RMSE for each model. Plot to CSV or JSON for visualization.\n",
    "3. Implement automatic threshold tuning: adjust k based on false positive rate from last 30 days. If >5% alerts are false (later retracted), reduce k; if <2% alerts, increase k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c2569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/config.py\n",
    "# file: src/chapter4/config.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MonitoringConfig:\n",
    "    db_path: str = \"artifacts/monitoring/monitoring.sqlite\"\n",
    "\n",
    "    # Restatement strategy (data can be revised)\n",
    "    restatement_lookback_hours: int = 336  # 2 weeks (matches your notes)\n",
    "\n",
    "    # Drift windows (rolling accuracy)\n",
    "    roll_7d_hours: int = 24 * 7\n",
    "    roll_14d_hours: int = 24 * 14\n",
    "\n",
    "    # Drift threshold policy (data-driven, computed from backtests)\n",
    "    # e.g. threshold = mean + k*std (k chosen)\n",
    "    drift_std_k: float = 2.0\n",
    "\n",
    "    # Alert routing (keep simple at first)\n",
    "    alert_print_only: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90d3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/db.py\n",
    "# file: src/chapter4/db.py\n",
    "from __future__ import annotations\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Tuple\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "def init_db(db_path: str) -> None:\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pipeline_runs (\n",
    "        run_id TEXT PRIMARY KEY,\n",
    "        ts_utc TEXT NOT NULL,\n",
    "        status TEXT NOT NULL,\n",
    "        step TEXT NOT NULL,\n",
    "        message TEXT,\n",
    "        raw_rows INTEGER,\n",
    "        clean_rows INTEGER,\n",
    "        duration_sec REAL\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS forecasts (\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_ts_utc TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        lo REAL,\n",
    "        hi REAL,\n",
    "        PRIMARY KEY (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS forecast_scores (\n",
    "        scored_ts_utc TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mape REAL,\n",
    "        coverage_pct REAL,\n",
    "        valid_rows INTEGER,\n",
    "        PRIMARY KEY (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS alerts (\n",
    "        alert_ts_utc TEXT NOT NULL,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        message TEXT NOT NULL,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3911f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/run_log.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/run_log.py\n",
    "# file: src/chapter4/run_log.py\n",
    "from __future__ import annotations\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional\n",
    "from src.chapter4.db import connect\n",
    "\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def log_run(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    status: str,\n",
    "    step: str,\n",
    "    message: Optional[str] = None,\n",
    "    raw_rows: Optional[int] = None,\n",
    "    clean_rows: Optional[int] = None,\n",
    "    duration_sec: Optional[float] = None,\n",
    ") -> None:\n",
    "    con = connect(db_path)\n",
    "    con.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO pipeline_runs\n",
    "        (run_id, ts_utc, status, step, message, raw_rows, clean_rows, duration_sec)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (run_id, now_utc_iso(), status, step, message, raw_rows, clean_rows, duration_sec))\n",
    "    con.commit()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "211f43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/forecast_store.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/forecast_store.py\n",
    "# file: src/chapter4/forecast_store.py\n",
    "from __future__ import annotations\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from src.chapter4.db import connect\n",
    "\n",
    "def _utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def persist_forecasts(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    forecast_df: pd.DataFrame,\n",
    "    confidence_level: int = 95\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Expects StatsForecast output:\n",
    "      columns include: unique_id, ds, <model1>, <model1>-lo-95, <model1>-hi-95, ...\n",
    "    Writes long format into SQLite.\n",
    "    \"\"\"\n",
    "    required = {\"unique_id\", \"ds\"}\n",
    "    if not required.issubset(forecast_df.columns):\n",
    "        raise ValueError(f\"forecast_df missing {required}, got {forecast_df.columns.tolist()}\")\n",
    "\n",
    "    df = forecast_df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"raise\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Model columns = non-metadata and not interval cols\n",
    "    model_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in (\"unique_id\", \"ds\")\n",
    "        and not c.endswith(f\"-lo-{confidence_level}\")\n",
    "        and not c.endswith(f\"-hi-{confidence_level}\")\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    created_ts = _utc_iso()\n",
    "\n",
    "    for m in model_cols:\n",
    "        lo_col = f\"{m}-lo-{confidence_level}\"\n",
    "        hi_col = f\"{m}-hi-{confidence_level}\"\n",
    "        has_int = (lo_col in df.columns) and (hi_col in df.columns)\n",
    "\n",
    "        tmp = df[[\"unique_id\", \"ds\", m]].rename(columns={m: \"yhat\"})\n",
    "        tmp[\"model\"] = m\n",
    "        tmp[\"lo\"] = df[lo_col] if has_int else None\n",
    "        tmp[\"hi\"] = df[hi_col] if has_int else None\n",
    "        tmp[\"run_id\"] = run_id\n",
    "        tmp[\"created_ts_utc\"] = created_ts\n",
    "\n",
    "        rows.append(tmp)\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    con = connect(db_path)\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO forecasts\n",
    "        (run_id, created_ts_utc, unique_id, ds, model, yhat, lo, hi)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", [\n",
    "        (\n",
    "            r.run_id,\n",
    "            r.created_ts_utc,\n",
    "            r.unique_id,\n",
    "            str(r.ds),\n",
    "            r.model,\n",
    "            None if pd.isna(r.yhat) else float(r.yhat),\n",
    "            None if pd.isna(r.lo) else float(r.lo),\n",
    "            None if pd.isna(r.hi) else float(r.hi),\n",
    "        )\n",
    "        for r in out.itertuples(index=False)\n",
    "    ])\n",
    "    con.commit()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c75cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/scoring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/scoring.py\n",
    "# file: src/chapter4/scoring.py\n",
    "from __future__ import annotations\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.chapter4.db import connect\n",
    "\n",
    "def _utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def score_forecasts(\n",
    "    db_path: str,\n",
    "    actuals_df: pd.DataFrame,\n",
    "    max_horizon_hours: int = 72\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    actuals_df must be statsforecast format: unique_id, ds, y (timezone-naive UTC preferred)\n",
    "\n",
    "    Scores all forecasts in DB that match actuals_df on (unique_id, ds).\n",
    "    Aggregates per (run_id, model, unique_id, horizon_hours).\n",
    "\n",
    "    horizon_hours is computed as ds - forecast_created_ts (rounded to hours).\n",
    "    \"\"\"\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    if not required.issubset(actuals_df.columns):\n",
    "        raise ValueError(f\"actuals_df missing {required}, got {actuals_df.columns.tolist()}\")\n",
    "\n",
    "    act = actuals_df.copy()\n",
    "    act[\"ds\"] = pd.to_datetime(act[\"ds\"], errors=\"raise\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    con = connect(db_path)\n",
    "    fc = pd.read_sql_query(\"SELECT * FROM forecasts\", con)\n",
    "    con.close()\n",
    "\n",
    "    if fc.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fc[\"ds\"] = pd.to_datetime(fc[\"ds\"], errors=\"raise\")\n",
    "    fc[\"created_ts_utc\"] = pd.to_datetime(fc[\"created_ts_utc\"], errors=\"raise\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    merged = fc.merge(act, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "    if merged.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    merged[\"horizon_hours\"] = ((merged[\"ds\"] - merged[\"created_ts_utc\"]).dt.total_seconds() / 3600.0).round().astype(int)\n",
    "    merged = merged[(merged[\"horizon_hours\"] >= 1) & (merged[\"horizon_hours\"] <= max_horizon_hours)]\n",
    "\n",
    "    def rmse(y, yhat):\n",
    "        m = np.isfinite(y) & np.isfinite(yhat)\n",
    "        if m.sum() == 0:\n",
    "            return np.nan, 0\n",
    "        return float(np.sqrt(np.mean((y[m] - yhat[m]) ** 2))), int(m.sum())\n",
    "\n",
    "    def mape(y, yhat):\n",
    "        m = np.isfinite(y) & np.isfinite(yhat) & (np.abs(y) > 1e-12)\n",
    "        if m.sum() == 0:\n",
    "            return np.nan\n",
    "        return float(np.mean(np.abs((y[m] - yhat[m]) / y[m])))\n",
    "\n",
    "    def coverage(y, lo, hi):\n",
    "        m = np.isfinite(y) & np.isfinite(lo) & np.isfinite(hi)\n",
    "        if m.sum() == 0:\n",
    "            return np.nan\n",
    "        return float(100.0 * np.mean((y[m] >= lo[m]) & (y[m] <= hi[m])))\n",
    "\n",
    "    rows = []\n",
    "    for (run_id, model, uid, h), g in merged.groupby([\"run_id\", \"model\", \"unique_id\", \"horizon_hours\"]):\n",
    "        y = g[\"y\"].to_numpy()\n",
    "        yhat = g[\"yhat\"].to_numpy()\n",
    "        lo = g[\"lo\"].to_numpy()\n",
    "        hi = g[\"hi\"].to_numpy()\n",
    "\n",
    "        r, n = rmse(y, yhat)\n",
    "        rows.append({\n",
    "            \"scored_ts_utc\": _utc_iso(),\n",
    "            \"run_id\": run_id,\n",
    "            \"model\": model,\n",
    "            \"unique_id\": uid,\n",
    "            \"horizon_hours\": int(h),\n",
    "            \"rmse\": r,\n",
    "            \"mape\": mape(y, yhat),\n",
    "            \"coverage_pct\": coverage(y, lo, hi),\n",
    "            \"valid_rows\": n,\n",
    "        })\n",
    "\n",
    "    scores = pd.DataFrame(rows)\n",
    "\n",
    "    con = connect(db_path)\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO forecast_scores\n",
    "        (scored_ts_utc, run_id, model, unique_id, horizon_hours, rmse, mape, coverage_pct, valid_rows)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", [\n",
    "        (\n",
    "            r.scored_ts_utc, r.run_id, r.model, r.unique_id, r.horizon_hours,\n",
    "            None if pd.isna(r.rmse) else float(r.rmse),\n",
    "            None if pd.isna(r.mape) else float(r.mape),\n",
    "            None if pd.isna(r.coverage_pct) else float(r.coverage_pct),\n",
    "            int(r.valid_rows),\n",
    "        )\n",
    "        for r in scores.itertuples(index=False)\n",
    "    ])\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54d0af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/chapter4/drift.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/drift.py\n",
    "# file: src/chapter4/drift.py\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from src.chapter4.db import connect\n",
    "\n",
    "def _utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def compute_drift_threshold_from_backtest(\n",
    "    leaderboard_path: str,\n",
    "    metric_col: str = \"mape_mean\",\n",
    "    std_col: str = \"mape_std\",\n",
    "    k: float = 2.0\n",
    ") -> float:\n",
    "    lb = pd.read_parquet(leaderboard_path)\n",
    "    if lb.empty:\n",
    "        raise ValueError(\"Leaderboard is empty; cannot compute drift threshold.\")\n",
    "    best = lb.iloc[0]\n",
    "    if metric_col not in lb.columns or std_col not in lb.columns:\n",
    "        raise ValueError(f\"Expected {metric_col} and {std_col} in leaderboard columns: {lb.columns.tolist()}\")\n",
    "    return float(best[metric_col] + k * best[std_col])\n",
    "\n",
    "def rolling_accuracy(\n",
    "    db_path: str,\n",
    "    model: str,\n",
    "    unique_id: str,\n",
    "    horizon_hours: int = 24\n",
    ") -> pd.DataFrame:\n",
    "    con = connect(db_path)\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "        SELECT scored_ts_utc, run_id, model, unique_id, horizon_hours, rmse, mape, coverage_pct, valid_rows\n",
    "        FROM forecast_scores\n",
    "        WHERE model = ? AND unique_id = ? AND horizon_hours = ?\n",
    "        ORDER BY scored_ts_utc ASC\n",
    "    \"\"\", con, params=(model, unique_id, int(horizon_hours)))\n",
    "    con.close()\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"scored_ts_utc\"] = pd.to_datetime(df[\"scored_ts_utc\"], errors=\"raise\", utc=True)\n",
    "    return df\n",
    "\n",
    "def detect_drift(\n",
    "    db_path: str,\n",
    "    leaderboard_path: str,\n",
    "    model: str,\n",
    "    unique_id: str,\n",
    "    horizon_hours: int,\n",
    "    k: float = 2.0,\n",
    ") -> dict:\n",
    "    thr = compute_drift_threshold_from_backtest(leaderboard_path, k=k)\n",
    "    hist = rolling_accuracy(db_path, model=model, unique_id=unique_id, horizon_hours=horizon_hours)\n",
    "\n",
    "    if hist.empty:\n",
    "        return {\"status\": \"no_data\", \"threshold\": thr}\n",
    "\n",
    "    latest = hist.iloc[-1]\n",
    "    drifted = (pd.notna(latest[\"mape\"]) and float(latest[\"mape\"]) > thr)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"drift\" if drifted else \"ok\",\n",
    "        \"threshold\": float(thr),\n",
    "        \"latest_mape\": None if pd.isna(latest[\"mape\"]) else float(latest[\"mape\"]),\n",
    "        \"latest_rmse\": None if pd.isna(latest[\"rmse\"]) else float(latest[\"rmse\"]),\n",
    "        \"latest_scored_ts\": str(latest[\"scored_ts_utc\"]),\n",
    "        \"model\": model,\n",
    "        \"unique_id\": unique_id,\n",
    "        \"horizon_hours\": int(horizon_hours),\n",
    "    }\n",
    "\n",
    "def write_alert(db_path: str, alert_type: str, severity: str, message: str, metadata: dict) -> None:\n",
    "    con = connect(db_path)\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO alerts (alert_ts_utc, alert_type, severity, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", (_utc_iso(), alert_type, severity, message, json.dumps(metadata)))\n",
    "    con.commit()\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ed02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/chapter4/dashboard_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter4/dashboard_app.py\n",
    "# file: src/chapter4/dashboard_app.py\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from src.chapter4.db import connect\n",
    "\n",
    "st.set_page_config(page_title=\"EIA Forecast Monitoring\", layout=\"wide\")\n",
    "st.title(\"EIA Forecast Monitoring\")\n",
    "\n",
    "db_path = st.sidebar.text_input(\"DB Path\", \"artifacts/monitoring/monitoring.sqlite\")\n",
    "\n",
    "con = connect(db_path)\n",
    "\n",
    "runs = pd.read_sql_query(\"SELECT * FROM pipeline_runs ORDER BY ts_utc DESC LIMIT 100\", con)\n",
    "alerts = pd.read_sql_query(\"SELECT * FROM alerts ORDER BY alert_ts_utc DESC LIMIT 200\", con)\n",
    "scores = pd.read_sql_query(\"SELECT * FROM forecast_scores ORDER BY scored_ts_utc DESC LIMIT 2000\", con)\n",
    "\n",
    "con.close()\n",
    "\n",
    "c1, c2 = st.columns(2)\n",
    "with c1:\n",
    "    st.subheader(\"Recent Pipeline Runs\")\n",
    "    st.dataframe(runs, use_container_width=True)\n",
    "\n",
    "with c2:\n",
    "    st.subheader(\"Alerts\")\n",
    "    st.dataframe(alerts, use_container_width=True)\n",
    "\n",
    "st.subheader(\"Accuracy (Scored Forecasts)\")\n",
    "if not scores.empty:\n",
    "    scores[\"scored_ts_utc\"] = pd.to_datetime(scores[\"scored_ts_utc\"], utc=True)\n",
    "    st.line_chart(\n",
    "        scores.sort_values(\"scored_ts_utc\").set_index(\"scored_ts_utc\")[[\"mape\", \"rmse\"]],\n",
    "        use_container_width=True\n",
    "    )\n",
    "    st.dataframe(scores.head(200), use_container_width=True)\n",
    "else:\n",
    "    st.info(\"No scored forecasts yet. Run the scoring job after actuals arrive.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
