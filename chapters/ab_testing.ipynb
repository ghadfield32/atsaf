{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30aeddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern A/B Testing Framework - Demo\n",
      "\n",
      "================================================================================\n",
      "DEMO 1: Binary Metric (Conversion Rate)\n",
      "================================================================================\n",
      "\n",
      "Control conversion: 10.14%\n",
      "Treatment conversion: 10.14%\n",
      "Absolute lift: 0.00%\n",
      "Relative lift: 0.0%\n",
      "P-value: 0.9993\n",
      "Result: ❌ NOT SIGNIFICANT\n",
      "\n",
      "================================================================================\n",
      "DEMO 2: Continuous Metric with CUPED (Revenue)\n",
      "================================================================================\n",
      "\n",
      "Raw Analysis:\n",
      "  Control mean: $22.16\n",
      "  Treatment mean: $24.10\n",
      "  Difference: $1.94\n",
      "  P-value: 0.0000\n",
      "\n",
      "CUPED-Adjusted Analysis:\n",
      "  Variance reduction: 60.9%\n",
      "  Control mean: $22.16\n",
      "  Treatment mean: $24.10\n",
      "  Difference: $1.94\n",
      "  P-value: 0.0000\n",
      "  Result: ✅ SIGNIFICANT (CUPED)\n",
      "\n",
      "================================================================================\n",
      "DEMO 3: Complete Experiment Readout\n",
      "================================================================================\n",
      "================================================================================\n",
      "EXPERIMENT READOUT: checkout_redesign_v1\n",
      "================================================================================\n",
      "\n",
      "Hypothesis: New checkout flow will increase conversion without hurting revenue\n",
      "Duration: 14 days\n",
      "Analysis timestamp: 2026-01-21T17:41:51.994799\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "SAMPLE SIZE\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Control (A): 4,008\n",
      "Treatment (B): 3,992\n",
      "Total: 8,000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "DATA QUALITY: SRM CHECK\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "P-value: 0.8580\n",
      "Status: ✅ No SRM detected\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "PRIMARY METRIC RESULTS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Metric: purchased\n",
      "Control rate: 15.245%\n",
      "Treatment rate: 17.159%\n",
      "Absolute lift: 1.915%\n",
      "Relative lift: 12.56%\n",
      "P-value: 0.0201\n",
      "Status: ✅ SIGNIFICANT\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "GUARDRAIL METRICS\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "revenue_per_user:\n",
      "  P-value: 0.0066\n",
      "  Status: ✅ SIGNIFICANT\n",
      "\n",
      "page_load_time:\n",
      "  P-value: 0.0000\n",
      "  Status: ✅ SIGNIFICANT\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "RECOMMENDATION\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✅ SHIP IT - Treatment wins with no guardrail violations\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Demo complete! See functions above for full implementation.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Modern A/B Testing Framework\n",
    "============================\n",
    "\n",
    "A comprehensive Python implementation of A/B testing with:\n",
    "- Deterministic assignment (SHA-256 hashing)\n",
    "- Data quality checks (SRM, A/A testing)\n",
    "- Binary and continuous metric analysis\n",
    "- CUPED variance reduction\n",
    "- Sequential testing support\n",
    "- Bayesian analysis\n",
    "- Automated reporting\n",
    "\n",
    "Author: Data Science Team\n",
    "Last Updated: January 2026\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "from statsmodels.stats import power as pwr\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for an A/B test experiment\"\"\"\n",
    "    experiment_id: str\n",
    "    hypothesis: str\n",
    "    primary_metric: str\n",
    "    metric_type: str  # 'binary' or 'continuous'\n",
    "    guardrail_metrics: List[str]\n",
    "    unit_of_randomization: str  # 'user', 'session', etc.\n",
    "    allocation_percent: float = 0.5  # 50% to treatment by default\n",
    "    alpha: float = 0.05\n",
    "    power: float = 0.80\n",
    "    minimum_detectable_effect: float = 0.05  # 5% relative lift\n",
    "    duration_days: int = 14\n",
    "\n",
    "\n",
    "# ==================== ASSIGNMENT ====================\n",
    "\n",
    "def assign_variant(\n",
    "    unit_id: str,\n",
    "    experiment_id: str = \"default_experiment\",\n",
    "    p_treatment: float = 0.5,\n",
    "    salt: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Deterministic assignment using SHA-256 hashing.\n",
    "    \n",
    "    Ensures:\n",
    "    - Same unit_id always gets same variant (consistency)\n",
    "    - ~50/50 split across population\n",
    "    - Independent across experiments (via experiment_id)\n",
    "    \n",
    "    Args:\n",
    "        unit_id: Unique identifier for randomization unit (e.g., user_id)\n",
    "        experiment_id: Unique experiment name\n",
    "        p_treatment: Probability of treatment assignment (default 0.5)\n",
    "        salt: Additional salt for hashing\n",
    "        \n",
    "    Returns:\n",
    "        'A' for control or 'B' for treatment\n",
    "    \"\"\"\n",
    "    # Create hash input\n",
    "    hash_input = f\"{experiment_id}:{unit_id}:{salt}\"\n",
    "    \n",
    "    # Hash and convert to [0, 1)\n",
    "    h = hashlib.sha256(hash_input.encode(\"utf-8\")).hexdigest()\n",
    "    hash_value = int(h[:16], 16) / (16 ** 16)\n",
    "    \n",
    "    # Assign variant\n",
    "    return \"B\" if hash_value < p_treatment else \"A\"\n",
    "\n",
    "\n",
    "# ==================== DATA QUALITY CHECKS ====================\n",
    "\n",
    "def srm_check(\n",
    "    count_a: int,\n",
    "    count_b: int,\n",
    "    expected_split: Tuple[float, float] = (0.5, 0.5),\n",
    "    alpha: float = 0.01\n",
    ") -> Dict[str, Union[float, bool]]:\n",
    "    \"\"\"\n",
    "    Sample Ratio Mismatch (SRM) check using chi-square goodness-of-fit.\n",
    "    \n",
    "    Detects data quality issues by checking if observed split matches expected.\n",
    "    \n",
    "    Args:\n",
    "        count_a: Number of users in variant A\n",
    "        count_b: Number of users in variant B\n",
    "        expected_split: Expected ratio (e.g., (0.5, 0.5) for 50/50)\n",
    "        alpha: Significance level (typically 0.01, stricter than experiment)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-value and whether SRM detected\n",
    "    \"\"\"\n",
    "    obs = np.array([count_a, count_b])\n",
    "    total = obs.sum()\n",
    "    exp = np.array(expected_split) * total\n",
    "    \n",
    "    # Chi-square test\n",
    "    chi2 = ((obs - exp) ** 2 / exp).sum()\n",
    "    p_value = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "    \n",
    "    # Flag if p-value is small (evidence of mismatch)\n",
    "    srm_detected = p_value < alpha\n",
    "    \n",
    "    return {\n",
    "        \"chi2_statistic\": float(chi2),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"srm_detected\": srm_detected,\n",
    "        \"observed_split\": (count_a / total, count_b / total),\n",
    "        \"expected_split\": expected_split,\n",
    "        \"interpretation\": \"⚠️ SRM DETECTED - DO NOT TRUST RESULTS\" if srm_detected else \"✅ No SRM detected\"\n",
    "    }\n",
    "\n",
    "\n",
    "def aa_test_expected_p_values(n_tests: int = 1000, alpha: float = 0.05) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Simulate A/A tests to validate false positive rate.\n",
    "    \n",
    "    In A/A test, both groups get same experience.\n",
    "    We expect ~5% of tests to be \"significant\" by chance at alpha=0.05.\n",
    "    \n",
    "    Args:\n",
    "        n_tests: Number of A/A tests to simulate\n",
    "        alpha: Significance threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with false positive rate and diagnostics\n",
    "    \"\"\"\n",
    "    p_values = []\n",
    "    \n",
    "    for _ in range(n_tests):\n",
    "        # Simulate A/A: both groups have same true conversion rate\n",
    "        true_rate = 0.10\n",
    "        n_per_group = 1000\n",
    "        \n",
    "        conversions_a = np.random.binomial(n_per_group, true_rate)\n",
    "        conversions_b = np.random.binomial(n_per_group, true_rate)\n",
    "        \n",
    "        # Run test\n",
    "        _, p_val = proportions_ztest(\n",
    "            [conversions_a, conversions_b],\n",
    "            [n_per_group, n_per_group]\n",
    "        )\n",
    "        p_values.append(p_val)\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    false_positive_rate = (p_values < alpha).mean()\n",
    "    \n",
    "    return {\n",
    "        \"n_tests\": n_tests,\n",
    "        \"false_positive_rate\": float(false_positive_rate),\n",
    "        \"expected_rate\": alpha,\n",
    "        \"deviation\": float(abs(false_positive_rate - alpha)),\n",
    "        \"p_values_sample\": p_values[:10].tolist(),\n",
    "        \"interpretation\": f\"✅ Within expected range\" if abs(false_positive_rate - alpha) < 0.02 else \"⚠️ Elevated false positive rate\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== POWER ANALYSIS ====================\n",
    "\n",
    "def calculate_sample_size_binary(\n",
    "    baseline_rate: float,\n",
    "    mde: float,  # Minimum detectable effect (absolute, e.g., 0.02 for 2 percentage points)\n",
    "    alpha: float = 0.05,\n",
    "    power: float = 0.80,\n",
    "    ratio: float = 1.0\n",
    ") -> Dict[str, Union[int, float]]:\n",
    "    \"\"\"\n",
    "    Calculate required sample size for binary metric (conversion rate).\n",
    "    \n",
    "    Args:\n",
    "        baseline_rate: Control group conversion rate (e.g., 0.10 for 10%)\n",
    "        mde: Minimum detectable absolute effect (e.g., 0.02 for 2 pp)\n",
    "        alpha: Type I error rate (false positive)\n",
    "        power: Statistical power (1 - Type II error rate)\n",
    "        ratio: Ratio of treatment to control size (typically 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sample size requirements\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.api import proportion_effectsize, NormalIndPower\n",
    "    \n",
    "    # Calculate effect size (Cohen's h)\n",
    "    treatment_rate = baseline_rate + mde\n",
    "    effect_size = proportion_effectsize(baseline_rate, treatment_rate)\n",
    "    \n",
    "    # Calculate sample size per group\n",
    "    analysis = NormalIndPower()\n",
    "    n_per_group = analysis.solve_power(\n",
    "        effect_size=effect_size,\n",
    "        power=power,\n",
    "        alpha=alpha,\n",
    "        ratio=ratio,\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Round up\n",
    "    n_per_group = int(np.ceil(n_per_group))\n",
    "    n_total = n_per_group * (1 + ratio)\n",
    "    \n",
    "    return {\n",
    "        \"n_per_group\": n_per_group,\n",
    "        \"n_total\": int(n_total),\n",
    "        \"baseline_rate\": baseline_rate,\n",
    "        \"treatment_rate\": treatment_rate,\n",
    "        \"mde_absolute\": mde,\n",
    "        \"mde_relative\": mde / baseline_rate if baseline_rate > 0 else np.inf,\n",
    "        \"effect_size\": float(effect_size),\n",
    "        \"power\": power,\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_sample_size_continuous(\n",
    "    baseline_mean: float,\n",
    "    baseline_std: float,\n",
    "    mde: float,  # Absolute change (e.g., $5 revenue increase)\n",
    "    alpha: float = 0.05,\n",
    "    power: float = 0.80,\n",
    "    ratio: float = 1.0\n",
    ") -> Dict[str, Union[int, float]]:\n",
    "    \"\"\"\n",
    "    Calculate required sample size for continuous metric (revenue, time, etc.).\n",
    "    \n",
    "    Args:\n",
    "        baseline_mean: Control group mean\n",
    "        baseline_std: Control group standard deviation\n",
    "        mde: Minimum detectable absolute effect\n",
    "        alpha: Type I error rate\n",
    "        power: Statistical power\n",
    "        ratio: Treatment to control ratio\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with sample size requirements\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.power import TTestIndPower\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    effect_size = mde / baseline_std\n",
    "    \n",
    "    # Calculate sample size per group\n",
    "    analysis = TTestIndPower()\n",
    "    n_per_group = analysis.solve_power(\n",
    "        effect_size=effect_size,\n",
    "        power=power,\n",
    "        alpha=alpha,\n",
    "        ratio=ratio,\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Round up\n",
    "    n_per_group = int(np.ceil(n_per_group))\n",
    "    n_total = n_per_group * (1 + ratio)\n",
    "    \n",
    "    return {\n",
    "        \"n_per_group\": n_per_group,\n",
    "        \"n_total\": int(n_total),\n",
    "        \"baseline_mean\": baseline_mean,\n",
    "        \"baseline_std\": baseline_std,\n",
    "        \"mde_absolute\": mde,\n",
    "        \"mde_relative\": mde / baseline_mean if baseline_mean != 0 else np.inf,\n",
    "        \"effect_size\": float(effect_size),\n",
    "        \"power\": power,\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== BINARY METRIC ANALYSIS ====================\n",
    "\n",
    "def analyze_conversion(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str = 'converted',\n",
    "    variant_col: str = 'variant',\n",
    "    alpha: float = 0.05\n",
    ") -> Dict[str, Union[float, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Analyze binary metric (conversion rate) with Z-test.\n",
    "    \n",
    "    Uses:\n",
    "    - Z-test for proportions (two-sided)\n",
    "    - Wilson confidence intervals (better for small samples)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with variant and metric columns\n",
    "        metric_col: Name of binary metric column (0/1)\n",
    "        variant_col: Name of variant column ('A' or 'B')\n",
    "        alpha: Significance level\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Aggregate by variant\n",
    "    agg = df.groupby(variant_col)[metric_col].agg(['sum', 'count'])\n",
    "    agg.columns = ['conversions', 'total']\n",
    "    \n",
    "    # Extract values\n",
    "    conv_a, n_a = int(agg.loc['A', 'conversions']), int(agg.loc['A', 'total'])\n",
    "    conv_b, n_b = int(agg.loc['B', 'conversions']), int(agg.loc['B', 'total'])\n",
    "    \n",
    "    # Conversion rates\n",
    "    p_a = conv_a / n_a if n_a > 0 else 0\n",
    "    p_b = conv_b / n_b if n_b > 0 else 0\n",
    "    \n",
    "    # Z-test for proportions\n",
    "    stat, p_val = proportions_ztest(\n",
    "        count=[conv_a, conv_b],\n",
    "        nobs=[n_a, n_b],\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Wilson confidence intervals (recommended)\n",
    "    ci_a = proportion_confint(conv_a, n_a, alpha=alpha, method='wilson')\n",
    "    ci_b = proportion_confint(conv_b, n_b, alpha=alpha, method='wilson')\n",
    "    \n",
    "    # Effect sizes\n",
    "    abs_lift = p_b - p_a\n",
    "    rel_lift = (p_b / p_a - 1) if p_a > 0 else np.inf\n",
    "    \n",
    "    # Confidence interval for difference\n",
    "    # Using normal approximation\n",
    "    se = np.sqrt(p_a * (1 - p_a) / n_a + p_b * (1 - p_b) / n_b)\n",
    "    z_crit = stats.norm.ppf(1 - alpha/2)\n",
    "    ci_diff = (abs_lift - z_crit * se, abs_lift + z_crit * se)\n",
    "    \n",
    "    return {\n",
    "        \"metric\": metric_col,\n",
    "        \"n_a\": n_a,\n",
    "        \"n_b\": n_b,\n",
    "        \"conversions_a\": conv_a,\n",
    "        \"conversions_b\": conv_b,\n",
    "        \"rate_a\": float(p_a),\n",
    "        \"rate_b\": float(p_b),\n",
    "        \"ci_a\": (float(ci_a[0]), float(ci_a[1])),\n",
    "        \"ci_b\": (float(ci_b[0]), float(ci_b[1])),\n",
    "        \"absolute_lift\": float(abs_lift),\n",
    "        \"relative_lift\": float(rel_lift),\n",
    "        \"ci_diff\": (float(ci_diff[0]), float(ci_diff[1])),\n",
    "        \"z_statistic\": float(stat),\n",
    "        \"p_value\": float(p_val),\n",
    "        \"significant\": p_val < alpha,\n",
    "        \"interpretation\": \"✅ SIGNIFICANT\" if p_val < alpha else \"❌ NOT SIGNIFICANT\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== CONTINUOUS METRIC ANALYSIS ====================\n",
    "\n",
    "def analyze_continuous(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    variant_col: str = 'variant',\n",
    "    alpha: float = 0.05,\n",
    "    equal_var: bool = False\n",
    ") -> Dict[str, Union[float, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Analyze continuous metric with t-test.\n",
    "    \n",
    "    Uses:\n",
    "    - Welch's t-test by default (doesn't assume equal variances)\n",
    "    - Student's t-test if equal_var=True\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with variant and metric columns\n",
    "        metric_col: Name of continuous metric column\n",
    "        variant_col: Name of variant column\n",
    "        alpha: Significance level\n",
    "        equal_var: Whether to assume equal variances (default False)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Split by variant\n",
    "    a_data = df[df[variant_col] == 'A'][metric_col].dropna()\n",
    "    b_data = df[df[variant_col] == 'B'][metric_col].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    mean_a, std_a, n_a = a_data.mean(), a_data.std(), len(a_data)\n",
    "    mean_b, std_b, n_b = b_data.mean(), b_data.std(), len(b_data)\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_val = stats.ttest_ind(\n",
    "        b_data, a_data,\n",
    "        equal_var=equal_var,\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Effect size\n",
    "    diff = mean_b - mean_a\n",
    "    rel_diff = (mean_b / mean_a - 1) if mean_a != 0 else np.inf\n",
    "    \n",
    "    # Cohen's d (standardized effect size)\n",
    "    pooled_std = np.sqrt(((n_a - 1) * std_a**2 + (n_b - 1) * std_b**2) / (n_a + n_b - 2))\n",
    "    cohens_d = diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Confidence interval for difference\n",
    "    se = np.sqrt(std_a**2 / n_a + std_b**2 / n_b)\n",
    "    df_welch = (std_a**2 / n_a + std_b**2 / n_b)**2 / (\n",
    "        (std_a**2 / n_a)**2 / (n_a - 1) + (std_b**2 / n_b)**2 / (n_b - 1)\n",
    "    )\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df=df_welch)\n",
    "    ci_diff = (diff - t_crit * se, diff + t_crit * se)\n",
    "    \n",
    "    return {\n",
    "        \"metric\": metric_col,\n",
    "        \"n_a\": n_a,\n",
    "        \"n_b\": n_b,\n",
    "        \"mean_a\": float(mean_a),\n",
    "        \"mean_b\": float(mean_b),\n",
    "        \"std_a\": float(std_a),\n",
    "        \"std_b\": float(std_b),\n",
    "        \"median_a\": float(a_data.median()),\n",
    "        \"median_b\": float(b_data.median()),\n",
    "        \"difference\": float(diff),\n",
    "        \"relative_difference\": float(rel_diff),\n",
    "        \"ci_diff\": (float(ci_diff[0]), float(ci_diff[1])),\n",
    "        \"cohens_d\": float(cohens_d),\n",
    "        \"t_statistic\": float(t_stat),\n",
    "        \"p_value\": float(p_val),\n",
    "        \"test_type\": \"Welch's t-test\" if not equal_var else \"Student's t-test\",\n",
    "        \"significant\": p_val < alpha,\n",
    "        \"interpretation\": \"✅ SIGNIFICANT\" if p_val < alpha else \"❌ NOT SIGNIFICANT\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== CUPED (VARIANCE REDUCTION) ====================\n",
    "\n",
    "def cuped_adjust(\n",
    "    y: np.ndarray,\n",
    "    x_pre: np.ndarray\n",
    ") -> Tuple[np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Apply CUPED (Controlled-experiment Using Pre-Experiment Data) adjustment.\n",
    "    \n",
    "    Reduces variance by using pre-experiment covariate that's correlated\n",
    "    with the outcome but independent of treatment assignment.\n",
    "    \n",
    "    Formula: y_adjusted = y - θ(x_pre - mean(x_pre))\n",
    "    Where θ = cov(y, x_pre) / var(x_pre)\n",
    "    \n",
    "    Args:\n",
    "        y: Outcome metric during experiment\n",
    "        x_pre: Pre-experiment covariate (e.g., past behavior)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (y_adjusted, theta, variance_reduction_pct)\n",
    "    \"\"\"\n",
    "    # Handle missing data\n",
    "    mask = ~(np.isnan(y) | np.isnan(x_pre))\n",
    "    y = y[mask]\n",
    "    x_pre = x_pre[mask]\n",
    "    \n",
    "    # Center the covariate\n",
    "    x_centered = x_pre - x_pre.mean()\n",
    "    \n",
    "    # Compute theta (optimal coefficient)\n",
    "    covariance = np.cov(y, x_pre)[0, 1]\n",
    "    variance_x = x_pre.var(ddof=1)\n",
    "    \n",
    "    if variance_x == 0:\n",
    "        # No variance in covariate, can't adjust\n",
    "        return y, 0.0, 0.0\n",
    "    \n",
    "    theta = covariance / variance_x\n",
    "    \n",
    "    # Adjust outcome\n",
    "    y_adjusted = y - theta * x_centered\n",
    "    \n",
    "    # Calculate variance reduction\n",
    "    var_original = y.var(ddof=1)\n",
    "    var_adjusted = y_adjusted.var(ddof=1)\n",
    "    variance_reduction = 1 - (var_adjusted / var_original) if var_original > 0 else 0\n",
    "    \n",
    "    return y_adjusted, float(theta), float(variance_reduction)\n",
    "\n",
    "\n",
    "def analyze_continuous_cuped(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    pre_metric_col: str,\n",
    "    variant_col: str = 'variant',\n",
    "    alpha: float = 0.05\n",
    ") -> Dict[str, Union[float, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Analyze continuous metric with CUPED variance reduction.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with variant, metric, and pre-metric columns\n",
    "        metric_col: Name of outcome metric column\n",
    "        pre_metric_col: Name of pre-experiment covariate column\n",
    "        variant_col: Name of variant column\n",
    "        alpha: Significance level\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test results (both raw and CUPED-adjusted)\n",
    "    \"\"\"\n",
    "    # First, run standard analysis\n",
    "    raw_results = analyze_continuous(df, metric_col, variant_col, alpha)\n",
    "    \n",
    "    # Apply CUPED to each variant\n",
    "    a_data = df[df[variant_col] == 'A']\n",
    "    b_data = df[df[variant_col] == 'B']\n",
    "    \n",
    "    y_a_adj, theta_a, vr_a = cuped_adjust(\n",
    "        a_data[metric_col].values,\n",
    "        a_data[pre_metric_col].values\n",
    "    )\n",
    "    \n",
    "    y_b_adj, theta_b, vr_b = cuped_adjust(\n",
    "        b_data[metric_col].values,\n",
    "        b_data[pre_metric_col].values\n",
    "    )\n",
    "    \n",
    "    # T-test on adjusted values\n",
    "    t_stat_adj, p_val_adj = stats.ttest_ind(\n",
    "        y_b_adj, y_a_adj,\n",
    "        equal_var=False,\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Statistics on adjusted values\n",
    "    mean_a_adj = y_a_adj.mean()\n",
    "    mean_b_adj = y_b_adj.mean()\n",
    "    diff_adj = mean_b_adj - mean_a_adj\n",
    "    \n",
    "    # Confidence interval\n",
    "    se_adj = np.sqrt(y_a_adj.var() / len(y_a_adj) + y_b_adj.var() / len(y_b_adj))\n",
    "    df_welch = len(y_a_adj) + len(y_b_adj) - 2\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df=df_welch)\n",
    "    ci_diff_adj = (diff_adj - t_crit * se_adj, diff_adj + t_crit * se_adj)\n",
    "    \n",
    "    return {\n",
    "        \"metric\": metric_col,\n",
    "        \"pre_metric\": pre_metric_col,\n",
    "        # Raw results\n",
    "        \"raw_mean_a\": raw_results[\"mean_a\"],\n",
    "        \"raw_mean_b\": raw_results[\"mean_b\"],\n",
    "        \"raw_difference\": raw_results[\"difference\"],\n",
    "        \"raw_p_value\": raw_results[\"p_value\"],\n",
    "        # CUPED results\n",
    "        \"theta_a\": theta_a,\n",
    "        \"theta_b\": theta_b,\n",
    "        \"variance_reduction_a\": vr_a,\n",
    "        \"variance_reduction_b\": vr_b,\n",
    "        \"cuped_mean_a\": float(mean_a_adj),\n",
    "        \"cuped_mean_b\": float(mean_b_adj),\n",
    "        \"cuped_difference\": float(diff_adj),\n",
    "        \"cuped_ci_diff\": (float(ci_diff_adj[0]), float(ci_diff_adj[1])),\n",
    "        \"cuped_t_statistic\": float(t_stat_adj),\n",
    "        \"cuped_p_value\": float(p_val_adj),\n",
    "        \"significant\": p_val_adj < alpha,\n",
    "        \"interpretation\": \"✅ SIGNIFICANT (CUPED)\" if p_val_adj < alpha else \"❌ NOT SIGNIFICANT (CUPED)\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== EXPERIMENT READOUT ====================\n",
    "\n",
    "def generate_experiment_readout(\n",
    "    df: pd.DataFrame,\n",
    "    config: ExperimentConfig,\n",
    "    pre_metric_col: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generate comprehensive experiment readout.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with experiment data\n",
    "        config: Experiment configuration\n",
    "        pre_metric_col: Optional pre-experiment metric for CUPED\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with complete experiment results\n",
    "    \"\"\"\n",
    "    readout = {\n",
    "        \"experiment_id\": config.experiment_id,\n",
    "        \"hypothesis\": config.hypothesis,\n",
    "        \"duration_days\": config.duration_days,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # 1. Sample size check\n",
    "    n_a = (df['variant'] == 'A').sum()\n",
    "    n_b = (df['variant'] == 'B').sum()\n",
    "    readout[\"sample_size\"] = {\"A\": int(n_a), \"B\": int(n_b), \"total\": int(n_a + n_b)}\n",
    "    \n",
    "    # 2. SRM check\n",
    "    readout[\"srm_check\"] = srm_check(n_a, n_b)\n",
    "    \n",
    "    if readout[\"srm_check\"][\"srm_detected\"]:\n",
    "        readout[\"warning\"] = \"⚠️ SRM DETECTED - Results may not be reliable!\"\n",
    "    \n",
    "    # 3. Primary metric analysis\n",
    "    if config.metric_type == 'binary':\n",
    "        primary_result = analyze_conversion(df, config.primary_metric)\n",
    "    else:\n",
    "        if pre_metric_col:\n",
    "            primary_result = analyze_continuous_cuped(\n",
    "                df, config.primary_metric, pre_metric_col\n",
    "            )\n",
    "        else:\n",
    "            primary_result = analyze_continuous(df, config.primary_metric)\n",
    "    \n",
    "    readout[\"primary_metric\"] = primary_result\n",
    "    \n",
    "    # 4. Guardrail metrics\n",
    "    readout[\"guardrail_metrics\"] = {}\n",
    "    for metric in config.guardrail_metrics:\n",
    "        if metric in df.columns:\n",
    "            # Infer metric type\n",
    "            if df[metric].nunique() <= 2:\n",
    "                result = analyze_conversion(df, metric)\n",
    "            else:\n",
    "                result = analyze_continuous(df, metric)\n",
    "            readout[\"guardrail_metrics\"][metric] = result\n",
    "    \n",
    "    # 5. Decision recommendation\n",
    "    primary_significant = primary_result.get(\"significant\", False)\n",
    "    \n",
    "    if config.metric_type == 'binary':\n",
    "        positive_effect = primary_result[\"relative_lift\"] > 0\n",
    "    else:\n",
    "        positive_effect = primary_result[\"difference\"] > 0\n",
    "    \n",
    "    guardrails_ok = all(\n",
    "        not result.get(\"significant\", False) or \n",
    "        (result.get(\"difference\", 0) >= 0 if config.metric_type == 'continuous'\n",
    "         else result.get(\"relative_lift\", 0) >= 0)\n",
    "        for result in readout[\"guardrail_metrics\"].values()\n",
    "    )\n",
    "    \n",
    "    if primary_significant and positive_effect and guardrails_ok:\n",
    "        recommendation = \"✅ SHIP IT - Treatment wins with no guardrail violations\"\n",
    "    elif primary_significant and positive_effect and not guardrails_ok:\n",
    "        recommendation = \"⚠️ INVESTIGATE - Treatment wins but guardrail concerns\"\n",
    "    elif primary_significant and not positive_effect:\n",
    "        recommendation = \"❌ DO NOT SHIP - Treatment performs worse\"\n",
    "    else:\n",
    "        recommendation = \"❌ NO EFFECT - No significant difference detected\"\n",
    "    \n",
    "    readout[\"recommendation\"] = recommendation\n",
    "    \n",
    "    return readout\n",
    "\n",
    "\n",
    "def print_readout(readout: Dict) -> None:\n",
    "    \"\"\"Pretty-print experiment readout\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"EXPERIMENT READOUT: {readout['experiment_id']}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nHypothesis: {readout['hypothesis']}\")\n",
    "    print(f\"Duration: {readout['duration_days']} days\")\n",
    "    print(f\"Analysis timestamp: {readout['timestamp']}\")\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"SAMPLE SIZE\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    ss = readout['sample_size']\n",
    "    print(f\"Control (A): {ss['A']:,}\")\n",
    "    print(f\"Treatment (B): {ss['B']:,}\")\n",
    "    print(f\"Total: {ss['total']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"DATA QUALITY: SRM CHECK\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    srm = readout['srm_check']\n",
    "    print(f\"P-value: {srm['p_value']:.4f}\")\n",
    "    print(f\"Status: {srm['interpretation']}\")\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"PRIMARY METRIC RESULTS\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    pm = readout['primary_metric']\n",
    "    print(f\"Metric: {pm['metric']}\")\n",
    "    \n",
    "    if 'rate_a' in pm:  # Binary metric\n",
    "        print(f\"Control rate: {pm['rate_a']:.3%}\")\n",
    "        print(f\"Treatment rate: {pm['rate_b']:.3%}\")\n",
    "        print(f\"Absolute lift: {pm['absolute_lift']:.3%}\")\n",
    "        print(f\"Relative lift: {pm['relative_lift']:.2%}\")\n",
    "    else:  # Continuous metric\n",
    "        if 'cuped_mean_a' in pm:  # CUPED version\n",
    "            print(f\"Control mean (CUPED): {pm['cuped_mean_a']:.2f}\")\n",
    "            print(f\"Treatment mean (CUPED): {pm['cuped_mean_b']:.2f}\")\n",
    "            print(f\"Difference: {pm['cuped_difference']:.2f}\")\n",
    "            print(f\"Variance reduction: {pm['variance_reduction_a']:.1%} (A), {pm['variance_reduction_b']:.1%} (B)\")\n",
    "        else:\n",
    "            print(f\"Control mean: {pm['mean_a']:.2f}\")\n",
    "            print(f\"Treatment mean: {pm['mean_b']:.2f}\")\n",
    "            print(f\"Difference: {pm['difference']:.2f}\")\n",
    "    \n",
    "    print(f\"P-value: {pm['p_value']:.4f}\")\n",
    "    print(f\"Status: {pm['interpretation']}\")\n",
    "    \n",
    "    if readout['guardrail_metrics']:\n",
    "        print(f\"\\n{'─' * 80}\")\n",
    "        print(\"GUARDRAIL METRICS\")\n",
    "        print(f\"{'─' * 80}\")\n",
    "        for metric_name, result in readout['guardrail_metrics'].items():\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            print(f\"  P-value: {result['p_value']:.4f}\")\n",
    "            print(f\"  Status: {result['interpretation']}\")\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    print(readout['recommendation'])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ==================== DEMO USAGE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Modern A/B Testing Framework - Demo\\n\")\n",
    "    \n",
    "    # ========== DEMO 1: Simple Binary Metric ==========\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DEMO 1: Binary Metric (Conversion Rate)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_users = 10000\n",
    "    user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "    \n",
    "    # Assign variants\n",
    "    variants = [assign_variant(uid, \"demo_experiment_1\") for uid in user_ids]\n",
    "    \n",
    "    # Simulate conversions (treatment has 1% absolute lift)\n",
    "    base_rate = 0.10\n",
    "    conversions = [\n",
    "        np.random.binomial(1, base_rate + (0.01 if v == 'B' else 0))\n",
    "        for v in variants\n",
    "    ]\n",
    "    \n",
    "    df1 = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'variant': variants,\n",
    "        'converted': conversions\n",
    "    })\n",
    "    \n",
    "    # Run analysis\n",
    "    result1 = analyze_conversion(df1, 'converted')\n",
    "    \n",
    "    print(f\"\\nControl conversion: {result1['rate_a']:.2%}\")\n",
    "    print(f\"Treatment conversion: {result1['rate_b']:.2%}\")\n",
    "    print(f\"Absolute lift: {result1['absolute_lift']:.2%}\")\n",
    "    print(f\"Relative lift: {result1['relative_lift']:.1%}\")\n",
    "    print(f\"P-value: {result1['p_value']:.4f}\")\n",
    "    print(f\"Result: {result1['interpretation']}\")\n",
    "    \n",
    "    # ========== DEMO 2: Continuous Metric with CUPED ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DEMO 2: Continuous Metric with CUPED (Revenue)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate sample data with pre-period revenue\n",
    "    n_users = 5000\n",
    "    user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "    variants = [assign_variant(uid, \"demo_experiment_2\") for uid in user_ids]\n",
    "    \n",
    "    # Pre-period revenue (correlated with test revenue)\n",
    "    pre_revenue = np.random.gamma(shape=2, scale=10, size=n_users)\n",
    "    \n",
    "    # Test revenue (correlated with pre, plus treatment effect)\n",
    "    test_revenue = (\n",
    "        0.6 * pre_revenue +\n",
    "        np.random.gamma(shape=2, scale=5, size=n_users) +\n",
    "        np.array([2.0 if v == 'B' else 0 for v in variants])\n",
    "    )\n",
    "    \n",
    "    df2 = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'variant': variants,\n",
    "        'pre_revenue': pre_revenue,\n",
    "        'test_revenue': test_revenue\n",
    "    })\n",
    "    \n",
    "    # Run analysis with CUPED\n",
    "    result2 = analyze_continuous_cuped(df2, 'test_revenue', 'pre_revenue')\n",
    "    \n",
    "    print(f\"\\nRaw Analysis:\")\n",
    "    print(f\"  Control mean: ${result2['raw_mean_a']:.2f}\")\n",
    "    print(f\"  Treatment mean: ${result2['raw_mean_b']:.2f}\")\n",
    "    print(f\"  Difference: ${result2['raw_difference']:.2f}\")\n",
    "    print(f\"  P-value: {result2['raw_p_value']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCUPED-Adjusted Analysis:\")\n",
    "    print(f\"  Variance reduction: {result2['variance_reduction_a']:.1%}\")\n",
    "    print(f\"  Control mean: ${result2['cuped_mean_a']:.2f}\")\n",
    "    print(f\"  Treatment mean: ${result2['cuped_mean_b']:.2f}\")\n",
    "    print(f\"  Difference: ${result2['cuped_difference']:.2f}\")\n",
    "    print(f\"  P-value: {result2['cuped_p_value']:.4f}\")\n",
    "    print(f\"  Result: {result2['interpretation']}\")\n",
    "    \n",
    "    # ========== DEMO 3: Full Experiment Readout ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DEMO 3: Complete Experiment Readout\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create experiment config\n",
    "    config = ExperimentConfig(\n",
    "        experiment_id=\"checkout_redesign_v1\",\n",
    "        hypothesis=\"New checkout flow will increase conversion without hurting revenue\",\n",
    "        primary_metric=\"purchased\",\n",
    "        metric_type=\"binary\",\n",
    "        guardrail_metrics=[\"revenue_per_user\", \"page_load_time\"],\n",
    "        unit_of_randomization=\"user\",\n",
    "        duration_days=14\n",
    "    )\n",
    "    \n",
    "    # Generate data\n",
    "    n_users = 8000\n",
    "    user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "    variants = [assign_variant(uid, config.experiment_id) for uid in user_ids]\n",
    "    \n",
    "    # Metrics\n",
    "    purchased = [np.random.binomial(1, 0.15 + (0.02 if v == 'B' else 0)) for v in variants]\n",
    "    revenue_per_user = [p * np.random.gamma(2, 25) for p in purchased]\n",
    "    page_load_time = [np.random.normal(2.0 + (0.1 if v == 'B' else 0), 0.5) for v in variants]\n",
    "    \n",
    "    df3 = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'variant': variants,\n",
    "        'purchased': purchased,\n",
    "        'revenue_per_user': revenue_per_user,\n",
    "        'page_load_time': page_load_time\n",
    "    })\n",
    "    \n",
    "    # Generate readout\n",
    "    readout = generate_experiment_readout(df3, config)\n",
    "    print_readout(readout)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Demo complete! See functions above for full implementation.\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f655b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
