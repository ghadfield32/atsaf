{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Applied Time Series Analysis and Forecasting\n",
    "\n",
    "## Complete Learning Course\n",
    "\n",
    "This notebook consolidates all learning materials for the ATSAF project. It covers the full lifecycle of time series forecasting:\n",
    "\n",
    "**Part I: Foundations**\n",
    "1. Introduction & Course Overview\n",
    "2. Time Series Objects & Contracts\n",
    "\n",
    "**Part II: Data Pipeline**\n",
    "3. Data Ingestion & Preparation\n",
    "4. Data Quality & Preprocessing\n",
    "\n",
    "**Part III: Analysis & Diagnostics**\n",
    "5. Transformations & Stationarity\n",
    "6. ACF, PACF & Residual Diagnostics\n",
    "\n",
    "**Part IV: Modeling & Evaluation**\n",
    "7. Backtesting & Cross-Validation\n",
    "8. Experimentation & Model Training\n",
    "9. Metrics & Evaluation\n",
    "10. Probabilistic Forecasting\n",
    "\n",
    "**Part V: Advanced Topics**\n",
    "11. Exogenous Regressors\n",
    "12. Hierarchical & Multi-Series\n",
    "13. Special Data Types\n",
    "14. Model Selection & Ensembling\n",
    "\n",
    "**Part VI: Production**\n",
    "15. Orchestration & Pipeline DAG\n",
    "16. Monitoring & Drift Detection\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Paths\n",
    "\n",
    "**Quick Path (~2 hours):** Sections 1, 2, 3, 8, 15\n",
    "\n",
    "**Full Path (~9 hours):** All sections with walkthroughs\n",
    "\n",
    "**Modification Path (~10+ hours):** Full path + exercises + source code reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SETUP: Run this cell first\n",
    "# ========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure src is importable\n",
    "root = Path.cwd()\n",
    "if (root / 'src').exists():\n",
    "    sys.path.insert(0, str(root))\n",
    "elif (root.parent / 'src').exists():\n",
    "    sys.path.insert(0, str(root.parent))\n",
    "    root = root.parent\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "api_key = os.getenv(\"EIA_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"EIA API key loaded successfully\")\n",
    "else:\n",
    "    print(\"WARNING: EIA_API_KEY not found in .env file\")\n",
    "\n",
    "print(f\"Working directory: {root}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part I: Foundations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Course Overview\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Understand the repository structure and purpose\n",
    "- [ ] Know how to navigate between learning materials and source code\n",
    "- [ ] Set up the environment for running examples\n",
    "\n",
    "### Overview\n",
    "\n",
    "This course teaches applied time series forecasting using real EIA (Energy Information Administration) electricity generation data. You'll learn:\n",
    "\n",
    "1. **Data handling**: Fetching, validating, and preparing time series data\n",
    "2. **Modeling**: Training and evaluating forecasting models with cross-validation\n",
    "3. **Production**: Building pipelines, monitoring drift, and triggering alerts\n",
    "\n",
    "### Repository Structure\n",
    "\n",
    "```\n",
    "atsaf/\n",
    "├── src/\n",
    "│   ├── chapter0/    # Time series object helpers\n",
    "│   ├── chapter1/    # Data ingestion & validation\n",
    "│   ├── chapter2/    # Backtesting & evaluation\n",
    "│   ├── chapter3/    # Pipeline orchestration\n",
    "│   └── chapter4/    # Monitoring & alerts\n",
    "├── chapters/        # Jupyter notebooks\n",
    "├── docs/            # Learning documentation\n",
    "├── data/            # Raw and processed data\n",
    "└── artifacts/       # Model outputs\n",
    "```\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Verifiability**: Every example is runnable, not pseudo-code\n",
    "2. **Fail-Loud**: Code raises clear errors when assumptions are violated\n",
    "3. **Idempotency**: Tasks can be re-run safely with same results\n",
    "4. **Metrics Over Promises**: Success is measured, not assumed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "## Section 2: Time Series Objects & Contracts\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Explain the Python equivalents of R ts / tsibble / timetk\n",
    "- [ ] Create a forecasting-ready DataFrame with columns unique_id, ds, y\n",
    "- [ ] Normalize timestamps to UTC and reason about DST edge cases\n",
    "- [ ] Validate time-series integrity before modeling\n",
    "\n",
    "### Concepts\n",
    "\n",
    "| R Concept | Python Equivalent | Description |\n",
    "|-----------|------------------|-------------|\n",
    "| `ts` | `pd.Series` with `DatetimeIndex` | Single time series |\n",
    "| `tsibble` | DataFrame with `unique_id, ds, y` | Tidy time series table |\n",
    "| `timetk` | pandas `.dt`, `shift`, `rolling` | Time-based feature helpers |\n",
    "\n",
    "### The Data Contract\n",
    "\n",
    "StatsForecast and our pipeline expect data in this format:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `unique_id` | string | Series identifier (e.g., \"NG_US48\") |\n",
    "| `ds` | datetime | Timestamp (UTC, timezone-naive) |\n",
    "| `y` | float | Numeric value to forecast |\n",
    "\n",
    "### Invariants (Must Always Hold)\n",
    "- `ds` is normalized to UTC (timezone-naive in the pipeline)\n",
    "- No duplicate `(unique_id, ds)` pairs\n",
    "- Regular hourly frequency with no gaps\n",
    "- Data sorted by `unique_id, ds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2-walkthrough1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 Walkthrough: Create a single-series \"ts\" object\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a UTC-aware time index\n",
    "idx = pd.date_range(\"2024-01-01\", periods=6, freq=\"h\", tz=\"UTC\")\n",
    "y = pd.Series([100, 102, 98, 101, 103, 99], index=idx)\n",
    "\n",
    "print(\"Type:\", type(y))\n",
    "print(\"Timezone:\", y.index.tz)\n",
    "print(\"\\nSeries:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2-walkthrough2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 Walkthrough: Convert to \"tsibble\" style table\n",
    "\n",
    "# Convert series to DataFrame with contract columns\n",
    "df = y.reset_index()\n",
    "df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "# Convert to timezone-naive UTC (required by StatsForecast)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"ds\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Add series identifier\n",
    "df[\"unique_id\"] = \"NG_US48\"\n",
    "\n",
    "# Reorder columns to match contract\n",
    "df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2-walkthrough3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 Walkthrough: Validate the time-series contract\n",
    "\n",
    "from src.chapter1.validate import validate_time_index, print_validation_report\n",
    "\n",
    "report = validate_time_index(df)\n",
    "print_validation_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2-walkthrough4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 Walkthrough: Add timetk-style features (safe, leakage-free)\n",
    "\n",
    "df_features = df.assign(\n",
    "    hour=df[\"ds\"].dt.hour,\n",
    "    dayofweek=df[\"ds\"].dt.dayofweek,\n",
    "    y_lag1=df[\"y\"].shift(1),\n",
    "    y_roll24=df[\"y\"].rolling(24, min_periods=1).mean()\n",
    ")\n",
    "print(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. What is the Python equivalent of an R `ts` object?</summary>\n",
    "\n",
    "A `pd.Series` with a `DatetimeIndex`.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. Why does the pipeline require `unique_id, ds, y` even for a single series?</summary>\n",
    "\n",
    "StatsForecast is multi-series-first; the contract stays consistent across one or many series.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>3. What two DST problems does the integrity check catch?</summary>\n",
    "\n",
    "Fall-back duplicates and spring-forward missing hours.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Data Pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "## Section 3: Data Ingestion & Preparation\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Pull raw EIA electricity data via REST API with pagination\n",
    "- [ ] Validate time-series data for duplicates, missing hours, DST edge cases\n",
    "- [ ] Transform raw data into forecasting-ready format (unique_id, ds, y)\n",
    "- [ ] Explain why UTC normalization and data sorting matter\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **API Pagination**: Splitting large datasets into fixed-size chunks\n",
    "- **Datetime Normalization**: Converting all timestamps to UTC\n",
    "- **Time-series Integrity**: Detecting duplicates, missing hours, repeated timestamps\n",
    "- **Monotonicity**: Ensuring timestamps are sorted chronologically\n",
    "\n",
    "### Architecture\n",
    "\n",
    "**Inputs:**\n",
    "- EIA API credentials (`EIA_API_KEY` in `.env`)\n",
    "- Date range (start_date, end_date)\n",
    "- Series identifier (respondent, fueltype)\n",
    "\n",
    "**Outputs:**\n",
    "- `raw.parquet`: Unmodified API response\n",
    "- `clean.parquet`: Normalized data with `unique_id, ds, y`\n",
    "- `metadata.json`: Data snapshot (row count, integrity report)\n",
    "\n",
    "### Files Touched\n",
    "- `src/chapter1/eia_data_simple.py` - Main orchestrator\n",
    "- `src/chapter1/ingest.py` - Paginated API calls\n",
    "- `src/chapter1/prepare.py` - Datetime parsing\n",
    "- `src/chapter1/validate.py` - Integrity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section3-walkthrough1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 Walkthrough: Initialize the EIA data fetcher\n",
    "\n",
    "from src.chapter1.eia_data_simple import EIADataFetcher\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"EIA_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"ERROR: Set EIA_API_KEY in .env file\")\n",
    "else:\n",
    "    fetcher = EIADataFetcher(api_key)\n",
    "    print(\"Fetcher initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section3-walkthrough2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 Walkthrough: Pull and prepare data\n",
    "# Note: This requires a valid EIA_API_KEY\n",
    "\n",
    "if api_key:\n",
    "    # Pull raw data (1 month for demo)\n",
    "    df_raw = fetcher.pull_data(\n",
    "        start_date=\"2023-06-01\",\n",
    "        end_date=\"2023-06-30\",\n",
    "        respondent=\"US48\",\n",
    "        fueltype=\"NG\"\n",
    "    )\n",
    "    print(f\"Raw rows: {len(df_raw)}, Columns: {df_raw.columns.tolist()}\")\n",
    "    \n",
    "    # Prepare (normalize) data\n",
    "    df_prepared = fetcher.prepare_data(df_raw)\n",
    "    print(f\"\\nPrepared data types:\\n{df_prepared.dtypes}\")\n",
    "else:\n",
    "    print(\"Skipped: API key not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section3-walkthrough3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 Walkthrough: Validate and format for forecasting\n",
    "\n",
    "if api_key:\n",
    "    # Validate integrity\n",
    "    is_valid = fetcher.validate_data(df_prepared)\n",
    "    print(f\"Basic validation: {is_valid}\")\n",
    "    \n",
    "    integrity = fetcher.validate_time_series_integrity(df_prepared, unique_id=\"respondent\")\n",
    "    print(f\"Integrity status: {integrity['status']}\")\n",
    "    \n",
    "    # Format for forecasting\n",
    "    df_forecast = fetcher.prepare_for_forecasting(df_prepared, unique_id=\"respondent\")\n",
    "    print(f\"\\nForecast-ready columns: {df_forecast.columns.tolist()}\")\n",
    "    print(df_forecast.head())\n",
    "else:\n",
    "    print(\"Skipped: API key not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. Why is UTC normalization required?</summary>\n",
    "\n",
    "Backtesting assumes monotonic, non-overlapping timestamps. Local timezones have repeated/missing hours during DST transitions. UTC eliminates this ambiguity.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. What are the three checks in `validate_time_series_integrity()`?</summary>\n",
    "\n",
    "Duplicates (same timestamp twice), missing hours (gaps in sequence), DST repeated hours (detected by duplicate).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "## Section 4: Data Quality & Preprocessing\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Detect missing timestamps, duplicates, and ordering issues\n",
    "- [ ] Standardize time zones and maintain clean unique_id/ds/y contract\n",
    "- [ ] Repair gaps without leaking future information\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Missing timestamps**: Gaps in the expected frequency\n",
    "- **Duplicates**: Multiple rows with the same (unique_id, ds)\n",
    "- **Integrity gate**: A hard check that stops the pipeline when data is invalid\n",
    "- **Resampling**: Creating a regular time index and aligning data to it\n",
    "\n",
    "### Failure Modes\n",
    "- DST creates duplicates or missing hours\n",
    "- API returns unsorted or partial data\n",
    "- Local time zone slips into modeling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section4-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 Walkthrough: Detect and handle data quality issues\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chapter1.validate import validate_time_index\n",
    "\n",
    "# Create a series with issues\n",
    "ds = pd.date_range('2024-01-01', periods=72, freq='h')\n",
    "df = pd.DataFrame({'unique_id': 'series_1', 'ds': ds, 'y': np.arange(len(ds))})\n",
    "\n",
    "# Introduce a missing hour and a duplicate\n",
    "df_broken = df.drop(index=[10]).reset_index(drop=True)\n",
    "df_broken = pd.concat([df_broken, df_broken.iloc[[20]]], ignore_index=True)\n",
    "\n",
    "# Validate\n",
    "result = validate_time_index(df_broken)\n",
    "print(\"Validation result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section4-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Repair the issues\n",
    "\n",
    "# Remove duplicates\n",
    "df_fixed = df_broken.drop_duplicates(subset=[\"unique_id\", \"ds\"]).sort_values(\"ds\")\n",
    "\n",
    "# Align to expected frequency\n",
    "full_index = pd.date_range(df_fixed[\"ds\"].min(), df_fixed[\"ds\"].max(), freq=\"h\")\n",
    "df_fixed = df_fixed.set_index(\"ds\").reindex(full_index).rename_axis(\"ds\").reset_index()\n",
    "df_fixed[\"unique_id\"] = df_fixed[\"unique_id\"].ffill()\n",
    "\n",
    "print(f\"Missing values after alignment:\\n{df_fixed.isna().sum()}\")\n",
    "\n",
    "# Forward fill short gaps (1 hour max)\n",
    "df_fixed[\"y\"] = df_fixed[\"y\"].ffill(limit=1)\n",
    "print(f\"\\nMissing values after forward fill:\\n{df_fixed.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. Why must ds be timezone-naive UTC before modeling?</summary>\n",
    "\n",
    "StatsForecast and backtesting assume a clean, monotonic UTC index.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. What is the difference between missing hours and duplicates?</summary>\n",
    "\n",
    "Missing hours are gaps in the expected frequency; duplicates are repeated timestamps.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part III: Analysis & Diagnostics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "## Section 5: Transformations & Stationarity\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Apply log or Box-Cox transforms safely\n",
    "- [ ] Difference a series to remove trend\n",
    "- [ ] Run a stationarity test when needed\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Transformations**: Log or Box-Cox to stabilize variance\n",
    "- **Differencing**: Subtract lagged values to remove trend\n",
    "- **Stationarity**: Stable mean and variance over time\n",
    "- **Unit root tests**: ADF or KPSS as a quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section5-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 Walkthrough: Transformations and stationarity testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a trending series\n",
    "ds = pd.date_range('2024-01-01', periods=200, freq='h')\n",
    "trend = 5 + 0.05 * np.arange(len(ds))\n",
    "noise = np.random.normal(scale=0.5, size=len(ds))\n",
    "y = trend + noise\n",
    "series = pd.Series(y, index=ds)\n",
    "\n",
    "# Apply transformations\n",
    "log_series = np.log1p(series)\n",
    "diff_series = series.diff().dropna()\n",
    "\n",
    "print('Original mean:', round(series.mean(), 2))\n",
    "print('Diff mean:', round(diff_series.mean(), 4))\n",
    "\n",
    "# Run ADF test (if statsmodels available)\n",
    "try:\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    p_value = adfuller(series.values)[1]\n",
    "    p_value_diff = adfuller(diff_series.values)[1]\n",
    "    print(f'\\nADF p-value (original): {p_value:.4f}')\n",
    "    print(f'ADF p-value (differenced): {p_value_diff:.4f}')\n",
    "    print('\\nInterpretation: p < 0.05 suggests stationarity')\n",
    "except Exception as exc:\n",
    "    print(f'statsmodels not available: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. When should you consider differencing a series?</summary>\n",
    "\n",
    "When trend dominates and residuals show non-stationarity.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. Why do we avoid log(0) without an offset?</summary>\n",
    "\n",
    "log(0) is undefined, so we use log1p or an offset.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "## Section 6: ACF, PACF & Residual Diagnostics\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Compute ACF/PACF for model clues\n",
    "- [ ] Run a basic residual diagnostic\n",
    "- [ ] Interpret lag structure\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **ACF (Autocorrelation Function)**: Correlation between values at different lags\n",
    "- **PACF (Partial ACF)**: Correlation after removing effects of intermediate lags\n",
    "- **Residual diagnostics**: Checking if model residuals are white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section6-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 Walkthrough: ACF and PACF analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a seasonal series\n",
    "ds = pd.date_range('2024-01-01', periods=200, freq='h')\n",
    "y = np.sin(np.arange(len(ds)) / 6.0) + np.random.normal(scale=0.3, size=len(ds))\n",
    "series = pd.Series(y, index=ds)\n",
    "\n",
    "# Compute ACF/PACF (if statsmodels available)\n",
    "try:\n",
    "    from statsmodels.tsa.stattools import acf, pacf\n",
    "    acf_vals = acf(series.values, nlags=10)\n",
    "    pacf_vals = pacf(series.values, nlags=10)\n",
    "    \n",
    "    print('ACF values (lags 0-10):')\n",
    "    print([round(v, 3) for v in acf_vals])\n",
    "    print('\\nPACF values (lags 0-10):')\n",
    "    print([round(v, 3) for v in pacf_vals])\n",
    "except Exception as exc:\n",
    "    print(f'statsmodels not available: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part IV: Modeling & Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "## Section 7: Backtesting & Cross-Validation\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Build rolling-origin splits without leakage\n",
    "- [ ] Compute RMSE, MAE, and MASE on holdout windows\n",
    "- [ ] Compare models fairly using identical splits\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Rolling-origin CV**: Move the train/test cutoff forward through time\n",
    "- **Horizon**: Number of steps to predict in each window\n",
    "- **Leakage**: Using future data to predict the past (always a bug)\n",
    "\n",
    "### Invariants\n",
    "- Train end < test start (no overlap)\n",
    "- Same splits for all models (fair comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section7-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7 Walkthrough: Rolling-origin backtesting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chapter2.backtesting import RollingWindowBacktest\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Create synthetic data\n",
    "ds = pd.date_range('2024-01-01', periods=240, freq='h')\n",
    "y = 10 + 0.1 * np.arange(len(ds)) + np.random.normal(scale=0.5, size=len(ds))\n",
    "df = pd.DataFrame({'unique_id': 'series_1', 'ds': ds, 'y': y})\n",
    "\n",
    "# Generate rolling splits\n",
    "backtest = RollingWindowBacktest(min_train_size=120, test_size=24, step_size=24)\n",
    "splits = backtest.generate_splits(df, unique_id='series_1')\n",
    "\n",
    "print(f\"Generated {len(splits)} splits\")\n",
    "print(f\"First split info: {splits[0].info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section7-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Score a naive baseline\n",
    "\n",
    "split = splits[0]\n",
    "train = df.iloc[split.train_indices]\n",
    "test = df.iloc[split.test_indices]\n",
    "\n",
    "# Naive baseline: repeat last training value\n",
    "yhat = np.repeat(train['y'].iloc[-1], len(test))\n",
    "\n",
    "rmse = ForecastMetrics.rmse(test['y'].values, yhat)\n",
    "mae = ForecastMetrics.mae(test['y'].values, yhat)\n",
    "\n",
    "print(f'Naive Baseline RMSE: {rmse:.3f}')\n",
    "print(f'Naive Baseline MAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. Why is rolling-origin CV preferred over random splits?</summary>\n",
    "\n",
    "It preserves time order and avoids leakage.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. What does MASE tell you that RMSE does not?</summary>\n",
    "\n",
    "MASE scales error relative to a seasonal naive baseline, so MASE < 1 means you're beating the baseline.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-header",
   "metadata": {},
   "source": [
    "## Section 8: Experimentation & Model Training\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Set up and run rolling-origin cross-validation\n",
    "- [ ] Build a leaderboard comparing multiple models\n",
    "- [ ] Explain why RMSE is primary and when MAPE fails\n",
    "- [ ] Interpret prediction interval coverage\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Expanding window**: Training set grows over time; test set is fixed\n",
    "- **Rolling window**: Both train and test windows slide forward\n",
    "- **Coverage**: Percentage of actual values inside prediction intervals\n",
    "- **MASE**: Mean Absolute Scaled Error (normalized by seasonal naive)\n",
    "\n",
    "### Files Touched\n",
    "- `src/chapter1/eia_data_simple.py` - cross_validate(), evaluate_forecast()\n",
    "- `src/chapter2/backtesting.py` - RollingWindowBacktest, ExpandingWindowBacktest\n",
    "- `src/chapter2/evaluation.py` - ForecastMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8 Walkthrough: Cross-validation with StatsForecast\n",
    "# Note: This requires a valid EIA_API_KEY and may take a few minutes\n",
    "\n",
    "if api_key:\n",
    "    from src.chapter1.eia_data_simple import EIADataFetcher, ExperimentConfig\n",
    "    \n",
    "    # Define experiment configuration\n",
    "    config = ExperimentConfig(\n",
    "        name=\"baseline_experiment\",\n",
    "        horizon=24,           # Forecast 24 hours ahead\n",
    "        n_windows=3,          # 3 train/test splits\n",
    "        step_size=168,        # Move forward 1 week each time\n",
    "        confidence_level=95,\n",
    "        models=[\"AutoARIMA\", \"SeasonalNaive\"],\n",
    "        metrics=[\"rmse\", \"mape\", \"mase\", \"coverage\"]\n",
    "    )\n",
    "    print(f\"Experiment config: {config}\")\n",
    "else:\n",
    "    print(\"Skipped: API key not available. See section 3 for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9-header",
   "metadata": {},
   "source": [
    "## Section 9: Metrics & Evaluation\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Compute RMSE, MAPE, MASE, and coverage\n",
    "- [ ] Understand when MAPE fails and what to use instead\n",
    "- [ ] Interpret leaderboard rankings\n",
    "\n",
    "### Metric Reference\n",
    "\n",
    "| Metric | Formula | Best When |\n",
    "|--------|---------|----------|\n",
    "| RMSE | √(mean(error²)) | Penalize large errors |\n",
    "| MAE | mean(|error|) | Robust to outliers |\n",
    "| MAPE | mean(|error/actual|) | y never near zero |\n",
    "| MASE | MAE / naive_MAE | Compare to baseline |\n",
    "| Coverage | % inside [lo, hi] | Interval calibration |\n",
    "\n",
    "### MAPE Pitfall\n",
    "\n",
    "**MAPE = |error| / |actual|** → explodes when actual ≈ 0\n",
    "\n",
    "Example: Solar generation at night = 0 → MAPE = ∞\n",
    "\n",
    "**Fix**: Use RMSE/MAE/MASE as primary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section9-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9 Walkthrough: Compute all metrics\n",
    "\n",
    "import numpy as np\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Simulated forecast results\n",
    "rng = np.random.default_rng(42)\n",
    "y_true = 50 + rng.normal(scale=5, size=100)\n",
    "y_pred = y_true + rng.normal(scale=3, size=100)\n",
    "\n",
    "# Compute metrics\n",
    "rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "mape = ForecastMetrics.mape(y_true, y_pred)\n",
    "mase = ForecastMetrics.mase(y_true, y_pred, y_true, season_length=24)\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"MAPE: {mape:.3f}%\")\n",
    "print(f\"MASE: {mase:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10-header",
   "metadata": {},
   "source": [
    "## Section 10: Probabilistic Forecasting\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Compute prediction interval coverage\n",
    "- [ ] Compare nominal and empirical coverage\n",
    "- [ ] Spot overconfident or underconfident intervals\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Prediction interval**: A range that should contain the true value\n",
    "- **Coverage**: Percent of actuals inside the interval\n",
    "- **Calibration**: Agreement between nominal and empirical coverage\n",
    "\n",
    "**Interpretation:**\n",
    "- Coverage >> nominal: Intervals too wide (overconfident)\n",
    "- Coverage << nominal: Intervals too tight (underconfident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10 Walkthrough: Prediction interval coverage\n",
    "\n",
    "import numpy as np\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Simulate predictions with intervals\n",
    "rng = np.random.default_rng(7)\n",
    "y_true = rng.normal(loc=100, scale=5, size=200)\n",
    "yhat = y_true + rng.normal(scale=2, size=200)\n",
    "\n",
    "# Create 95% prediction interval\n",
    "interval_width = 4.0\n",
    "lower = yhat - interval_width\n",
    "upper = yhat + interval_width\n",
    "\n",
    "# Compute coverage\n",
    "coverage = ForecastMetrics.coverage(y_true, lower, upper)\n",
    "\n",
    "print(f\"Empirical coverage: {coverage:.1f}%\")\n",
    "print(f\"Nominal coverage: 95%\")\n",
    "print(f\"Gap: {95 - coverage:.1f}%\")\n",
    "\n",
    "if coverage < 90:\n",
    "    print(\"\\nWarning: Intervals too tight (underconfident)\")\n",
    "elif coverage > 98:\n",
    "    print(\"\\nWarning: Intervals too wide (overconfident)\")\n",
    "else:\n",
    "    print(\"\\nIntervals are well-calibrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part V: Advanced Topics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section11-header",
   "metadata": {},
   "source": [
    "## Section 11: Exogenous Regressors\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Add weather, holiday, or event features\n",
    "- [ ] Align features to avoid leakage\n",
    "- [ ] Measure feature impact\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Exogenous variables**: External drivers that influence the forecast\n",
    "- **Feature alignment**: Ensuring features are available at forecast time\n",
    "- **Leakage prevention**: Only use past/known values for future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section11-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11 Walkthrough: Exogenous features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Create data with exogenous drivers\n",
    "ds = pd.date_range('2024-01-01', periods=240, freq='h')\n",
    "temp = 20 + 5 * np.sin(np.arange(len(ds)) / 24.0)  # Temperature\n",
    "event = (np.arange(len(ds)) % 72 == 0).astype(int)  # Events every 3 days\n",
    "y = 50 + 0.8 * temp + 10 * event + np.random.normal(scale=2.0, size=len(ds))\n",
    "\n",
    "df = pd.DataFrame({'ds': ds, 'temp': temp, 'event': event, 'y': y})\n",
    "\n",
    "# Train/test split\n",
    "train = df.iloc[:-24]\n",
    "test = df.iloc[-24:]\n",
    "\n",
    "# Model with exogenous features\n",
    "model = LinearRegression()\n",
    "model.fit(train[['temp', 'event']], train['y'])\n",
    "pred = model.predict(test[['temp', 'event']])\n",
    "\n",
    "rmse = ForecastMetrics.rmse(test['y'].values, pred)\n",
    "print(f'RMSE with exogenous features: {rmse:.3f}')\n",
    "print(f'Feature coefficients: temp={model.coef_[0]:.2f}, event={model.coef_[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section12-header",
   "metadata": {},
   "source": [
    "## Section 12: Hierarchical & Multi-Series\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Structure multiple series with unique_id\n",
    "- [ ] Compare local vs global summaries\n",
    "- [ ] Understand why reconciliation matters for totals\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Multi-series**: Multiple time series in one dataset\n",
    "- **Local models**: Fit separately per series\n",
    "- **Global models**: Fit once across all series\n",
    "- **Reconciliation**: Ensuring forecasts sum correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section12-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12 Walkthrough: Multi-series handling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create multi-series data\n",
    "ds = pd.date_range('2024-01-01', periods=48, freq='h')\n",
    "df = pd.DataFrame({\n",
    "    'unique_id': ['series_a'] * len(ds) + ['series_b'] * len(ds),\n",
    "    'ds': list(ds) + list(ds),\n",
    "    'y': np.concatenate([\n",
    "        10 + np.random.normal(scale=1.0, size=len(ds)),\n",
    "        20 + np.random.normal(scale=1.5, size=len(ds)),\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Compare local vs global summaries\n",
    "local_means = df.groupby('unique_id')['y'].mean()\n",
    "global_mean = df['y'].mean()\n",
    "\n",
    "print('Local means (per series):')\n",
    "print(local_means)\n",
    "print(f'\\nGlobal mean (all series): {global_mean:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section13-header",
   "metadata": {},
   "source": [
    "## Section 13: Special Data Types\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Handle zero-heavy or count series\n",
    "- [ ] Know which metrics fail on zeros\n",
    "\n",
    "### Special Cases\n",
    "\n",
    "- **Intermittent demand**: Many zeros with occasional spikes\n",
    "- **Count data**: Non-negative integers (e.g., units sold)\n",
    "- **Zero-heavy series**: Solar at night, wind on calm days\n",
    "\n",
    "### Metric Behavior with Zeros\n",
    "\n",
    "| Metric | With Zeros |\n",
    "|--------|------------|\n",
    "| RMSE | Works fine |\n",
    "| MAE | Works fine |\n",
    "| MAPE | Undefined (divide by zero) |\n",
    "| MASE | Works if baseline exists |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section13-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13 Walkthrough: Handling zero-heavy data\n",
    "\n",
    "import numpy as np\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Create intermittent demand series\n",
    "rng = np.random.default_rng(42)\n",
    "y_true = rng.poisson(lam=1.0, size=100)\n",
    "y_true[rng.choice(len(y_true), size=40, replace=False)] = 0  # Add more zeros\n",
    "\n",
    "yhat = np.maximum(0, y_true + rng.normal(scale=0.5, size=len(y_true)))\n",
    "\n",
    "# Compute metrics\n",
    "mae = ForecastMetrics.mae(y_true, yhat)\n",
    "mape = ForecastMetrics.mape(y_true, yhat)\n",
    "mase = ForecastMetrics.mase(y_true, yhat, y_true, season_length=1)\n",
    "\n",
    "print(f'MAE: {mae:.3f}')\n",
    "print(f'MAPE: {mape:.3f}% (may be unreliable with zeros)')\n",
    "print(f'MASE: {mase:.3f}')\n",
    "print(f'\\nZero count: {(y_true == 0).sum()} / {len(y_true)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section14-header",
   "metadata": {},
   "source": [
    "## Section 14: Model Selection & Ensembling\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Pick a champion from a leaderboard\n",
    "- [ ] Build a simple ensemble baseline\n",
    "- [ ] Understand why a naive baseline should never be dropped\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Champion model**: Best performer on primary metric\n",
    "- **Ensemble**: Combine multiple model predictions\n",
    "- **Naive baseline**: Always include to sanity-check complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section14-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14 Walkthrough: Simple ensemble\n",
    "\n",
    "import numpy as np\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Simulate two model predictions\n",
    "rng = np.random.default_rng(123)\n",
    "y_true = rng.normal(loc=50, scale=3, size=60)\n",
    "model_a = y_true + rng.normal(scale=2, size=60)\n",
    "model_b = y_true + rng.normal(scale=2.5, size=60)\n",
    "\n",
    "# Simple mean ensemble\n",
    "ensemble = (model_a + model_b) / 2\n",
    "\n",
    "# Compare performance\n",
    "rmse_a = ForecastMetrics.rmse(y_true, model_a)\n",
    "rmse_b = ForecastMetrics.rmse(y_true, model_b)\n",
    "rmse_ens = ForecastMetrics.rmse(y_true, ensemble)\n",
    "\n",
    "print(f'Model A RMSE: {rmse_a:.3f}')\n",
    "print(f'Model B RMSE: {rmse_b:.3f}')\n",
    "print(f'Ensemble RMSE: {rmse_ens:.3f}')\n",
    "\n",
    "if rmse_ens < min(rmse_a, rmse_b):\n",
    "    print('\\nEnsemble outperforms individual models!')\n",
    "else:\n",
    "    print('\\nBest single model is the winner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part VI: Production\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section15-header",
   "metadata": {},
   "source": [
    "## Section 15: Orchestration & Pipeline DAG\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Run an end-to-end forecasting pipeline from CLI\n",
    "- [ ] Understand task decomposition and why idempotency matters\n",
    "- [ ] Deploy the pipeline as an Airflow DAG\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Task**: Atomic unit of work (pull data, validate, train, forecast)\n",
    "- **DAG**: Directed Acyclic Graph of task dependencies\n",
    "- **Idempotency**: Re-running produces same outputs\n",
    "- **Atomic writes**: All-or-nothing file operations\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```\n",
    "ingest_eia() → prepare_clean() → validate_clean() → train_backtest_select() → register_champion() → forecast_publish()\n",
    "```\n",
    "\n",
    "### Files Touched\n",
    "- `src/chapter3/tasks.py` - 6 pipeline tasks\n",
    "- `src/chapter3/dag_builder.py` - Airflow DAG definition\n",
    "- `src/chapter3/cli.py` - Command-line interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section15-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 15 Walkthrough: View the pipeline DAG\n",
    "\n",
    "try:\n",
    "    from src.chapter3.dag_builder import build_dag_dot\n",
    "    dot_string = build_dag_dot()\n",
    "    print(\"Pipeline DAG (DOT format):\")\n",
    "    print(dot_string)\n",
    "    print(\"\\nVisualize at: https://dreampuf.github.io/GraphvizOnline/\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not build DAG: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section15-cli",
   "metadata": {},
   "source": [
    "### Running the Pipeline via CLI\n",
    "\n",
    "```bash\n",
    "# Run full pipeline\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24 \\\n",
    "  --output-dir artifacts/\n",
    "\n",
    "# Re-run with overwrite\n",
    "python -m src.chapter3.cli run \\\n",
    "  --start-date 2023-06-01 \\\n",
    "  --end-date 2023-09-30 \\\n",
    "  --horizon 24 \\\n",
    "  --overwrite\n",
    "```\n",
    "\n",
    "### Expected Outputs\n",
    "- `data/raw.parquet` - Raw API response\n",
    "- `data/clean.parquet` - Normalized data\n",
    "- `artifacts/cv_results.parquet` - Cross-validation results\n",
    "- `artifacts/leaderboard.parquet` - Model rankings\n",
    "- `artifacts/predictions.parquet` - Final forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section15-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. What is idempotency and why does it matter?</summary>\n",
    "\n",
    "Idempotency means re-running with same inputs yields same outputs (no hidden state). It matters because tasks can be safely re-run on failure.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. If validate_clean() is skipped, what could go wrong?</summary>\n",
    "\n",
    "Bad data (duplicates, missing hours) enters training. Model learns on corrupt series → poor forecasts.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section16-header",
   "metadata": {},
   "source": [
    "## Section 16: Monitoring, Drift Detection & Alerts\n",
    "\n",
    "### Learning Outcomes\n",
    "- [ ] Persist forecasts and actuals into a queryable database\n",
    "- [ ] Compute rolling accuracy metrics and detect model drift\n",
    "- [ ] Set alert thresholds based on backtest performance\n",
    "- [ ] Run health checks (freshness, completeness, staleness)\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- **Forecast persistence**: Store predictions for later scoring\n",
    "- **Scoring**: Compare predictions vs actuals\n",
    "- **Drift**: Model performance degrades over time\n",
    "- **Alert threshold**: Metric value that triggers action\n",
    "\n",
    "### Monitoring Database Schema\n",
    "\n",
    "```sql\n",
    "pipeline_runs     -- Execution log\n",
    "forecasts         -- Stored predictions\n",
    "forecast_scores   -- Rolling metrics\n",
    "alerts            -- Alert events\n",
    "```\n",
    "\n",
    "### Alert Types\n",
    "- `STALE_DATA` - Data too old\n",
    "- `MISSING_DATA` - Gaps in data\n",
    "- `STALE_FORECAST` - Forecasts not scored\n",
    "- `MODEL_DRIFT` - Performance degraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section16-walkthrough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 16 Walkthrough: Initialize monitoring database\n",
    "\n",
    "try:\n",
    "    from src.chapter4.db import init_monitoring_db\n",
    "    import sqlite3\n",
    "    \n",
    "    db_path = root / \"monitoring_demo.db\"\n",
    "    init_monitoring_db(str(db_path))\n",
    "    \n",
    "    # Verify tables created\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Monitoring database created at: {db_path}\")\n",
    "    print(f\"Tables: {[t[0] for t in tables]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize monitoring: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section16-drift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 16 Walkthrough: Drift threshold calculation\n",
    "\n",
    "# Drift threshold = mean + k*std (from backtest)\n",
    "# k = 2 is a good starting point (2 standard deviations)\n",
    "\n",
    "backtest_rmse_mean = 10.5\n",
    "backtest_rmse_std = 1.2\n",
    "k = 2.0\n",
    "\n",
    "threshold = backtest_rmse_mean + k * backtest_rmse_std\n",
    "\n",
    "print(f\"Backtest RMSE: {backtest_rmse_mean} ± {backtest_rmse_std}\")\n",
    "print(f\"Drift threshold (k={k}): {threshold:.2f}\")\n",
    "print(f\"\\nAlert triggers when RMSE > {threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section16-checkpoint",
   "metadata": {},
   "source": [
    "### Checkpoint Questions\n",
    "\n",
    "<details>\n",
    "<summary>1. Why can't we score a 24-hour forecast immediately after generating it?</summary>\n",
    "\n",
    "The actual value isn't observed yet (it's 24 hours in the future). Safe to score once actuals are available.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>2. What does k control in the threshold formula?</summary>\n",
    "\n",
    "k is the std multiplier. Higher k = wider band (fewer alerts). Lower k = tighter band (more alerts). k=2 is standard.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-a",
   "metadata": {},
   "source": [
    "## Appendix A: Quick Reference\n",
    "\n",
    "### Common Imports\n",
    "\n",
    "```python\n",
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Data ingestion\n",
    "from src.chapter1.eia_data_simple import EIADataFetcher\n",
    "from src.chapter1.validate import validate_time_index\n",
    "\n",
    "# Backtesting\n",
    "from src.chapter2.backtesting import RollingWindowBacktest\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "# Pipeline\n",
    "from src.chapter3.cli import run as pipeline_run\n",
    "\n",
    "# Monitoring\n",
    "from src.chapter4.db import init_monitoring_db\n",
    "from src.chapter4.drift import detect_drift\n",
    "```\n",
    "\n",
    "### Data Contract\n",
    "\n",
    "```python\n",
    "# Required columns for forecasting\n",
    "df.columns  # ['unique_id', 'ds', 'y']\n",
    "df['ds']    # datetime64[ns], timezone-naive UTC\n",
    "df['y']     # float64, the target variable\n",
    "```\n",
    "\n",
    "### CLI Commands\n",
    "\n",
    "```bash\n",
    "# Run pipeline\n",
    "python -m src.chapter3.cli run --start-date 2023-06-01 --end-date 2023-09-30 --horizon 24\n",
    "\n",
    "# Run tests\n",
    "pytest tests/ -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-b",
   "metadata": {},
   "source": [
    "## Appendix B: All Checkpoint Answers\n",
    "\n",
    "### Section 2: Time Series Objects\n",
    "1. Python equivalent of R `ts`: `pd.Series` with `DatetimeIndex`\n",
    "2. Why `unique_id, ds, y`: StatsForecast is multi-series-first\n",
    "3. DST problems caught: Fall-back duplicates, spring-forward missing hours\n",
    "\n",
    "### Section 3: Data Ingestion\n",
    "1. UTC required: Eliminates DST ambiguity for backtesting\n",
    "2. Integrity checks: Duplicates, missing hours, DST repeated hours\n",
    "\n",
    "### Section 4: Data Quality\n",
    "1. UTC before modeling: StatsForecast assumes clean monotonic UTC index\n",
    "2. Missing vs duplicates: Gaps in frequency vs repeated timestamps\n",
    "\n",
    "### Section 5: Transformations\n",
    "1. When to difference: When trend dominates and residuals are non-stationary\n",
    "2. Avoid log(0): Undefined, use log1p or offset\n",
    "\n",
    "### Section 7: Backtesting\n",
    "1. Rolling-origin preferred: Preserves time order, avoids leakage\n",
    "2. MASE vs RMSE: MASE scales error relative to baseline\n",
    "\n",
    "### Section 15: Orchestration\n",
    "1. Idempotency: Re-run produces same outputs, safe on failure\n",
    "2. Skip validate: Bad data enters training, poor forecasts\n",
    "\n",
    "### Section 16: Monitoring\n",
    "1. Can't score immediately: Actuals not observed for 24 hours\n",
    "2. k in threshold: Std multiplier, k=2 is standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "course-complete",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Course Complete!\n",
    "\n",
    "You've covered the full time series forecasting lifecycle:\n",
    "\n",
    "1. **Foundations**: Time series objects and data contracts\n",
    "2. **Data Pipeline**: Ingestion, validation, and quality\n",
    "3. **Analysis**: Transformations and diagnostics\n",
    "4. **Modeling**: Backtesting, training, and evaluation\n",
    "5. **Advanced**: Exogenous features, multi-series, ensembling\n",
    "6. **Production**: Pipeline orchestration and monitoring\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Run the full pipeline on your own data\n",
    "- [ ] Deploy to Airflow for scheduled runs\n",
    "- [ ] Set up monitoring alerts for production\n",
    "- [ ] Extend with additional models and features\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Source code: `src/chapter[0-4]/`\n",
    "- Documentation: `docs/chapter*.md`\n",
    "- Learning guide: `docs/LEARNING_GUIDE.md`\n",
    "\n",
    "---\n",
    "\n",
    "*Last updated: January 2026*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
