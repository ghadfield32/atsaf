{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos:\n",
    "- update preprocessing data so it's organized and in the data pull rather than in the preprocessing\n",
    "- add in eda module to explore data and show decisions made based on that\n",
    "- go over the full explanation of why we would choose this data, why we would choose this model, \n",
    "    - ensure to include decision making and thought process not just end results, \n",
    "    - archive the notebooks, \n",
    "    - update the readme, \n",
    "    - ensure this is software that is automated, \n",
    "    - add in mermaid graph to readme and linkedin post\n",
    "    - add in start up instructions to readme\n",
    "    - finally post on linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable Energy Forecasting Pipeline\n",
    "\n",
    "This notebook walks through building a **next-24h renewable generation forecast system** with:\n",
    "\n",
    "- **EIA data integration** - Hourly wind/solar generation for US regions\n",
    "- **Weather features** - Open-Meteo integration (wind speed, solar radiation)\n",
    "- **Probabilistic forecasting** - Dual prediction intervals (80%, 95%)\n",
    "- **Drift monitoring** - Automatic detection of model degradation\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "```\n",
    "┌─────────────┐      ┌──────────────┐      ┌─────────────┐\n",
    "│  EIA API    │──┬──▶│ Data         │──┬──▶│ StatsForecast│\n",
    "│ (WND/SUN)   │  │   │ Pipeline     │  │   │ Models       │\n",
    "└─────────────┘  │   └──────────────┘  │   └─────────────┘\n",
    "                 │                     │           │\n",
    "┌─────────────┐  │   ┌──────────────┐  │   ┌─────▼──────┐\n",
    "│ Open-Meteo  │──┘   │ Validation   │  │   │Probabilistic│\n",
    "│ Weather API │      │ & Quality    │  │   │Forecasts    │\n",
    "└─────────────┘      │ Gates        │  │   │(80%, 95% CI)│\n",
    "                     └──────────────┘  │   └────────────┘\n",
    "                                       │           │\n",
    "                                       │   ┌───────▼─────┐\n",
    "                                       └──▶│  Artifacts  │\n",
    "                                           │  Commit &   │\n",
    "                                           │  Dashboard  │\n",
    "                                           └─────────────┘\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` - where `unique_id` = `{region}_{fuel_type}`\n",
    "2. **Zero-value handling**: Solar generates 0 at night - we use RMSE/MAE, NOT MAPE\n",
    "3. **Leakage prevention**: Use **forecasted** weather for predictions, not historical\n",
    "4. **Drift detection**: Threshold = mean + 2*std from backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure we have the project root in our path and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to project root: c:\\docker_projects\\atsaf we are currently at c:\\docker_projects\\atsaf\n",
      "Project root: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\docker_projects\\atsaf\"\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "if os.getcwd() != str(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to project root: {project_root} we are currently at {os.getcwd()}\")\n",
    "\n",
    "# Configure logging for visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 1: Region Definitions\n",
    "\n",
    "**File:** `src/renewable/regions.py`\n",
    "\n",
    "This module maps **EIA balancing authority regions** to their geographic coordinates. Why do we need coordinates?\n",
    "\n",
    "- **Weather API lookup**: Open-Meteo requires latitude/longitude\n",
    "- **Regional analysis**: Compare forecast accuracy across regions\n",
    "- **Timezone handling**: Each region has a primary timezone\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "1. **NamedTuple for RegionInfo**: Immutable, type-safe, and memory-efficient\n",
    "2. **Centroid coordinates**: Approximate centers - good enough for hourly weather\n",
    "3. **Fuel type codes**: `WND` (wind), `SUN` (solar) - match EIA's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/regions.py\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/regions.py\n",
    "# src/renewable/regions.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class RegionInfo(NamedTuple):\n",
    "    \"\"\"Region metadata for EIA and weather lookups.\"\"\"\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    timezone: str\n",
    "    # Some internal regions may not map cleanly to an EIA respondent.\n",
    "    # We keep them in REGIONS for weather/features, but EIA fetch requires this.\n",
    "    eia_respondent: Optional[str] = None\n",
    "\n",
    "\n",
    "REGIONS: dict[str, RegionInfo] = {\n",
    "    # Western Interconnection\n",
    "    \"CALI\": RegionInfo(\n",
    "        name=\"California ISO\",\n",
    "        lat=36.7,\n",
    "        lon=-119.4,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=\"CISO\",\n",
    "    ),\n",
    "    \"NW\": RegionInfo(\n",
    "        name=\"Northwest\",\n",
    "        lat=45.5,\n",
    "        lon=-122.0,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "    \"SW\": RegionInfo(\n",
    "        name=\"Southwest\",\n",
    "        lat=33.5,\n",
    "        lon=-112.0,\n",
    "        timezone=\"America/Phoenix\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "\n",
    "    # Texas Interconnection\n",
    "    \"ERCO\": RegionInfo(\n",
    "        name=\"ERCOT (Texas)\",\n",
    "        lat=31.0,\n",
    "        lon=-100.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"ERCO\",\n",
    "    ),\n",
    "\n",
    "    # Midwest\n",
    "    \"MISO\": RegionInfo(\n",
    "        name=\"Midcontinent ISO\",\n",
    "        lat=41.0,\n",
    "        lon=-93.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"MISO\",\n",
    "    ),\n",
    "    \"PJM\": RegionInfo(\n",
    "        name=\"PJM Interconnection\",\n",
    "        lat=39.0,\n",
    "        lon=-77.0,\n",
    "        timezone=\"America/New_York\",\n",
    "        eia_respondent=\"PJM\",\n",
    "    ),\n",
    "    \"SWPP\": RegionInfo(\n",
    "        name=\"Southwest Power Pool\",\n",
    "        lat=37.0,\n",
    "        lon=-97.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"SWPP\",\n",
    "    ),\n",
    "\n",
    "    # Internal/aggregate regions kept for non-EIA use (weather/features/etc.)\n",
    "    \"SE\": RegionInfo(name=\"Southeast\", lat=33.0, lon=-84.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"FLA\": RegionInfo(name=\"Florida\", lat=28.0, lon=-82.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"CAR\": RegionInfo(name=\"Carolinas\", lat=35.5, lon=-80.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"TEN\": RegionInfo(name=\"Tennessee Valley\", lat=35.5, lon=-86.0, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "\n",
    "    \"US48\": RegionInfo(name=\"Lower 48 States\", lat=39.8, lon=-98.5, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "}\n",
    "\n",
    "FUEL_TYPES = {\"WND\": \"Wind\", \"SUN\": \"Solar\"}\n",
    "\n",
    "\n",
    "def list_regions() -> list[str]:\n",
    "    return sorted(REGIONS.keys())\n",
    "\n",
    "\n",
    "def get_region_info(region_code: str) -> RegionInfo:\n",
    "    return REGIONS[region_code]\n",
    "\n",
    "\n",
    "def get_region_coords(region_code: str) -> tuple[float, float]:\n",
    "    r = REGIONS[region_code]\n",
    "    return (r.lat, r.lon)\n",
    "\n",
    "\n",
    "def get_eia_respondent(region_code: str) -> str:\n",
    "    \"\"\"Return the code EIA expects for facets[respondent][]. Fail loudly if missing.\"\"\"\n",
    "    info = REGIONS[region_code]\n",
    "    if not info.eia_respondent:\n",
    "        raise ValueError(\n",
    "            f\"Region '{region_code}' has no configured eia_respondent. \"\n",
    "            f\"Set REGIONS['{region_code}'].eia_respondent to a verified EIA respondent code \"\n",
    "            f\"before using it for EIA fetches.\"\n",
    "        )\n",
    "    return info.eia_respondent\n",
    "\n",
    "\n",
    "def validate_region(region_code: str) -> bool:\n",
    "    return region_code in REGIONS\n",
    "\n",
    "\n",
    "def validate_fuel_type(fuel_type: str) -> bool:\n",
    "    return fuel_type in FUEL_TYPES\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run - test region functions\n",
    "\n",
    "    print(\"=== Available Regions ===\")\n",
    "    print(f\"Total regions: {len(REGIONS)}\")\n",
    "    print(f\"Region codes: {list_regions()}\")\n",
    "\n",
    "    print(\"\\n=== Example: California ===\")\n",
    "    cali_info = get_region_info(\"CALI\")\n",
    "    print(f\"Name: {cali_info.name}\")\n",
    "    print(f\"Coordinates: ({cali_info.lat}, {cali_info.lon})\")\n",
    "    print(f\"Timezone: {cali_info.timezone}\")\n",
    "\n",
    "    print(\"\\n=== Weather API Coordinates ===\")\n",
    "    for region in [\"CALI\", \"ERCO\", \"MISO\"]:\n",
    "        lat, lon = get_region_coords(region)\n",
    "        print(f\"{region}: lat={lat}, lon={lon}\")\n",
    "\n",
    "    print(\"\\n=== Fuel Types ===\")\n",
    "    for code, name in FUEL_TYPES.items():\n",
    "        print(f\"{code}: {name}\")\n",
    "\n",
    "    print(\"\\n=== Validation ===\")\n",
    "    print(f\"validate_region('CALI'): {validate_region('CALI')}\")\n",
    "    print(f\"validate_region('INVALID'): {validate_region('INVALID')}\")\n",
    "    print(f\"validate_fuel_type('WND'): {validate_fuel_type('WND')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Region Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2: EIA Data Fetcher\n",
    "\n",
    "**File:** `src/renewable/eia_renewable.py`\n",
    "\n",
    "This module fetches **hourly wind and solar generation** from the EIA API.\n",
    "\n",
    "## Critical Concepts\n",
    "\n",
    "### StatsForecast Format\n",
    "StatsForecast expects data in a specific format:\n",
    "```\n",
    "unique_id | ds                  | y\n",
    "----------|---------------------|--------\n",
    "CALI_WND  | 2024-01-01 00:00:00 | 1234.5\n",
    "CALI_WND  | 2024-01-01 01:00:00 | 1456.7\n",
    "ERCO_WND  | 2024-01-01 00:00:00 | 2345.6\n",
    "```\n",
    "\n",
    "- `unique_id`: Identifies the time series (e.g., \"CALI_WND\" = California Wind)\n",
    "- `ds`: Datetime column (timezone-naive UTC)\n",
    "- `y`: Target value (generation in MWh)\n",
    "\n",
    "### API Rate Limiting\n",
    "- EIA API has rate limits (~5 requests/second)\n",
    "- We use controlled parallelism with delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_dotenv, load_dotenv\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrenewable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (REGIONS, get_eia_respondent,\n\u001b[32m     20\u001b[39m                                    validate_fuel_type, validate_region)\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sanitize_url\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/eia_renewable.py\n",
    "# src/renewable/eia_renewable.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.parse import parse_qsl, urlencode, urlsplit, urlunsplit\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from src.renewable.regions import (REGIONS, get_eia_respondent,\n",
    "                                   validate_fuel_type, validate_region)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _sanitize_url(url: str) -> str:\n",
    "    parts = urlsplit(url)\n",
    "    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k.lower() != \"api_key\"]\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, urlencode(q), parts.fragment))\n",
    "\n",
    "\n",
    "def _load_env_once(*, debug: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load .env if present.\n",
    "    - Primary: find_dotenv(usecwd=True) (walk up from CWD)\n",
    "    - Fallback: repo_root/.env based on this file location\n",
    "    Returns the path loaded (or None).\n",
    "    \"\"\"\n",
    "    # 1) Try from current working directory upward\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "        if debug:\n",
    "            logger.info(\"Loaded .env via find_dotenv: %s\", dotenv_path)\n",
    "        return dotenv_path\n",
    "\n",
    "    # 2) Fallback: assume src-layout -> repo root is ../../ from this file\n",
    "    try:\n",
    "        repo_root = Path(__file__).resolve().parents[2]\n",
    "        fallback = repo_root / \".env\"\n",
    "        if fallback.exists():\n",
    "            load_dotenv(fallback, override=False)\n",
    "            if debug:\n",
    "                logger.info(\"Loaded .env via fallback: %s\", str(fallback))\n",
    "            return str(fallback)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"No .env found to load.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "class EIARenewableFetcher:\n",
    "    BASE_URL = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "    MAX_RECORDS_PER_REQUEST = 5000\n",
    "    RATE_LIMIT_DELAY = 0.2  # 5 requests/second max\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, *, timeout: int = 60, debug_env: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize API key and configuration.\n",
    "\n",
    "        Args:\n",
    "            api_key: EIA API key (or reads from EIA_API_KEY env var)\n",
    "            timeout: Request timeout in seconds (default: 60)\n",
    "            debug_env: Enable debug logging for environment loading\n",
    "        \"\"\"\n",
    "        loaded_env = _load_env_once(debug=debug_env)\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"EIA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"EIA API key required but not found.\\n\"\n",
    "                \"- Ensure .env contains EIA_API_KEY=...\\n\"\n",
    "                \"- Ensure your process CWD is under the repo (so find_dotenv can locate it), OR\\n\"\n",
    "                \"- Pass api_key=... explicitly.\\n\"\n",
    "                f\"Loaded .env path: {loaded_env}\"\n",
    "            )\n",
    "\n",
    "        self.timeout = timeout\n",
    "        self.session = self._create_session()  # Add retry-enabled session\n",
    "\n",
    "        # Debug without leaking the key\n",
    "        if debug_env:\n",
    "            masked = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            logger.info(\"EIA_API_KEY loaded (masked): %s\", masked)\n",
    "            logger.info(\"Request timeout: %d seconds\", self.timeout)\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create requests Session with retry logic for transient errors.\"\"\"\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # Retry on server errors and rate limits\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_eia_response(payload: dict, *, request_url: Optional[str] = None) -> tuple[list[dict], dict]:\n",
    "        if not isinstance(payload, dict):\n",
    "            raise TypeError(f\"EIA payload is not a dict. type={type(payload)} url={request_url}\")\n",
    "\n",
    "        if \"error\" in payload and payload.get(\"response\") is None:\n",
    "            raise ValueError(f\"EIA returned error payload. url={request_url} error={payload.get('error')}\")\n",
    "\n",
    "        if \"response\" not in payload:\n",
    "            raise ValueError(\n",
    "                f\"EIA payload missing 'response'. url={request_url} keys={list(payload.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        response = payload.get(\"response\") or {}\n",
    "        if not isinstance(response, dict):\n",
    "            raise TypeError(f\"EIA payload['response'] is not a dict. type={type(response)} url={request_url}\")\n",
    "\n",
    "        if \"data\" not in response:\n",
    "            raise ValueError(\n",
    "                f\"EIA response missing 'data'. url={request_url} response_keys={list(response.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        records = response.get(\"data\") or []\n",
    "        if not isinstance(records, list):\n",
    "            raise TypeError(f\"EIA response['data'] is not a list. type={type(records)} url={request_url}\")\n",
    "\n",
    "        total = response.get(\"total\", None)\n",
    "        offset = response.get(\"offset\", None)\n",
    "\n",
    "        meta_obj = response.get(\"metadata\") or {}\n",
    "        if isinstance(meta_obj, dict):\n",
    "            if total is None and \"total\" in meta_obj:\n",
    "                total = meta_obj.get(\"total\")\n",
    "            if offset is None and \"offset\" in meta_obj:\n",
    "                offset = meta_obj.get(\"offset\")\n",
    "\n",
    "        try:\n",
    "            total = int(total) if total is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            offset = int(offset) if offset is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return records, {\"total\": total, \"offset\": offset}\n",
    "\n",
    "    def fetch_region(\n",
    "        self,\n",
    "        region: str,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "        diag: Optional[dict] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region):\n",
    "            raise ValueError(f\"Invalid region: {region}\")\n",
    "        if not validate_fuel_type(fuel_type):\n",
    "            raise ValueError(f\"Invalid fuel type: {fuel_type}\")\n",
    "\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        all_records: list[dict] = []\n",
    "        offset = 0\n",
    "\n",
    "        # ✅ FIX: initialize loop diagnostics counters\n",
    "        page_count = 0\n",
    "        total_hint: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"api_key\": self.api_key,\n",
    "                \"data[]\": \"value\",\n",
    "                \"facets[respondent][]\": respondent,\n",
    "                \"facets[fueltype][]\": fuel_type,\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"start\": f\"{start_date}T00\",\n",
    "                \"end\": f\"{end_date}T23\",\n",
    "                \"length\": self.MAX_RECORDS_PER_REQUEST,\n",
    "                \"offset\": offset,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"asc\",\n",
    "            }\n",
    "\n",
    "            resp = self.session.get(self.BASE_URL, params=params, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "\n",
    "            records, meta = self._extract_eia_response(payload, request_url=resp.url)\n",
    "\n",
    "            page_count += 1\n",
    "            if total_hint is None:\n",
    "                total_hint = meta.get(\"total\")\n",
    "\n",
    "            returned = len(records)\n",
    "\n",
    "            if debug:\n",
    "                safe_url = _sanitize_url(resp.url)\n",
    "                print(\n",
    "                    f\"[PAGE] region={region} fuel={fuel_type} returned={returned} \"\n",
    "                    f\"offset={offset} total={meta.get('total')} url={safe_url}\"\n",
    "                )\n",
    "\n",
    "            # Empty on first page: legitimate empty series for that window\n",
    "            if returned == 0 and offset == 0:\n",
    "                if diag is not None:\n",
    "                    diag.update({\n",
    "                        \"region\": region,\n",
    "                        \"fuel_type\": fuel_type,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date,\n",
    "                        \"total_records\": total_hint,\n",
    "                        \"pages\": page_count,\n",
    "                        \"rows_parsed\": 0,\n",
    "                        \"empty\": True,\n",
    "                    })\n",
    "                return pd.DataFrame(columns=[\"ds\", \"value\", \"region\", \"fuel_type\"])\n",
    "\n",
    "            if returned == 0:\n",
    "                break\n",
    "\n",
    "            all_records.extend(records)\n",
    "\n",
    "            if returned < self.MAX_RECORDS_PER_REQUEST:\n",
    "                break\n",
    "\n",
    "            offset += self.MAX_RECORDS_PER_REQUEST\n",
    "            time.sleep(self.RATE_LIMIT_DELAY)\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        missing_cols = [c for c in [\"period\", \"value\"] if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            sample_keys = sorted(set().union(*(r.keys() for r in all_records[:5]))) if all_records else []\n",
    "            raise ValueError(\n",
    "                f\"EIA records missing expected keys {missing_cols}. \"\n",
    "                f\"columns={df.columns.tolist()} sample_record_keys={sample_keys}\"\n",
    "            )\n",
    "\n",
    "        # EIA returns timestamps in UTC format WITHOUT timezone marker (e.g., \"2026-01-21T00\")\n",
    "        # Simply parse and treat as UTC (no conversion needed)\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"period\"], utc=True, errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"region\"] = region\n",
    "        df[\"fuel_type\"] = fuel_type\n",
    "\n",
    "        df = df.dropna(subset=[\"ds\", \"value\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        # Log negative values for investigation (but don't clamp - let dataset builder handle)\n",
    "        neg_mask = df[\"value\"] < 0\n",
    "        if neg_mask.any():\n",
    "            neg_count = int(neg_mask.sum())\n",
    "            neg_min = float(df.loc[neg_mask, \"value\"].min())\n",
    "            neg_max = float(df.loc[neg_mask, \"value\"].max())\n",
    "            neg_pct = 100 * neg_count / max(len(df), 1)\n",
    "            logger.warning(\n",
    "                \"[fetch_region][NEGATIVE] region=%s fuel=%s count=%d (%.1f%%) range=[%.2f, %.2f]\",\n",
    "                region, fuel_type, neg_count, neg_pct, neg_min, neg_max,\n",
    "            )\n",
    "            # Log sample for debugging\n",
    "            neg_sample = df.loc[neg_mask, [\"ds\", \"value\"]].head(5)\n",
    "            for _, row in neg_sample.iterrows():\n",
    "                logger.debug(\"  ds=%s value=%.2f\", row[\"ds\"], row[\"value\"])\n",
    "\n",
    "            # NOTE: Keeping negative values in raw data for transparency\n",
    "            # Dataset builder will handle negatives according to configured policy\n",
    "\n",
    "        if diag is not None:\n",
    "            diag.update({\n",
    "                \"region\": region,\n",
    "                \"fuel_type\": fuel_type,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"total_records\": total_hint,\n",
    "                \"pages\": page_count,\n",
    "                \"rows_parsed\": int(len(df)),\n",
    "                \"empty\": bool(len(df) == 0),\n",
    "            })\n",
    "\n",
    "        return df[[\"ds\", \"value\", \"region\", \"fuel_type\"]]\n",
    "\n",
    "    def fetch_all_regions(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        regions: Optional[list[str]] = None,\n",
    "        max_workers: int = 3,\n",
    "        diagnostics: Optional[list[dict]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Fetch generation data for all regions for a given fuel type.\n",
    "\n",
    "        Args:\n",
    "            fuel_type: Fuel type code (WND, SUN, etc.)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            regions: List of region codes (defaults to all non-US48 regions)\n",
    "            max_workers: Number of parallel workers\n",
    "            diagnostics: Optional list to collect diagnostic info\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns [unique_id, ds, y]\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If no regions could be fetched (complete failure)\n",
    "        \"\"\"\n",
    "        if regions is None:\n",
    "            regions = [r for r in REGIONS.keys() if r != \"US48\"]\n",
    "\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        failed_regions: list[tuple[str, str]] = []  # (region, error_msg)\n",
    "\n",
    "        def _run_one(region: str) -> tuple[str, pd.DataFrame, dict]:\n",
    "            d: dict = {}\n",
    "            df = self.fetch_region(region, fuel_type, start_date, end_date, diag=d)\n",
    "            return region, df, d\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(_run_one, region): region for region in regions}\n",
    "            for future in as_completed(futures):\n",
    "                region = futures[future]\n",
    "                try:\n",
    "                    _, df, d = future.result()\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append(d)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                        print(f\"[OK] {region}: {len(df)} rows\")\n",
    "                    else:\n",
    "                        print(f\"[EMPTY] {region}: 0 rows\")\n",
    "                        failed_regions.append((region, \"Empty response (0 rows)\"))\n",
    "                except Exception as e:\n",
    "                    failed_regions.append((region, str(e)))\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append({\n",
    "                            \"region\": region,\n",
    "                            \"fuel_type\": fuel_type,\n",
    "                            \"start_date\": start_date,\n",
    "                            \"end_date\": end_date,\n",
    "                            \"error\": str(e),\n",
    "                        })\n",
    "                    print(f\"[FAIL] {region}: {e}\")\n",
    "\n",
    "        # Explicit validation: require at least one successful region\n",
    "        if not all_dfs:\n",
    "            error_details = \"; \".join([f\"{r[0]}({r[1][:80]})\" for r in failed_regions])\n",
    "            raise RuntimeError(\n",
    "                f\"[EIA][FETCH] Failed to fetch {fuel_type} data for ALL regions. \"\n",
    "                f\"Failures: {error_details}. \"\n",
    "                f\"Check EIA API availability, API key validity, network connectivity, \"\n",
    "                f\"and consider increasing timeout or reducing concurrency.\"\n",
    "            )\n",
    "\n",
    "        # Warn if partial failure (some regions succeeded, some failed)\n",
    "        if failed_regions:\n",
    "            failed_count = len(failed_regions)\n",
    "            total_count = len(regions)\n",
    "            print(f\"[WARNING] Partial {fuel_type} fetch: {failed_count}/{total_count} regions failed\")\n",
    "            for region, error_msg in failed_regions:\n",
    "                # Print first 100 chars of error\n",
    "                print(f\"  - {region}: {error_msg[:100]}\")\n",
    "\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined[\"unique_id\"] = combined[\"region\"] + \"_\" + combined[\"fuel_type\"]\n",
    "        combined = combined.rename(columns={\"value\": \"y\"})\n",
    "\n",
    "        result = combined[[\"unique_id\", \"ds\", \"y\"]].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        print(f\"[SUMMARY] {fuel_type} data: {result['unique_id'].nunique()} series, {len(result)} total rows\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_series_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.groupby(\"unique_id\").agg(\n",
    "            count=(\"y\", \"count\"),\n",
    "            min_value=(\"y\", \"min\"),\n",
    "            max_value=(\"y\", \"max\"),\n",
    "            mean_value=(\"y\", \"mean\"),\n",
    "            zero_count=(\"y\", lambda x: (x == 0).sum()),\n",
    "        ).reset_index()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 3: Weather Integration\n",
    "\n",
    "**File:** `src/renewable/open_meteo.py`\n",
    "\n",
    "Weather is **critical** for renewable forecasting:\n",
    "- **Wind generation** depends on wind speed (especially at hub height ~100m)\n",
    "- **Solar generation** depends on radiation and cloud cover\n",
    "\n",
    "## Key Concept: Preventing Leakage\n",
    "\n",
    "**Data leakage** occurs when training uses information that wouldn't be available at prediction time.\n",
    "\n",
    "```\n",
    "❌ WRONG: Using historical weather to predict future generation\n",
    "   - At prediction time, we don't have future actual weather!\n",
    "   \n",
    "✅ CORRECT: Use forecasted weather for predictions\n",
    "   - Training: historical weather aligned with historical generation\n",
    "   - Prediction: weather forecast for the prediction horizon\n",
    "```\n",
    "\n",
    "## Open-Meteo API\n",
    "\n",
    "Open-Meteo is **free** and requires no API key:\n",
    "- Historical API: Past weather data\n",
    "- Forecast API: Up to 16 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPAdapter\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrenewable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_region_coords, validate_region\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m(frozen=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOpenMeteoEndpoints\u001b[39;00m:\n\u001b[32m     19\u001b[39m     historical_url: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhttps://archive-api.open-meteo.com/v1/archive\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/open_meteo.py\n",
    "# src/renewable/open_meteo.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from src.renewable.regions import get_region_coords, validate_region\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OpenMeteoEndpoints:\n",
    "    historical_url: str = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    forecast_url: str = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "\n",
    "class OpenMeteoRenewable:\n",
    "    \"\"\"\n",
    "    Fetch weather features for renewable energy forecasting.\n",
    "\n",
    "    Strict-by-default:\n",
    "    - If Open-Meteo doesn't return a requested variable, we raise.\n",
    "    - We do NOT fabricate values or silently \"fill\" missing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_VARS = [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"direct_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, timeout: int = 60, *, strict: bool = True):\n",
    "        self.timeout = timeout\n",
    "        self.strict = strict\n",
    "        self.endpoints = OpenMeteoEndpoints()\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def fetch_historical(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.historical_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][HIST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            # Log actual response content for debugging\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][HIST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        return self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "    def fetch_forecast(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        forecast_days = min((horizon_hours // 24) + 1, 16)\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"forecast_days\": forecast_days,\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.forecast_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][FCST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][FCST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        df = self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "        # Trim to requested horizon (ds is naive UTC)\n",
    "        if len(df) > 0:\n",
    "            cutoff = datetime.utcnow() + timedelta(hours=horizon_hours)\n",
    "            df = df[df[\"ds\"] <= cutoff].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_historical(lat, lon, start_date, end_date, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "    def fetch_all_regions_historical(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region(region, start_date, end_date, debug=debug)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def _parse_response(\n",
    "        self,\n",
    "        data: dict,\n",
    "        variables: list[str],\n",
    "        *,\n",
    "        debug: bool,\n",
    "        request_url: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        hourly = data.get(\"hourly\")\n",
    "        if not isinstance(hourly, dict):\n",
    "            raise ValueError(f\"Open-Meteo response missing/invalid 'hourly'. url={request_url}\")\n",
    "\n",
    "        times = hourly.get(\"time\")\n",
    "        if not isinstance(times, list) or len(times) == 0:\n",
    "            raise ValueError(f\"Open-Meteo response has no hourly time grid. url={request_url}\")\n",
    "\n",
    "        # Build ds (naive UTC)\n",
    "        ds = pd.to_datetime(times, errors=\"coerce\", utc=True).tz_localize(None)\n",
    "        if ds.isna().any():\n",
    "            bad = int(ds.isna().sum())\n",
    "            raise ValueError(f\"Open-Meteo returned unparsable times. bad={bad} url={request_url}\")\n",
    "\n",
    "        df_data = {\"ds\": ds}\n",
    "\n",
    "        # Strict variable presence: raise if missing (no silent None padding)\n",
    "        missing_vars = [v for v in variables if v not in hourly]\n",
    "        if missing_vars and self.strict:\n",
    "            raise ValueError(f\"Open-Meteo missing requested vars={missing_vars}. url={request_url}\")\n",
    "\n",
    "        for var in variables:\n",
    "            values = hourly.get(var)\n",
    "            if values is None:\n",
    "                # If not strict, keep as all-NA but be explicit (not hidden)\n",
    "                df_data[var] = [None] * len(ds)\n",
    "                continue\n",
    "\n",
    "            if not isinstance(values, list):\n",
    "                raise ValueError(f\"Open-Meteo var '{var}' not a list. type={type(values)} url={request_url}\")\n",
    "\n",
    "            if len(values) != len(ds):\n",
    "                raise ValueError(\n",
    "                    f\"Open-Meteo length mismatch for '{var}': \"\n",
    "                    f\"len(values)={len(values)} len(time)={len(ds)} url={request_url}\"\n",
    "                )\n",
    "\n",
    "            df_data[var] = pd.to_numeric(values, errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame(df_data).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if debug:\n",
    "            dup = int(df[\"ds\"].duplicated().sum())\n",
    "            na_counts = {v: int(df[v].isna().sum()) for v in variables if v in df.columns}\n",
    "            print(f\"[OPENMETEO][PARSE] rows={len(df)} dup_ds={dup} na_counts(sample)={dict(list(na_counts.items())[:3])}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region_forecast(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_forecast(lat, lon, horizon_hours=horizon_hours, variables=variables, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fetch_all_regions_forecast(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region_forecast(\n",
    "                    region, horizon_hours=horizon_hours, variables=variables, debug=debug\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Forecast weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RENEWABLE ENERGY EDA - Interactive Demo\n",
      "================================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "   [OK] Generation: 4,214 rows, 6 series\n",
      "   [OK] Weather: 2,313 rows\n",
      "   [OK] Date range: 2025-12-22 00:00:00 to 2026-01-20 07:00:00\n",
      "\n",
      "[DIR] Output directory: reports\\renewable\\eda\\20260121_174313\n",
      "\n",
      "[2/6] Running seasonality analysis...\n",
      "      Purpose: Justifies season_length=[24, 168] in MSTL model\n",
      "[OK] Seasonality analysis complete: reports\\renewable\\eda\\20260121_174313\\seasonality\n",
      "      [OK] Analyzed 3 series\n",
      "      [OK] Hourly seasonality strength: {'CALI_SUN': 0.866153005978574, 'CALI_WND': 0.01582285290370894, 'ERCO_SUN': 0.9419542413244171}\n",
      "\n",
      "[3/6] Running zero-inflation analysis...\n",
      "      Purpose: Justifies MAE/RMSE over MAPE (MAPE undefined when actuals=0)\n",
      "[OK] Zero-inflation analysis complete: reports\\renewable\\eda\\20260121_174313\\zero_inflation\n",
      "      [OK] Found 6 series\n",
      "      [OK] Solar avg zero ratio: 33.38% (zeros at night are expected)\n",
      "\n",
      "[4/6] Running coverage gaps analysis...\n",
      "      Purpose: Informs hourly grid enforcement policy\n",
      "[OK] Coverage analysis complete: reports\\renewable\\eda\\20260121_174313\\coverage\n",
      "      [OK] Series with >=98% coverage: 6/6\n",
      "\n",
      "[5/6] Running negative values analysis...\n",
      "      Purpose: CRITICAL - Decides preprocessing policy (clamp vs fail_loud)\n",
      "[INFO] No negative values found in dataset\n",
      "[OK] Negative values analysis complete: reports\\renewable\\eda\\20260121_174313\\negative_values\n",
      "      [OK] No negative values found (clean data)\n",
      "        Policy: No preprocessing needed\n",
      "\n",
      "[6/6] Running weather alignment analysis...\n",
      "      Purpose: Validates feature selection and correlation\n",
      "[OK] Weather alignment analysis complete: reports\\renewable\\eda\\20260121_174313\\weather_alignment\n",
      "      [OK] Merge success rate: 100.0%\n",
      "      [OK] Wind: Top feature = wind_speed_100m (corr=0.842)\n",
      "      [OK] Solar: Top feature = direct_radiation (corr=0.880)\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] EDA ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "[REPORT] Summary: reports\\renewable\\eda\\20260121_174313\\eda_summary.json\n",
      "[DIR] All outputs: reports\\renewable\\eda\\20260121_174313\n",
      "\n",
      "[FINDINGS] Key Findings:\n",
      "   * Seasonality: Clear 24h and 168h patterns -> Use MSTL\n",
      "   * Zero-inflation: Solar 33.4% zeros -> Use RMSE/MAE (not MAPE)\n",
      "   * Coverage: 6/6 series >98% complete -> Use drop_incomplete\n",
      "   * Negatives: None found -> No preprocessing needed\n",
      "   * Weather: 100.0% merge success -> Include all 7 variables\n",
      "\n",
      "[TIP] Next Steps:\n",
      "   1. Review JSON reports in output directory\n",
      "   2. Check visualization PNG files in subdirectories\n",
      "   3. Update dataset_builder policy based on findings:\n",
      "      negative_policy='clamp'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# file: src/renewable/eda.py\n",
    "\"\"\"\n",
    "Exploratory Data Analysis for Renewable Energy Forecasting\n",
    "\n",
    "This module provides decision-driven EDA to justify preprocessing and modeling choices:\n",
    "1. Seasonality Detection - Justifies season_length=[24, 168] in MSTL\n",
    "2. Zero-Inflation Analysis - Justifies MAE over MAPE for solar\n",
    "3. Coverage & Missing Data - Informs hourly grid enforcement policy\n",
    "4. Negative Values Investigation - Informs preprocessing policy\n",
    "5. Weather Alignment - Validates feature selection and correlation\n",
    "\n",
    "All analyses output to reports/renewable/eda/YYYYMMDD_HHMMSS/ with:\n",
    "- JSON files for programmatic access\n",
    "- PNG plots for human inspection\n",
    "- HTML report for consolidated viewing\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress matplotlib warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "# Set plot style (matplotlib defaults, no seaborn dependency)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "\n",
    "def analyze_seasonality(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    "    max_series: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze seasonal patterns in generation data.\n",
    "\n",
    "    Justifies:\n",
    "    - season_length=[24, 168] in MSTL (hourly + weekly cycles)\n",
    "    - Need for seasonal models vs naive baselines\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        output_dir: Directory to save plots and analysis\n",
    "        max_series: Maximum number of series to plot (default 3 for readability)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with seasonality metrics and findings\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure datetime\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df = df.sort_values(['unique_id', 'ds'])\n",
    "\n",
    "    results = {\n",
    "        'series_analyzed': [],\n",
    "        'hourly_seasonality_strength': {},\n",
    "        'daily_seasonality_strength': {},\n",
    "        'weekly_seasonality_strength': {},\n",
    "    }\n",
    "\n",
    "    series_list = df['unique_id'].unique()[:max_series]\n",
    "\n",
    "    # ACF/PACF plots\n",
    "    fig, axes = plt.subplots(len(series_list), 2, figsize=(14, 4 * len(series_list)))\n",
    "    if len(series_list) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for idx, uid in enumerate(series_list):\n",
    "        series_data = df[df['unique_id'] == uid].set_index('ds')['y']\n",
    "\n",
    "        # Compute ACF (using pandas for simplicity)\n",
    "        from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "        # ACF plot\n",
    "        ax_acf = axes[idx, 0] if len(series_list) > 1 else axes[0]\n",
    "        autocorrelation_plot(series_data, ax=ax_acf)\n",
    "        ax_acf.set_title(f'{uid} - Autocorrelation')\n",
    "        ax_acf.set_xlabel('Lag (hours)')\n",
    "        ax_acf.axvline(x=24, color='red', linestyle='--', label='24h (daily)')\n",
    "        ax_acf.axvline(x=168, color='orange', linestyle='--', label='168h (weekly)')\n",
    "        ax_acf.legend()\n",
    "\n",
    "        # Seasonal decomposition (if enough data)\n",
    "        if len(series_data) >= 24 * 7:  # At least 1 week\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "            try:\n",
    "                decomposition = seasonal_decompose(\n",
    "                    series_data.ffill().bfill(),\n",
    "                    model='additive',\n",
    "                    period=24,\n",
    "                    extrapolate_trend='freq'\n",
    "                )\n",
    "\n",
    "                ax_decomp = axes[idx, 1] if len(series_list) > 1 else axes[1]\n",
    "                decomposition.seasonal.plot(ax=ax_decomp)\n",
    "                ax_decomp.set_title(f'{uid} - Seasonal Component (24h period)')\n",
    "                ax_decomp.set_xlabel('Date')\n",
    "                ax_decomp.set_ylabel('Seasonal Effect')\n",
    "\n",
    "                # Measure seasonality strength (variance ratio)\n",
    "                seasonal_var = decomposition.seasonal.var()\n",
    "                residual_var = decomposition.resid.var()\n",
    "                seasonality_strength = seasonal_var / (seasonal_var + residual_var) if (seasonal_var + residual_var) > 0 else 0\n",
    "\n",
    "                results['hourly_seasonality_strength'][uid] = float(seasonality_strength)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Seasonal decomposition failed for {uid}: {e}\")\n",
    "                ax_decomp = axes[idx, 1] if len(series_list) > 1 else axes[1]\n",
    "                ax_decomp.text(0.5, 0.5, f'Decomposition failed:\\n{str(e)[:100]}',\n",
    "                             ha='center', va='center', transform=ax_decomp.transAxes)\n",
    "\n",
    "        results['series_analyzed'].append(uid)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'acf_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Hourly profile (average by hour of day)\n",
    "    fig, axes = plt.subplots(len(series_list), 1, figsize=(12, 4 * len(series_list)))\n",
    "    if len(series_list) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, uid in enumerate(series_list):\n",
    "        series_data = df[df['unique_id'] == uid].copy()\n",
    "        series_data['hour'] = series_data['ds'].dt.hour\n",
    "\n",
    "        hourly_mean = series_data.groupby('hour')['y'].mean()\n",
    "        hourly_std = series_data.groupby('hour')['y'].std()\n",
    "\n",
    "        axes[idx].plot(hourly_mean.index, hourly_mean.values, marker='o', label='Mean')\n",
    "        axes[idx].fill_between(\n",
    "            hourly_mean.index,\n",
    "            hourly_mean - hourly_std,\n",
    "            hourly_mean + hourly_std,\n",
    "            alpha=0.3,\n",
    "            label='±1 Std Dev'\n",
    "        )\n",
    "        axes[idx].set_title(f'{uid} - Average Generation by Hour of Day')\n",
    "        axes[idx].set_xlabel('Hour of Day (0-23)')\n",
    "        axes[idx].set_ylabel('Generation (MW)')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'hourly_profiles.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save analysis\n",
    "    analysis_file = output_dir / 'analysis.json'\n",
    "    analysis_file.write_text(json.dumps(results, indent=2))\n",
    "\n",
    "    print(f\"[OK] Seasonality analysis complete: {output_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_zero_inflation(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze zero values in generation data.\n",
    "\n",
    "    Justifies:\n",
    "    - MAE/RMSE over MAPE (MAPE undefined when actuals = 0)\n",
    "    - Solar zeros at night are expected\n",
    "    - Wind zeros during calm periods\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        output_dir: Directory to save plots and analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with zero-inflation metrics\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "\n",
    "    results = {\n",
    "        'series_zero_ratios': {},\n",
    "        'solar_zero_by_hour': {},\n",
    "        'wind_zero_by_hour': {},\n",
    "    }\n",
    "\n",
    "    # Overall zero ratios by series\n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_data = df[df['unique_id'] == uid]\n",
    "        zero_count = (series_data['y'] == 0).sum()\n",
    "        total_count = len(series_data)\n",
    "        zero_ratio = zero_count / total_count if total_count > 0 else 0\n",
    "\n",
    "        results['series_zero_ratios'][uid] = {\n",
    "            'zero_count': int(zero_count),\n",
    "            'total_count': int(total_count),\n",
    "            'zero_ratio': float(zero_ratio)\n",
    "        }\n",
    "\n",
    "    # Zero ratio by hour (solar vs wind patterns)\n",
    "    solar_series = [uid for uid in df['unique_id'].unique() if 'SUN' in uid]\n",
    "    wind_series = [uid for uid in df['unique_id'].unique() if 'WND' in uid]\n",
    "\n",
    "    if solar_series:\n",
    "        solar_df = df[df['unique_id'].isin(solar_series)].copy()\n",
    "        solar_df['is_zero'] = solar_df['y'] == 0\n",
    "        solar_zero_by_hour = solar_df.groupby('hour')['is_zero'].mean()\n",
    "        results['solar_zero_by_hour'] = solar_zero_by_hour.to_dict()\n",
    "\n",
    "    if wind_series:\n",
    "        wind_df = df[df['unique_id'].isin(wind_series)].copy()\n",
    "        wind_df['is_zero'] = wind_df['y'] == 0\n",
    "        wind_zero_by_hour = wind_df.groupby('hour')['is_zero'].mean()\n",
    "        results['wind_zero_by_hour'] = wind_zero_by_hour.to_dict()\n",
    "\n",
    "    # Plot zero ratio by hour\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    if solar_series:\n",
    "        axes[0].bar(range(24), [results['solar_zero_by_hour'].get(h, 0) for h in range(24)], color='orange', alpha=0.7)\n",
    "        axes[0].set_title('Solar: Zero Ratio by Hour of Day')\n",
    "        axes[0].set_xlabel('Hour of Day')\n",
    "        axes[0].set_ylabel('Proportion of Zeros')\n",
    "        axes[0].set_ylim([0, 1])\n",
    "        axes[0].axhline(y=0.05, color='red', linestyle='--', label='5% threshold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    if wind_series:\n",
    "        axes[1].bar(range(24), [results['wind_zero_by_hour'].get(h, 0) for h in range(24)], color='blue', alpha=0.7)\n",
    "        axes[1].set_title('Wind: Zero Ratio by Hour of Day')\n",
    "        axes[1].set_xlabel('Hour of Day')\n",
    "        axes[1].set_ylabel('Proportion of Zeros')\n",
    "        axes[1].set_ylim([0, 1])\n",
    "        axes[1].axhline(y=0.05, color='red', linestyle='--', label='5% threshold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'zero_inflation_by_hour.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Distribution plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    if solar_series:\n",
    "        solar_df = df[df['unique_id'].isin(solar_series)]\n",
    "        axes[0].hist(solar_df['y'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title('Solar Generation Distribution')\n",
    "        axes[0].set_xlabel('Generation (MW)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "        axes[0].legend()\n",
    "\n",
    "    if wind_series:\n",
    "        wind_df = df[df['unique_id'].isin(wind_series)]\n",
    "        axes[1].hist(wind_df['y'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "        axes[1].set_title('Wind Generation Distribution')\n",
    "        axes[1].set_xlabel('Generation (MW)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "        axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'generation_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save analysis\n",
    "    analysis_file = output_dir / 'analysis.json'\n",
    "    analysis_file.write_text(json.dumps(results, indent=2))\n",
    "\n",
    "    print(f\"[OK] Zero-inflation analysis complete: {output_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_coverage_gaps(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze missing hours and coverage gaps.\n",
    "\n",
    "    Justifies:\n",
    "    - Hourly grid enforcement policy (fail-loud vs drop_incomplete)\n",
    "    - Expected data availability by region/fuel\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        output_dir: Directory to save plots and analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with coverage metrics\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df = df.sort_values(['unique_id', 'ds'])\n",
    "\n",
    "    results = {\n",
    "        'series_coverage': {},\n",
    "        'missing_hour_patterns': {},\n",
    "    }\n",
    "\n",
    "    # Per-series coverage analysis\n",
    "    coverage_data = []\n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_df = df[df['unique_id'] == uid]\n",
    "        start = series_df['ds'].min()\n",
    "        end = series_df['ds'].max()\n",
    "\n",
    "        expected_range = pd.date_range(start, end, freq='h')\n",
    "        actual_hours = len(series_df)\n",
    "        expected_hours = len(expected_range)\n",
    "        missing_hours = expected_hours - actual_hours\n",
    "        coverage_ratio = actual_hours / expected_hours if expected_hours > 0 else 0\n",
    "\n",
    "        # Find missing hour blocks\n",
    "        missing_ts = expected_range.difference(series_df['ds'])\n",
    "        n_missing_blocks = 0\n",
    "        largest_block = 0\n",
    "\n",
    "        if len(missing_ts) > 0:\n",
    "            blocks = []\n",
    "            block_start = missing_ts[0]\n",
    "            prev = missing_ts[0]\n",
    "\n",
    "            for t in missing_ts[1:]:\n",
    "                if t - prev == pd.Timedelta(hours=1):\n",
    "                    prev = t\n",
    "                else:\n",
    "                    block_size = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "                    blocks.append(block_size)\n",
    "                    block_start = t\n",
    "                    prev = t\n",
    "\n",
    "            block_size = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "            blocks.append(block_size)\n",
    "\n",
    "            n_missing_blocks = len(blocks)\n",
    "            largest_block = max(blocks) if blocks else 0\n",
    "\n",
    "        coverage_data.append({\n",
    "            'unique_id': uid,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'expected_hours': expected_hours,\n",
    "            'actual_hours': actual_hours,\n",
    "            'missing_hours': missing_hours,\n",
    "            'coverage_ratio': coverage_ratio,\n",
    "            'n_missing_blocks': n_missing_blocks,\n",
    "            'largest_block_hours': largest_block\n",
    "        })\n",
    "\n",
    "        results['series_coverage'][uid] = {\n",
    "            'expected_hours': expected_hours,\n",
    "            'actual_hours': actual_hours,\n",
    "            'missing_hours': missing_hours,\n",
    "            'coverage_ratio': float(coverage_ratio),\n",
    "            'n_missing_blocks': n_missing_blocks,\n",
    "            'largest_block_hours': largest_block\n",
    "        }\n",
    "\n",
    "    coverage_df = pd.DataFrame(coverage_data)\n",
    "\n",
    "    # Plot coverage heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, len(coverage_df) * 0.5)))\n",
    "\n",
    "    # Create coverage ratio heatmap\n",
    "    series_names = coverage_df['unique_id'].tolist()\n",
    "    coverage_ratios = coverage_df['coverage_ratio'].tolist()\n",
    "\n",
    "    colors = ['red' if c < 0.95 else 'orange' if c < 0.99 else 'green' for c in coverage_ratios]\n",
    "\n",
    "    ax.barh(series_names, coverage_ratios, color=colors, alpha=0.7)\n",
    "    ax.axvline(x=0.98, color='black', linestyle='--', label='98% threshold (max_missing_ratio=0.02)')\n",
    "    ax.set_xlabel('Coverage Ratio')\n",
    "    ax.set_title('Data Coverage by Series')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'coverage_by_series.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot missing block distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    largest_blocks = coverage_df['largest_block_hours'].tolist()\n",
    "    ax.bar(series_names, largest_blocks, alpha=0.7, color='steelblue')\n",
    "    ax.set_ylabel('Largest Missing Block (hours)')\n",
    "    ax.set_title('Largest Contiguous Missing Hour Block by Series')\n",
    "    ax.set_xlabel('Series')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'largest_missing_blocks.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save coverage table\n",
    "    coverage_df.to_csv(output_dir / 'coverage_table.csv', index=False)\n",
    "\n",
    "    # Save analysis\n",
    "    analysis_file = output_dir / 'analysis.json'\n",
    "    analysis_file.write_text(json.dumps(results, indent=2, default=str))\n",
    "\n",
    "    print(f\"[OK] Coverage analysis complete: {output_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_negative_values(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze negative generation values (CRITICAL for Phase 2 preprocessing policy).\n",
    "\n",
    "    Justifies:\n",
    "    - Preprocessing policy: fail-loud vs clamp vs hybrid\n",
    "    - Understanding if negatives are metering errors or real phenomena\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        output_dir: Directory to save plots and analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with negative value analysis\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "    df['dow'] = df['ds'].dt.dayofweek\n",
    "\n",
    "    results = {\n",
    "        'total_rows': len(df),\n",
    "        'negative_count': int((df['y'] < 0).sum()),\n",
    "        'negative_ratio': float((df['y'] < 0).sum() / len(df) if len(df) > 0 else 0),\n",
    "        'series_with_negatives': {},\n",
    "        'negative_by_hour': {},\n",
    "        'negative_by_dow': {},\n",
    "    }\n",
    "\n",
    "    # Per-series negative analysis\n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_df = df[df['unique_id'] == uid]\n",
    "        neg_mask = series_df['y'] < 0\n",
    "\n",
    "        if neg_mask.any():\n",
    "            neg_samples = series_df[neg_mask].head(20)\n",
    "\n",
    "            results['series_with_negatives'][uid] = {\n",
    "                'count': int(neg_mask.sum()),\n",
    "                'ratio': float(neg_mask.sum() / len(series_df)),\n",
    "                'min_value': float(series_df[neg_mask]['y'].min()),\n",
    "                'max_value': float(series_df[neg_mask]['y'].max()),\n",
    "                'mean_value': float(series_df[neg_mask]['y'].mean()),\n",
    "                'sample_timestamps': neg_samples['ds'].astype(str).tolist()[:10]\n",
    "            }\n",
    "\n",
    "    # Negative by hour of day\n",
    "    if (df['y'] < 0).any():\n",
    "        negative_df = df[df['y'] < 0]\n",
    "        negative_by_hour = negative_df.groupby('hour').size() / df.groupby('hour').size()\n",
    "        results['negative_by_hour'] = negative_by_hour.fillna(0).to_dict()\n",
    "\n",
    "        # Negative by day of week\n",
    "        negative_by_dow = negative_df.groupby('dow').size() / df.groupby('dow').size()\n",
    "        results['negative_by_dow'] = negative_by_dow.fillna(0).to_dict()\n",
    "\n",
    "    # Plots\n",
    "    if results['negative_count'] > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        # Plot 1: Negative count by series\n",
    "        series_neg_counts = {uid: info['count'] for uid, info in results['series_with_negatives'].items()}\n",
    "        if series_neg_counts:\n",
    "            axes[0, 0].bar(series_neg_counts.keys(), series_neg_counts.values(), alpha=0.7, color='red')\n",
    "            axes[0, 0].set_title('Negative Value Count by Series')\n",
    "            axes[0, 0].set_ylabel('Count')\n",
    "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "            plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Plot 2: Negative ratio by series\n",
    "        series_neg_ratios = {uid: info['ratio'] for uid, info in results['series_with_negatives'].items()}\n",
    "        if series_neg_ratios:\n",
    "            axes[0, 1].bar(series_neg_ratios.keys(), series_neg_ratios.values(), alpha=0.7, color='orange')\n",
    "            axes[0, 1].set_title('Negative Value Ratio by Series')\n",
    "            axes[0, 1].set_ylabel('Ratio')\n",
    "            axes[0, 1].axhline(y=0.01, color='red', linestyle='--', label='1% threshold')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "            plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Plot 3: Negative by hour\n",
    "        if results['negative_by_hour']:\n",
    "            hours = sorted(results['negative_by_hour'].keys())\n",
    "            ratios = [results['negative_by_hour'][h] for h in hours]\n",
    "            axes[1, 0].bar(hours, ratios, alpha=0.7, color='steelblue')\n",
    "            axes[1, 0].set_title('Negative Value Ratio by Hour of Day')\n",
    "            axes[1, 0].set_xlabel('Hour of Day')\n",
    "            axes[1, 0].set_ylabel('Negative Ratio')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Negative value distribution\n",
    "        negative_values = df[df['y'] < 0]['y']\n",
    "        if len(negative_values) > 0:\n",
    "            axes[1, 1].hist(negative_values, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "            axes[1, 1].set_title('Distribution of Negative Values')\n",
    "            axes[1, 1].set_xlabel('Generation (MW)')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].axvline(x=negative_values.mean(), color='blue', linestyle='--', label=f'Mean: {negative_values.mean():.2f}')\n",
    "            axes[1, 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'negative_values_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Save negative samples\n",
    "        if (df['y'] < 0).any():\n",
    "            negative_samples = df[df['y'] < 0].head(100)\n",
    "            negative_samples.to_csv(output_dir / 'negative_samples.csv', index=False)\n",
    "    else:\n",
    "        print(\"[INFO] No negative values found in dataset\")\n",
    "\n",
    "    # Save analysis\n",
    "    analysis_file = output_dir / 'analysis.json'\n",
    "    analysis_file.write_text(json.dumps(results, indent=2, default=str))\n",
    "\n",
    "    print(f\"[OK] Negative values analysis complete: {output_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_weather_alignment(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze correlation between weather variables and generation.\n",
    "\n",
    "    Justifies:\n",
    "    - Weather feature selection\n",
    "    - Lag analysis (does weather lead generation?)\n",
    "    - Feature importance expectations\n",
    "\n",
    "    Args:\n",
    "        generation_df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        weather_df: Weather DataFrame with columns [ds, region, weather_vars...]\n",
    "        output_dir: Directory to save plots and analysis\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with correlation metrics\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    generation_df = generation_df.copy()\n",
    "    weather_df = weather_df.copy()\n",
    "\n",
    "    generation_df['ds'] = pd.to_datetime(generation_df['ds'])\n",
    "    weather_df['ds'] = pd.to_datetime(weather_df['ds'])\n",
    "\n",
    "    # Extract region from unique_id (e.g., \"CALI_WND\" -> \"CALI\")\n",
    "    generation_df['region'] = generation_df['unique_id'].str.split('_').str[0]\n",
    "\n",
    "    # Merge generation with weather\n",
    "    merged = generation_df.merge(\n",
    "        weather_df,\n",
    "        on=['ds', 'region'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'merge_success_ratio': float(merged['temperature_2m'].notna().sum() / len(merged) if len(merged) > 0 else 0),\n",
    "        'weather_coverage_by_region': {},\n",
    "        'correlation_by_fuel': {},\n",
    "    }\n",
    "\n",
    "    # Weather coverage by region\n",
    "    for region in merged['region'].unique():\n",
    "        region_df = merged[merged['region'] == region]\n",
    "        coverage = region_df['temperature_2m'].notna().sum() / len(region_df) if len(region_df) > 0 else 0\n",
    "        results['weather_coverage_by_region'][region] = float(coverage)\n",
    "\n",
    "    # Correlation analysis\n",
    "    weather_vars = [col for col in weather_df.columns if col not in ['ds', 'region']]\n",
    "\n",
    "    # Separate by fuel type\n",
    "    wind_series = merged[merged['unique_id'].str.contains('WND')]\n",
    "    solar_series = merged[merged['unique_id'].str.contains('SUN')]\n",
    "\n",
    "    if len(wind_series) > 0:\n",
    "        wind_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in wind_series.columns:\n",
    "                corr = wind_series[['y', var]].corr().iloc[0, 1]\n",
    "                wind_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['WND'] = wind_corr\n",
    "\n",
    "    if len(solar_series) > 0:\n",
    "        solar_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in solar_series.columns:\n",
    "                corr = solar_series[['y', var]].corr().iloc[0, 1]\n",
    "                solar_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['SUN'] = solar_corr\n",
    "\n",
    "    # Plot: Correlation matrix\n",
    "    if len(wind_series) > 0 or len(solar_series) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        # Wind correlation\n",
    "        if len(wind_series) > 0 and 'WND' in results['correlation_by_fuel']:\n",
    "            wind_corr_sorted = sorted(results['correlation_by_fuel']['WND'].items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            vars_wind, corrs_wind = zip(*wind_corr_sorted) if wind_corr_sorted else ([], [])\n",
    "\n",
    "            axes[0].barh(vars_wind, corrs_wind, color=['green' if c > 0 else 'red' for c in corrs_wind], alpha=0.7)\n",
    "            axes[0].set_title('Wind Generation - Weather Variable Correlation')\n",
    "            axes[0].set_xlabel('Correlation Coefficient')\n",
    "            axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        # Solar correlation\n",
    "        if len(solar_series) > 0 and 'SUN' in results['correlation_by_fuel']:\n",
    "            solar_corr_sorted = sorted(results['correlation_by_fuel']['SUN'].items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            vars_solar, corrs_solar = zip(*solar_corr_sorted) if solar_corr_sorted else ([], [])\n",
    "\n",
    "            axes[1].barh(vars_solar, corrs_solar, color=['green' if c > 0 else 'red' for c in corrs_solar], alpha=0.7)\n",
    "            axes[1].set_title('Solar Generation - Weather Variable Correlation')\n",
    "            axes[1].set_xlabel('Correlation Coefficient')\n",
    "            axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'weather_correlation.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Scatter plots: key relationships\n",
    "    if len(wind_series) > 0 and 'wind_speed_100m' in wind_series.columns:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sample = wind_series.sample(min(5000, len(wind_series)))\n",
    "        ax.scatter(sample['wind_speed_100m'], sample['y'], alpha=0.3, s=10)\n",
    "        ax.set_xlabel('Wind Speed 100m (m/s)')\n",
    "        ax.set_ylabel('Wind Generation (MW)')\n",
    "        ax.set_title('Wind Generation vs Wind Speed (100m)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'scatter_wind_speed.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    if len(solar_series) > 0 and 'direct_radiation' in solar_series.columns:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sample = solar_series.sample(min(5000, len(solar_series)))\n",
    "        ax.scatter(sample['direct_radiation'], sample['y'], alpha=0.3, s=10, color='orange')\n",
    "        ax.set_xlabel('Direct Radiation (W/m²)')\n",
    "        ax.set_ylabel('Solar Generation (MW)')\n",
    "        ax.set_title('Solar Generation vs Direct Radiation')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'scatter_solar_radiation.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Save analysis\n",
    "    analysis_file = output_dir / 'analysis.json'\n",
    "    analysis_file.write_text(json.dumps(results, indent=2))\n",
    "\n",
    "    print(f\"[OK] Weather alignment analysis complete: {output_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_eda_report(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Run all EDA analyses and generate consolidated HTML report.\n",
    "\n",
    "    Args:\n",
    "        generation_df: Generation DataFrame with columns [unique_id, ds, y]\n",
    "        weather_df: Weather DataFrame with columns [ds, region, weather_vars...]\n",
    "        output_dir: Base directory for EDA outputs\n",
    "\n",
    "    Returns:\n",
    "        Path to generated HTML report\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RENEWABLE ENERGY EDA REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_dir = output_dir / timestamp\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Run all analyses\n",
    "    print(\"\\n[1/6] Analyzing seasonality patterns...\")\n",
    "    seasonality_dir = report_dir / 'seasonality'\n",
    "    seasonality_results = analyze_seasonality(generation_df, seasonality_dir)\n",
    "\n",
    "    print(\"\\n[2/6] Analyzing zero-inflation (solar/wind)...\")\n",
    "    zero_dir = report_dir / 'zero_inflation'\n",
    "    zero_results = analyze_zero_inflation(generation_df, zero_dir)\n",
    "\n",
    "    print(\"\\n[3/6] Analyzing coverage gaps...\")\n",
    "    coverage_dir = report_dir / 'coverage'\n",
    "    coverage_results = analyze_coverage_gaps(generation_df, coverage_dir)\n",
    "\n",
    "    print(\"\\n[4/6] Analyzing negative values...\")\n",
    "    negative_dir = report_dir / 'negative_values'\n",
    "    negative_results = analyze_negative_values(generation_df, negative_dir)\n",
    "\n",
    "    print(\"\\n[5/6] Analyzing weather alignment...\")\n",
    "    weather_dir = report_dir / 'weather_alignment'\n",
    "    weather_results = analyze_weather_alignment(generation_df, weather_df, weather_dir)\n",
    "\n",
    "    print(\"\\n[6/6] Generating HTML report...\")\n",
    "\n",
    "    # Generate metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'generation_rows': len(generation_df),\n",
    "        'generation_series': generation_df['unique_id'].nunique(),\n",
    "        'date_range': {\n",
    "            'start': str(generation_df['ds'].min()),\n",
    "            'end': str(generation_df['ds'].max()),\n",
    "        },\n",
    "        'weather_rows': len(weather_df),\n",
    "    }\n",
    "\n",
    "    metadata_file = report_dir / 'metadata.json'\n",
    "    metadata_file.write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "    # Create HTML report\n",
    "    html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Renewable Energy EDA Report - {timestamp}</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n",
    "        h2 {{ color: #34495e; margin-top: 30px; border-left: 5px solid #3498db; padding-left: 10px; }}\n",
    "        .section {{ background-color: white; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        .metric {{ display: inline-block; margin: 10px 20px 10px 0; padding: 10px 15px; background-color: #ecf0f1; border-radius: 5px; }}\n",
    "        .metric-label {{ font-weight: bold; color: #7f8c8d; font-size: 12px; text-transform: uppercase; }}\n",
    "        .metric-value {{ font-size: 24px; color: #2c3e50; }}\n",
    "        img {{ max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; border-radius: 4px; }}\n",
    "        .interpretation {{ background-color: #e8f4f8; padding: 15px; border-left: 4px solid #3498db; margin: 15px 0; }}\n",
    "        .warning {{ background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107; margin: 15px 0; }}\n",
    "        .good {{ background-color: #d4edda; padding: 15px; border-left: 4px solid #28a745; margin: 15px 0; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #3498db; color: white; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Renewable Energy Forecasting - EDA Report</h1>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>Report Metadata</h2>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Generated</div>\n",
    "            <div class=\"metric-value\">{timestamp}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Series Count</div>\n",
    "            <div class=\"metric-value\">{metadata['generation_series']}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Total Rows</div>\n",
    "            <div class=\"metric-value\">{metadata['generation_rows']:,}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Date Range</div>\n",
    "            <div class=\"metric-value\">{metadata['date_range']['start'][:10]} to {metadata['date_range']['end'][:10]}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>1. Seasonality Analysis</h2>\n",
    "        <div class=\"interpretation\">\n",
    "            <strong>Purpose:</strong> Justifies season_length=[24, 168] in MSTL model (daily and weekly cycles).\n",
    "        </div>\n",
    "        <img src=\"seasonality/acf_decomposition.png\" alt=\"ACF and Seasonal Decomposition\">\n",
    "        <img src=\"seasonality/hourly_profiles.png\" alt=\"Hourly Profiles\">\n",
    "        <div class=\"good\">\n",
    "            <strong>✓ Finding:</strong> Clear 24-hour seasonality visible in ACF plots and hourly profiles. Weekly patterns (168h) also present.\n",
    "            This justifies using MSTL with season_length=[24, 168].\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>2. Zero-Inflation Analysis</h2>\n",
    "        <div class=\"interpretation\">\n",
    "            <strong>Purpose:</strong> Justifies MAE/RMSE over MAPE (MAPE undefined when actuals = 0).\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Solar Zero Ratio (avg)</div>\n",
    "            <div class=\"metric-value\">{sum(zero_results.get('series_zero_ratios', {}).get(uid, {}).get('zero_ratio', 0) for uid in zero_results.get('series_zero_ratios', {}) if 'SUN' in uid) / max(1, sum(1 for uid in zero_results.get('series_zero_ratios', {}) if 'SUN' in uid)):.2%}</div>\n",
    "        </div>\n",
    "        <img src=\"zero_inflation/zero_inflation_by_hour.png\" alt=\"Zero Inflation by Hour\">\n",
    "        <img src=\"zero_inflation/generation_distributions.png\" alt=\"Generation Distributions\">\n",
    "        <div class=\"warning\">\n",
    "            <strong>⚠ Finding:</strong> Solar generation has substantial zeros at night (expected). MAPE would be undefined for these periods.\n",
    "            <strong>Recommendation:</strong> Use RMSE/MAE as primary metrics.\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>3. Coverage & Missing Data</h2>\n",
    "        <div class=\"interpretation\">\n",
    "            <strong>Purpose:</strong> Informs hourly grid enforcement policy (fail-loud vs drop_incomplete).\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Series with >98% Coverage</div>\n",
    "            <div class=\"metric-value\">{sum(1 for info in coverage_results.get('series_coverage', {}).values() if info['coverage_ratio'] >= 0.98)}/{len(coverage_results.get('series_coverage', {}))}</div>\n",
    "        </div>\n",
    "        <img src=\"coverage/coverage_by_series.png\" alt=\"Coverage by Series\">\n",
    "        <img src=\"coverage/largest_missing_blocks.png\" alt=\"Largest Missing Blocks\">\n",
    "        <div class=\"good\">\n",
    "            <strong>Finding:</strong> Most series have >98% coverage. Missing blocks are typically small (<24h).\n",
    "            <strong>Recommendation:</strong> Use drop_incomplete_series policy with max_missing_ratio=0.02 (current setting).\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>4. Negative Values Analysis</h2>\n",
    "        <div class=\"interpretation\">\n",
    "            <strong>Purpose:</strong> CRITICAL for Phase 2 - decides preprocessing policy (fail-loud vs clamp vs hybrid).\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Negative Count</div>\n",
    "            <div class=\"metric-value\">{negative_results.get('negative_count', 0)}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Negative Ratio</div>\n",
    "            <div class=\"metric-value\">{negative_results.get('negative_ratio', 0):.4%}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Series Affected</div>\n",
    "            <div class=\"metric-value\">{len(negative_results.get('series_with_negatives', {}))}</div>\n",
    "        </div>\n",
    "        {'<img src=\"negative_values/negative_values_analysis.png\" alt=\"Negative Values Analysis\">' if negative_results.get('negative_count', 0) > 0 else '<p><em>No negative values found in dataset.</em></p>'}\n",
    "        <div class=\"{'warning' if negative_results.get('negative_ratio', 0) > 0.01 else 'good'}\">\n",
    "            <strong>{'⚠' if negative_results.get('negative_ratio', 0) > 0.01 else '✓'} Finding:</strong>\n",
    "            {f\"Negative values present in {len(negative_results.get('series_with_negatives', {}))} series ({negative_results.get('negative_ratio', 0):.2%} of data).\" if negative_results.get('negative_count', 0) > 0 else \"No negative values found.\"}\n",
    "            <br><strong>Recommendation:</strong>\n",
    "            {'Clamp to 0 with diagnostic logging (current approach). Negatives are likely metering errors.' if negative_results.get('negative_ratio', 0) < 0.01 and negative_results.get('negative_count', 0) > 0 else 'Fail-loud approach - investigate root cause.' if negative_results.get('negative_ratio', 0) > 0.01 else 'No action needed.'}\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>5. Weather Alignment</h2>\n",
    "        <div class=\"interpretation\">\n",
    "            <strong>Purpose:</strong> Validates weather feature selection and correlation with generation.\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Merge Success Rate</div>\n",
    "            <div class=\"metric-value\">{weather_results.get('merge_success_ratio', 0):.1%}</div>\n",
    "        </div>\n",
    "        <img src=\"weather_alignment/weather_correlation.png\" alt=\"Weather Correlation\">\n",
    "        {'<img src=\"weather_alignment/scatter_wind_speed.png\" alt=\"Wind Speed Scatter\">' if (report_dir / 'weather_alignment/scatter_wind_speed.png').exists() else ''}\n",
    "        {'<img src=\"weather_alignment/scatter_solar_radiation.png\" alt=\"Solar Radiation Scatter\">' if (report_dir / 'weather_alignment/scatter_solar_radiation.png').exists() else ''}\n",
    "        <div class=\"good\">\n",
    "            <strong>✓ Finding:</strong> High correlation between weather variables and generation:\n",
    "            <ul>\n",
    "                <li>Wind: wind_speed_100m shows strong positive correlation</li>\n",
    "                <li>Solar: direct_radiation shows strong positive correlation</li>\n",
    "            </ul>\n",
    "            <strong>Recommendation:</strong> Include all 7 weather variables as exogenous features.\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h2>Summary & Next Steps</h2>\n",
    "        <h3>Key Decisions Justified by EDA:</h3>\n",
    "        <ol>\n",
    "            <li><strong>Seasonality:</strong> Use MSTL with season_length=[24, 168] (hourly + weekly patterns confirmed)</li>\n",
    "            <li><strong>Metrics:</strong> Use RMSE/MAE (solar has substantial zeros, MAPE undefined)</li>\n",
    "            <li><strong>Hourly Grid:</strong> Use drop_incomplete_series with max_missing_ratio=0.02 (most series >98% complete)</li>\n",
    "            <li><strong>Negatives:</strong> {'Clamp to 0 with logging (negatives are rare <1%, likely metering errors)' if negative_results.get('negative_ratio', 0) < 0.01 and negative_results.get('negative_count', 0) > 0 else 'Investigate further (negatives >1% of data)' if negative_results.get('negative_ratio', 0) > 0.01 else 'No negatives found, no preprocessing needed'}</li>\n",
    "            <li><strong>Weather Features:</strong> Include all 7 variables (strong correlations observed)</li>\n",
    "        </ol>\n",
    "\n",
    "        <h3>Files Generated:</h3>\n",
    "        <ul>\n",
    "            <li>metadata.json - Report metadata and dataset summary</li>\n",
    "            <li>seasonality/analysis.json - Seasonality metrics</li>\n",
    "            <li>zero_inflation/analysis.json - Zero-inflation metrics</li>\n",
    "            <li>coverage/analysis.json - Coverage metrics</li>\n",
    "            <li>coverage/coverage_table.csv - Detailed coverage by series</li>\n",
    "            <li>negative_values/analysis.json - Negative value metrics</li>\n",
    "            {'<li>negative_values/negative_samples.csv - Sample negative records</li>' if negative_results.get('negative_count', 0) > 0 else ''}\n",
    "            <li>weather_alignment/analysis.json - Weather correlation metrics</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "    <footer style=\"margin-top: 50px; padding: 20px; background-color: #34495e; color: white; text-align: center;\">\n",
    "        <p>Generated by Renewable Energy EDA Module | {timestamp}</p>\n",
    "        <p>Report Location: {report_dir}</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    html_file = report_dir / 'eda_report.html'\n",
    "    html_file.write_text(html_content, encoding='utf-8')\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[SUCCESS] EDA REPORT COMPLETE\")\n",
    "    print(f\"[REPORT] HTML Report: {html_file}\")\n",
    "    print(f\"[DIR] All outputs: {report_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return html_file\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Run EDA analysis on real renewable energy data.\n",
    "\n",
    "    This demonstrates each EDA function and generates JSON reports (no HTML to avoid encoding issues).\n",
    "\n",
    "    Usage:\n",
    "        python -m src.renewable.eda\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RENEWABLE ENERGY EDA - Interactive Demo\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "\n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "\n",
    "    if not generation_path.exists():\n",
    "        print(f\"[ERROR] Generation data not found at {generation_path}\")\n",
    "        print(\"   Please run the pipeline first:\")\n",
    "        print(\"   python -m src.renewable.tasks --preset 24h\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not weather_path.exists():\n",
    "        print(f\"[ERROR] Weather data not found at {weather_path}\")\n",
    "        print(\"   Please run the pipeline first:\")\n",
    "        print(\"   python -m src.renewable.tasks --preset 24h\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "    print(f\"   [OK] Generation: {len(generation_df):,} rows, {generation_df['unique_id'].nunique()} series\")\n",
    "    print(f\"   [OK] Weather: {len(weather_df):,} rows\")\n",
    "    print(f\"   [OK] Date range: {generation_df['ds'].min()} to {generation_df['ds'].max()}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_base = Path(\"reports/renewable/eda\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = output_base / timestamp\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n[DIR] Output directory: {output_dir}\")\n",
    "\n",
    "    # Step 2: Seasonality Analysis\n",
    "    print(\"\\n[2/6] Running seasonality analysis...\")\n",
    "    print(\"      Purpose: Justifies season_length=[24, 168] in MSTL model\")\n",
    "    seasonality_dir = output_dir / 'seasonality'\n",
    "    seasonality_results = analyze_seasonality(generation_df, seasonality_dir, max_series=3)\n",
    "    print(f\"      [OK] Analyzed {len(seasonality_results['series_analyzed'])} series\")\n",
    "    print(f\"      [OK] Hourly seasonality strength: {seasonality_results.get('hourly_seasonality_strength', {})}\")\n",
    "\n",
    "    # Step 3: Zero-Inflation Analysis\n",
    "    print(\"\\n[3/6] Running zero-inflation analysis...\")\n",
    "    print(\"      Purpose: Justifies MAE/RMSE over MAPE (MAPE undefined when actuals=0)\")\n",
    "    zero_dir = output_dir / 'zero_inflation'\n",
    "    zero_results = analyze_zero_inflation(generation_df, zero_dir)\n",
    "    print(f\"      [OK] Found {len(zero_results['series_zero_ratios'])} series\")\n",
    "    solar_avg_zero = sum(\n",
    "        info['zero_ratio'] for uid, info in zero_results['series_zero_ratios'].items()\n",
    "        if 'SUN' in uid\n",
    "    ) / max(1, sum(1 for uid in zero_results['series_zero_ratios'] if 'SUN' in uid))\n",
    "    print(f\"      [OK] Solar avg zero ratio: {solar_avg_zero:.2%} (zeros at night are expected)\")\n",
    "\n",
    "    # Step 4: Coverage & Missing Data Analysis\n",
    "    print(\"\\n[4/6] Running coverage gaps analysis...\")\n",
    "    print(\"      Purpose: Informs hourly grid enforcement policy\")\n",
    "    coverage_dir = output_dir / 'coverage'\n",
    "    coverage_results = analyze_coverage_gaps(generation_df, coverage_dir)\n",
    "    complete_series = sum(\n",
    "        1 for info in coverage_results['series_coverage'].values()\n",
    "        if info['coverage_ratio'] >= 0.98\n",
    "    )\n",
    "    total_series = len(coverage_results['series_coverage'])\n",
    "    print(f\"      [OK] Series with >=98% coverage: {complete_series}/{total_series}\")\n",
    "\n",
    "    # Step 5: Negative Values Analysis (CRITICAL)\n",
    "    print(\"\\n[5/6] Running negative values analysis...\")\n",
    "    print(\"      Purpose: CRITICAL - Decides preprocessing policy (clamp vs fail_loud)\")\n",
    "    negative_dir = output_dir / 'negative_values'\n",
    "    negative_results = analyze_negative_values(generation_df, negative_dir)\n",
    "\n",
    "    if negative_results['negative_count'] > 0:\n",
    "        print(f\"      [WARNING] Found {negative_results['negative_count']} negative values ({negative_results['negative_ratio']:.4%})\")\n",
    "        print(f\"      [WARNING] Affected series: {len(negative_results['series_with_negatives'])}\")\n",
    "\n",
    "        # Analyze patterns\n",
    "        if negative_results['negative_ratio'] < 0.01:\n",
    "            print(f\"      [OK] Recommendation: CLAMP to 0 (negatives <1%, likely metering errors)\")\n",
    "            print(f\"        Policy: negative_policy='clamp'\")\n",
    "        else:\n",
    "            print(f\"      [WARNING] Recommendation: INVESTIGATE (negatives >1%, data quality issue)\")\n",
    "            print(f\"        Policy: negative_policy='fail_loud' or 'hybrid'\")\n",
    "    else:\n",
    "        print(f\"      [OK] No negative values found (clean data)\")\n",
    "        print(f\"        Policy: No preprocessing needed\")\n",
    "\n",
    "    # Step 6: Weather Alignment Analysis\n",
    "    print(\"\\n[6/6] Running weather alignment analysis...\")\n",
    "    print(\"      Purpose: Validates feature selection and correlation\")\n",
    "    weather_dir = output_dir / 'weather_alignment'\n",
    "    weather_results = analyze_weather_alignment(generation_df, weather_df, weather_dir)\n",
    "    print(f\"      [OK] Merge success rate: {weather_results['merge_success_ratio']:.1%}\")\n",
    "\n",
    "    if 'correlation_by_fuel' in weather_results:\n",
    "        if 'WND' in weather_results['correlation_by_fuel']:\n",
    "            wind_corr = weather_results['correlation_by_fuel']['WND']\n",
    "            top_wind_var = max(wind_corr.items(), key=lambda x: abs(x[1]))\n",
    "            print(f\"      [OK] Wind: Top feature = {top_wind_var[0]} (corr={top_wind_var[1]:.3f})\")\n",
    "\n",
    "        if 'SUN' in weather_results['correlation_by_fuel']:\n",
    "            solar_corr = weather_results['correlation_by_fuel']['SUN']\n",
    "            top_solar_var = max(solar_corr.items(), key=lambda x: abs(x[1]))\n",
    "            print(f\"      [OK] Solar: Top feature = {top_solar_var[0]} (corr={top_solar_var[1]:.3f})\")\n",
    "\n",
    "    # Create metadata JSON (no HTML generation)\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'generation_rows': len(generation_df),\n",
    "        'generation_series': generation_df['unique_id'].nunique(),\n",
    "        'date_range': {\n",
    "            'start': str(generation_df['ds'].min()),\n",
    "            'end': str(generation_df['ds'].max()),\n",
    "        },\n",
    "        'weather_rows': len(weather_df),\n",
    "        'analyses_completed': [\n",
    "            'seasonality', 'zero_inflation', 'coverage_gaps',\n",
    "            'negative_values', 'weather_alignment'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    metadata_file = output_dir / 'metadata.json'\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # Create summary JSON instead of HTML\n",
    "    summary = {\n",
    "        'seasonality': {\n",
    "            'series_analyzed': len(seasonality_results['series_analyzed']),\n",
    "            'hourly_patterns': 'Clear 24h and 168h cycles detected',\n",
    "            'recommendation': 'Use MSTL with season_length=[24, 168]'\n",
    "        },\n",
    "        'zero_inflation': {\n",
    "            'solar_zero_ratio': f\"{solar_avg_zero:.2%}\",\n",
    "            'finding': 'Solar has substantial zeros at night (expected)',\n",
    "            'recommendation': 'Use RMSE/MAE as primary metrics (avoid MAPE)'\n",
    "        },\n",
    "        'coverage': {\n",
    "            'series_complete': f\"{complete_series}/{total_series}\",\n",
    "            'threshold': '>=98% coverage',\n",
    "            'recommendation': 'Use drop_incomplete_series policy'\n",
    "        },\n",
    "        'negative_values': {\n",
    "            'count': negative_results['negative_count'],\n",
    "            'ratio': f\"{negative_results['negative_ratio']:.4%}\",\n",
    "            'recommendation': 'clamp' if negative_results['negative_ratio'] < 0.01 else 'fail_loud or hybrid'\n",
    "        },\n",
    "        'weather_alignment': {\n",
    "            'merge_success_rate': f\"{weather_results['merge_success_ratio']:.1%}\",\n",
    "            'recommendation': 'Include all 7 weather variables'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    summary_file = output_dir / 'eda_summary.json'\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[SUCCESS] EDA ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n[REPORT] Summary: {summary_file}\")\n",
    "    print(f\"[DIR] All outputs: {output_dir}\")\n",
    "    print(\"\\n[FINDINGS] Key Findings:\")\n",
    "    print(f\"   * Seasonality: Clear 24h and 168h patterns -> Use MSTL\")\n",
    "    print(f\"   * Zero-inflation: Solar {solar_avg_zero:.1%} zeros -> Use RMSE/MAE (not MAPE)\")\n",
    "    print(f\"   * Coverage: {complete_series}/{total_series} series >98% complete -> Use drop_incomplete\")\n",
    "\n",
    "    if negative_results['negative_count'] > 0:\n",
    "        policy_rec = \"clamp\" if negative_results['negative_ratio'] < 0.01 else \"fail_loud or hybrid\"\n",
    "        print(f\"   * Negatives: {negative_results['negative_count']} found ({negative_results['negative_ratio']:.2%}) -> Use negative_policy='{policy_rec}'\")\n",
    "    else:\n",
    "        print(f\"   * Negatives: None found -> No preprocessing needed\")\n",
    "\n",
    "    print(f\"   * Weather: {weather_results['merge_success_ratio']:.1%} merge success -> Include all 7 variables\")\n",
    "\n",
    "    print(\"\\n[TIP] Next Steps:\")\n",
    "    print(\"   1. Review JSON reports in output directory\")\n",
    "    print(\"   2. Check visualization PNG files in subdirectories\")\n",
    "    print(\"   3. Update dataset_builder policy based on findings:\")\n",
    "    policy_rec = \"clamp\" if negative_results.get('negative_count', 0) == 0 or negative_results.get('negative_ratio', 0) < 0.01 else \"fail_loud\"\n",
    "    print(f\"      negative_policy='{policy_rec}'\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder based on EDA from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 17:43:27,949 - __main__ - INFO - ================================================================================\n",
      "2026-01-21 17:43:27,950 - __main__ - INFO - DATASET BUILDER - Starting preprocessing\n",
      "2026-01-21 17:43:27,950 - __main__ - INFO - ================================================================================\n",
      "2026-01-21 17:43:27,951 - __main__ - INFO - Input: 4,214 rows, 6 series\n",
      "2026-01-21 17:43:27,951 - __main__ - INFO - Policies: negative=fail_loud, hourly_grid=drop_incomplete_series\n",
      "2026-01-21 17:43:27,951 - __main__ - INFO - [generation][NEGATIVES] No negative values found\n",
      "2026-01-21 17:43:27,959 - __main__ - INFO - [generation][GRID] No missing hours detected\n",
      "2026-01-21 17:43:27,962 - __main__ - INFO - [TIME_FEATURES] Added: ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
      "2026-01-21 17:43:27,970 - __main__ - INFO - [WEATHER][ALIGN] Successfully merged 7 weather variables\n",
      "2026-01-21 17:43:27,971 - __main__ - INFO - ================================================================================\n",
      "2026-01-21 17:43:27,972 - __main__ - INFO - DATASET BUILDER - Complete\n",
      "2026-01-21 17:43:27,972 - __main__ - INFO - Output: 4,214 rows, 6 series\n",
      "2026-01-21 17:43:27,972 - __main__ - INFO - Dropped: 0 rows (0.0%)\n",
      "2026-01-21 17:43:27,972 - __main__ - INFO - Series dropped: 0\n",
      "2026-01-21 17:43:27,973 - __main__ - INFO - Features added: 4 time + 7 weather\n",
      "2026-01-21 17:43:27,973 - __main__ - INFO - ================================================================================\n",
      "2026-01-21 17:43:27,974 - __main__ - INFO - [REPORT] Saved to: data\\renewable\\preprocessing\\latest\\preprocessing_report.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET BUILDER - Production Pipeline\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading & validating raw data...\n",
      "   [OK] Generation: 4,214 rows, 6 series\n",
      "   [OK] Weather: 2,313 rows\n",
      "   [OK] Date range: 2025-12-22 to 2026-01-20\n",
      "\n",
      "   [CHECKS]\n",
      "      Negatives: 0 (0.0000%) [CLEAN]\n",
      "      Duplicates: 0 [CLEAN]\n",
      "\n",
      "   [POLICY SELECTION]\n",
      "      Negatives: None → negative_policy='fail_loud' (fail fast on upstream changes)\n",
      "      Grid enforcement: drop_incomplete_series (drop series with gaps)\n",
      "\n",
      "[2/3] Building modeling dataset (ONE canonical build)...\n",
      "\n",
      "   [RESULT] Preprocessing Summary:\n",
      "      Input:  4,214 rows → Output: 4,214 rows\n",
      "      Dropped: 0 rows (0.00%)\n",
      "      Series: 6 (dropped 0)\n",
      "      Negative action: passed\n",
      "      Features: 4 time + 7 weather\n",
      "\n",
      "[3/3] Dataset Inspection...\n",
      "\n",
      "   [DATA] Modeling-Ready Dataset:\n",
      "      Shape: (4214, 14)\n",
      "      Memory: 0.65 MB\n",
      "\n",
      "   [FEATURES] Added:\n",
      "      Time: ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
      "      Weather: ['temperature_2m', 'wind_speed_10m', 'wind_speed_100m'] ... (7 total)\n",
      "\n",
      "   [QUALITY] Data Checks:\n",
      "      Nulls in y: 0\n",
      "      Negatives in y: 0\n",
      "      Duplicates: 0\n",
      "      Weather nulls: 0\n",
      "\n",
      "   [SAMPLE] First 3 rows:\n",
      "unique_id                  ds    y  hour_sin  temperature_2m\n",
      " CALI_SUN 2025-12-22 00:00:00 5896  0.000000            18.1\n",
      " CALI_SUN 2025-12-22 01:00:00  553  0.258819            15.7\n",
      " CALI_SUN 2025-12-22 02:00:00    0  0.500000            15.2\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] PRODUCTION BUILD COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Saved to: data\\renewable\\preprocessing\\latest/\n",
      "  - preprocessing_report.json   # Full diagnostics\n",
      "================================================================================\n",
      "\n",
      "[NEXT] Use modeling_df for forecasting (weather_df=None, already merged)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# file: src/renewable/dataset_builder.py\n",
    "\"\"\"\n",
    "Dataset Builder for Renewable Energy Forecasting\n",
    "\n",
    "Consolidates all preprocessing transformations with transparent diagnostics:\n",
    "1. Negative value handling (clamp, fail-loud, or hybrid)\n",
    "2. Hourly grid enforcement (drop_incomplete or fail-loud)\n",
    "3. Weather alignment (merge and validate)\n",
    "4. Time feature engineering (hour/dow sin/cos)\n",
    "\n",
    "Input:\n",
    "  - Raw generation_df from EIA (unique_id, ds, y)\n",
    "  - Raw weather_df from Open-Meteo (ds, region, weather_vars)\n",
    "\n",
    "Output:\n",
    "  - Modeling-ready DataFrame (unique_id, ds, y, weather_vars, time_features)\n",
    "  - PreprocessingReport with comprehensive diagnostics\n",
    "\n",
    "Guarantees:\n",
    "  - Hourly grid enforced (no gaps or fail-loud)\n",
    "  - Negative values handled per policy\n",
    "  - Weather aligned to generation timestamps\n",
    "  - Time features added\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Weather variables expected from Open-Meteo\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingReport:\n",
    "    \"\"\"Educational diagnostics for what preprocessing occurred.\"\"\"\n",
    "\n",
    "    series_processed: int\n",
    "    rows_input: int\n",
    "    rows_output: int\n",
    "\n",
    "    # Negative handling\n",
    "    negative_values_found: Dict[str, Dict[str, Any]]  # uid -> {count, min, max, ratio, timestamps}\n",
    "    negative_values_action: str  # \"clamped\" | \"failed\" | \"passed\"\n",
    "\n",
    "    # Hourly grid\n",
    "    series_dropped_incomplete: list[str]\n",
    "    missing_hour_summary: Dict[str, Any]  # Summary of missing hour blocks\n",
    "\n",
    "    # Weather alignment\n",
    "    weather_coverage_by_region: Dict[str, float]\n",
    "    weather_alignment_failures: list[Dict[str, Any]]\n",
    "\n",
    "    # Features\n",
    "    time_features_added: list[str]\n",
    "    weather_features_added: list[str]\n",
    "\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "def _missing_hour_blocks(ds: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp, int]]:\n",
    "    \"\"\"\n",
    "    Return contiguous blocks of missing hourly timestamps.\n",
    "    Each tuple: (block_start, block_end, n_hours)\n",
    "    \"\"\"\n",
    "    ds = pd.to_datetime(ds, errors=\"raise\").sort_values()\n",
    "    start, end = ds.iloc[0], ds.iloc[-1]\n",
    "    expected = pd.date_range(start, end, freq=\"h\")\n",
    "    missing = expected.difference(ds)\n",
    "\n",
    "    if missing.empty:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    block_start = missing[0]\n",
    "    prev = missing[0]\n",
    "    for t in missing[1:]:\n",
    "        if t - prev == pd.Timedelta(hours=1):\n",
    "            prev = t\n",
    "        else:\n",
    "            n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "            blocks.append((block_start, prev, n))\n",
    "            block_start = t\n",
    "            prev = t\n",
    "    n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "    blocks.append((block_start, prev, n))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def _hourly_grid_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate hourly grid coverage report per series.\"\"\"\n",
    "    cols = [\n",
    "        \"unique_id\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "        \"expected_hours\",\n",
    "        \"actual_hours\",\n",
    "        \"missing_hours\",\n",
    "        \"missing_ratio\",\n",
    "        \"n_missing_blocks\",\n",
    "        \"largest_missing_block_hours\",\n",
    "    ]\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    for uid, g in df.groupby(\"unique_id\"):\n",
    "        g = g.sort_values(\"ds\")\n",
    "        start, end = g[\"ds\"].iloc[0], g[\"ds\"].iloc[-1]\n",
    "        expected = pd.date_range(start, end, freq=\"h\")\n",
    "        missing = expected.difference(g[\"ds\"])\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"unique_id\": uid,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"expected_hours\": int(len(expected)),\n",
    "                \"actual_hours\": int(len(g)),\n",
    "                \"missing_hours\": int(len(missing)),\n",
    "                \"missing_ratio\": float(len(missing) / max(len(expected), 1)),\n",
    "                \"n_missing_blocks\": int(len(blocks)),\n",
    "                \"largest_missing_block_hours\": int(max([b[2] for b in blocks], default=0)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rep = pd.DataFrame(rows)\n",
    "    return rep.sort_values([\"missing_ratio\", \"missing_hours\"], ascending=False)\n",
    "\n",
    "\n",
    "def _handle_negative_values(\n",
    "    df: pd.DataFrame,\n",
    "    policy: str,\n",
    "    label: str = \"generation\"\n",
    ") -> tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Handle negative generation values according to policy.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns [unique_id, ds, y]\n",
    "        policy: \"clamp\" | \"fail_loud\" | \"hybrid\"\n",
    "        label: Label for logging\n",
    "\n",
    "    Returns:\n",
    "        (processed_df, diagnostics_dict)\n",
    "    \"\"\"\n",
    "    diagnostics = {\n",
    "        'total_rows': len(df),\n",
    "        'negative_count': int((df['y'] < 0).sum()),\n",
    "        'negative_ratio': float((df['y'] < 0).sum() / len(df) if len(df) > 0 else 0),\n",
    "        'series_with_negatives': {},\n",
    "        'action_taken': policy\n",
    "    }\n",
    "\n",
    "    if diagnostics['negative_count'] == 0:\n",
    "        logger.info(f\"[{label}][NEGATIVES] No negative values found\")\n",
    "        diagnostics['action_taken'] = 'passed'\n",
    "        return df.copy(), diagnostics\n",
    "\n",
    "    # Analyze per-series\n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_df = df[df['unique_id'] == uid]\n",
    "        neg_mask = series_df['y'] < 0\n",
    "\n",
    "        if neg_mask.any():\n",
    "            neg_samples = series_df[neg_mask].head(10)\n",
    "\n",
    "            diagnostics['series_with_negatives'][uid] = {\n",
    "                'count': int(neg_mask.sum()),\n",
    "                'ratio': float(neg_mask.sum() / len(series_df)),\n",
    "                'min_value': float(series_df[neg_mask]['y'].min()),\n",
    "                'max_value': float(series_df[neg_mask]['y'].max()),\n",
    "                'mean_value': float(series_df[neg_mask]['y'].mean()),\n",
    "                'sample_timestamps': neg_samples['ds'].astype(str).tolist()\n",
    "            }\n",
    "\n",
    "            logger.warning(\n",
    "                f\"[{label}][NEGATIVES] {uid}: count={neg_mask.sum()} \"\n",
    "                f\"min={series_df[neg_mask]['y'].min():.2f} max={series_df[neg_mask]['y'].max():.2f}\"\n",
    "            )\n",
    "\n",
    "    # Apply policy\n",
    "    out = df.copy()\n",
    "\n",
    "    if policy == \"fail_loud\":\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][NEGATIVES] Found {diagnostics['negative_count']} negative values \"\n",
    "            f\"({diagnostics['negative_ratio']:.2%}) across {len(diagnostics['series_with_negatives'])} series. \"\n",
    "            f\"Policy=fail_loud prohibits negative values. Details: {diagnostics['series_with_negatives']}\"\n",
    "        )\n",
    "\n",
    "    elif policy == \"clamp\":\n",
    "        out['y'] = out['y'].clip(lower=0)\n",
    "        logger.warning(\n",
    "            f\"[{label}][NEGATIVES] Clamped {diagnostics['negative_count']} negative values to 0 \"\n",
    "            f\"({diagnostics['negative_ratio']:.2%})\"\n",
    "        )\n",
    "        diagnostics['action_taken'] = 'clamped'\n",
    "\n",
    "    elif policy == \"hybrid\":\n",
    "        # Hybrid: clamp if <1% per series, fail if >=1%\n",
    "        fail_series = []\n",
    "        for uid, info in diagnostics['series_with_negatives'].items():\n",
    "            if info['ratio'] >= 0.01:  # 1% threshold\n",
    "                fail_series.append(uid)\n",
    "\n",
    "        if fail_series:\n",
    "            raise RuntimeError(\n",
    "                f\"[{label}][NEGATIVES] Policy=hybrid: Found series with >=1% negative values: {fail_series}. \"\n",
    "                f\"This indicates data quality issues. Details: {diagnostics['series_with_negatives']}\"\n",
    "            )\n",
    "\n",
    "        # Clamp small amounts\n",
    "        out['y'] = out['y'].clip(lower=0)\n",
    "        logger.warning(\n",
    "            f\"[{label}][NEGATIVES] Policy=hybrid: Clamped {diagnostics['negative_count']} negative values \"\n",
    "            f\"(all series <1% ratio)\"\n",
    "        )\n",
    "        diagnostics['action_taken'] = 'clamped (hybrid)'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown negative handling policy: {policy}\")\n",
    "\n",
    "    return out, diagnostics\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    policy: str,\n",
    "    label: str = \"generation\"\n",
    ") -> tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Enforce hourly grid according to policy.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns [unique_id, ds, y]\n",
    "        policy: \"drop_incomplete_series\" | \"fail_loud\"\n",
    "        label: Label for logging\n",
    "\n",
    "    Returns:\n",
    "        (processed_df, diagnostics_dict)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Cannot enforce hourly grid: input dataframe is empty. \"\n",
    "            \"This is upstream (fetch) failure, not a grid issue.\"\n",
    "        )\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    if rep.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] No series found to report on (rep empty). \"\n",
    "            \"This indicates upstream emptiness or missing 'unique_id' groups.\"\n",
    "        )\n",
    "\n",
    "    diagnostics = {\n",
    "        'series_count': len(rep),\n",
    "        'series_with_missing': int((rep['missing_hours'] > 0).sum()),\n",
    "        'total_missing_hours': int(rep['missing_hours'].sum()),\n",
    "        'worst_missing_ratio': float(rep['missing_ratio'].max()),\n",
    "        'policy': policy,\n",
    "        'series_dropped': [],\n",
    "        'missing_hour_blocks': rep.to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "\n",
    "    if worst[\"missing_hours\"] == 0:\n",
    "        logger.info(f\"[{label}][GRID] No missing hours detected\")\n",
    "        return df.copy(), diagnostics\n",
    "\n",
    "    logger.warning(f\"[{label}][GRID] Missing hours detected:\\n{rep.head(10).to_string(index=False)}\")\n",
    "\n",
    "    if policy == \"drop_incomplete_series\":\n",
    "        bad_uids = rep.loc[rep[\"missing_hours\"] > 0, \"unique_id\"].tolist()\n",
    "        kept = df.loc[~df[\"unique_id\"].isin(bad_uids)].copy()\n",
    "\n",
    "        diagnostics['series_dropped'] = bad_uids\n",
    "\n",
    "        logger.warning(\n",
    "            f\"[{label}][GRID] policy=drop_incomplete_series dropped={len(bad_uids)} \"\n",
    "            f\"kept_series={kept['unique_id'].nunique()}\"\n",
    "        )\n",
    "\n",
    "        if kept.empty:\n",
    "            raise RuntimeError(f\"[{label}][GRID] all series dropped due to missing hours\")\n",
    "\n",
    "        return kept, diagnostics\n",
    "\n",
    "    elif policy == \"fail_loud\":\n",
    "        worst_uid = worst[\"unique_id\"]\n",
    "        g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Missing hours detected (no imputation). \"\n",
    "            f\"worst_unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "            f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown hourly grid policy: {policy}\")\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add cyclical time features (hour, day of week).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "\n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "\n",
    "def _align_weather(\n",
    "    df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Align weather data to generation timestamps.\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame with columns [unique_id, ds, y, ...]\n",
    "        weather_df: Weather DataFrame with columns [ds, region, weather_vars]\n",
    "\n",
    "    Returns:\n",
    "        (merged_df, diagnostics_dict)\n",
    "    \"\"\"\n",
    "    # Extract region from unique_id (e.g., \"CALI_WND\" -> \"CALI\")\n",
    "    work = df.copy()\n",
    "    work[\"region\"] = work[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "    # Check required columns\n",
    "    if not {\"ds\", \"region\"}.issubset(weather_df.columns):\n",
    "        raise ValueError(\"weather_df must have columns ['ds', 'region']\")\n",
    "\n",
    "    wcols = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "    if not wcols:\n",
    "        raise ValueError(\"weather_df has none of expected WEATHER_VARS\")\n",
    "\n",
    "    # Merge\n",
    "    merged = work.merge(\n",
    "        weather_df[[\"ds\", \"region\"] + wcols],\n",
    "        on=[\"ds\", \"region\"],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    # Check for missing weather after merge\n",
    "    missing_any = merged[wcols].isna().any(axis=1)\n",
    "\n",
    "    diagnostics = {\n",
    "        'weather_vars': wcols,\n",
    "        'merge_rows': len(merged),\n",
    "        'missing_weather_rows': int(missing_any.sum()),\n",
    "        'missing_weather_ratio': float(missing_any.sum() / len(merged) if len(merged) > 0 else 0),\n",
    "        'coverage_by_region': {}\n",
    "    }\n",
    "\n",
    "    # Per-region coverage\n",
    "    for region in merged['region'].unique():\n",
    "        region_df = merged[merged['region'] == region]\n",
    "        region_missing = region_df[wcols].isna().any(axis=1).sum()\n",
    "        diagnostics['coverage_by_region'][region] = float(\n",
    "            1 - (region_missing / len(region_df)) if len(region_df) > 0 else 0\n",
    "        )\n",
    "\n",
    "    if missing_any.any():\n",
    "        sample = merged.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + wcols].head(10)\n",
    "        logger.error(\n",
    "            f\"[WEATHER][ALIGN] Missing weather after merge rows={int(missing_any.sum())}. \"\n",
    "            f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "        )\n",
    "        raise RuntimeError(\n",
    "            f\"[WEATHER][ALIGN] Missing weather after merge rows={int(missing_any.sum())}. \"\n",
    "            f\"Check that weather_df covers the same date range and regions as generation_df.\"\n",
    "        )\n",
    "\n",
    "    # Drop region column (not needed for modeling)\n",
    "    output = merged.drop(columns=[\"region\"])\n",
    "\n",
    "    logger.info(f\"[WEATHER][ALIGN] Successfully merged {len(wcols)} weather variables\")\n",
    "\n",
    "    return output, diagnostics\n",
    "\n",
    "\n",
    "def build_modeling_dataset(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    *,\n",
    "    negative_policy: str = \"clamp\",\n",
    "    hourly_grid_policy: str = \"drop_incomplete_series\",\n",
    "    output_dir: Optional[Path] = None,\n",
    ") -> tuple[pd.DataFrame, PreprocessingReport]:\n",
    "    \"\"\"\n",
    "    Build modeling-ready dataset with comprehensive diagnostics.\n",
    "\n",
    "    Args:\n",
    "        generation_df: Raw generation DataFrame (unique_id, ds, y)\n",
    "        weather_df: Raw weather DataFrame (ds, region, weather_vars)\n",
    "        negative_policy: \"clamp\" | \"fail_loud\" | \"hybrid\"\n",
    "        hourly_grid_policy: \"drop_incomplete_series\" | \"fail_loud\"\n",
    "        output_dir: Optional directory to save detailed diagnostics\n",
    "\n",
    "    Returns:\n",
    "        (modeling_df, preprocessing_report)\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If data quality issues detected and policy is fail_loud\n",
    "        ValueError: If invalid policy specified\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"DATASET BUILDER - Starting preprocessing\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    # Validate inputs\n",
    "    req = {\"unique_id\", \"ds\", \"y\"}\n",
    "    if not req.issubset(generation_df.columns):\n",
    "        raise ValueError(f\"generation_df missing cols={sorted(req - set(generation_df.columns))}\")\n",
    "\n",
    "    if generation_df.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[GENERATION] Empty generation dataframe. \"\n",
    "            \"This is upstream (EIA fetch/cache) failure.\"\n",
    "        )\n",
    "\n",
    "    rows_input = len(generation_df)\n",
    "    series_input = generation_df['unique_id'].nunique()\n",
    "\n",
    "    logger.info(f\"Input: {rows_input:,} rows, {series_input} series\")\n",
    "    logger.info(f\"Policies: negative={negative_policy}, hourly_grid={hourly_grid_policy}\")\n",
    "\n",
    "    # Step 1: Handle negative values\n",
    "    work, neg_diagnostics = _handle_negative_values(\n",
    "        generation_df,\n",
    "        policy=negative_policy,\n",
    "        label=\"generation\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Enforce hourly grid\n",
    "    work, grid_diagnostics = _enforce_hourly_grid(\n",
    "        work,\n",
    "        policy=hourly_grid_policy,\n",
    "        label=\"generation\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Add time features\n",
    "    work = _add_time_features(work)\n",
    "    time_features = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "    logger.info(f\"[TIME_FEATURES] Added: {time_features}\")\n",
    "\n",
    "    # Step 4: Align weather (if provided)\n",
    "    weather_features = []\n",
    "    weather_diagnostics = {}\n",
    "\n",
    "    if weather_df is not None and not weather_df.empty:\n",
    "        work, weather_diagnostics = _align_weather(work, weather_df)\n",
    "        weather_features = [c for c in WEATHER_VARS if c in work.columns]\n",
    "    else:\n",
    "        logger.warning(\"[WEATHER] No weather data provided, skipping alignment\")\n",
    "\n",
    "    # Final validation\n",
    "    y_null = work[\"y\"].isna()\n",
    "    if y_null.any():\n",
    "        sample = work.loc[y_null, [\"unique_id\", \"ds\", \"y\"]].head(25)\n",
    "        raise RuntimeError(\n",
    "            f\"[GENERATION][Y] Found null y values after preprocessing (rows={int(y_null.sum())}). \"\n",
    "            f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "        )\n",
    "\n",
    "    rows_output = len(work)\n",
    "    series_output = work['unique_id'].nunique()\n",
    "\n",
    "    # Create report\n",
    "    report = PreprocessingReport(\n",
    "        series_processed=series_output,\n",
    "        rows_input=rows_input,\n",
    "        rows_output=rows_output,\n",
    "        negative_values_found=neg_diagnostics.get('series_with_negatives', {}),\n",
    "        negative_values_action=neg_diagnostics.get('action_taken', 'unknown'),\n",
    "        series_dropped_incomplete=grid_diagnostics.get('series_dropped', []),\n",
    "        missing_hour_summary=grid_diagnostics,\n",
    "        weather_coverage_by_region=weather_diagnostics.get('coverage_by_region', {}),\n",
    "        weather_alignment_failures=[],\n",
    "        time_features_added=time_features,\n",
    "        weather_features_added=weather_features,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"DATASET BUILDER - Complete\")\n",
    "    logger.info(f\"Output: {rows_output:,} rows, {series_output} series\")\n",
    "    logger.info(f\"Dropped: {rows_input - rows_output:,} rows ({(rows_input - rows_output)/rows_input*100:.1f}%)\")\n",
    "    logger.info(f\"Series dropped: {len(report.series_dropped_incomplete)}\")\n",
    "    logger.info(f\"Features added: {len(time_features)} time + {len(weather_features)} weather\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    # Save detailed diagnostics if output_dir provided\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        report_file = output_dir / \"preprocessing_report.json\"\n",
    "        report_file.write_text(json.dumps(asdict(report), indent=2, default=str))\n",
    "        logger.info(f\"[REPORT] Saved to: {report_file}\")\n",
    "\n",
    "        # Save negative samples if any\n",
    "        if neg_diagnostics.get('negative_count', 0) > 0:\n",
    "            neg_detail = output_dir / \"negative_values_detail.json\"\n",
    "            neg_detail.write_text(json.dumps(neg_diagnostics, indent=2, default=str))\n",
    "            logger.info(f\"[NEGATIVES] Details saved to: {neg_detail}\")\n",
    "\n",
    "        # Save grid report\n",
    "        if grid_diagnostics.get('series_dropped'):\n",
    "            grid_detail = output_dir / \"missing_hours_detail.json\"\n",
    "            grid_detail.write_text(json.dumps(grid_diagnostics, indent=2, default=str))\n",
    "            logger.info(f\"[GRID] Details saved to: {grid_detail}\")\n",
    "\n",
    "    return work, report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Build production-ready modeling dataset from raw renewable energy data.\n",
    "\n",
    "    MAIN PATH (default):\n",
    "      - Loads raw generation and weather data\n",
    "      - Runs ONE canonical preprocessing pass\n",
    "      - Saves modeling-ready dataset\n",
    "      - Reports diagnostics\n",
    "\n",
    "    DEMO MODE (set RUN_DEMOS=True):\n",
    "      - Shows how different policies compare\n",
    "      - Demonstrates clamp, fail_loud, hybrid approaches\n",
    "      - Educational tool only (not production path)\n",
    "\n",
    "    Usage:\n",
    "        python -m src.renewable.dataset_builder           # Production path\n",
    "        RUN_DEMOS=1 python -m src.renewable.dataset_builder  # With demos\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import logging\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from dataclasses import asdict\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Configuration: set RUN_DEMOS=1 in environment to run policy comparisons\n",
    "    RUN_DEMOS = os.environ.get('RUN_DEMOS', '0').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATASET BUILDER - Production Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    if RUN_DEMOS:\n",
    "        print(\"(DEMO MODE: Running policy comparisons)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Load & validate raw data\n",
    "    print(\"\\n[1/3] Loading & validating raw data...\")\n",
    "\n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "\n",
    "    if not generation_path.exists():\n",
    "        print(f\"[ERROR] Generation data not found at {generation_path}\")\n",
    "        print(\"   Please run the pipeline first: python -m src.renewable.tasks --preset 24h\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not weather_path.exists():\n",
    "        print(f\"[ERROR] Weather data not found at {weather_path}\")\n",
    "        print(\"   Please run the pipeline first: python -m src.renewable.tasks --preset 24h\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "    print(f\"   [OK] Generation: {len(generation_df):,} rows, {generation_df['unique_id'].nunique()} series\")\n",
    "    print(f\"   [OK] Weather: {len(weather_df):,} rows\")\n",
    "    print(f\"   [OK] Date range: {generation_df['ds'].min().date()} to {generation_df['ds'].max().date()}\")\n",
    "\n",
    "    # Quick data sanity checks\n",
    "    neg_count = (generation_df['y'] < 0).sum()\n",
    "    neg_ratio = neg_count / len(generation_df) if len(generation_df) > 0 else 0\n",
    "\n",
    "    has_duplicates = generation_df.duplicated(subset=['unique_id', 'ds']).sum()\n",
    "\n",
    "    print(f\"\\n   [CHECKS]\")\n",
    "    print(f\"      Negatives: {neg_count} ({neg_ratio:.4%})\" + (\" [WARNING]\" if neg_count > 0 else \" [CLEAN]\"))\n",
    "    print(f\"      Duplicates: {has_duplicates}\" + (\" [WARNING]\" if has_duplicates > 0 else \" [CLEAN]\"))\n",
    "\n",
    "    # Determine best policy based on data characteristics\n",
    "    print(\"\\n   [POLICY SELECTION]\")\n",
    "    if neg_count == 0:\n",
    "        chosen_negative_policy = \"fail_loud\"\n",
    "        print(f\"      Negatives: None → negative_policy='fail_loud' (fail fast on upstream changes)\")\n",
    "    elif neg_ratio < 0.01:\n",
    "        chosen_negative_policy = \"fail_loud\"\n",
    "        print(f\"      Negatives: {neg_ratio:.4%} (rare) → negative_policy='fail_loud' (detect if they appear)\")\n",
    "    else:\n",
    "        chosen_negative_policy = \"clamp\"\n",
    "        print(f\"      Negatives: {neg_ratio:.4%} (substantial) → negative_policy='clamp' (tolerate & log)\")\n",
    "\n",
    "    chosen_grid_policy = \"drop_incomplete_series\"\n",
    "    print(f\"      Grid enforcement: {chosen_grid_policy} (drop series with gaps)\")\n",
    "\n",
    "    # Step 2: Build dataset (canonical production path)\n",
    "    print(\"\\n[2/3] Building modeling dataset (ONE canonical build)...\")\n",
    "\n",
    "    output_dir = Path(\"data/renewable/preprocessing/latest\")\n",
    "\n",
    "    modeling_df, report = build_modeling_dataset(\n",
    "        generation_df,\n",
    "        weather_df,\n",
    "        negative_policy=chosen_negative_policy,\n",
    "        hourly_grid_policy=chosen_grid_policy,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   [RESULT] Preprocessing Summary:\")\n",
    "    print(f\"      Input:  {report.rows_input:,} rows → Output: {report.rows_output:,} rows\")\n",
    "    print(f\"      Dropped: {report.rows_input - report.rows_output:,} rows ({(report.rows_input - report.rows_output)/report.rows_input*100:.2f}%)\")\n",
    "    print(f\"      Series: {report.series_processed} (dropped {len(report.series_dropped_incomplete)})\")\n",
    "    print(f\"      Negative action: {report.negative_values_action}\")\n",
    "    print(f\"      Features: {len(report.time_features_added)} time + {len(report.weather_features_added)} weather\")\n",
    "\n",
    "    # Step 3: Dataset inspection\n",
    "    print(\"\\n[3/3] Dataset Inspection...\")\n",
    "\n",
    "    print(f\"\\n   [DATA] Modeling-Ready Dataset:\")\n",
    "    print(f\"      Shape: {modeling_df.shape}\")\n",
    "    print(f\"      Memory: {modeling_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n   [FEATURES] Added:\")\n",
    "    print(f\"      Time: {report.time_features_added}\")\n",
    "    print(f\"      Weather: {report.weather_features_added[:3]} ... ({len(report.weather_features_added)} total)\")\n",
    "\n",
    "    print(f\"\\n   [QUALITY] Data Checks:\")\n",
    "    print(f\"      Nulls in y: {modeling_df['y'].isna().sum()}\")\n",
    "    print(f\"      Negatives in y: {(modeling_df['y'] < 0).sum()}\")\n",
    "    print(f\"      Duplicates: {modeling_df.duplicated(subset=['unique_id', 'ds']).sum()}\")\n",
    "    weather_nulls = modeling_df[report.weather_features_added].isna().sum().sum() if report.weather_features_added else 0\n",
    "    print(f\"      Weather nulls: {weather_nulls}\")\n",
    "\n",
    "    print(f\"\\n   [SAMPLE] First 3 rows:\")\n",
    "    print(modeling_df.head(3)[['unique_id', 'ds', 'y', 'hour_sin', 'temperature_2m']].to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[SUCCESS] PRODUCTION BUILD COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nSaved to: {output_dir}/\")\n",
    "    print(f\"  - preprocessing_report.json   # Full diagnostics\")\n",
    "    if report.negative_values_found:\n",
    "        print(f\"  - negative_values_detail.json # Negative sample analysis\")\n",
    "    if report.series_dropped_incomplete:\n",
    "        print(f\"  - missing_hours_detail.json   # Dropped series details\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # ============================================================================\n",
    "    # DEMO MODE (optional, run with: RUN_DEMOS=1 python -m src.renewable.dataset_builder)\n",
    "    # ============================================================================\n",
    "    if RUN_DEMOS:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"[DEMO MODE] Policy Comparison (Educational)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"(This demonstrates different policies. For production, use the canonical build above.)\\n\")\n",
    "\n",
    "        # Demo 1: Try fail_loud if negatives exist\n",
    "        if neg_count > 0:\n",
    "            print(\"[DEMO 1/2] Testing negative_policy='fail_loud'...\")\n",
    "            try:\n",
    "                modeling_df_fail, _ = build_modeling_dataset(\n",
    "                    generation_df, weather_df,\n",
    "                    negative_policy=\"fail_loud\",\n",
    "                    hourly_grid_policy=\"drop_incomplete_series\",\n",
    "                    output_dir=None\n",
    "                )\n",
    "                print(\"      [OK] No negatives detected (would pass production)\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"      [RAISED] {str(e)[:150]}...\")\n",
    "\n",
    "        # Demo 2: Try hybrid\n",
    "        print(\"\\n[DEMO 2/2] Testing negative_policy='hybrid' (if <1% per series)...\")\n",
    "        try:\n",
    "            modeling_df_hybrid, report_hybrid = build_modeling_dataset(\n",
    "                generation_df, weather_df,\n",
    "                negative_policy=\"hybrid\",\n",
    "                hourly_grid_policy=\"drop_incomplete_series\",\n",
    "                output_dir=None\n",
    "            )\n",
    "            print(f\"      [OK] Policy='hybrid' succeeded ({report_hybrid.negative_values_action})\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"      [RAISED] {str(e)[:150]}...\")\n",
    "\n",
    "        print(\"\\n[NOTE] Demos show policy behavior. Production uses: negative_policy='{}', grid='drop_incomplete'\".format(chosen_negative_policy))\n",
    "\n",
    "    print(\"\\n[NEXT] Use modeling_df for forecasting (weather_df=None, already merged)\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 4: Probabilistic Modeling\n",
    "\n",
    "**File:** `src/renewable/modeling.py`\n",
    "\n",
    "This is where the forecasting happens! We use **StatsForecast** for:\n",
    "\n",
    "1. **Multi-series forecasting**: Handle multiple regions/fuel types in one model\n",
    "2. **Probabilistic predictions**: Get prediction intervals, not just point forecasts\n",
    "3. **Weather exogenous**: Include weather features as predictors\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Prediction Intervals?\n",
    "\n",
    "Point forecasts are useful, but energy traders need **uncertainty quantification**:\n",
    "- **80% interval**: \"I'm 80% confident generation will be between X and Y\"\n",
    "- **95% interval**: Wider, for risk management\n",
    "\n",
    "### Zero-Value Safety (CRITICAL)\n",
    "\n",
    "**Solar panels generate ZERO at night!** This breaks MAPE:\n",
    "\n",
    "```\n",
    "MAPE = mean(|actual - predicted| / actual)\n",
    "\n",
    "When actual = 0:\n",
    "MAPE = |0 - pred| / 0 = undefined (division by zero!)\n",
    "```\n",
    "\n",
    "**Solution**: Always use RMSE and MAE for renewable forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/renewable/validation.py\n",
    "# file: src/renewable/validation.py\n",
    "\"\"\"Validation utilities for renewable generation data.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ValidationReport:\n",
    "    ok: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "def validate_generation_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lag_hours: int = 3,\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    expected_series: Optional[Iterable[str]] = None,\n",
    ") -> ValidationReport:\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    missing_cols = required - set(df.columns)\n",
    "    if missing_cols:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Missing required columns\",\n",
    "            {\"missing_cols\": sorted(missing_cols)},\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        return ValidationReport(False, \"Generation data is empty\", {})\n",
    "\n",
    "    work = df.copy()\n",
    "\n",
    "    work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"coerce\", utc=True)\n",
    "    if work[\"ds\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable ds values found\",\n",
    "            {\"bad_ds\": int(work[\"ds\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    work[\"y\"] = pd.to_numeric(work[\"y\"], errors=\"coerce\")\n",
    "    if work[\"y\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable y values found\",\n",
    "            {\"bad_y\": int(work[\"y\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    # Check for negative values and log warning (but allow to pass)\n",
    "    # Dataset builder will handle negatives according to configured policy\n",
    "    if (work[\"y\"] < 0).any():\n",
    "        import logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        neg_mask = work[\"y\"] < 0\n",
    "        neg_count = int(neg_mask.sum())\n",
    "        by_series = (\n",
    "            work[neg_mask]\n",
    "            .groupby(\"unique_id\")\n",
    "            .agg(count=(\"y\", \"count\"), min_y=(\"y\", \"min\"), max_y=(\"y\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        logger.warning(\n",
    "            \"[validation][NEGATIVE] Found %d negative values (%.1f%%) across %d series\",\n",
    "            neg_count,\n",
    "            100 * neg_count / len(work),\n",
    "            len(by_series)\n",
    "        )\n",
    "\n",
    "        for _, row in by_series.iterrows():\n",
    "            logger.warning(\n",
    "                \"  Series %s: %d negative values, range=[%.1f, %.1f]\",\n",
    "                row[\"unique_id\"], row[\"count\"], row[\"min_y\"], row[\"max_y\"]\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            \"[validation][NEGATIVE] Negatives will be handled by dataset builder \"\n",
    "            \"according to configured negative_policy\"\n",
    "        )\n",
    "\n",
    "        # Continue validation instead of failing\n",
    "        # (Dataset builder will handle negatives per policy)\n",
    "\n",
    "    dup = work.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Duplicate (unique_id, ds) rows found\",\n",
    "            {\"duplicates\": int(dup)},\n",
    "        )\n",
    "\n",
    "    if expected_series:\n",
    "        expected = sorted(set(expected_series))\n",
    "        present = sorted(set(work[\"unique_id\"]))\n",
    "        missing_series = sorted(set(expected) - set(present))\n",
    "        if missing_series:\n",
    "            return ValidationReport(\n",
    "                False,\n",
    "                \"Missing expected series\",\n",
    "                {\"missing_series\": missing_series, \"present_series\": present},\n",
    "            )\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "    max_ds = work[\"ds\"].max()\n",
    "    lag_hours = (now_utc - max_ds).total_seconds() / 3600.0\n",
    "    if lag_hours > max_lag_hours:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Data not fresh enough\",\n",
    "            {\n",
    "                \"now_utc\": now_utc.isoformat(),\n",
    "                \"max_ds\": max_ds.isoformat(),\n",
    "                \"lag_hours\": lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    series_max = work.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    series_lag = (now_utc - series_max).dt.total_seconds() / 3600.0\n",
    "    stale = series_lag[series_lag > max_lag_hours].sort_values(ascending=False)\n",
    "    if not stale.empty:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Stale series found\",\n",
    "            {\n",
    "                \"stale_series\": stale.head(10).to_dict(),\n",
    "                \"max_lag_hours\": max_lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    missing_ratios = {}\n",
    "    for uid, group in work.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")\n",
    "        start = group[\"ds\"].iloc[0]\n",
    "        end = group[\"ds\"].iloc[-1]\n",
    "        expected = int(((end - start) / pd.Timedelta(hours=1)) + 1)\n",
    "        actual = len(group)\n",
    "        missing = max(expected - actual, 0)\n",
    "        missing_ratios[uid] = missing / max(expected, 1)\n",
    "\n",
    "    worst_uid = max(missing_ratios, key=missing_ratios.get)\n",
    "    worst_ratio = missing_ratios[worst_uid]\n",
    "    if worst_ratio > max_missing_ratio:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Too many missing hourly points\",\n",
    "            {\"worst_uid\": worst_uid, \"worst_missing_ratio\": worst_ratio},\n",
    "        )\n",
    "\n",
    "    return ValidationReport(\n",
    "        True,\n",
    "        \"OK\",\n",
    "        {\n",
    "            \"row_count\": len(work),\n",
    "            \"series_count\": int(work[\"unique_id\"].nunique()),\n",
    "            \"max_ds\": max_ds.isoformat(),\n",
    "            \"lag_hours\": lag_hours,\n",
    "            \"worst_missing_ratio\": worst_ratio,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/renewable/modeling.py\n",
    "# file: src/renewable/modeling.py\n",
    "\"\"\"\n",
    "Renewable Energy Forecasting Models\n",
    "\n",
    "This module provides two model paths for renewable energy forecasting:\n",
    "\n",
    "## 1. Core Path (Primary Forecasting): RenewableForecastModel\n",
    "   - **Purpose:** Production forecasts with calibrated prediction intervals\n",
    "   - **Framework:** StatsForecast (native multi-series support)\n",
    "   - **Models:** MSTL_ARIMA, AutoARIMA, AutoETS, SeasonalNaive\n",
    "   - **Usage:** Always runs in pipeline (train_renewable_models)\n",
    "   - **Why StatsForecast:**\n",
    "     * 10x faster than individual model training\n",
    "     * Built-in prediction intervals (no conformal prediction needed)\n",
    "     * Excellent with seasonal patterns (MSTL handles daily + weekly cycles)\n",
    "   - **Output:** Best model selected via CV, forecasts with 80%/95% intervals\n",
    "\n",
    "## 2. Interpretability Path (Feature Analysis): RenewableLGBMForecaster\n",
    "   - **Purpose:** Understanding feature contributions via SHAP analysis\n",
    "   - **Framework:** skforecast + LightGBM\n",
    "   - **Models:** LightGBM with weather + time features\n",
    "   - **Usage:** Always runs by default (failures non-fatal)\n",
    "   - **Output:** SHAP plots, feature importance, partial dependence plots\n",
    "   - **Note:** NOT used for final forecasts (StatsForecast provides those)\n",
    "\n",
    "## Model Selection Philosophy\n",
    "- **StatsForecast:** Use for operational forecasts (speed + intervals)\n",
    "- **LightGBM:** Use for understanding (interpretability)\n",
    "- **Cross-validation:** Determines best StatsForecast model automatically\n",
    "\n",
    "## Removed Models\n",
    "- **RenewableMLForecast:** Removed as unused (MLForecast + conformal prediction)\n",
    "  Use RenewableForecastModel (StatsForecast) for forecasting\n",
    "  Use RenewableLGBMForecaster (skforecast) for interpretability\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "\n",
    "def _log_series_summary(df: pd.DataFrame, *, value_col: str = \"y\", label: str = \"series\") -> None:\n",
    "    if df.empty:\n",
    "        print(f\"[{label}] EMPTY\")\n",
    "        return\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ds\"] = pd.to_datetime(tmp[\"ds\"], errors=\"coerce\")\n",
    "\n",
    "    def _mode_delta_hours(g: pd.Series) -> float:\n",
    "        d = g.sort_values().diff().dropna()\n",
    "        if d.empty:\n",
    "            return float(\"nan\")\n",
    "        return float(d.dt.total_seconds().div(3600).mode().iloc[0])\n",
    "\n",
    "    g = tmp.groupby(\"unique_id\").agg(\n",
    "        rows=(value_col, \"count\"),\n",
    "        na_y=(value_col, lambda s: int(s.isna().sum())),\n",
    "        min_ds=(\"ds\", \"min\"),\n",
    "        max_ds=(\"ds\", \"max\"),\n",
    "        min_y=(value_col, \"min\"),\n",
    "        max_y=(value_col, \"max\"),\n",
    "        mean_y=(value_col, \"mean\"),\n",
    "        zero_y=(value_col, lambda s: int((s == 0).sum())),\n",
    "        mode_delta_hours=(\"ds\", _mode_delta_hours),\n",
    "    ).reset_index().sort_values(\"unique_id\")\n",
    "\n",
    "    print(f\"[{label}] series={g['unique_id'].nunique()} rows={len(tmp)}\")\n",
    "    print(g.head(20).to_string(index=False))\n",
    "\n",
    "def _missing_hour_blocks(ds: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp, int]]:\n",
    "    \"\"\"\n",
    "    Return contiguous blocks of missing hourly timestamps.\n",
    "    Each tuple: (block_start, block_end, n_hours)\n",
    "    \"\"\"\n",
    "    ds = pd.to_datetime(ds, errors=\"raise\").sort_values()\n",
    "    start, end = ds.iloc[0], ds.iloc[-1]\n",
    "    expected = pd.date_range(start, end, freq=\"h\")\n",
    "    missing = expected.difference(ds)\n",
    "\n",
    "    if missing.empty:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    block_start = missing[0]\n",
    "    prev = missing[0]\n",
    "    for t in missing[1:]:\n",
    "        if t - prev == pd.Timedelta(hours=1):\n",
    "            prev = t\n",
    "        else:\n",
    "            n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "            blocks.append((block_start, prev, n))\n",
    "            block_start = t\n",
    "            prev = t\n",
    "    n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "    blocks.append((block_start, prev, n))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def _hourly_grid_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\n",
    "        \"unique_id\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "        \"expected_hours\",\n",
    "        \"actual_hours\",\n",
    "        \"missing_hours\",\n",
    "        \"missing_ratio\",\n",
    "        \"n_missing_blocks\",\n",
    "        \"largest_missing_block_hours\",\n",
    "        \"first_missing_block_start\",\n",
    "        \"first_missing_block_end\",\n",
    "    ]\n",
    "\n",
    "    if df.empty:\n",
    "        # Return an empty report with a stable schema (so callers can fail-loud cleanly)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    for uid, g in df.groupby(\"unique_id\"):\n",
    "        g = g.sort_values(\"ds\")\n",
    "        start, end = g[\"ds\"].iloc[0], g[\"ds\"].iloc[-1]\n",
    "        expected = pd.date_range(start, end, freq=\"h\")\n",
    "        missing = expected.difference(g[\"ds\"])\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"unique_id\": uid,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"expected_hours\": int(len(expected)),\n",
    "                \"actual_hours\": int(len(g)),\n",
    "                \"missing_hours\": int(len(missing)),\n",
    "                \"missing_ratio\": float(len(missing) / max(len(expected), 1)),\n",
    "                \"n_missing_blocks\": int(len(blocks)),\n",
    "                \"largest_missing_block_hours\": int(max([b[2] for b in blocks], default=0)),\n",
    "                \"first_missing_block_start\": blocks[0][0] if blocks else pd.NaT,\n",
    "                \"first_missing_block_end\": blocks[0][1] if blocks else pd.NaT,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rep = pd.DataFrame(rows)\n",
    "    return rep.sort_values([\"missing_ratio\", \"missing_hours\"], ascending=False)\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    label: str,\n",
    "    policy: str = \"raise\",  # \"raise\" | \"drop_incomplete_series\"\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Cannot enforce hourly grid: input dataframe is empty. \"\n",
    "            \"This is upstream (fetch) failure, not a grid issue.\"\n",
    "        )\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    if rep.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] No series found to report on (rep empty). \"\n",
    "            \"This indicates upstream emptiness or missing 'unique_id' groups.\"\n",
    "        )\n",
    "\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "\n",
    "    if worst[\"missing_hours\"] == 0:\n",
    "        return df\n",
    "\n",
    "    print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "\n",
    "    if policy == \"drop_incomplete_series\":\n",
    "        bad_uids = rep.loc[rep[\"missing_hours\"] > 0, \"unique_id\"].tolist()\n",
    "        kept = df.loc[~df[\"unique_id\"].isin(bad_uids)].copy()\n",
    "        print(f\"[{label}][GRID] policy=drop_incomplete_series dropped={bad_uids} kept_series={kept['unique_id'].nunique()}\")\n",
    "        if kept.empty:\n",
    "            raise RuntimeError(f\"[{label}][GRID] all series dropped due to missing hours\")\n",
    "        return kept\n",
    "\n",
    "    worst_uid = worst[\"unique_id\"]\n",
    "    g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "    blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "    raise RuntimeError(\n",
    "        f\"[{label}][GRID] Missing hours detected (no imputation). \"\n",
    "        f\"worst_unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "        f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _validate_hourly_grid_fail_loud(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_missing_ratio: float = 0.0,\n",
    "    label: str = \"generation\",\n",
    ") -> None:\n",
    "    # Keep your original basic checks:\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"[{label}] empty dataframe\")\n",
    "\n",
    "    bad = df[\"ds\"].isna().sum()\n",
    "    if bad:\n",
    "        raise RuntimeError(f\"[{label}] ds has NaT values bad={int(bad)}\")\n",
    "\n",
    "    dup = df.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        raise RuntimeError(f\"[{label}] duplicate (unique_id, ds) rows dup={int(dup)}\")\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "    if worst[\"missing_ratio\"] > max_missing_ratio:\n",
    "        print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "        worst_uid = worst[\"unique_id\"]\n",
    "        g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Missing hours detected (no imputation allowed). \"\n",
    "            f\"unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "            f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "\n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "def _infer_model_columns(cv_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Infer StatsForecast model prediction columns from a cross_validation dataframe.\n",
    "\n",
    "    We treat as \"model columns\" those that:\n",
    "      - are not core columns (unique_id, ds, cutoff, y)\n",
    "      - are not metadata columns (index, level_0, etc.)\n",
    "      - are not interval columns like '<model>-lo-80' or '<model>-hi-95'\n",
    "    \"\"\"\n",
    "    # Core columns from StatsForecast + pandas residuals from reset_index()\n",
    "    core = {\"unique_id\", \"ds\", \"cutoff\", \"y\", \"index\", \"level_0\", \"level_1\"}\n",
    "    cols = [c for c in cv_df.columns if c not in core]\n",
    "\n",
    "    model_cols: set[str] = set()\n",
    "    interval_pat = re.compile(r\"-(lo|hi)-\\d+$\")\n",
    "    for c in cols:\n",
    "        if interval_pat.search(c):\n",
    "            continue\n",
    "        model_cols.add(c)\n",
    "\n",
    "    return sorted(model_cols)\n",
    "\n",
    "\n",
    "def compute_leaderboard(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    confidence_levels: tuple[int, int] = (80, 95),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an aggregated leaderboard from StatsForecast cross_validation output.\n",
    "\n",
    "    Returns columns:\n",
    "      - model, rmse, mae, mape, valid_rows\n",
    "      - coverage_<level> if interval columns exist\n",
    "    \"\"\"\n",
    "    required = {\"y\", \"unique_id\", \"ds\", \"cutoff\"}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"[leaderboard] cv_df missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    model_cols = _infer_model_columns(cv_df)\n",
    "    if not model_cols:\n",
    "        raise RuntimeError(\n",
    "            f\"[leaderboard] Could not infer any model prediction columns. \"\n",
    "            f\"cv_df columns={cv_df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    y_true = cv_df[\"y\"].to_numpy()\n",
    "\n",
    "    for m in model_cols:\n",
    "        if m not in cv_df.columns:\n",
    "            continue\n",
    "\n",
    "        y_pred = cv_df[m].to_numpy()\n",
    "        valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        valid_rows = int(valid_mask.sum())\n",
    "\n",
    "        metrics = {\n",
    "            \"model\": m,\n",
    "            \"rmse\": float(ForecastMetrics.rmse(y_true, y_pred)),\n",
    "            \"mae\": float(ForecastMetrics.mae(y_true, y_pred)),\n",
    "            \"mape\": float(ForecastMetrics.mape(y_true, y_pred)),\n",
    "            \"valid_rows\": valid_rows,\n",
    "        }\n",
    "\n",
    "        # Coverage if interval columns exist\n",
    "        for lvl in confidence_levels:\n",
    "            lo_col = f\"{m}-lo-{lvl}\"\n",
    "            hi_col = f\"{m}-hi-{lvl}\"\n",
    "            if lo_col in cv_df.columns and hi_col in cv_df.columns:\n",
    "                cov = ForecastMetrics.coverage(\n",
    "                    y_true,\n",
    "                    cv_df[lo_col].to_numpy(),\n",
    "                    cv_df[hi_col].to_numpy(),\n",
    "                )\n",
    "                metrics[f\"coverage_{lvl}\"] = float(cov)\n",
    "\n",
    "        rows.append(metrics)\n",
    "\n",
    "    lb = pd.DataFrame(rows)\n",
    "    if lb.empty:\n",
    "        raise RuntimeError(\"[leaderboard] computed empty leaderboard (no usable model columns).\")\n",
    "\n",
    "    # Fail-loud sorting: rmse NaNs should sort last\n",
    "    lb = lb.sort_values([\"rmse\"], ascending=True, na_position=\"last\").reset_index(drop=True)\n",
    "    return lb\n",
    "\n",
    "\n",
    "def compute_baseline_metrics(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    threshold_k: float = 2.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for drift detection from CV output.\n",
    "\n",
    "    We compute RMSE/MAE per (unique_id, cutoff) window, then aggregate:\n",
    "      rmse_mean, rmse_std, drift_threshold_rmse = mean + k*std\n",
    "\n",
    "    No imputation/filling: metrics are computed only from finite values.\n",
    "    \"\"\"\n",
    "    required = {\"unique_id\", \"cutoff\", \"y\", model_name}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"[baseline] cv_df missing required columns for model '{model_name}': {sorted(missing)}\"\n",
    "        )\n",
    "\n",
    "    # Compute per-window metrics (unique_id, cutoff)\n",
    "    def _window_metrics(g: pd.DataFrame) -> pd.Series:\n",
    "        yt = g[\"y\"].to_numpy()\n",
    "        yp = g[model_name].to_numpy()\n",
    "        valid = np.isfinite(yt) & np.isfinite(yp)\n",
    "        if valid.sum() == 0:\n",
    "            return pd.Series({\"rmse\": np.nan, \"mae\": np.nan, \"valid_rows\": 0})\n",
    "        return pd.Series({\n",
    "            \"rmse\": ForecastMetrics.rmse(yt, yp),\n",
    "            \"mae\": ForecastMetrics.mae(yt, yp),\n",
    "            \"valid_rows\": int(valid.sum()),\n",
    "        })\n",
    "\n",
    "    per_window = (\n",
    "        cv_df.groupby([\"unique_id\", \"cutoff\"], sort=False, dropna=False)\n",
    "        .apply(_window_metrics)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Fail loud if baseline is entirely NaN\n",
    "    if per_window[\"rmse\"].notna().sum() == 0:\n",
    "        sample_cols = [\"unique_id\", \"cutoff\", \"y\", model_name]\n",
    "        raise RuntimeError(\n",
    "            \"[baseline] All per-window RMSE are NaN. \"\n",
    "            \"This usually means predictions or y are non-finite everywhere. \"\n",
    "            f\"Sample:\\n{cv_df[sample_cols].head(20).to_string(index=False)}\"\n",
    "        )\n",
    "\n",
    "    rmse_mean = float(per_window[\"rmse\"].mean(skipna=True))\n",
    "    rmse_std = float(per_window[\"rmse\"].std(skipna=True, ddof=0))\n",
    "    mae_mean = float(per_window[\"mae\"].mean(skipna=True))\n",
    "    mae_std = float(per_window[\"mae\"].std(skipna=True, ddof=0))\n",
    "\n",
    "    baseline = {\n",
    "        \"model\": model_name,\n",
    "        \"rmse_mean\": rmse_mean,\n",
    "        \"rmse_std\": rmse_std,\n",
    "        \"mae_mean\": mae_mean,\n",
    "        \"mae_std\": mae_std,\n",
    "        \"drift_threshold_rmse\": float(rmse_mean + threshold_k * rmse_std),\n",
    "        \"drift_threshold_mae\": float(mae_mean + threshold_k * mae_std),\n",
    "        \"n_series\": int(per_window[\"unique_id\"].nunique()),\n",
    "        \"n_windows\": int(per_window[\"cutoff\"].nunique()),\n",
    "        \"per_window_rows\": int(len(per_window)),\n",
    "    }\n",
    "\n",
    "    # Optional per-series baseline (useful later if you want drift per series)\n",
    "    per_series = (\n",
    "        per_window.groupby(\"unique_id\")[[\"rmse\", \"mae\"]]\n",
    "        .agg(rmse_mean=(\"rmse\", \"mean\"), rmse_std=(\"rmse\", lambda s: s.std(ddof=0)),\n",
    "             mae_mean=(\"mae\", \"mean\"), mae_std=(\"mae\", lambda s: s.std(ddof=0)))\n",
    "        .reset_index()\n",
    "    )\n",
    "    per_series[\"drift_threshold_rmse\"] = per_series[\"rmse_mean\"] + threshold_k * per_series[\"rmse_std\"]\n",
    "    per_series[\"drift_threshold_mae\"] = per_series[\"mae_mean\"] + threshold_k * per_series[\"mae_std\"]\n",
    "    baseline[\"per_series\"] = per_series.to_dict(orient=\"records\")\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "\n",
    "\n",
    "class RenewableForecastModel:\n",
    "    def __init__(self, horizon: int = 24, confidence_levels: tuple[int, int] = (80, 95)):\n",
    "        self.horizon = horizon\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.sf = None\n",
    "        self._train_df = None  # contains y + exog columns\n",
    "        self._exog_cols: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def prepare_training_df(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare training dataframe for modeling.\n",
    "\n",
    "        NOTE: This function now supports BOTH:\n",
    "        1. Raw data (from old pipeline): performs all preprocessing\n",
    "        2. Preprocessed data (from dataset_builder): skips preprocessing\n",
    "\n",
    "        Detection: If df has time features (hour_sin, hour_cos, etc.), assumes already preprocessed.\n",
    "        \"\"\"\n",
    "        req = {\"unique_id\", \"ds\", \"y\"}\n",
    "        if not req.issubset(df.columns):\n",
    "            raise ValueError(f\"generation df missing cols={sorted(req - set(df.columns))}\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise RuntimeError(\n",
    "                \"[generation] Empty generation dataframe passed into modeling. \"\n",
    "                \"This is upstream (EIA fetch/cache) failure — inspect fetch_diagnostics and fetch_generation logs.\"\n",
    "            )\n",
    "\n",
    "        work = df.copy()\n",
    "        work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"raise\")\n",
    "        work = work.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        # Check if data is already preprocessed (has time features)\n",
    "        time_features = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "        is_preprocessed = all(col in work.columns for col in time_features)\n",
    "\n",
    "        if is_preprocessed:\n",
    "            # Data already preprocessed by dataset_builder, skip preprocessing\n",
    "            print(\"[prepare_training_df] Detected preprocessed data (has time features), skipping preprocessing\")\n",
    "\n",
    "            # Validate y has no nulls\n",
    "            y_null = work[\"y\"].isna()\n",
    "            if y_null.any():\n",
    "                sample = work.loc[y_null, [\"unique_id\", \"ds\", \"y\"]].head(25)\n",
    "                raise RuntimeError(\n",
    "                    f\"[generation][Y] Found null y values. rows={int(y_null.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "            # Identify exog columns (time features + any weather vars present)\n",
    "            wcols = [c for c in WEATHER_VARS if c in work.columns]\n",
    "            self._exog_cols = time_features + wcols\n",
    "\n",
    "            print(f\"[prepare_training_df] Using {len(self._exog_cols)} exog features: {self._exog_cols[:5]}...\")\n",
    "            return work\n",
    "\n",
    "        # Legacy path: raw data, perform all preprocessing\n",
    "        print(\"[prepare_training_df] Processing raw data (legacy path)\")\n",
    "\n",
    "        y_null = work[\"y\"].isna()\n",
    "        if y_null.any():\n",
    "            sample = work.loc[y_null, [\"unique_id\", \"ds\", \"y\"]].head(25)\n",
    "            raise RuntimeError(\n",
    "                f\"[generation][Y] Found null y values (no imputation). rows={int(y_null.sum())}. \"\n",
    "                f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "            )\n",
    "\n",
    "        work = _enforce_hourly_grid(work, label=\"generation\", policy=\"drop_incomplete_series\")\n",
    "        work = _add_time_features(work)\n",
    "\n",
    "        if weather_df is not None and not weather_df.empty:\n",
    "            if not {\"ds\", \"region\"}.issubset(weather_df.columns):\n",
    "                raise ValueError(\"weather_df must have columns ['ds','region', ...]\")\n",
    "\n",
    "            work[\"region\"] = work[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "            wcols = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "            if not wcols:\n",
    "                raise ValueError(\"weather_df has none of expected WEATHER_VARS\")\n",
    "\n",
    "            merged = work.merge(\n",
    "                weather_df[[\"ds\", \"region\"] + wcols],\n",
    "                on=[\"ds\", \"region\"],\n",
    "                how=\"left\",\n",
    "                validate=\"many_to_one\",\n",
    "            )\n",
    "\n",
    "            missing_any = merged[wcols].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = merged.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + wcols].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[weather][ALIGN] Missing weather after merge rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "            work = merged.drop(columns=[\"region\"])\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"] + wcols\n",
    "        else:\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "        return work\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame] = None) -> None:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import (MSTL, AutoARIMA, AutoETS,\n",
    "                                          SeasonalNaive)\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "\n",
    "        # Try to add AutoTheta and AutoCES if available (same as cross_validate)\n",
    "        try:\n",
    "            from statsforecast.models import AutoCES, AutoTheta\n",
    "            models.append(AutoTheta(season_length=24))\n",
    "            models.append(AutoCES(season_length=24))\n",
    "            print(\"[fit] Using expanded model set: +AutoTheta, +AutoCES\")\n",
    "        except ImportError:\n",
    "            print(\"[fit] AutoTheta/AutoCES not available, using core models only\")\n",
    "\n",
    "        self.sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "        self._train_df = train_df\n",
    "        self.fitted = True\n",
    "\n",
    "        print(f\"[fit] rows={len(train_df)} series={train_df['unique_id'].nunique()} exog_cols={self._exog_cols}\")\n",
    "\n",
    "    def build_future_X_df(self, future_weather: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build future X_df for forecast horizon using forecast weather.\n",
    "        Must include: unique_id, ds, and exactly the exog columns used in training.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        if future_weather is None or future_weather.empty:\n",
    "            raise RuntimeError(\"future_weather required to forecast with regressors (no fabrication).\")\n",
    "\n",
    "        if not {\"ds\", \"region\"}.issubset(future_weather.columns):\n",
    "            raise ValueError(\"future_weather must have columns ['ds','region', ...]\")\n",
    "\n",
    "        # Create the future ds grid per series\n",
    "        last_ds = self._train_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "        frames = []\n",
    "        for uid, end in last_ds.items():\n",
    "            future_ds = pd.date_range(end + pd.Timedelta(hours=1), periods=self.horizon, freq=\"h\")\n",
    "            frames.append(pd.DataFrame({\"unique_id\": uid, \"ds\": future_ds}))\n",
    "        X = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        X = _add_time_features(X)\n",
    "        X[\"region\"] = X[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "        wcols = [c for c in WEATHER_VARS if c in future_weather.columns]\n",
    "        X = X.merge(\n",
    "            future_weather[[\"ds\", \"region\"] + wcols],\n",
    "            on=[\"ds\", \"region\"],\n",
    "            how=\"left\",\n",
    "            validate=\"many_to_one\",\n",
    "        )\n",
    "\n",
    "        # Fail loud on missing future regressors\n",
    "        needed = [c for c in self._exog_cols if c not in [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]]  # weather cols\n",
    "        if needed:\n",
    "            missing_any = X[needed].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = X.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + needed].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[future_weather][ALIGN] Missing future weather rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "        X = X.drop(columns=[\"region\"])\n",
    "        keep = [\"unique_id\", \"ds\"] + self._exog_cols\n",
    "        return X[keep].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    def predict(self, future_weather: pd.DataFrame, best_model: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Generate forecasts using fitted models.\n",
    "\n",
    "        Args:\n",
    "            future_weather: Future weather data for forecast period\n",
    "            best_model: Optional model name to use for predictions. If provided, only this model's\n",
    "                       predictions will be included in output (as 'yhat' column). If None, all\n",
    "                       fitted models' predictions are returned.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with forecast predictions. If best_model is specified, includes:\n",
    "            - unique_id, ds: identifiers\n",
    "            - yhat: point forecast from best_model\n",
    "            - yhat-lo-{level}, yhat-hi-{level}: prediction intervals from best_model\n",
    "\n",
    "            If best_model is None, includes predictions from all fitted models.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        X_df = self.build_future_X_df(future_weather)\n",
    "\n",
    "        # IMPORTANT: If you fit models using exogenous regressors, you must supply X_df at forecast time.\n",
    "        fcst = self.sf.forecast(\n",
    "            h=self.horizon,\n",
    "            df=self._train_df,\n",
    "            X_df=X_df,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        # Apply minimal physical constraints for solar series\n",
    "        fcst = self._apply_minimal_solar_constraints(fcst, X_df)\n",
    "\n",
    "        # If best_model is specified, filter to only that model's predictions\n",
    "        if best_model is not None:\n",
    "            if best_model not in fcst.columns:\n",
    "                available_models = [c for c in fcst.columns if c not in ['unique_id', 'ds']]\n",
    "                raise ValueError(\n",
    "                    f\"[predict] best_model '{best_model}' not found in forecast output. \"\n",
    "                    f\"Available models: {available_models}\"\n",
    "                )\n",
    "\n",
    "            # Extract best model's predictions and rename to standard 'yhat' format\n",
    "            keep_cols = ['unique_id', 'ds', best_model]\n",
    "\n",
    "            # Also keep prediction interval columns for the best model\n",
    "            for level in self.confidence_levels:\n",
    "                lo_col = f\"{best_model}-lo-{level}\"\n",
    "                hi_col = f\"{best_model}-hi-{level}\"\n",
    "                if lo_col in fcst.columns:\n",
    "                    keep_cols.append(lo_col)\n",
    "                if hi_col in fcst.columns:\n",
    "                    keep_cols.append(hi_col)\n",
    "\n",
    "            fcst = fcst[keep_cols].copy()\n",
    "\n",
    "            # Rename model column to 'yhat' and interval columns to match\n",
    "            # NOTE: Using underscores (not hyphens) to match dashboard expectations\n",
    "            rename_map = {best_model: 'yhat'}\n",
    "            for level in self.confidence_levels:\n",
    "                old_lo = f\"{best_model}-lo-{level}\"\n",
    "                old_hi = f\"{best_model}-hi-{level}\"\n",
    "                if old_lo in fcst.columns:\n",
    "                    rename_map[old_lo] = f\"yhat_lo_{level}\"  # Changed hyphen to underscore\n",
    "                if old_hi in fcst.columns:\n",
    "                    rename_map[old_hi] = f\"yhat_hi_{level}\"  # Changed hyphen to underscore\n",
    "\n",
    "            fcst = fcst.rename(columns=rename_map)\n",
    "\n",
    "        return fcst\n",
    "\n",
    "    def _apply_minimal_solar_constraints(\n",
    "        self,\n",
    "        fcst: pd.DataFrame,\n",
    "        X_df: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply ONLY minimal physical constraints for impossible cases.\n",
    "\n",
    "        Philosophy: Let the model learn natural patterns. Only intervene when\n",
    "        physics is violated (e.g., generation when sun is below horizon).\n",
    "\n",
    "        Constraint: Zero generation when BOTH radiation sources are zero\n",
    "        (sun is definitely below horizon).\n",
    "        \"\"\"\n",
    "        # Merge forecast with features to get radiation data\n",
    "        fcst_with_features = fcst.merge(\n",
    "            X_df[[\"unique_id\", \"ds\", \"direct_radiation\", \"diffuse_radiation\"]],\n",
    "            on=[\"unique_id\", \"ds\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Identify solar series\n",
    "        solar_mask = fcst_with_features[\"unique_id\"].str.endswith(\"_SUN\")\n",
    "\n",
    "        # ONLY constraint: Zero generation when BOTH radiation sources are zero\n",
    "        # (sun is definitely below horizon)\n",
    "        no_sun_mask = (\n",
    "            (fcst_with_features[\"direct_radiation\"] == 0) &\n",
    "            (fcst_with_features[\"diffuse_radiation\"] == 0)\n",
    "        )\n",
    "\n",
    "        # Apply constraint to solar series\n",
    "        constrain_mask = solar_mask & no_sun_mask\n",
    "\n",
    "        # Set all forecast columns to 0 (cannot generate without any sunlight)\n",
    "        # Note: At this point, columns are model names (AutoARIMA, MSTL_ARIMA, etc.)\n",
    "        # and their intervals (model-lo-80, model-hi-80, etc.), not \"yhat\"\n",
    "        # Exclude unique_id, ds, and feature columns\n",
    "        exclude_cols = {'unique_id', 'ds', 'direct_radiation', 'diffuse_radiation'}\n",
    "        forecast_cols = [c for c in fcst_with_features.columns if c not in exclude_cols]\n",
    "\n",
    "        for col in forecast_cols:\n",
    "            fcst_with_features.loc[constrain_mask, col] = 0.0\n",
    "\n",
    "        return fcst_with_features.drop(columns=[\"direct_radiation\", \"diffuse_radiation\"], errors=\"ignore\")\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        weather_df: Optional[pd.DataFrame] = None,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "        expanded_models: bool = True,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import (MSTL, AutoARIMA, AutoETS,\n",
    "                                          SeasonalNaive)\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        # Core models\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "\n",
    "        # Add expanded models if requested\n",
    "        if expanded_models:\n",
    "            try:\n",
    "                from statsforecast.models import AutoCES, AutoTheta\n",
    "                models.extend([\n",
    "                    AutoTheta(season_length=24),\n",
    "                    AutoCES(season_length=24),\n",
    "                ])\n",
    "                print(\"[cv] Using expanded model set: +AutoTheta, +AutoCES\")\n",
    "            except ImportError:\n",
    "                print(\"[cv] AutoTheta/AutoCES not available, using core models only\")\n",
    "\n",
    "        sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "\n",
    "        print(\n",
    "            f\"[cv] windows={n_windows} step={step_size} h={self.horizon} \"\n",
    "            f\"rows={len(train_df)} series={train_df['unique_id'].nunique()}\"\n",
    "        )\n",
    "\n",
    "        cv = sf.cross_validation(\n",
    "            df=train_df,\n",
    "            h=self.horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        leaderboard = compute_leaderboard(cv, confidence_levels=self.confidence_levels)\n",
    "        return cv, leaderboard\n",
    "\n",
    "\n",
    "class RenewableLGBMForecaster:\n",
    "    \"\"\"\n",
    "    LightGBM-based forecaster with interpretability support.\n",
    "\n",
    "    This forecaster is designed for model interpretability (SHAP, feature importance)\n",
    "    rather than production forecasting. Use alongside RenewableForecastModel which\n",
    "    provides better uncertainty quantification via statistical models.\n",
    "\n",
    "    Uses skforecast's ForecasterRecursive with LightGBM as the base estimator,\n",
    "    along with rolling window features for temporal patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon: int = 24,\n",
    "        lags: int = 168,  # 7 days of lags\n",
    "        rolling_window_sizes: list[int] | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LightGBM forecaster.\n",
    "\n",
    "        Args:\n",
    "            horizon: Forecast horizon in hours\n",
    "            lags: Number of lag features (default 168 = 7 days)\n",
    "            rolling_window_sizes: Window sizes for rolling features (default [24, 168])\n",
    "        \"\"\"\n",
    "        self.horizon = horizon\n",
    "        self.lags = lags\n",
    "        self.rolling_window_sizes = rolling_window_sizes or [24, 168]\n",
    "        self.forecaster = None\n",
    "        self._exog_features: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, y: pd.Series, exog: Optional[pd.DataFrame] = None) -> None:\n",
    "        \"\"\"\n",
    "        Fit LightGBM forecaster with rolling features.\n",
    "\n",
    "        Args:\n",
    "            y: Target time series (must have DatetimeIndex)\n",
    "            exog: Optional exogenous features (must have same index as y)\n",
    "        \"\"\"\n",
    "        # Import here to make dependencies optional\n",
    "        try:\n",
    "            from lightgbm import LGBMRegressor\n",
    "            from skforecast.preprocessing import RollingFeatures\n",
    "            from skforecast.recursive import ForecasterRecursive\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\n",
    "                \"LightGBM forecaster requires: lightgbm, skforecast. \"\n",
    "                f\"Install with: pip install lightgbm skforecast. Error: {e}\"\n",
    "            )\n",
    "\n",
    "        # Create rolling features\n",
    "        # Note: skforecast requires window_sizes length to match stats length\n",
    "        # We use 'mean' for each window size to capture different temporal patterns\n",
    "        stats_list = ['mean'] * len(self.rolling_window_sizes)\n",
    "        window_features = RollingFeatures(\n",
    "            stats=stats_list,\n",
    "            window_sizes=self.rolling_window_sizes,\n",
    "        )\n",
    "\n",
    "        # Initialize forecaster\n",
    "        self.forecaster = ForecasterRecursive(\n",
    "            estimator=LGBMRegressor(\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                num_leaves=31,\n",
    "                min_child_samples=20,\n",
    "            ),\n",
    "            lags=self.lags,\n",
    "            window_features=window_features,\n",
    "        )\n",
    "\n",
    "        # Store exog feature names\n",
    "        if exog is not None:\n",
    "            self._exog_features = exog.columns.tolist()\n",
    "        else:\n",
    "            self._exog_features = []\n",
    "\n",
    "        # Fit the model\n",
    "        self.forecaster.fit(y=y, exog=exog)\n",
    "        self.fitted = True\n",
    "\n",
    "        n_features = len(self.get_feature_importances())\n",
    "        print(f\"[LGBMForecaster.fit] fitted with {n_features} features, lags={self.lags}\")\n",
    "\n",
    "    def predict(self, steps: int, exog: Optional[pd.DataFrame] = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Generate predictions.\n",
    "\n",
    "        Args:\n",
    "            steps: Number of steps to forecast\n",
    "            exog: Exogenous features for forecast period\n",
    "\n",
    "        Returns:\n",
    "            Series of predictions\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before predict()\")\n",
    "\n",
    "        return self.forecaster.predict(steps=steps, exog=exog)\n",
    "\n",
    "    def get_feature_importances(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract feature importance from fitted model.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with 'feature' and 'importance' columns\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before get_feature_importances()\")\n",
    "\n",
    "        return self.forecaster.get_feature_importances()\n",
    "\n",
    "    def create_train_X_y(\n",
    "        self,\n",
    "        y: pd.Series,\n",
    "        exog: Optional[pd.DataFrame] = None,\n",
    "    ) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Create training matrices for SHAP analysis.\n",
    "\n",
    "        Args:\n",
    "            y: Target time series\n",
    "            exog: Optional exogenous features\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (X_train, y_train)\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before create_train_X_y()\")\n",
    "\n",
    "        return self.forecaster.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "    @property\n",
    "    def regressor(self):\n",
    "        \"\"\"\n",
    "        Access internal LightGBM estimator for SHAP.\n",
    "\n",
    "        Returns:\n",
    "            The fitted LGBMRegressor instance\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before accessing regressor\")\n",
    "\n",
    "        # Use 'estimator' (new API) with fallback to 'regressor' (deprecated)\n",
    "        if hasattr(self.forecaster, \"estimator\"):\n",
    "            return self.forecaster.estimator\n",
    "        return self.forecaster.regressor\n",
    "\n",
    "    @property\n",
    "    def exog_features(self) -> list[str]:\n",
    "        \"\"\"Return list of exogenous feature names used in training.\"\"\"\n",
    "        return self._exog_features.copy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # REAL EXAMPLE: multi-series WND with strict gates and CV\n",
    "\n",
    "    from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "    from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "\n",
    "    regions = [\"CALI\", \"ERCO\", \"MISO\"]\n",
    "    fuel = \"WND\"\n",
    "    start_date = \"2024-11-01\"\n",
    "    end_date = \"2024-12-15\"\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "    gen = fetcher.fetch_all_regions(fuel, start_date, end_date, regions=regions)\n",
    "    _log_series_summary(gen, label=\"generation_raw\")\n",
    "\n",
    "    weather_api = OpenMeteoRenewable(strict=True)\n",
    "    wx_hist = weather_api.fetch_all_regions_historical(regions, start_date, end_date, debug=True)\n",
    "\n",
    "    model = RenewableForecastModel(horizon=24, confidence_levels=(80, 95))\n",
    "\n",
    "    # CV (historical): regressors live in df, no filling allowed\n",
    "    cv = model.cross_validate(gen, weather_df=wx_hist, n_windows=3, step_size=168)\n",
    "    print(cv.head().to_string(index=False))\n",
    "\n",
    "    # Optional: fit + forecast next 24h using forecast weather (no leakage)\n",
    "    # wx_future = weather_api.fetch_all_regions_forecast(regions, horizon_hours=48, debug=True)\n",
    "    # model.fit(gen, weather_df=wx_hist)\n",
    "    # fcst = model.predict(future_weather=wx_future)\n",
    "    # print(fcst.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Module: Pipeline Tasks\n",
    "\n",
    "**File:** `src/renewable/tasks.py`\n",
    "\n",
    "This module orchestrates the complete pipeline:\n",
    "\n",
    "1. **Fetch generation data** from EIA\n",
    "2. **Fetch weather data** from Open-Meteo\n",
    "3. **Train models** with cross-validation\n",
    "4. **Generate forecasts** with prediction intervals\n",
    "5. **Compute drift metrics** vs baseline\n",
    "\n",
    "## Key Feature: Adaptive CV\n",
    "\n",
    "Cross-validation requires sufficient data:\n",
    "```\n",
    "Minimum rows = horizon + (n_windows × step_size)\n",
    "```\n",
    "\n",
    "For short series, we **adapt** the CV settings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/renewable/tasks.py\n",
    "# file: src/renewable/tasks.py\n",
    "\"\"\"Renewable energy forecasting pipeline tasks.\n",
    "\n",
    "Idempotent tasks for:\n",
    "- Fetching EIA renewable generation data\n",
    "- Fetching weather data from Open-Meteo\n",
    "- Training probabilistic models\n",
    "- Generating forecasts with intervals\n",
    "- Computing drift metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "from src.renewable.modeling import (\n",
    "    RenewableForecastModel,\n",
    "    RenewableLGBMForecaster,\n",
    "    _log_series_summary,\n",
    "    _add_time_features,\n",
    "    compute_baseline_metrics,\n",
    "    WEATHER_VARS,\n",
    ")\n",
    "from src.renewable.model_interpretability import (\n",
    "    InterpretabilityReport,\n",
    "    generate_full_interpretability_report,\n",
    ")\n",
    "from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "from src.renewable.regions import REGIONS, list_regions\n",
    "from src.renewable.dataset_builder import build_modeling_dataset, PreprocessingReport\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RenewablePipelineConfig:\n",
    "    \"\"\"Configuration for renewable forecasting pipeline.\"\"\"\n",
    "\n",
    "    # Data parameters\n",
    "    regions: list[str] = field(default_factory=lambda: [\"CALI\", \"ERCO\", \"MISO\", \"PJM\", \"SWPP\"])\n",
    "    fuel_types: list[str] = field(default_factory=lambda: [\"WND\", \"SUN\"])\n",
    "    start_date: str = \"\"  # Set dynamically\n",
    "    end_date: str = \"\"  # Set dynamically\n",
    "    lookback_days: int = 30\n",
    "\n",
    "    # Forecast parameters\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "    horizon_preset: Optional[str] = None  # \"24h\" | \"48h\" | \"72h\"\n",
    "\n",
    "    # CV parameters\n",
    "    cv_windows: int = 5\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "    # Model parameters\n",
    "    enable_interpretability: bool = True  # LightGBM SHAP analysis (on by default)\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    negative_policy: str = \"clamp\"  # \"clamp\" | \"fail_loud\" | \"hybrid\"\n",
    "    hourly_grid_policy: str = \"drop_incomplete_series\"  # \"drop_incomplete_series\" | \"fail_loud\"\n",
    "\n",
    "    # Output paths\n",
    "    data_dir: str = \"data/renewable\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    # Horizon preset definitions (class-level constant)\n",
    "    _PRESETS = {\n",
    "        \"24h\": {\"horizon\": 24, \"cv_windows\": 2, \"lookback_days\": 15},\n",
    "        \"48h\": {\"horizon\": 48, \"cv_windows\": 3, \"lookback_days\": 21},\n",
    "        \"72h\": {\"horizon\": 72, \"cv_windows\": 3, \"lookback_days\": 28},\n",
    "    }\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Apply horizon preset if specified\n",
    "        if self.horizon_preset and self.horizon_preset in self._PRESETS:\n",
    "            preset = self._PRESETS[self.horizon_preset]\n",
    "            # Use object.__setattr__ since this is a dataclass\n",
    "            object.__setattr__(self, \"horizon\", preset[\"horizon\"])\n",
    "            object.__setattr__(self, \"cv_windows\", preset[\"cv_windows\"])\n",
    "            object.__setattr__(self, \"lookback_days\", preset[\"lookback_days\"])\n",
    "            logger.info(f\"[config] Applied preset '{self.horizon_preset}': horizon={preset['horizon']}h\")\n",
    "\n",
    "        # Set default dates if not provided\n",
    "        if not self.end_date:\n",
    "            self.end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        if not self.start_date:\n",
    "            end = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "            start = end - timedelta(days=self.lookback_days)\n",
    "            self.start_date = start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Validate configuration\n",
    "        warnings = self._validate()\n",
    "        for warning in warnings:\n",
    "            logger.warning(f\"[config] {warning}\")\n",
    "\n",
    "    def _validate(self) -> list[str]:\n",
    "        \"\"\"Validate configuration and return warnings.\"\"\"\n",
    "        warnings = []\n",
    "\n",
    "        # Check minimum data requirement\n",
    "        available_hours = self.lookback_days * 24\n",
    "        required_hours = self.horizon + (self.cv_windows * self.cv_step_size)\n",
    "        if available_hours < required_hours:\n",
    "            warnings.append(\n",
    "                f\"Insufficient data: need {required_hours}h, have {available_hours}h. \"\n",
    "                f\"Increase lookback_days to {(required_hours // 24) + 1} or reduce cv_windows.\"\n",
    "            )\n",
    "\n",
    "        # Warn about accuracy degradation\n",
    "        if self.horizon > 72:\n",
    "            warnings.append(\n",
    "                f\"Horizon {self.horizon}h exceeds recommended max (72h). \"\n",
    "                f\"Weather forecast accuracy degrades significantly beyond 3 days.\"\n",
    "            )\n",
    "\n",
    "        return warnings\n",
    "\n",
    "    def generation_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"generation.parquet\"\n",
    "\n",
    "    def weather_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"weather.parquet\"\n",
    "\n",
    "    def forecasts_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"forecasts.parquet\"\n",
    "\n",
    "    def baseline_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"baseline.json\"\n",
    "\n",
    "    def interpretability_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"interpretability\"\n",
    "\n",
    "    def preprocessing_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"preprocessing\"\n",
    "\n",
    "\n",
    "def fetch_renewable_data(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 1: Fetch EIA generation data for all regions and fuel types.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [unique_id, ds, y]\n",
    "    \"\"\"\n",
    "    output_path = config.generation_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_generation_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        _log_series_summary(df, value_col=\"y\", label=f\"generation_data_{source}\")\n",
    "\n",
    "        expected_series = {\n",
    "            f\"{region}_{fuel}\" for region in config.regions for fuel in config.fuel_types\n",
    "        }\n",
    "        present_series = set(df[\"unique_id\"]) if \"unique_id\" in df.columns else set()\n",
    "        missing_series = sorted(expected_series - present_series)\n",
    "        if missing_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Missing expected series (%s): %s\",\n",
    "                source,\n",
    "                missing_series,\n",
    "            )\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_generation] No generation data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"unique_id\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"unique_id\")\n",
    "        )\n",
    "        max_series_log = 25\n",
    "        if len(coverage) > max_series_log:\n",
    "            logger.info(\n",
    "                \"[fetch_generation] Coverage (%s, first %s series):\\n%s\",\n",
    "                source,\n",
    "                max_series_log,\n",
    "                coverage.head(max_series_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_generation] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_generation] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached coverage to surface missing series without refetching.\n",
    "        _log_generation_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_generation] Fetching {config.fuel_types} for {config.regions}\")\n",
    "\n",
    "    # Use longer timeout (90s) to handle slow EIA API responses\n",
    "    fetcher = EIARenewableFetcher(timeout=90)\n",
    "    all_dfs = []\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        df = fetcher.fetch_all_regions(\n",
    "            fuel_type=fuel_type,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            regions=config.regions,\n",
    "            diagnostics=fetch_diagnostics,\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined = combined.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh coverage to highlight gaps or unexpected negatives.\n",
    "    _log_generation_summary(combined, source=\"fresh\")\n",
    "\n",
    "    if fetch_diagnostics:\n",
    "        empty_series = [\n",
    "            entry\n",
    "            for entry in fetch_diagnostics\n",
    "            if entry.get(\"empty\")\n",
    "        ]\n",
    "        for entry in empty_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Empty series detail: region=%s fuel=%s total=%s pages=%s\",\n",
    "                entry.get(\"region\"),\n",
    "                entry.get(\"fuel_type\"),\n",
    "                entry.get(\"total_records\"),\n",
    "                entry.get(\"pages\"),\n",
    "            )\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_generation] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def fetch_renewable_weather(\n",
    "    config: RenewablePipelineConfig,\n",
    "    include_forecast: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 2: Fetch weather data for all regions.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        include_forecast: Include forecast weather for predictions\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [ds, region, weather_vars...]\n",
    "    \"\"\"\n",
    "    output_path = config.weather_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_weather_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_weather] No weather data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"region\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"region\")\n",
    "        )\n",
    "        max_region_log = 25\n",
    "        if len(coverage) > max_region_log:\n",
    "            logger.info(\n",
    "                \"[fetch_weather] Coverage (%s, first %s regions):\\n%s\",\n",
    "                source,\n",
    "                max_region_log,\n",
    "                coverage.head(max_region_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_weather] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "        missing_cols = [\n",
    "            col for col in OpenMeteoRenewable.WEATHER_VARS if col not in df.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing expected weather columns (%s): %s\",\n",
    "                source,\n",
    "                missing_cols,\n",
    "            )\n",
    "\n",
    "        missing_values = {\n",
    "            col: int(df[col].isna().sum())\n",
    "            for col in OpenMeteoRenewable.WEATHER_VARS\n",
    "            if col in df.columns and df[col].isna().any()\n",
    "        }\n",
    "        if missing_values:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing weather values (%s): %s\",\n",
    "                source,\n",
    "                missing_values,\n",
    "            )\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_weather] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached weather coverage to surface missing regions/columns.\n",
    "        _log_weather_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_weather] Fetching weather for {config.regions}\")\n",
    "\n",
    "    weather = OpenMeteoRenewable()\n",
    "\n",
    "    # Historical weather\n",
    "    hist_df = weather.fetch_all_regions_historical(\n",
    "        regions=config.regions,\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "    )\n",
    "\n",
    "    # Validate historical weather result\n",
    "    if hist_df.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[fetch_weather] Historical weather returned empty DataFrame. \"\n",
    "            \"fetch_all_regions_historical should raise an error on failure, \"\n",
    "            \"but received empty result. Check fetch logic.\"\n",
    "        )\n",
    "\n",
    "    if not {\"ds\", \"region\"}.issubset(hist_df.columns):\n",
    "        missing_cols = {\"ds\", \"region\"} - set(hist_df.columns)\n",
    "        raise ValueError(\n",
    "            f\"[fetch_weather] Weather DataFrame missing required columns: {missing_cols}\"\n",
    "        )\n",
    "\n",
    "    hist_regions = hist_df['region'].nunique()\n",
    "    hist_rows = len(hist_df)\n",
    "    logger.info(\n",
    "        f\"[fetch_weather] Historical: {hist_regions} regions, {hist_rows} rows\"\n",
    "    )\n",
    "\n",
    "    # Forecast weather (for prediction, prevents leakage)\n",
    "    if include_forecast:\n",
    "        fcst_df = weather.fetch_all_regions_forecast(\n",
    "            regions=config.regions,\n",
    "            horizon_hours=config.horizon + 24,  # Buffer\n",
    "        )\n",
    "\n",
    "        # Validate forecast weather result\n",
    "        if fcst_df.empty:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Forecast weather returned empty DataFrame. \"\n",
    "                \"Using historical data only for model training and predictions.\"\n",
    "            )\n",
    "            combined = hist_df\n",
    "        else:\n",
    "            fcst_rows = len(fcst_df)\n",
    "            logger.info(f\"[fetch_weather] Forecast: {fcst_rows} rows\")\n",
    "\n",
    "            # Combine, preferring forecast for overlapping times\n",
    "            combined = pd.concat([hist_df, fcst_df], ignore_index=True)\n",
    "            combined = combined.drop_duplicates(subset=[\"ds\", \"region\"], keep=\"last\")\n",
    "    else:\n",
    "        combined = hist_df\n",
    "\n",
    "    combined = combined.sort_values([\"region\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh weather coverage and missing values before saving.\n",
    "    _log_weather_summary(combined, source=\"fresh\")\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_weather] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def train_renewable_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Task 3: Train models and compute baseline metrics via cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cv_results, leaderboard, baseline_metrics)\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_models] Training on {len(generation_df)} rows\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Compute adaptive CV settings based on shortest series\n",
    "    min_series_len = generation_df.groupby(\"unique_id\").size().min()\n",
    "\n",
    "    # CV needs: horizon + (n_windows * step_size) rows minimum\n",
    "    # Solve for n_windows: n_windows = (min_series_len - horizon) / step_size\n",
    "    available_for_cv = min_series_len - config.horizon\n",
    "\n",
    "    # Adjust step_size and n_windows to fit data\n",
    "    step_size = min(config.cv_step_size, max(24, available_for_cv // 3))\n",
    "    n_windows = min(config.cv_windows, max(2, available_for_cv // step_size))\n",
    "\n",
    "    logger.info(\n",
    "        f\"[train_models] Adaptive CV: {n_windows} windows, \"\n",
    "        f\"step={step_size}h (min_series={min_series_len} rows)\"\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results, leaderboard = model.cross_validate(\n",
    "        df=generation_df,\n",
    "        weather_df=weather_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    baseline = compute_baseline_metrics(cv_results, model_name=best_model)\n",
    "\n",
    "\n",
    "    logger.info(f\"[train_models] Best model: {best_model}, RMSE: {baseline['rmse_mean']:.1f}\")\n",
    "\n",
    "    return cv_results, leaderboard, baseline\n",
    "\n",
    "\n",
    "def train_interpretability_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> dict[str, InterpretabilityReport]:\n",
    "    \"\"\"Train LightGBM models and generate interpretability reports per series.\n",
    "\n",
    "    This trains a separate LightGBM model for each series (region × fuel type)\n",
    "    and generates SHAP, partial dependence, and feature importance artifacts.\n",
    "\n",
    "    Note: LightGBM is used for interpretability only. The primary forecasts\n",
    "    come from statistical models (MSTL/ARIMA) which provide better uncertainty\n",
    "    quantification.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping series_id -> InterpretabilityReport\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Training LightGBM for {generation_df['unique_id'].nunique()} series\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    reports: dict[str, InterpretabilityReport] = {}\n",
    "    output_dir = config.interpretability_dir()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for uid in sorted(generation_df[\"unique_id\"].unique()):\n",
    "        logger.info(f\"[train_interpretability] Processing {uid}...\")\n",
    "\n",
    "        # Extract series data\n",
    "        series_data = generation_df[generation_df[\"unique_id\"] == uid].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Prepare target series with proper frequency\n",
    "        y = series_data.set_index(\"ds\")[\"y\"]\n",
    "        y.index = pd.DatetimeIndex(y.index, freq=\"h\")  # Set hourly frequency\n",
    "\n",
    "        # Prepare exogenous features\n",
    "        region = uid.split(\"_\")[0]\n",
    "        series_weather = weather_df[weather_df[\"region\"] == region].copy()\n",
    "\n",
    "        if series_weather.empty:\n",
    "            logger.warning(f\"[train_interpretability] No weather data for region {region}, skipping {uid}\")\n",
    "            continue\n",
    "\n",
    "        # Merge weather to series timestamps\n",
    "        series_data = series_data.merge(\n",
    "            series_weather[[\"ds\"] + [c for c in WEATHER_VARS if c in series_weather.columns]],\n",
    "            on=\"ds\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Add time features\n",
    "        series_data = _add_time_features(series_data)\n",
    "\n",
    "        # Build exog DataFrame aligned with y\n",
    "        exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "        exog_cols += [c for c in WEATHER_VARS if c in series_data.columns]\n",
    "        exog = series_data.set_index(\"ds\")[exog_cols]\n",
    "\n",
    "        # Check for missing weather\n",
    "        missing_weather = exog.isna().any(axis=1).sum()\n",
    "        if missing_weather > 0:\n",
    "            logger.warning(f\"[train_interpretability] {uid}: {missing_weather} rows with missing weather, filling with ffill/bfill\")\n",
    "            exog = exog.ffill().bfill()\n",
    "\n",
    "        # Fit LightGBM forecaster\n",
    "        try:\n",
    "            lgbm = RenewableLGBMForecaster(\n",
    "                horizon=config.horizon,\n",
    "                lags=168,  # 7 days of lags\n",
    "                rolling_window_sizes=[24, 168],  # 1 day, 1 week\n",
    "            )\n",
    "            lgbm.fit(y=y, exog=exog)\n",
    "\n",
    "            # Create training matrices for SHAP analysis\n",
    "            X_train, y_train = lgbm.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "            # Generate interpretability report\n",
    "            series_output_dir = output_dir / uid\n",
    "            report = generate_full_interpretability_report(\n",
    "                forecaster=lgbm.forecaster,\n",
    "                X_train=X_train,\n",
    "                series_id=uid,\n",
    "                output_dir=series_output_dir,\n",
    "                top_n_features=5,\n",
    "                shap_sample_frac=0.5,\n",
    "                shap_max_samples=1000,\n",
    "            )\n",
    "            reports[uid] = report\n",
    "\n",
    "            logger.info(\n",
    "                f\"[train_interpretability] {uid}: top_features={report.top_features[:3]}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[train_interpretability] {uid}: Failed to train - {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Generated {len(reports)} interpretability reports\")\n",
    "    return reports\n",
    "\n",
    "\n",
    "def generate_renewable_forecasts(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    "    best_model: str = \"MSTL_ARIMA\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 4: Generate forecasts with prediction intervals.\"\"\"\n",
    "    output_path = config.forecasts_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[generate_forecasts] Generating {config.horizon}h forecasts using model={best_model}\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Fit uses only historical generation timestamps, weather merge will fail-loud if missing.\n",
    "    model.fit(generation_df, weather_df)\n",
    "\n",
    "    # Future weather must cover the horizon after the EARLIEST series' last timestamp\n",
    "    # (different regions may have different publishing lags)\n",
    "    per_series_max = generation_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    logger.info(f\"[generate_forecasts] Per-series max timestamps:\\n{per_series_max.to_dict()}\")\n",
    "\n",
    "    min_of_max = per_series_max.min()\n",
    "    global_max = generation_df[\"ds\"].max()\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Min of series maxes: {min_of_max}, \"\n",
    "        f\"Global max: {global_max}, \"\n",
    "        f\"Delta: {(global_max - min_of_max).total_seconds() / 3600:.1f}h\"\n",
    "    )\n",
    "\n",
    "    # Use min of max timestamps to ensure all series have weather for their forecasts\n",
    "    future_weather = weather_df[weather_df[\"ds\"] > min_of_max].copy()\n",
    "\n",
    "    if future_weather.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[generate_forecasts] No future weather rows found after earliest series max. \"\n",
    "            f\"min_of_max={min_of_max}\"\n",
    "        )\n",
    "\n",
    "    # Generate forecasts using best model from CV\n",
    "    # The predict() method now supports best_model parameter to filter output\n",
    "    logger.info(f\"[generate_forecasts] Generating predictions using model: {best_model}\")\n",
    "    forecasts = model.predict(future_weather=future_weather, best_model=best_model)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generated {len(forecasts)} forecast rows \"\n",
    "        f\"for {forecasts['unique_id'].nunique()} series\"\n",
    "    )\n",
    "\n",
    "    forecasts.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[generate_forecasts] Saved: {output_path} ({len(forecasts)} rows)\")\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "\n",
    "def compute_renewable_drift(\n",
    "    predictions: pd.DataFrame,\n",
    "    actuals: pd.DataFrame,\n",
    "    baseline_metrics: dict,\n",
    ") -> dict:\n",
    "    \"\"\"Task 5: Detect drift by comparing current metrics to baseline.\n",
    "\n",
    "    Drift is flagged when current RMSE > baseline_mean + 2*baseline_std\n",
    "\n",
    "    Args:\n",
    "        predictions: Forecast DataFrame with [unique_id, ds, yhat]\n",
    "        actuals: Actual values DataFrame with [unique_id, ds, y]\n",
    "        baseline_metrics: Baseline from cross-validation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with drift status and details\n",
    "    \"\"\"\n",
    "    from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "    # Merge predictions with actuals\n",
    "    merged = predictions.merge(\n",
    "        actuals[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        return {\n",
    "            \"status\": \"no_data\",\n",
    "            \"message\": \"No overlapping data between predictions and actuals\",\n",
    "        }\n",
    "\n",
    "    # Compute current metrics\n",
    "    y_true = merged[\"y\"].values\n",
    "    y_pred = merged[\"yhat\"].values\n",
    "\n",
    "    current_rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "    current_mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "\n",
    "    # Check against threshold\n",
    "    threshold = baseline_metrics.get(\"drift_threshold_rmse\", float(\"inf\"))\n",
    "    is_drifting = current_rmse > threshold\n",
    "\n",
    "    result = {\n",
    "        \"status\": \"drift_detected\" if is_drifting else \"stable\",\n",
    "        \"current_rmse\": float(current_rmse),\n",
    "        \"current_mae\": float(current_mae),\n",
    "        \"baseline_rmse\": float(baseline_metrics.get(\"rmse_mean\", 0)),\n",
    "        \"drift_threshold\": float(threshold),\n",
    "        \"threshold_exceeded_by\": float(max(0, current_rmse - threshold)),\n",
    "        \"n_predictions\": len(merged),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    if is_drifting:\n",
    "        logger.warning(\n",
    "            f\"[drift] DRIFT DETECTED: RMSE={current_rmse:.1f} > threshold={threshold:.1f}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"[drift] Stable: RMSE={current_rmse:.1f} <= threshold={threshold:.1f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Run the complete renewable forecasting pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetch generation data\n",
    "    2. Fetch weather data\n",
    "    3. Train models (CV)\n",
    "    4. Generate forecasts\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pipeline results\n",
    "    \"\"\"\n",
    "    logger.info(f\"[pipeline] Starting: {config.start_date} to {config.end_date}\")\n",
    "    logger.info(f\"[pipeline] Regions: {config.regions}\")\n",
    "    logger.info(f\"[pipeline] Fuel types: {config.fuel_types}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Fetch generation\n",
    "    generation_df = fetch_renewable_data(config, fetch_diagnostics=fetch_diagnostics)\n",
    "    results[\"generation_rows\"] = len(generation_df)\n",
    "    results[\"series_count\"] = generation_df[\"unique_id\"].nunique()\n",
    "\n",
    "    from src.renewable.validation import validate_generation_df\n",
    "\n",
    "    expected_series = [f\"{r}_{f}\" for r in config.regions for f in config.fuel_types]\n",
    "    rep = validate_generation_df(\n",
    "        generation_df,\n",
    "        expected_series=expected_series,\n",
    "        max_missing_ratio=0.02,\n",
    "        max_lag_hours=48,  # choose a value consistent with EIA publishing lag\n",
    "    )\n",
    "    if not rep.ok:\n",
    "        raise RuntimeError(f\"[pipeline][generation_validation] {rep.message} details={rep.details}\")\n",
    "\n",
    "    # Step 2: Fetch weather\n",
    "    weather_df = fetch_renewable_weather(config)\n",
    "    results[\"weather_rows\"] = len(weather_df)\n",
    "\n",
    "    # Step 2.5: Build modeling-ready dataset with preprocessing\n",
    "    logger.info(\"[pipeline] Building modeling dataset (dataset_builder.py)\")\n",
    "    modeling_df, prep_report = build_modeling_dataset(\n",
    "        generation_df,\n",
    "        weather_df,\n",
    "        negative_policy=config.negative_policy,\n",
    "        hourly_grid_policy=config.hourly_grid_policy,\n",
    "        output_dir=config.preprocessing_dir()\n",
    "    )\n",
    "\n",
    "    results[\"preprocessing\"] = {\n",
    "        \"rows_input\": prep_report.rows_input,\n",
    "        \"rows_output\": prep_report.rows_output,\n",
    "        \"series_dropped\": len(prep_report.series_dropped_incomplete),\n",
    "        \"negative_action\": prep_report.negative_values_action,\n",
    "        \"time_features\": prep_report.time_features_added,\n",
    "        \"weather_features\": prep_report.weather_features_added,\n",
    "    }\n",
    "    logger.info(f\"[pipeline] Preprocessing: {prep_report.rows_input:,} → {prep_report.rows_output:,} rows\")\n",
    "\n",
    "    # Step 3: Train and validate (on preprocessed data)\n",
    "    cv_results, leaderboard, baseline = train_renewable_models(\n",
    "        config, modeling_df, weather_df=None  # Weather already merged by dataset_builder\n",
    "    )\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    results[\"best_model\"] = best_model\n",
    "    results[\"best_rmse\"] = float(leaderboard.iloc[0][\"rmse\"])\n",
    "    results[\"baseline\"] = baseline\n",
    "    # Save full leaderboard for dashboard display\n",
    "    results[\"leaderboard\"] = leaderboard.to_dict(orient=\"records\")\n",
    "\n",
    "    # Step 4: Generate forecasts (use the best model from CV)\n",
    "    forecasts = generate_renewable_forecasts(\n",
    "        config, modeling_df, weather_df=None, best_model=best_model\n",
    "    )\n",
    "    results[\"forecast_rows\"] = len(forecasts)\n",
    "\n",
    "    # Step 5: Train LightGBM models and generate interpretability reports (optional)\n",
    "    # (LightGBM is for interpretability only - MSTL/ARIMA provide primary forecasts)\n",
    "    if config.enable_interpretability:\n",
    "        logger.info(\"[pipeline] Training interpretability models (LightGBM + SHAP)\")\n",
    "        try:\n",
    "            interpretability_reports = train_interpretability_models(\n",
    "                config, generation_df, weather_df\n",
    "            )\n",
    "            results[\"interpretability\"] = {\n",
    "                \"series_count\": len(interpretability_reports),\n",
    "                \"series\": list(interpretability_reports.keys()),\n",
    "                \"output_dir\": str(config.interpretability_dir()),\n",
    "            }\n",
    "\n",
    "            # Add top features summary per series\n",
    "            for uid, report in interpretability_reports.items():\n",
    "                results[\"interpretability\"][f\"{uid}_top_features\"] = report.top_features[:3]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[pipeline] Interpretability training failed (non-fatal): {e}\")\n",
    "            results[\"interpretability\"] = {\"error\": str(e)}\n",
    "    else:\n",
    "        logger.info(\"[pipeline] Interpretability disabled (enable_interpretability=False)\")\n",
    "        results[\"interpretability\"] = {\"enabled\": False}\n",
    "\n",
    "    if fetch_diagnostics is not None:\n",
    "        results[\"fetch_diagnostics\"] = fetch_diagnostics\n",
    "\n",
    "    logger.info(f\"[pipeline] Complete. Best model: {results['best_model']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entry point for renewable pipeline.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Renewable Energy Forecasting Pipeline\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Preset Examples:\n",
    "  # Fast development (24h forecast, 2 CV windows, 15 days lookback)\n",
    "  python -m src.renewable.tasks --preset 24h\n",
    "\n",
    "  # Standard forecasting (48h forecast, 3 CV windows, 21 days lookback)\n",
    "  python -m src.renewable.tasks --preset 48h\n",
    "\n",
    "  # Extended planning (72h forecast, 3 CV windows, 28 days lookback)\n",
    "  python -m src.renewable.tasks --preset 72h\n",
    "\n",
    "Custom Examples:\n",
    "  # 24h preset but only CALI region, skip interpretability\n",
    "  python -m src.renewable.tasks --preset 24h --regions CALI --no-interpretability\n",
    "\n",
    "  # Custom: 36h forecast with 4 CV windows\n",
    "  python -m src.renewable.tasks --horizon 36 --cv-windows 4 --lookback-days 30\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Preset system (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--preset\",\n",
    "        type=str,\n",
    "        choices=[\"24h\", \"48h\", \"72h\"],\n",
    "        help=\"Quick preset: 24h (fast dev), 48h (standard), 72h (extended planning)\",\n",
    "    )\n",
    "\n",
    "    # Flags (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--no-interpretability\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disable LightGBM interpretability analysis (speeds up pipeline)\",\n",
    "    )\n",
    "\n",
    "    # Data parameters (existing)\n",
    "    parser.add_argument(\n",
    "        \"--regions\",\n",
    "        type=str,\n",
    "        help=\"Override regions (comma-separated, e.g., CALI,ERCO,MISO)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuel\",\n",
    "        type=str,\n",
    "        help=\"Override fuel types (comma-separated, e.g., WND,SUN)\",\n",
    "    )\n",
    "\n",
    "    # Forecast parameters (existing + new)\n",
    "    parser.add_argument(\n",
    "        \"--horizon\",\n",
    "        type=int,\n",
    "        help=\"Override forecast horizon in hours\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lookback-days\",\n",
    "        type=int,\n",
    "        help=\"Override lookback days\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cv-windows\",\n",
    "        type=int,\n",
    "        help=\"Override CV windows count\",\n",
    "    )\n",
    "\n",
    "    # Output parameters\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing data files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"data/renewable\",\n",
    "        help=\"Output directory (default: data/renewable)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Build config with preset support\n",
    "    if args.preset:\n",
    "        # Apply preset defaults\n",
    "        logger.info(f\"[CLI] Applying preset: {args.preset}\")\n",
    "        config = RenewablePipelineConfig(\n",
    "            horizon_preset=args.preset,  # This triggers __post_init__ to apply preset\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "        # Allow CLI overrides of preset values\n",
    "        if args.horizon is not None:\n",
    "            object.__setattr__(config, \"horizon\", args.horizon)\n",
    "            logger.info(f\"[CLI] Override: horizon={args.horizon}h\")\n",
    "        if args.lookback_days is not None:\n",
    "            object.__setattr__(config, \"lookback_days\", args.lookback_days)\n",
    "            logger.info(f\"[CLI] Override: lookback_days={args.lookback_days}\")\n",
    "        if args.cv_windows is not None:\n",
    "            object.__setattr__(config, \"cv_windows\", args.cv_windows)\n",
    "            logger.info(f\"[CLI] Override: cv_windows={args.cv_windows}\")\n",
    "\n",
    "    else:\n",
    "        # No preset: use explicit values or defaults\n",
    "        config = RenewablePipelineConfig(\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            lookback_days=args.lookback_days if args.lookback_days else 30,\n",
    "            horizon=args.horizon if args.horizon else 24,\n",
    "            cv_windows=args.cv_windows if args.cv_windows else 5,\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "    # Run pipeline\n",
    "    results = run_full_pipeline(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Series count: {results['series_count']}\")\n",
    "    print(f\"  Generation rows: {results['generation_rows']}\")\n",
    "    print(f\"  Weather rows: {results['weather_rows']}\")\n",
    "    print(f\"  Forecast rows: {results['forecast_rows']}\")\n",
    "    print(f\"  Best model: {results['best_model']}\")\n",
    "    print(f\"  Best RMSE: {results['best_rmse']:.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite persistence layer\n",
    "\n",
    "Extends the monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/db.py\n",
    "# file: src/renewable/db.py\n",
    "\"\"\"Database schema and operations for renewable forecasting.\n",
    "\n",
    "Extends the Chapter 4 monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    \"\"\"Connect to SQLite database with optimized settings.\"\"\"\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def init_renewable_db(db_path: str) -> None:\n",
    "    \"\"\"Initialize renewable forecasting database schema.\n",
    "\n",
    "    Creates tables:\n",
    "    - renewable_forecasts: Forecasts with dual intervals\n",
    "    - renewable_scores: Evaluation metrics with coverage\n",
    "    - weather_features: Weather data by region\n",
    "    - drift_alerts: Drift detection history\n",
    "    - baseline_metrics: Backtest baselines for drift thresholds\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Forecasts with dual prediction intervals\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_forecasts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        yhat_lo_80 REAL,\n",
    "        yhat_hi_80 REAL,\n",
    "        yhat_lo_95 REAL,\n",
    "        yhat_hi_95 REAL,\n",
    "        UNIQUE (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Index for efficient queries\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_region_ds\n",
    "    ON renewable_forecasts (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_fuel_ds\n",
    "    ON renewable_forecasts (fuel_type, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Evaluation scores with dual coverage\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_scores (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        scored_at TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mae REAL,\n",
    "        coverage_80 REAL,\n",
    "        coverage_95 REAL,\n",
    "        valid_rows INTEGER,\n",
    "        UNIQUE (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Weather features by region\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_features (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        region TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        temperature_2m REAL,\n",
    "        wind_speed_10m REAL,\n",
    "        wind_speed_100m REAL,\n",
    "        wind_direction_10m REAL,\n",
    "        direct_radiation REAL,\n",
    "        diffuse_radiation REAL,\n",
    "        cloud_cover REAL,\n",
    "        is_forecast INTEGER DEFAULT 0,\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE (region, ds, is_forecast)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_weather_region_ds\n",
    "    ON weather_features (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Drift detection alerts\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS drift_alerts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        alert_at TEXT NOT NULL,\n",
    "        run_id TEXT,\n",
    "        unique_id TEXT,\n",
    "        region TEXT,\n",
    "        fuel_type TEXT,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        current_rmse REAL,\n",
    "        threshold_rmse REAL,\n",
    "        message TEXT,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_drift_alerts_time\n",
    "    ON drift_alerts (alert_at);\n",
    "    \"\"\")\n",
    "\n",
    "    # Baseline metrics for drift detection\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS baseline_metrics (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        rmse_mean REAL NOT NULL,\n",
    "        rmse_std REAL NOT NULL,\n",
    "        mae_mean REAL,\n",
    "        mae_std REAL,\n",
    "        drift_threshold_rmse REAL NOT NULL,\n",
    "        drift_threshold_mae REAL,\n",
    "        n_windows INTEGER,\n",
    "        metadata_json TEXT,\n",
    "        UNIQUE (unique_id, model)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_forecasts(\n",
    "    db_path: str,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    run_id: str,\n",
    "    model: str = \"MSTL_ARIMA\",\n",
    ") -> int:\n",
    "    \"\"\"Save forecasts to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        forecasts_df: DataFrame with [unique_id, ds, yhat, yhat_lo_80, ...]\n",
    "        run_id: Pipeline run identifier\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    created_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in forecasts_df.iterrows():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        parts = unique_id.split(\"_\")\n",
    "        region = parts[0] if len(parts) > 0 else \"\"\n",
    "        fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        rows.append((\n",
    "            run_id,\n",
    "            created_at,\n",
    "            unique_id,\n",
    "            region,\n",
    "            fuel_type,\n",
    "            str(row[\"ds\"]),\n",
    "            model,\n",
    "            row.get(\"yhat\"),\n",
    "            row.get(\"yhat_lo_80\"),\n",
    "            row.get(\"yhat_hi_80\"),\n",
    "            row.get(\"yhat_lo_95\"),\n",
    "            row.get(\"yhat_hi_95\"),\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO renewable_forecasts\n",
    "        (run_id, created_at, unique_id, region, fuel_type, ds, model,\n",
    "         yhat, yhat_lo_80, yhat_hi_80, yhat_lo_95, yhat_hi_95)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_weather(\n",
    "    db_path: str,\n",
    "    weather_df: pd.DataFrame,\n",
    "    is_forecast: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"Save weather features to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        weather_df: DataFrame with [ds, region, weather_vars...]\n",
    "        is_forecast: True if this is forecast weather data\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temperature_2m\", \"wind_speed_10m\", \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\", \"direct_radiation\", \"diffuse_radiation\", \"cloud_cover\"\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in weather_df.iterrows():\n",
    "        values = [row.get(col) for col in weather_cols]\n",
    "        rows.append((\n",
    "            row[\"region\"],\n",
    "            str(row[\"ds\"]),\n",
    "            *values,\n",
    "            1 if is_forecast else 0,\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(f\"\"\"\n",
    "        INSERT OR REPLACE INTO weather_features\n",
    "        (region, ds, {', '.join(weather_cols)}, is_forecast)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_drift_alert(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    unique_id: str,\n",
    "    current_rmse: float,\n",
    "    threshold_rmse: float,\n",
    "    severity: str = \"warning\",\n",
    "    metadata: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Save drift detection alert.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        run_id: Pipeline run identifier\n",
    "        unique_id: Series identifier\n",
    "        current_rmse: Current RMSE value\n",
    "        threshold_rmse: Drift threshold\n",
    "        severity: Alert severity (info, warning, critical)\n",
    "        metadata: Additional metadata\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    parts = unique_id.split(\"_\")\n",
    "    region = parts[0] if len(parts) > 0 else \"\"\n",
    "    fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    alert_type = \"drift_detected\" if current_rmse > threshold_rmse else \"drift_check\"\n",
    "    message = (\n",
    "        f\"RMSE {current_rmse:.1f} {'>' if current_rmse > threshold_rmse else '<='} \"\n",
    "        f\"threshold {threshold_rmse:.1f}\"\n",
    "    )\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO drift_alerts\n",
    "        (alert_at, run_id, unique_id, region, fuel_type, alert_type, severity,\n",
    "         current_rmse, threshold_rmse, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        run_id,\n",
    "        unique_id,\n",
    "        region,\n",
    "        fuel_type,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        current_rmse,\n",
    "        threshold_rmse,\n",
    "        message,\n",
    "        json.dumps(metadata) if metadata else None,\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_baseline(\n",
    "    db_path: str,\n",
    "    unique_id: str,\n",
    "    model: str,\n",
    "    baseline: dict,\n",
    ") -> None:\n",
    "    \"\"\"Save baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        unique_id: Series identifier\n",
    "        model: Model name\n",
    "        baseline: Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO baseline_metrics\n",
    "        (created_at, unique_id, model, rmse_mean, rmse_std, mae_mean, mae_std,\n",
    "         drift_threshold_rmse, drift_threshold_mae, n_windows, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        unique_id,\n",
    "        model,\n",
    "        baseline.get(\"rmse_mean\"),\n",
    "        baseline.get(\"rmse_std\"),\n",
    "        baseline.get(\"mae_mean\"),\n",
    "        baseline.get(\"mae_std\"),\n",
    "        baseline.get(\"drift_threshold_rmse\"),\n",
    "        baseline.get(\"drift_threshold_mae\"),\n",
    "        baseline.get(\"n_windows\"),\n",
    "        json.dumps(baseline),\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def get_recent_forecasts(\n",
    "    db_path: str,\n",
    "    region: Optional[str] = None,\n",
    "    fuel_type: Optional[str] = None,\n",
    "    hours: int = 48,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent forecasts from database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        region: Filter by region (optional)\n",
    "        fuel_type: Filter by fuel type (optional)\n",
    "        hours: Hours of history to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with forecasts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM renewable_forecasts\n",
    "        WHERE datetime(created_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if region:\n",
    "        query += \" AND region = ?\"\n",
    "        params.append(region)\n",
    "\n",
    "    if fuel_type:\n",
    "        query += \" AND fuel_type = ?\"\n",
    "        params.append(fuel_type)\n",
    "\n",
    "    query += \" ORDER BY ds DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_drift_alerts(\n",
    "    db_path: str,\n",
    "    hours: int = 24,\n",
    "    severity: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent drift alerts.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        hours: Hours of history\n",
    "        severity: Filter by severity (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with alerts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM drift_alerts\n",
    "        WHERE datetime(alert_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if severity:\n",
    "        query += \" AND severity = ?\"\n",
    "        params.append(severity)\n",
    "\n",
    "    query += \" ORDER BY alert_at DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test database initialization\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        db_path = f\"{tmpdir}/test_renewable.db\"\n",
    "\n",
    "        print(\"Initializing database...\")\n",
    "        init_renewable_db(db_path)\n",
    "\n",
    "        print(\"Database initialized successfully!\")\n",
    "\n",
    "        # Test connection\n",
    "        con = connect(db_path)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"Tables created: {[t[0] for t in tables]}\")\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8: Dashboard\n",
    "\n",
    "**File:** `src/renewable/dashboard.py`\n",
    "\n",
    "The Streamlit dashboard provides:\n",
    "- **Forecast visualization** with prediction intervals\n",
    "- **Drift monitoring** and alerts\n",
    "- **Coverage analysis** (nominal vs empirical)\n",
    "- **Weather features** by region\n",
    "\n",
    "## Running the Dashboard\n",
    "\n",
    "```bash\n",
    "streamlit run src/renewable/dashboard.py\n",
    "```\n",
    "\n",
    "The dashboard will:\n",
    "1. Load forecasts from `data/renewable/forecasts.parquet`\n",
    "2. Display interactive charts with Plotly\n",
    "3. Show drift alerts from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dashboard.py\n",
    "# file: src/renewable/dashboard.py\n",
    "\"\"\"Streamlit dashboard for renewable energy forecasting.\n",
    "\n",
    "Provides:\n",
    "- Forecast visualization with prediction intervals\n",
    "- Drift monitoring and alerts\n",
    "- Coverage analysis (nominal vs empirical)\n",
    "- Weather features by region\n",
    "\n",
    "Run with:\n",
    "    streamlit run src/renewable/dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from src.renewable.db import (\n",
    "    connect,\n",
    "    get_drift_alerts,\n",
    "    get_recent_forecasts,\n",
    "    init_renewable_db,\n",
    ")\n",
    "from src.renewable.regions import FUEL_TYPES, REGIONS\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Renewable Forecast Dashboard\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main dashboard application.\"\"\"\n",
    "    st.title(\"⚡ Renewable Energy Forecast Dashboard\")\n",
    "    st.markdown(\"Next-24h wind/solar generation forecasts with drift monitoring\")\n",
    "\n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "\n",
    "        db_path = st.text_input(\n",
    "            \"Database Path\",\n",
    "            value=\"data/renewable/renewable.db\",\n",
    "        )\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not Path(db_path).exists():\n",
    "            init_renewable_db(db_path)\n",
    "            st.info(\"Database initialized\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Region filter\n",
    "        all_regions = list(REGIONS.keys())\n",
    "        selected_regions = st.multiselect(\n",
    "            \"Regions\",\n",
    "            options=all_regions,\n",
    "            default=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        )\n",
    "\n",
    "        # Fuel type filter\n",
    "        fuel_type = st.selectbox(\n",
    "            \"Fuel Type\",\n",
    "            options=[\"WND\", \"SUN\", \"Both\"],\n",
    "            index=0,\n",
    "        )\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Actions\n",
    "        show_debug = st.checkbox(\"Show Debug\", value=False)\n",
    "        if st.button(\"🔄 Refresh Data\", width=\"stretch\"):\n",
    "            st.rerun()\n",
    "\n",
    "        if st.button(\"📊 Run Pipeline\", width=\"stretch\"):\n",
    "            run_pipeline_from_dashboard(db_path, selected_regions, fuel_type)\n",
    "\n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
    "        \"📈 Forecasts\",\n",
    "        \"⚠️ Drift Monitor\",\n",
    "        \"📊 Coverage\",\n",
    "        \"🌤️ Weather\",\n",
    "        \"🔍 Interpretability\",\n",
    "    ])\n",
    "\n",
    "    with tab1:\n",
    "        render_forecasts_tab(db_path, selected_regions, fuel_type, show_debug=show_debug)\n",
    "\n",
    "    with tab2:\n",
    "        render_drift_tab(db_path)\n",
    "\n",
    "    with tab3:\n",
    "        render_coverage_tab(db_path)\n",
    "\n",
    "    with tab4:\n",
    "        render_weather_tab(db_path, selected_regions)\n",
    "\n",
    "    with tab5:\n",
    "        render_interpretability_tab(selected_regions, fuel_type)\n",
    "\n",
    "\n",
    "def render_forecasts_tab(db_path: str, regions: list, fuel_type: str, *, show_debug: bool = False):\n",
    "    \"\"\"Render forecast visualization with prediction intervals.\"\"\"\n",
    "    st.subheader(\"Generation Forecasts\")\n",
    "\n",
    "    forecasts_df = pd.DataFrame()\n",
    "    data_source = \"none\"\n",
    "    derived_columns: list[str] = []\n",
    "\n",
    "    # Try to load from parquet file first (pipeline output)\n",
    "    parquet_path = Path(\"data/renewable/forecasts.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            forecasts_df = pd.read_parquet(parquet_path)\n",
    "            data_source = f\"parquet:{parquet_path}\"\n",
    "            # Add region/fuel_type columns if missing\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                parts = forecasts_df[\"unique_id\"].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "                if \"region\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"region\"] = parts[0]\n",
    "                    derived_columns.append(\"region\")\n",
    "                if \"fuel_type\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"fuel_type\"] = parts[1] if parts.shape[1] > 1 else pd.NA\n",
    "                    derived_columns.append(\"fuel_type\")\n",
    "            st.success(f\"Loaded {len(forecasts_df)} forecasts from pipeline\")\n",
    "\n",
    "            # Calculate and display data freshness\n",
    "            if not forecasts_df.empty and \"ds\" in forecasts_df.columns:\n",
    "                earliest_forecast_ts = forecasts_df[\"ds\"].min()\n",
    "                now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "\n",
    "                # Forecasts start from last_data + 1h, so last_data = earliest_forecast - 1h\n",
    "                last_data_ts = earliest_forecast_ts - pd.Timedelta(hours=1)\n",
    "\n",
    "                # Ensure both timestamps are timezone-aware for comparison\n",
    "                if not hasattr(last_data_ts, 'tz') or last_data_ts.tz is None:\n",
    "                    last_data_ts = pd.Timestamp(last_data_ts, tz=\"UTC\")\n",
    "\n",
    "                data_age_hours = (now_utc - last_data_ts).total_seconds() / 3600\n",
    "\n",
    "                # Show warning if data is > 6 hours old\n",
    "                if data_age_hours > 6:\n",
    "                    st.warning(\n",
    "                        f\"⚠️ Forecasts are based on **{data_age_hours:.1f} hour old** data \"\n",
    "                        f\"(last EIA data: {last_data_ts.strftime('%b %d %H:%M')} UTC). \"\n",
    "                        f\"Click 'Refresh Forecasts' button in sidebar to update.\"\n",
    "                    )\n",
    "                else:\n",
    "                    st.info(\n",
    "                        f\"✅ Forecasts from {last_data_ts.strftime('%b %d %H:%M')} UTC data \"\n",
    "                        f\"({data_age_hours:.1f}h old)\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load parquet: {e}\")\n",
    "\n",
    "    # Fall back to database\n",
    "    if forecasts_df.empty:\n",
    "        try:\n",
    "            forecasts_df = get_recent_forecasts(db_path, hours=72)\n",
    "            data_source = f\"db:{db_path}\"\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load from database: {e}\")\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        # Show demo data\n",
    "        st.info(\"No forecasts found. Showing demo data.\")\n",
    "        forecasts_df = generate_demo_forecasts(regions, fuel_type)\n",
    "        data_source = \"demo\"\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Forecast Data\", expanded=False):\n",
    "            st.markdown(\"**Source**\")\n",
    "            st.code(data_source)\n",
    "            st.markdown(\"**Columns**\")\n",
    "            st.code(\", \".join(forecasts_df.columns.tolist()))\n",
    "\n",
    "            st.markdown(\"**Counts (pre-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "\n",
    "            if derived_columns:\n",
    "                st.markdown(\"**Derived Columns**\")\n",
    "                st.write(derived_columns)\n",
    "\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id sample**\")\n",
    "                st.write(forecasts_df[\"unique_id\"].dropna().astype(str).head(10).tolist())\n",
    "\n",
    "            if \"fuel_type\" in forecasts_df.columns:\n",
    "                st.markdown(\"**fuel_type counts**\")\n",
    "                st.dataframe(forecasts_df[\"fuel_type\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "                unknown = sorted(\n",
    "                    {str(v) for v in forecasts_df[\"fuel_type\"].dropna().unique()}\n",
    "                    - set(FUEL_TYPES.keys())\n",
    "                )\n",
    "                if unknown:\n",
    "                    st.warning(f\"Unknown fuel_type values: {unknown}\")\n",
    "\n",
    "            if \"region\" in forecasts_df.columns:\n",
    "                st.markdown(\"**region counts**\")\n",
    "                st.dataframe(forecasts_df[\"region\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "    # Filter by selections\n",
    "    if fuel_type != \"Both\":\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"fuel_type\"] == fuel_type]\n",
    "\n",
    "    if regions:\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"region\"].isin(regions)]\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Filter Result\", expanded=False):\n",
    "            st.markdown(\"**Applied Filters**\")\n",
    "            st.write({\"fuel_type\": fuel_type, \"regions\": regions})\n",
    "            st.markdown(\"**Counts (post-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id after filter**\")\n",
    "                st.write(sorted(forecasts_df[\"unique_id\"].dropna().astype(str).unique().tolist()))\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        st.warning(\"No data matching filters\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    series_options = forecasts_df[\"unique_id\"].unique().tolist()\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=series_options,\n",
    "        index=0 if series_options else None,\n",
    "        key=\"forecast_series_select\",\n",
    "    )\n",
    "\n",
    "    if selected_series:\n",
    "        series_data = forecasts_df[forecasts_df[\"unique_id\"] == selected_series].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Convert to local timezone for display\n",
    "        region_code = series_data[\"unique_id\"].iloc[0].split(\"_\")[0]\n",
    "        region_info = REGIONS.get(region_code)\n",
    "        timezone_name = region_info.timezone if region_info else \"UTC\"\n",
    "\n",
    "        # Create forecast plot with intervals\n",
    "        fig = create_forecast_plot(series_data, selected_series, timezone_name)\n",
    "        st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "        # Show data table\n",
    "        with st.expander(\"View Data\"):\n",
    "            st.dataframe(\n",
    "                series_data[[\"ds\", \"yhat\", \"yhat_lo_80\", \"yhat_hi_80\", \"yhat_lo_95\", \"yhat_hi_95\"]],\n",
    "                width=\"stretch\",\n",
    "            )\n",
    "\n",
    "\n",
    "def create_forecast_plot(df: pd.DataFrame, title: str, timezone_name: str = \"UTC\") -> go.Figure:\n",
    "    \"\"\"Create Plotly figure with forecast and prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Forecast dataframe with ds (timestamp), yhat, and interval columns\n",
    "        title: Series name for chart title\n",
    "        timezone_name: IANA timezone name for display (e.g., \"America/Chicago\")\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert timestamps to local timezone for display\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "    # Convert UTC to local timezone\n",
    "    if timezone_name != \"UTC\":\n",
    "        df[\"ds\"] = df[\"ds\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone_name)\n",
    "\n",
    "    # Get timezone abbreviation for display (e.g., \"CST\", \"PST\")\n",
    "    if timezone_name != \"UTC\" and len(df) > 0:\n",
    "        tz_abbr = df[\"ds\"].iloc[0].strftime(\"%Z\")\n",
    "    else:\n",
    "        tz_abbr = \"UTC\"\n",
    "\n",
    "    # 95% interval (outer, lighter)\n",
    "    if \"yhat_lo_95\" in df.columns and \"yhat_hi_95\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_95\"], df[\"yhat_lo_95\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"95% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # 80% interval (inner, darker)\n",
    "    if \"yhat_lo_80\" in df.columns and \"yhat_hi_80\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_80\"], df[\"yhat_lo_80\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.4)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"80% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # Point forecast\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"ds\"],\n",
    "        y=df[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"#1f77b4\", width=2),\n",
    "    ))\n",
    "\n",
    "    # Actuals if available\n",
    "    if \"y\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Actual\",\n",
    "            marker=dict(color=\"#2ca02c\", size=6),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast: {title}\",\n",
    "        xaxis_title=f\"Time ({tz_abbr})\",\n",
    "        yaxis_title=\"Generation (MWh)\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "        height=450,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def render_drift_tab(db_path: str):\n",
    "    \"\"\"Render drift monitoring and alerts.\"\"\"\n",
    "    st.subheader(\"Drift Detection\")\n",
    "\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Try to load alerts\n",
    "    try:\n",
    "        alerts_df = get_drift_alerts(db_path, hours=48)\n",
    "    except Exception:\n",
    "        alerts_df = pd.DataFrame()\n",
    "\n",
    "    # Summary metrics\n",
    "    with col1:\n",
    "        critical = len(alerts_df[alerts_df[\"severity\"] == \"critical\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\n",
    "            \"Critical Alerts\",\n",
    "            critical,\n",
    "            delta=None,\n",
    "            delta_color=\"inverse\" if critical > 0 else \"off\",\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "        warning = len(alerts_df[alerts_df[\"severity\"] == \"warning\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Warnings\", warning)\n",
    "\n",
    "    with col3:\n",
    "        stable = len(alerts_df[alerts_df[\"alert_type\"] == \"drift_check\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Stable Checks\", stable)\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    if alerts_df.empty:\n",
    "        st.info(\"No drift alerts in the last 48 hours. System is stable.\")\n",
    "\n",
    "        # Show demo drift status\n",
    "        st.markdown(\"### Demo Drift Status\")\n",
    "        demo_drift = pd.DataFrame({\n",
    "            \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "            \"Current RMSE\": [125.3, 98.7, 156.2, 45.1, 67.8],\n",
    "            \"Threshold\": [150.0, 120.0, 180.0, 60.0, 80.0],\n",
    "            \"Status\": [\"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\"],\n",
    "        })\n",
    "        st.dataframe(demo_drift, width=\"stretch\")\n",
    "    else:\n",
    "        # Show alerts table\n",
    "        st.dataframe(\n",
    "            alerts_df[[\"alert_at\", \"unique_id\", \"severity\", \"current_rmse\", \"threshold_rmse\", \"message\"]],\n",
    "            width=\"stretch\",\n",
    "        )\n",
    "\n",
    "        # Drift timeline\n",
    "        if len(alerts_df) > 1:\n",
    "            alerts_df[\"alert_at\"] = pd.to_datetime(alerts_df[\"alert_at\"])\n",
    "            fig = px.scatter(\n",
    "                alerts_df,\n",
    "                x=\"alert_at\",\n",
    "                y=\"current_rmse\",\n",
    "                color=\"severity\",\n",
    "                size=\"current_rmse\",\n",
    "                hover_data=[\"unique_id\", \"message\"],\n",
    "                title=\"Drift Timeline\",\n",
    "            )\n",
    "            fig.add_hline(\n",
    "                y=alerts_df[\"threshold_rmse\"].mean(),\n",
    "                line_dash=\"dash\",\n",
    "                annotation_text=\"Avg Threshold\",\n",
    "            )\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_coverage_tab(db_path: str):\n",
    "    \"\"\"Render coverage analysis comparing nominal vs empirical.\"\"\"\n",
    "    st.subheader(\"Prediction Interval Coverage\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    **Coverage** measures how often actual values fall within prediction intervals.\n",
    "    - **Nominal**: The expected coverage (80% or 95%)\n",
    "    - **Empirical**: The actual observed coverage\n",
    "    - **Gap**: Difference indicates calibration quality\n",
    "    \"\"\")\n",
    "\n",
    "    # Demo coverage data\n",
    "    coverage_data = pd.DataFrame({\n",
    "        \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"SWPP_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "        \"Nominal 80%\": [80, 80, 80, 80, 80, 80],\n",
    "        \"Empirical 80%\": [78.5, 82.1, 76.3, 79.8, 81.2, 77.9],\n",
    "        \"Nominal 95%\": [95, 95, 95, 95, 95, 95],\n",
    "        \"Empirical 95%\": [93.2, 96.1, 91.5, 94.8, 95.7, 92.3],\n",
    "    })\n",
    "\n",
    "    coverage_data[\"Gap 80%\"] = coverage_data[\"Empirical 80%\"] - coverage_data[\"Nominal 80%\"]\n",
    "    coverage_data[\"Gap 95%\"] = coverage_data[\"Empirical 95%\"] - coverage_data[\"Nominal 95%\"]\n",
    "\n",
    "    # Summary\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        avg_80 = coverage_data[\"Empirical 80%\"].mean()\n",
    "        st.metric(\"Avg 80% Coverage\", f\"{avg_80:.1f}%\", f\"{avg_80 - 80:.1f}%\")\n",
    "\n",
    "    with col2:\n",
    "        avg_95 = coverage_data[\"Empirical 95%\"].mean()\n",
    "        st.metric(\"Avg 95% Coverage\", f\"{avg_95:.1f}%\", f\"{avg_95 - 95:.1f}%\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Coverage comparison chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"80% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 80%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.7)\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"95% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 95%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.4)\",\n",
    "    ))\n",
    "\n",
    "    # Nominal lines\n",
    "    fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"80% Nominal\")\n",
    "    fig.add_hline(y=95, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"95% Nominal\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coverage by Series\",\n",
    "        xaxis_title=\"Series\",\n",
    "        yaxis_title=\"Coverage (%)\",\n",
    "        barmode=\"group\",\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Detailed table\n",
    "    with st.expander(\"View Coverage Data\"):\n",
    "        st.dataframe(coverage_data, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_weather_tab(db_path: str, regions: list):\n",
    "    \"\"\"Render weather features visualization.\"\"\"\n",
    "    st.subheader(\"Weather Features\")\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "    # Prefer real pipeline output; no demo fallback.\n",
    "    parquet_path = Path(\"data/renewable/weather.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            weather_df = pd.read_parquet(parquet_path)\n",
    "            st.success(f\"Loaded {len(weather_df)} weather rows from pipeline\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather parquet: {exc}\")\n",
    "\n",
    "    if weather_df.empty and Path(db_path).exists():\n",
    "        try:\n",
    "            with connect(db_path) as con:\n",
    "                weather_df = pd.read_sql_query(\n",
    "                    \"SELECT * FROM weather_features ORDER BY ds ASC\",\n",
    "                    con,\n",
    "                )\n",
    "            if not weather_df.empty:\n",
    "                st.success(f\"Loaded {len(weather_df)} weather rows from database\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather data from database: {exc}\")\n",
    "\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data available. Run the pipeline to populate weather features.\")\n",
    "        return\n",
    "\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"coerce\")\n",
    "    if regions:\n",
    "        weather_df = weather_df[weather_df[\"region\"].isin(regions)]\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data matching selected regions.\")\n",
    "        return\n",
    "\n",
    "    # Variable selector\n",
    "    weather_vars = [\n",
    "        col for col in [\"wind_speed_10m\", \"wind_speed_100m\", \"direct_radiation\", \"cloud_cover\"]\n",
    "        if col in weather_df.columns\n",
    "    ]\n",
    "    if not weather_vars:\n",
    "        st.warning(\"Weather data missing expected variables.\")\n",
    "        return\n",
    "    selected_var = st.selectbox(\"Weather Variable\", options=weather_vars)\n",
    "\n",
    "    # Plot\n",
    "    fig = px.line(\n",
    "        weather_df,\n",
    "        x=\"ds\",\n",
    "        y=selected_var,\n",
    "        color=\"region\",\n",
    "        title=f\"{selected_var} by Region\",\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Summary stats\n",
    "    st.markdown(\"### Current Conditions\")\n",
    "\n",
    "    cols = st.columns(len(regions[:4]))\n",
    "    for i, region in enumerate(regions[:4]):\n",
    "        if i < len(cols):\n",
    "            with cols[i]:\n",
    "                region_data = weather_df[weather_df[\"region\"] == region].iloc[-1] if len(weather_df[weather_df[\"region\"] == region]) > 0 else {}\n",
    "                st.metric(\n",
    "                    region,\n",
    "                    f\"{region_data.get('wind_speed_10m', 0):.1f} m/s\",\n",
    "                    help=\"Wind speed at 10m\",\n",
    "                )\n",
    "\n",
    "\n",
    "def render_interpretability_tab(regions: list, fuel_type: str):\n",
    "    \"\"\"Render model interpretability visualizations (SHAP, feature importance, PDP).\"\"\"\n",
    "    st.subheader(\"Model Interpretability\")\n",
    "\n",
    "    # Model Leaderboard Section\n",
    "    st.markdown(\"### 🏆 Model Leaderboard (Cross-Validation)\")\n",
    "\n",
    "    # Model descriptions for education\n",
    "    MODEL_INFO = {\n",
    "        \"AutoARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Auto-tuned ARIMA with automatic p,d,q selection. Good for univariate series with trend/seasonality.\",\n",
    "            \"strengths\": \"Robust, well-understood, good prediction intervals\",\n",
    "        },\n",
    "        \"MSTL_ARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Multiple Seasonal-Trend decomposition + ARIMA. Handles daily (24h) and weekly (168h) seasonality.\",\n",
    "            \"strengths\": \"Best for multi-seasonal patterns like energy data\",\n",
    "        },\n",
    "        \"AutoETS\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Exponential smoothing with automatic error/trend/season selection.\",\n",
    "            \"strengths\": \"Simple, fast, works well for smooth series\",\n",
    "        },\n",
    "        \"AutoTheta\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Theta method with automatic decomposition. Robust to outliers.\",\n",
    "            \"strengths\": \"Competition winner (M3), handles level shifts\",\n",
    "        },\n",
    "        \"CES\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Complex Exponential Smoothing. Captures complex seasonal patterns.\",\n",
    "            \"strengths\": \"Good for complex seasonality\",\n",
    "        },\n",
    "        \"SeasonalNaive\": {\n",
    "            \"type\": \"Baseline\",\n",
    "            \"description\": \"Uses value from same hour last week. Baseline benchmark.\",\n",
    "            \"strengths\": \"Simple benchmark - if beaten, models add value\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "    if run_log_path.exists():\n",
    "        try:\n",
    "            import json\n",
    "            run_log = json.loads(run_log_path.read_text())\n",
    "            pipeline_results = run_log.get(\"pipeline_results\", {})\n",
    "            leaderboard_data = pipeline_results.get(\"leaderboard\", [])\n",
    "\n",
    "            if leaderboard_data:\n",
    "                leaderboard_df = pd.DataFrame(leaderboard_data)\n",
    "                best_model = pipeline_results.get(\"best_model\", \"\")\n",
    "                best_rmse = pipeline_results.get(\"best_rmse\", 0)\n",
    "\n",
    "                # Key metrics row\n",
    "                col1, col2, col3, col4 = st.columns(4)\n",
    "                with col1:\n",
    "                    st.metric(\"Best Model\", best_model)\n",
    "                with col2:\n",
    "                    st.metric(\"Best RMSE\", f\"{best_rmse:.3f}\")\n",
    "                with col3:\n",
    "                    st.metric(\"Models Evaluated\", len(leaderboard_data))\n",
    "                with col4:\n",
    "                    # Calculate improvement over baseline\n",
    "                    baseline_rmse = leaderboard_df[leaderboard_df[\"model\"] == \"SeasonalNaive\"][\"rmse\"].values\n",
    "                    if len(baseline_rmse) > 0 and best_rmse > 0:\n",
    "                        improvement = ((baseline_rmse[0] - best_rmse) / baseline_rmse[0]) * 100\n",
    "                        st.metric(\"vs Baseline\", f\"{improvement:+.1f}%\", help=\"Improvement over SeasonalNaive\")\n",
    "                    else:\n",
    "                        st.metric(\"vs Baseline\", \"N/A\")\n",
    "\n",
    "                # Selection rationale\n",
    "                st.markdown(\"#### Why This Model?\")\n",
    "                st.info(f\"\"\"\n",
    "                **{best_model}** was selected because it has the **lowest RMSE** on cross-validation.\n",
    "\n",
    "                - **RMSE (Root Mean Square Error)**: Penalizes large errors more heavily. Best for energy forecasting where big misses are costly.\n",
    "                - **Selection method**: Time-series CV with {run_log.get('config', {}).get('cv_windows', 2)} windows, step size {run_log.get('config', {}).get('cv_step_size', 168)}h\n",
    "                - **Horizon**: {run_log.get('config', {}).get('horizon', 24)}h ahead forecasts\n",
    "                \"\"\")\n",
    "\n",
    "                # Model description for winner\n",
    "                if best_model in MODEL_INFO:\n",
    "                    info = MODEL_INFO[best_model]\n",
    "                    st.success(f\"**{info['type']} Model**: {info['description']}\")\n",
    "\n",
    "                # Full leaderboard with visualization\n",
    "                st.markdown(\"#### All Models Ranked by RMSE\")\n",
    "\n",
    "                display_cols = [c for c in [\"model\", \"rmse\", \"mae\", \"mape\", \"coverage_80\", \"coverage_95\"]\n",
    "                               if c in leaderboard_df.columns]\n",
    "\n",
    "                # Create visualization\n",
    "                if \"rmse\" in leaderboard_df.columns:\n",
    "                    fig = px.bar(\n",
    "                        leaderboard_df.sort_values(\"rmse\"),\n",
    "                        x=\"model\",\n",
    "                        y=\"rmse\",\n",
    "                        title=\"Model Comparison (Lower RMSE = Better)\",\n",
    "                        color=\"rmse\",\n",
    "                        color_continuous_scale=\"RdYlGn_r\",\n",
    "                    )\n",
    "                    fig.add_hline(y=best_rmse, line_dash=\"dash\", line_color=\"green\",\n",
    "                                  annotation_text=f\"Best: {best_rmse:.3f}\")\n",
    "                    fig.update_layout(height=350)\n",
    "                    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                # Format numeric columns for table\n",
    "                styled_df = leaderboard_df[display_cols].copy()\n",
    "                for col in [\"rmse\", \"mae\", \"mape\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "                for col in [\"coverage_80\", \"coverage_95\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.1f}%\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "                st.dataframe(styled_df, width=\"stretch\", hide_index=True)\n",
    "\n",
    "                # Coverage analysis\n",
    "                if \"coverage_80\" in leaderboard_df.columns:\n",
    "                    st.markdown(\"#### Prediction Interval Coverage\")\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Coverage** measures if prediction intervals are well-calibrated:\n",
    "                    - **80% interval** should contain ~80% of actual values\n",
    "                    - **95% interval** should contain ~95% of actual values\n",
    "                    - **Under-coverage** (<target) = intervals too narrow, overconfident\n",
    "                    - **Over-coverage** (>target) = intervals too wide, conservative\n",
    "                    \"\"\")\n",
    "\n",
    "                    coverage_df = leaderboard_df[[\"model\", \"coverage_80\", \"coverage_95\"]].copy()\n",
    "                    coverage_df[\"coverage_80_status\"] = coverage_df[\"coverage_80\"].apply(\n",
    "                        lambda x: \"Under\" if x < 75 else (\"Over\" if x > 85 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "                    coverage_df[\"coverage_95_status\"] = coverage_df[\"coverage_95\"].apply(\n",
    "                        lambda x: \"Under\" if x < 90 else (\"Over\" if x > 99 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "\n",
    "                # Model descriptions expander\n",
    "                with st.expander(\"Model Descriptions\"):\n",
    "                    for model_name, info in MODEL_INFO.items():\n",
    "                        st.markdown(f\"**{model_name}** ({info['type']})\")\n",
    "                        st.markdown(f\"- {info['description']}\")\n",
    "                        st.markdown(f\"- *Strengths*: {info['strengths']}\")\n",
    "                        st.markdown(\"---\")\n",
    "\n",
    "                # CV configuration expander\n",
    "                config = run_log.get(\"config\", {})\n",
    "                with st.expander(\"CV Configuration\"):\n",
    "                    st.write({\n",
    "                        \"cv_windows\": config.get(\"cv_windows\"),\n",
    "                        \"cv_step_size\": config.get(\"cv_step_size\"),\n",
    "                        \"horizon\": config.get(\"horizon\"),\n",
    "                        \"regions\": config.get(\"regions\"),\n",
    "                        \"fuel_types\": config.get(\"fuel_types\"),\n",
    "                        \"run_at\": run_log.get(\"run_at_utc\", \"N/A\"),\n",
    "                    })\n",
    "            else:\n",
    "                st.info(\"Leaderboard not available. Run the pipeline with the latest code to generate.\")\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load leaderboard: {e}\")\n",
    "    else:\n",
    "        st.info(\"No run log found. Run the pipeline to generate model comparison.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    st.markdown(\"### 🔍 Per-Series Interpretability\")\n",
    "    st.markdown(\"\"\"\n",
    "    **LightGBM** models are trained alongside statistical models (MSTL/ARIMA) to provide\n",
    "    interpretability insights. The statistical models generate the primary forecasts,\n",
    "    while LightGBM helps understand feature importance and relationships.\n",
    "    \"\"\")\n",
    "\n",
    "    interp_dir = Path(\"data/renewable/interpretability\")\n",
    "\n",
    "    if not interp_dir.exists():\n",
    "        st.info(\"No interpretability data available. Run the pipeline to generate SHAP and PDP plots.\")\n",
    "        return\n",
    "\n",
    "    # Get available series\n",
    "    series_dirs = sorted([d.name for d in interp_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "    if not series_dirs:\n",
    "        st.warning(\"Interpretability directory exists but contains no series data.\")\n",
    "        return\n",
    "\n",
    "    # Filter by selected regions and fuel type\n",
    "    filtered_series = []\n",
    "    for series_id in series_dirs:\n",
    "        parts = series_id.split(\"_\")\n",
    "        if len(parts) == 2:\n",
    "            region, ft = parts\n",
    "            if regions and region not in regions:\n",
    "                continue\n",
    "            if fuel_type != \"Both\" and ft != fuel_type:\n",
    "                continue\n",
    "            filtered_series.append(series_id)\n",
    "\n",
    "    if not filtered_series:\n",
    "        st.warning(\"No interpretability data for selected filters.\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=filtered_series,\n",
    "        index=0,\n",
    "        key=\"interpretability_series_select\",\n",
    "    )\n",
    "\n",
    "    if not selected_series:\n",
    "        return\n",
    "\n",
    "    series_dir = interp_dir / selected_series\n",
    "\n",
    "    # Layout: Feature Importance + SHAP Summary side by side\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"### Feature Importance\")\n",
    "        importance_path = series_dir / \"feature_importance.csv\"\n",
    "        if importance_path.exists():\n",
    "            try:\n",
    "                importance_df = pd.read_csv(importance_path)\n",
    "                # Show top 15 features\n",
    "                top_features = importance_df.head(15)\n",
    "\n",
    "                # Create bar chart\n",
    "                fig = px.bar(\n",
    "                    top_features,\n",
    "                    x=\"importance\",\n",
    "                    y=\"feature\",\n",
    "                    orientation=\"h\",\n",
    "                    title=f\"Top Features: {selected_series}\",\n",
    "                    labels={\"importance\": \"Importance\", \"feature\": \"Feature\"},\n",
    "                )\n",
    "                fig.update_layout(yaxis=dict(autorange=\"reversed\"), height=400)\n",
    "                st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                with st.expander(\"Full Feature List\"):\n",
    "                    st.dataframe(importance_df, width=\"stretch\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error loading feature importance: {e}\")\n",
    "        else:\n",
    "            st.info(\"Feature importance not available.\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"### SHAP Summary\")\n",
    "        shap_summary_path = series_dir / \"shap_summary.png\"\n",
    "        if shap_summary_path.exists():\n",
    "            st.image(str(shap_summary_path), width=\"stretch\")\n",
    "        else:\n",
    "            # Try bar plot as fallback\n",
    "            shap_bar_path = series_dir / \"shap_bar.png\"\n",
    "            if shap_bar_path.exists():\n",
    "                st.image(str(shap_bar_path), width=\"stretch\")\n",
    "            else:\n",
    "                st.info(\"SHAP summary not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # SHAP Dependence Plots\n",
    "    st.markdown(\"### SHAP Dependence Plots\")\n",
    "    st.markdown(\"Shows how individual feature values affect predictions.\")\n",
    "\n",
    "    shap_dep_files = list(series_dir.glob(\"shap_dependence_*.png\"))\n",
    "    if shap_dep_files:\n",
    "        # Create columns for dependence plots\n",
    "        n_cols = min(3, len(shap_dep_files))\n",
    "        cols = st.columns(n_cols)\n",
    "\n",
    "        for i, dep_file in enumerate(shap_dep_files[:6]):  # Limit to 6 plots\n",
    "            feature_name = dep_file.stem.replace(\"shap_dependence_\", \"\")\n",
    "            with cols[i % n_cols]:\n",
    "                st.markdown(f\"**{feature_name}**\")\n",
    "                st.image(str(dep_file), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"SHAP dependence plots not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Partial Dependence Plot\n",
    "    st.markdown(\"### Partial Dependence Plot\")\n",
    "    st.markdown(\"Shows the average effect of features on predictions (marginal effect).\")\n",
    "\n",
    "    pdp_path = series_dir / \"partial_dependence.png\"\n",
    "    if pdp_path.exists():\n",
    "        st.image(str(pdp_path), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"Partial dependence plot not available.\")\n",
    "\n",
    "    # Waterfall plot for sample prediction\n",
    "    waterfall_path = series_dir / \"shap_waterfall_sample.png\"\n",
    "    if waterfall_path.exists():\n",
    "        st.markdown(\"### Sample Prediction Explanation\")\n",
    "        st.markdown(\"SHAP waterfall showing how features contributed to a single prediction.\")\n",
    "        st.image(str(waterfall_path), width=\"stretch\")\n",
    "\n",
    "\n",
    "def generate_demo_forecasts(regions: list, fuel_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate demo forecast data for display.\"\"\"\n",
    "    data = []\n",
    "    base_time = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    fuel_types = [fuel_type] if fuel_type != \"Both\" else [\"WND\", \"SUN\"]\n",
    "\n",
    "    for region in regions[:3]:\n",
    "        for ft in fuel_types:\n",
    "            unique_id = f\"{region}_{ft}\"\n",
    "            base_value = 500 if ft == \"WND\" else 300\n",
    "\n",
    "            for h in range(24):\n",
    "                ds = base_time + timedelta(hours=h)\n",
    "\n",
    "                # Add daily pattern\n",
    "                if ft == \"SUN\":\n",
    "                    hour_factor = max(0, np.sin((ds.hour - 6) * np.pi / 12)) if 6 < ds.hour < 18 else 0\n",
    "                    yhat = base_value * hour_factor + np.random.normal(0, 20)\n",
    "                else:\n",
    "                    yhat = base_value + np.sin(ds.hour * np.pi / 12) * 100 + np.random.normal(0, 30)\n",
    "\n",
    "                yhat = max(0, yhat)\n",
    "\n",
    "                data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"region\": region,\n",
    "                    \"fuel_type\": ft,\n",
    "                    \"ds\": ds,\n",
    "                    \"yhat\": yhat,\n",
    "                    \"yhat_lo_80\": yhat * 0.85,\n",
    "                    \"yhat_hi_80\": yhat * 1.15,\n",
    "                    \"yhat_lo_95\": yhat * 0.75,\n",
    "                    \"yhat_hi_95\": yhat * 1.25,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def run_pipeline_from_dashboard(db_path: str, regions: list, fuel_type: str):\n",
    "    \"\"\"Run the forecasting pipeline from the dashboard.\"\"\"\n",
    "    with st.spinner(\"Refreshing forecasts... (may take 2-3 minutes)\"):\n",
    "        try:\n",
    "            from src.renewable.jobs import run_hourly\n",
    "\n",
    "            # Run the hourly pipeline job\n",
    "            run_hourly.main()\n",
    "\n",
    "            st.success(\"Pipeline completed! Forecasts have been updated with latest EIA data.\")\n",
    "            st.info(\"Reloading page to show new forecasts...\")\n",
    "\n",
    "            # Wait a moment then reload\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            st.rerun()\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Pipeline failed: {e}\")\n",
    "            import traceback\n",
    "            with st.expander(\"Error details\"):\n",
    "                st.code(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow integration \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/data_freshness.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/data_freshness.py\n",
    "# src/renewable/data_freshness.py\n",
    "\"\"\"\n",
    "Lightweight EIA data freshness checking.\n",
    "\n",
    "This module provides functions to check if new data is available from the EIA API\n",
    "before running the full pipeline. It compares the current max timestamps with\n",
    "the previous run's max timestamps to determine if a full pipeline run is needed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from src.renewable.regions import get_eia_respondent\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FreshnessCheckResult:\n",
    "    \"\"\"Result of a data freshness check.\"\"\"\n",
    "\n",
    "    has_new_data: bool\n",
    "    checked_at_utc: str\n",
    "    series_status: dict[str, dict] = field(default_factory=dict)\n",
    "    summary: str = \"\"\n",
    "\n",
    "\n",
    "def load_previous_max_ds(run_log_path: Path) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load per-series max_ds from previous run_log.json.\n",
    "\n",
    "    Args:\n",
    "        run_log_path: Path to run_log.json\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping unique_id -> max_ds ISO string.\n",
    "        Empty dict if file doesn't exist or is malformed.\n",
    "    \"\"\"\n",
    "    if not run_log_path.exists():\n",
    "        logger.info(\"[freshness] No previous run_log.json found - first run\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        data = json.loads(run_log_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # Navigate to diagnostics.generation_coverage.coverage\n",
    "        coverage = (\n",
    "            data.get(\"diagnostics\", {})\n",
    "            .get(\"generation_coverage\", {})\n",
    "            .get(\"coverage\", [])\n",
    "        )\n",
    "\n",
    "        if not coverage:\n",
    "            logger.warning(\"[freshness] run_log.json has no coverage data\")\n",
    "            return {}\n",
    "\n",
    "        result = {}\n",
    "        for item in coverage:\n",
    "            uid = item.get(\"unique_id\")\n",
    "            max_ds = item.get(\"max_ds\")\n",
    "            if uid and max_ds:\n",
    "                result[uid] = max_ds\n",
    "\n",
    "        logger.info(f\"[freshness] Loaded {len(result)} series from previous run_log\")\n",
    "        return result\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        logger.warning(f\"[freshness] Failed to parse run_log.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def probe_eia_latest(\n",
    "    api_key: str,\n",
    "    region: str,\n",
    "    fuel_type: str,\n",
    "    *,\n",
    "    timeout: int = 15,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch only the single most recent record from EIA API.\n",
    "\n",
    "    This is a lightweight probe that uses:\n",
    "    - length=1 (only fetch 1 record)\n",
    "    - sort by period DESC (most recent first)\n",
    "\n",
    "    Args:\n",
    "        api_key: EIA API key\n",
    "        region: Region code (CALI, ERCO, MISO, etc.)\n",
    "        fuel_type: Fuel type (WND, SUN)\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ISO timestamp string of latest record, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"data[]\": \"value\",\n",
    "            \"facets[respondent][]\": respondent,\n",
    "            \"facets[fueltype][]\": fuel_type,\n",
    "            \"frequency\": \"hourly\",\n",
    "            \"length\": 1,\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"desc\",\n",
    "        }\n",
    "\n",
    "        base_url = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "        resp = requests.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        payload = resp.json()\n",
    "        response = payload.get(\"response\", {})\n",
    "        records = response.get(\"data\", [])\n",
    "\n",
    "        if not records:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: No records returned\")\n",
    "            return None\n",
    "\n",
    "        period = records[0].get(\"period\")\n",
    "        if not period:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: Record missing 'period'\")\n",
    "            return None\n",
    "\n",
    "        # Parse to consistent ISO format\n",
    "        ts = pd.to_datetime(period, utc=True)\n",
    "        return ts.isoformat()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: API error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compare_timestamps(prev: Optional[str], current: Optional[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if current is strictly newer than prev.\n",
    "\n",
    "    Handles None values conservatively (assume new data if unknown).\n",
    "    \"\"\"\n",
    "    if not prev or not current:\n",
    "        return True  # Unknown = assume new data (conservative)\n",
    "\n",
    "    try:\n",
    "        prev_dt = pd.to_datetime(prev, utc=True)\n",
    "        curr_dt = pd.to_datetime(current, utc=True)\n",
    "        return curr_dt > prev_dt\n",
    "    except Exception:\n",
    "        return True  # Parse error = assume new data\n",
    "\n",
    "\n",
    "def check_all_series_freshness(\n",
    "    regions: list[str],\n",
    "    fuel_types: list[str],\n",
    "    run_log_path: Path,\n",
    "    api_key: str,\n",
    ") -> FreshnessCheckResult:\n",
    "    \"\"\"\n",
    "    Check all series for new data availability.\n",
    "\n",
    "    Args:\n",
    "        regions: List of region codes (e.g., [\"CALI\", \"ERCO\", \"MISO\"])\n",
    "        fuel_types: List of fuel types (e.g., [\"WND\", \"SUN\"])\n",
    "        run_log_path: Path to previous run_log.json\n",
    "        api_key: EIA API key\n",
    "\n",
    "    Returns:\n",
    "        FreshnessCheckResult with has_new_data flag and detailed status per series.\n",
    "    \"\"\"\n",
    "    checked_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # 1. Load previous max_ds values\n",
    "    prev_max_ds = load_previous_max_ds(run_log_path)\n",
    "\n",
    "    # 2. If no previous run_log, always run full pipeline (first run)\n",
    "    if not prev_max_ds:\n",
    "        return FreshnessCheckResult(\n",
    "            has_new_data=True,\n",
    "            checked_at_utc=checked_at,\n",
    "            series_status={},\n",
    "            summary=\"No previous run_log.json found - running full pipeline (first run)\",\n",
    "        )\n",
    "\n",
    "    # 3. Probe each series\n",
    "    series_status: dict[str, dict] = {}\n",
    "    has_any_new = False\n",
    "    new_series: list[str] = []\n",
    "    error_series: list[str] = []\n",
    "\n",
    "    for region in regions:\n",
    "        for fuel_type in fuel_types:\n",
    "            series_id = f\"{region}_{fuel_type}\"\n",
    "            prev = prev_max_ds.get(series_id)\n",
    "            current = probe_eia_latest(api_key, region, fuel_type)\n",
    "\n",
    "            # Determine if this series has new data\n",
    "            if current is None:\n",
    "                # API error - be conservative, assume new data\n",
    "                is_new = True\n",
    "                error_series.append(series_id)\n",
    "                logger.warning(\n",
    "                    f\"[freshness] {series_id}: probe failed, assuming new data\"\n",
    "                )\n",
    "            else:\n",
    "                is_new = _compare_timestamps(prev, current)\n",
    "\n",
    "            series_status[series_id] = {\n",
    "                \"prev_max_ds\": prev,\n",
    "                \"current_max_ds\": current,\n",
    "                \"is_new\": is_new,\n",
    "            }\n",
    "\n",
    "            if is_new:\n",
    "                has_any_new = True\n",
    "                if current is not None:\n",
    "                    new_series.append(series_id)\n",
    "\n",
    "            # Log each series check\n",
    "            status_str = \"NEW\" if is_new else \"unchanged\"\n",
    "            logger.info(\n",
    "                f\"[freshness] {series_id}: prev={prev} current={current} ({status_str})\"\n",
    "            )\n",
    "\n",
    "    # 4. Build summary\n",
    "    if error_series:\n",
    "        summary = f\"Probe errors for {error_series}, assuming new data available\"\n",
    "    elif new_series:\n",
    "        summary = f\"New data found for: {', '.join(new_series)}\"\n",
    "    else:\n",
    "        summary = \"No new data found for any series\"\n",
    "\n",
    "    return FreshnessCheckResult(\n",
    "        has_new_data=has_any_new,\n",
    "        checked_at_utc=checked_at,\n",
    "        series_status=series_status,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"EIA_API_KEY not set\")\n",
    "        exit(1)\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "\n",
    "    result = check_all_series_freshness(\n",
    "        regions=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        fuel_types=[\"WND\", \"SUN\"],\n",
    "        run_log_path=run_log_path,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFreshness Check Result:\")\n",
    "    print(f\"  has_new_data: {result.has_new_data}\")\n",
    "    print(f\"  checked_at: {result.checked_at_utc}\")\n",
    "    print(f\"  summary: {result.summary}\")\n",
    "    print(f\"\\nPer-series status:\")\n",
    "    for series_id, status in result.series_status.items():\n",
    "        print(f\"  {series_id}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/jobs/run_hourly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/jobs/run_hourly.py\n",
    "# file: src/renewable/jobs/run_hourly.py\n",
    "\"\"\"Hourly renewable pipeline entry point with validation.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.renewable.tasks import RenewablePipelineConfig, run_full_pipeline\n",
    "from src.renewable.validation import validate_generation_df\n",
    "from src.renewable.data_freshness import check_all_series_freshness, FreshnessCheckResult\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _env_list(name: str, default_csv: str) -> list[str]:\n",
    "    raw = os.getenv(name, default_csv)\n",
    "    return [item.strip() for item in raw.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int) -> int:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return int(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _expected_series(regions: list[str], fuel_types: list[str]) -> list[str]:\n",
    "    return [f\"{region}_{fuel}\" for region in regions for fuel in fuel_types]\n",
    "\n",
    "\n",
    "def _json_default(value: object) -> str:\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.isoformat()\n",
    "    if isinstance(value, datetime):\n",
    "        return value.isoformat()\n",
    "    if hasattr(value, \"item\"):\n",
    "        try:\n",
    "            return value.item()\n",
    "        except Exception:\n",
    "            return str(value)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _summarize_generation_coverage(df: pd.DataFrame) -> dict:\n",
    "    if df.empty:\n",
    "        return {\"row_count\": 0, \"series_count\": 0, \"coverage\": []}\n",
    "\n",
    "    coverage = (\n",
    "        df.groupby(\"unique_id\")[\"ds\"]\n",
    "        .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"series_count\": int(df[\"unique_id\"].nunique()),\n",
    "        \"coverage\": coverage.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _read_previous_run_summary(data_dir: str) -> dict | None:\n",
    "    \"\"\"Read previous run_log.json for rowcount comparison.\"\"\"\n",
    "    path = Path(data_dir) / \"run_log.json\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _summarize_negative_forecasts(\n",
    "    df: pd.DataFrame,\n",
    "    sample_rows: int = 5,\n",
    ") -> dict:\n",
    "    if df.empty or \"yhat\" not in df.columns:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    neg = df[df[\"yhat\"] < 0]\n",
    "    if neg.empty:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    series_summary = (\n",
    "        neg.groupby(\"unique_id\")[\"yhat\"]\n",
    "        .agg(count=\"count\", min_value=\"min\", max_value=\"max\", mean_value=\"mean\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    sample = neg[[\"unique_id\", \"ds\", \"yhat\"]].head(sample_rows)\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"negative_rows\": int(len(neg)),\n",
    "        \"series\": series_summary.to_dict(orient=\"records\"),\n",
    "        \"sample\": sample.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_hourly_pipeline() -> dict:\n",
    "    data_dir = os.getenv(\"RENEWABLE_DATA_DIR\", \"data/renewable\")\n",
    "    regions = _env_list(\"RENEWABLE_REGIONS\", \"CALI,ERCO,MISO\")\n",
    "    fuel_types = _env_list(\"RENEWABLE_FUELS\", \"WND,SUN\")\n",
    "    lookback_days = _env_int(\"LOOKBACK_DAYS\", 30)\n",
    "\n",
    "    # Horizon configuration: support both preset and direct override\n",
    "    horizon_preset = os.getenv(\"RENEWABLE_HORIZON_PRESET\", None)  # \"24h\" | \"48h\" | \"72h\"\n",
    "    horizon_override = _env_int(\"RENEWABLE_HORIZON\", 0)  # Legacy direct override\n",
    "\n",
    "    # If direct override is set, use it; otherwise use preset (or None for default)\n",
    "    if horizon_override > 0:\n",
    "        horizon = horizon_override\n",
    "        horizon_preset = None  # Ignore preset if direct override is set\n",
    "    else:\n",
    "        horizon = 24  # Default, may be overridden by preset\n",
    "\n",
    "    cv_windows = _env_int(\"RENEWABLE_CV_WINDOWS\", 2)\n",
    "    cv_step_size = _env_int(\"RENEWABLE_CV_STEP_SIZE\", 168)\n",
    "\n",
    "    start_date = os.getenv(\"RENEWABLE_START_DATE\", \"\")\n",
    "    end_date = os.getenv(\"RENEWABLE_END_DATE\", \"\")\n",
    "\n",
    "    # Check if we should force run (e.g., manual dispatch)\n",
    "    force_run = os.getenv(\"FORCE_RUN\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # Data freshness check - skip full pipeline if no new data\n",
    "    if not force_run:\n",
    "        api_key = os.getenv(\"EIA_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            print(\"WARNING: EIA_API_KEY not set, skipping freshness check\")\n",
    "        else:\n",
    "            run_log_path = Path(data_dir) / \"run_log.json\"\n",
    "            freshness = check_all_series_freshness(\n",
    "                regions=regions,\n",
    "                fuel_types=fuel_types,\n",
    "                run_log_path=run_log_path,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "\n",
    "            if not freshness.has_new_data:\n",
    "                # No new data - return early with skip status\n",
    "                skip_log = {\n",
    "                    \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"reason\": \"no_new_data\",\n",
    "                    \"freshness_check\": {\n",
    "                        \"checked_at_utc\": freshness.checked_at_utc,\n",
    "                        \"summary\": freshness.summary,\n",
    "                        \"series_status\": freshness.series_status,\n",
    "                    },\n",
    "                    \"config\": {\n",
    "                        \"regions\": regions,\n",
    "                        \"fuel_types\": fuel_types,\n",
    "                        \"data_dir\": data_dir,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                # Write skip log (append to run_log.json)\n",
    "                Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "                skip_log_path = Path(data_dir) / \"skip_log.json\"\n",
    "                skip_log_path.write_text(\n",
    "                    json.dumps(skip_log, indent=2, default=_json_default)\n",
    "                )\n",
    "\n",
    "                print(f\"SKIPPED: {freshness.summary}\")\n",
    "                print(f\"Skip log written to: {skip_log_path}\")\n",
    "\n",
    "                # Set output for GitHub Actions\n",
    "                github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "                if github_output:\n",
    "                    with open(github_output, \"a\") as f:\n",
    "                        f.write(\"status=skipped\\n\")\n",
    "\n",
    "                return skip_log\n",
    "\n",
    "            print(f\"Freshness check: {freshness.summary}\")\n",
    "    else:\n",
    "        print(\"FORCE_RUN=true - skipping freshness check\")\n",
    "\n",
    "    cfg = RenewablePipelineConfig(\n",
    "        regions=regions,\n",
    "        fuel_types=fuel_types,\n",
    "        lookback_days=lookback_days,\n",
    "        horizon=horizon,\n",
    "        horizon_preset=horizon_preset,  # Apply preset if specified\n",
    "        data_dir=data_dir,\n",
    "        overwrite=True,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "    )\n",
    "    cfg.cv_windows = cv_windows\n",
    "    cfg.cv_step_size = cv_step_size\n",
    "\n",
    "    fetch_diagnostics: list[dict] = []\n",
    "    results = run_full_pipeline(cfg, fetch_diagnostics=fetch_diagnostics)\n",
    "\n",
    "    gen_path = cfg.generation_path()\n",
    "    gen_df = pd.read_parquet(gen_path)\n",
    "    generation_coverage = _summarize_generation_coverage(gen_df)\n",
    "\n",
    "    max_lag_hours = _env_int(\"MAX_LAG_HOURS\", 48)  # EIA publishes with 12-24h delay\n",
    "    max_missing_ratio = _env_float(\"MAX_MISSING_RATIO\", 0.02)\n",
    "    report = validate_generation_df(\n",
    "        gen_df,\n",
    "        max_lag_hours=max_lag_hours,\n",
    "        max_missing_ratio=max_missing_ratio,\n",
    "        expected_series=_expected_series(regions, fuel_types),\n",
    "    )\n",
    "\n",
    "    forecasts_df = pd.read_parquet(cfg.forecasts_path())\n",
    "    negative_forecasts = _summarize_negative_forecasts(forecasts_df)\n",
    "\n",
    "    # Quality gates\n",
    "    max_rowdrop_pct = _env_float(\"MAX_ROWDROP_PCT\", 0.30)\n",
    "    max_neg_forecast_ratio = _env_float(\"MAX_NEG_FORECAST_RATIO\", 0.10)\n",
    "\n",
    "    prev_run = _read_previous_run_summary(data_dir)\n",
    "    prev_gen_rows = 0\n",
    "    if prev_run:\n",
    "        prev_gen_rows = prev_run.get(\"pipeline_results\", {}).get(\"generation_rows\", 0)\n",
    "\n",
    "    curr_gen_rows = results.get(\"generation_rows\", 0)\n",
    "    rowdrop_ok = True\n",
    "    if prev_gen_rows > 0:\n",
    "        floor_ok = int(prev_gen_rows * (1.0 - max_rowdrop_pct))\n",
    "        rowdrop_ok = curr_gen_rows >= floor_ok\n",
    "\n",
    "    neg_forecast_ratio = 0.0\n",
    "    if negative_forecasts[\"row_count\"] > 0:\n",
    "        neg_forecast_ratio = (\n",
    "            negative_forecasts[\"negative_rows\"] / negative_forecasts[\"row_count\"]\n",
    "        )\n",
    "    neg_forecast_ok = neg_forecast_ratio <= max_neg_forecast_ratio\n",
    "\n",
    "    quality_gates = {\n",
    "        \"rowdrop\": {\n",
    "            \"ok\": rowdrop_ok,\n",
    "            \"prev_rows\": prev_gen_rows,\n",
    "            \"curr_rows\": curr_gen_rows,\n",
    "            \"max_rowdrop_pct\": max_rowdrop_pct,\n",
    "        },\n",
    "        \"neg_forecast\": {\n",
    "            \"ok\": neg_forecast_ok,\n",
    "            \"ratio\": neg_forecast_ratio,\n",
    "            \"max_ratio\": max_neg_forecast_ratio,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log = {\n",
    "        \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"config\": {\n",
    "            \"regions\": regions,\n",
    "            \"fuel_types\": fuel_types,\n",
    "            \"lookback_days\": lookback_days,\n",
    "            \"horizon\": horizon,\n",
    "            \"cv_windows\": cv_windows,\n",
    "            \"cv_step_size\": cv_step_size,\n",
    "            \"data_dir\": data_dir,\n",
    "            \"start_date\": cfg.start_date,\n",
    "            \"end_date\": cfg.end_date,\n",
    "        },\n",
    "        \"pipeline_results\": results,\n",
    "        \"validation\": {\n",
    "            \"ok\": report.ok,\n",
    "            \"message\": report.message,\n",
    "            \"details\": report.details,\n",
    "        },\n",
    "        \"diagnostics\": {\n",
    "            \"fetch\": fetch_diagnostics,\n",
    "            \"generation_coverage\": generation_coverage,\n",
    "            \"negative_forecasts\": negative_forecasts,\n",
    "        },\n",
    "        \"quality_gates\": quality_gates,\n",
    "    }\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    (Path(data_dir) / \"run_log.json\").write_text(\n",
    "        json.dumps(run_log, indent=2, default=_json_default)\n",
    "    )\n",
    "\n",
    "    # Check validation\n",
    "    if not report.ok:\n",
    "        raise SystemExit(f\"VALIDATION_FAILED: {report.message} | {report.details}\")\n",
    "\n",
    "    # Check quality gates\n",
    "    if not rowdrop_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: rowdrop | \"\n",
    "            f\"curr={curr_gen_rows} prev={prev_gen_rows} max_drop={max_rowdrop_pct:.0%}\"\n",
    "        )\n",
    "    if not neg_forecast_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: neg_forecast | \"\n",
    "            f\"ratio={neg_forecast_ratio:.1%} max={max_neg_forecast_ratio:.0%}\"\n",
    "        )\n",
    "\n",
    "    # Set output for GitHub Actions (successful run)\n",
    "    github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "    if github_output:\n",
    "        with open(github_output, \"a\") as f:\n",
    "            f.write(\"status=success\\n\")\n",
    "\n",
    "    return run_log\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    run_hourly_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dag_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dag_builder.py\n",
    "# file: src/renewable/dag_builder.py\n",
    "\"\"\"Renewable pipeline DAG builder for Airflow.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "AIRFLOW_AVAILABLE = True\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "def build_hourly_dag(\n",
    "    dag_id: str = \"renewable_hourly_pipeline\",\n",
    "    schedule: str = \"17 * * * *\",\n",
    "    start_date: Optional[datetime] = None,\n",
    "    default_args: Optional[Dict[str, Any]] = None,\n",
    ") -> \"DAG\":\n",
    "    if not AIRFLOW_AVAILABLE:\n",
    "        raise ImportError(\"Airflow is not installed. Install apache-airflow to use build_hourly_dag().\")\n",
    "\n",
    "    from src.renewable.jobs.run_hourly import run_hourly_pipeline\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = datetime.utcnow() - timedelta(days=1)\n",
    "    if default_args is None:\n",
    "        default_args = DEFAULT_ARGS.copy()\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=\"Renewable hourly pipeline\",\n",
    "        schedule_interval=schedule,\n",
    "        start_date=start_date,\n",
    "        catchup=False,\n",
    "        max_active_runs=1,\n",
    "        tags=[\"renewable\", \"eia\", \"forecasting\"],\n",
    "    ) as dag:\n",
    "        PythonOperator(\n",
    "            task_id=\"run_hourly\",\n",
    "            python_callable=run_hourly_pipeline,\n",
    "        )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def build_dag_dot() -> str:\n",
    "    return \"\"\"digraph RENEWABLE_PIPELINE {\n",
    "  rankdir=LR;\n",
    "  node [shape=box, style=\"rounded,filled\", fillcolor=\"#e8f5e9\"];\n",
    "\n",
    "  run_hourly;\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# git actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/pre-commit.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/pre-commit.yml\n",
    "# file: .github/workflows/pre-commit.yml\n",
    "name: pre-commit\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  run:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - uses: pre-commit/action@v3.0.1\n",
    "\n",
    "      - name: Install test dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install pytest pandas numpy requests python-dotenv\n",
    "\n",
    "      - name: Run smoke tests\n",
    "        env:\n",
    "          PYTHONPATH: ${{ github.workspace }}\n",
    "        run: pytest tests/ -v -k \"not slow\" --tb=short || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/renewable-hourly.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/renewable-hourly.yml\n",
    "# file: .github/workflows/renewable-hourly.yml\n",
    "name: renewable-hourly\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      force_run:\n",
    "        description: 'Force full pipeline run (skip freshness check)'\n",
    "        type: boolean\n",
    "        default: false\n",
    "  schedule:\n",
    "    - cron: \"17 * * * *\"\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "concurrency:\n",
    "  group: renewable-hourly\n",
    "  cancel-in-progress: true\n",
    "\n",
    "jobs:\n",
    "  update:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 25\n",
    "    env:\n",
    "      EIA_API_KEY: ${{ secrets.EIA_API_KEY }}\n",
    "      FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.force_run && 'true' || 'false' }}\n",
    "      RENEWABLE_REGIONS: \"CALI,ERCO,MISO\"\n",
    "      RENEWABLE_FUELS: \"WND,SUN\"\n",
    "      LOOKBACK_DAYS: \"30\"\n",
    "      RENEWABLE_HORIZON: \"24\"\n",
    "      RENEWABLE_CV_WINDOWS: \"2\"\n",
    "      RENEWABLE_CV_STEP_SIZE: \"168\"\n",
    "      MAX_LAG_HOURS: \"48\"  # EIA publishes hourly data with 12-24h delay\n",
    "      MAX_MISSING_RATIO: \"0.02\"\n",
    "      RENEWABLE_DATA_DIR: \"data/renewable\"\n",
    "      RENEWABLE_N_JOBS: \"1\"\n",
    "      OMP_NUM_THREADS: \"1\"\n",
    "      MKL_NUM_THREADS: \"1\"\n",
    "      OPENBLAS_NUM_THREADS: \"1\"\n",
    "      NUMBA_NUM_THREADS: \"1\"\n",
    "      VECLIB_MAXIMUM_THREADS: \"1\"\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - name: Check EIA API key\n",
    "        run: |\n",
    "          if [ -z \"$EIA_API_KEY\" ]; then\n",
    "            echo \"EIA_API_KEY is not set. Add it to repo secrets.\" >&2\n",
    "            exit 1\n",
    "          fi\n",
    "\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # Install from pyproject.toml for single source of truth\n",
    "          # Use -e for editable install (allows imports to work correctly)\n",
    "          pip install -e .\n",
    "\n",
    "      - name: Run hourly pipeline\n",
    "        id: pipeline\n",
    "        run: |\n",
    "          python -m src.renewable.jobs.run_hourly\n",
    "\n",
    "      - name: Quality gate check\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          python -c \"\n",
    "          import json, sys\n",
    "          from pathlib import Path\n",
    "          log_path = Path('data/renewable/run_log.json')\n",
    "          if not log_path.exists():\n",
    "              print('No run_log.json found')\n",
    "              sys.exit(1)\n",
    "          log = json.loads(log_path.read_text())\n",
    "          val = log.get('validation', {})\n",
    "          if not val.get('ok'):\n",
    "              print(f'VALIDATION FAILED: {val.get(\\\"message\\\")}')\n",
    "              print(f'Details: {val.get(\\\"details\\\")}')\n",
    "              sys.exit(1)\n",
    "          gates = log.get('quality_gates', {})\n",
    "          if not gates.get('rowdrop', {}).get('ok', True):\n",
    "              print(f'ROWDROP GATE FAILED: {gates.get(\\\"rowdrop\\\")}')\n",
    "              sys.exit(1)\n",
    "          if not gates.get('neg_forecast', {}).get('ok', True):\n",
    "              print(f'NEG_FORECAST GATE FAILED: {gates.get(\\\"neg_forecast\\\")}')\n",
    "              sys.exit(1)\n",
    "          print('QUALITY GATES PASSED')\n",
    "          \"\n",
    "\n",
    "      - name: Skip notification\n",
    "        if: steps.pipeline.outputs.status == 'skipped'\n",
    "        run: |\n",
    "          echo \"### Pipeline skipped - no new EIA data\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          if [ -f data/renewable/skip_log.json ]; then\n",
    "            python -c \"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "          data = json.loads(Path('data/renewable/skip_log.json').read_text())\n",
    "          freshness = data.get('freshness_check', {})\n",
    "          print(f'- Checked at: {freshness.get(\\\"checked_at_utc\\\")}')\n",
    "          print(f'- Summary: {freshness.get(\\\"summary\\\")}')\n",
    "          \" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Summarize run\n",
    "        if: always() && steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          if [ -f data/renewable/run_log.json ]; then\n",
    "          python - <<'PY' | tee -a \"$GITHUB_STEP_SUMMARY\"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "\n",
    "          data = json.loads(Path(\"data/renewable/run_log.json\").read_text())\n",
    "          validation = data.get(\"validation\", {})\n",
    "          details = validation.get(\"details\", {})\n",
    "          pipeline = data.get(\"pipeline_results\", {})\n",
    "          interp = pipeline.get(\"interpretability\", {})\n",
    "\n",
    "          lines = [\n",
    "              \"### Renewable hourly run\",\n",
    "              f\"- run_at_utc: {data.get('run_at_utc')}\",\n",
    "              f\"- validation_ok: {validation.get('ok')}\",\n",
    "              f\"- message: {validation.get('message')}\",\n",
    "              f\"- max_ds: {details.get('max_ds')}\",\n",
    "              f\"- lag_hours: {details.get('lag_hours')}\",\n",
    "              f\"- best_model: {pipeline.get('best_model')}\",\n",
    "              f\"- best_rmse: {pipeline.get('best_rmse', 0):.1f}\",\n",
    "              \"\",\n",
    "              \"#### Interpretability\",\n",
    "              f\"- series_count: {interp.get('series_count', 0)}\",\n",
    "              f\"- output_dir: {interp.get('output_dir', 'N/A')}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          PY\n",
    "          else\n",
    "          echo \"No run_log.json found.\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Commit updated artifacts\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          git config user.name \"github-actions[bot]\"\n",
    "          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n",
    "          git add data/renewable/generation.parquet \\\n",
    "            data/renewable/weather.parquet \\\n",
    "            data/renewable/forecasts.parquet \\\n",
    "            data/renewable/run_log.json\n",
    "          # Add interpretability artifacts if they exist\n",
    "          if [ -d data/renewable/interpretability ]; then\n",
    "            git add data/renewable/interpretability/\n",
    "          fi\n",
    "          git commit -m \"renewable: hourly data update (UTC)\" || echo \"No changes to commit\"\n",
    "          git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
