{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable Energy Forecasting Pipeline\n",
    "\n",
    "This notebook walks through building a **next-24h renewable generation forecast system** with:\n",
    "\n",
    "- **EIA data integration** - Hourly wind/solar generation for US regions\n",
    "- **Weather features** - Open-Meteo integration (wind speed, solar radiation)\n",
    "- **Probabilistic forecasting** - Dual prediction intervals (80%, 95%)\n",
    "- **Drift monitoring** - Automatic detection of model degradation\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "EIA API (WND/SUN) ──┐\n",
    "                    ├──► Data Pipeline ──► StatsForecast ──► Predictions\n",
    "Open-Meteo API ─────┘         │                  │              │\n",
    "                              ▼                  ▼              ▼\n",
    "                         Validation        Multi-Series    Probabilistic\n",
    "                         & Quality         [unique_id,     (80%, 95%\n",
    "                                           ds, y, X]       intervals)\n",
    "                                                              │\n",
    "                                                              ▼\n",
    "                                                         Streamlit\n",
    "                                                         Dashboard\n",
    "                                                         (drift, alerts)\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` - where `unique_id` = `{region}_{fuel_type}`\n",
    "2. **Zero-value handling**: Solar generates 0 at night - we use RMSE/MAE, NOT MAPE\n",
    "3. **Leakage prevention**: Use **forecasted** weather for predictions, not historical\n",
    "4. **Drift detection**: Threshold = mean + 2*std from backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure we have the project root in our path and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\docker_projects\\atsaf\"\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "if os.getcwd() != str(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to project root: {project_root} we are currently at {os.getcwd()}\")\n",
    "\n",
    "# Configure logging for visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 1: Region Definitions\n",
    "\n",
    "**File:** `src/renewable/regions.py`\n",
    "\n",
    "This module maps **EIA balancing authority regions** to their geographic coordinates. Why do we need coordinates?\n",
    "\n",
    "- **Weather API lookup**: Open-Meteo requires latitude/longitude\n",
    "- **Regional analysis**: Compare forecast accuracy across regions\n",
    "- **Timezone handling**: Each region has a primary timezone\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "1. **NamedTuple for RegionInfo**: Immutable, type-safe, and memory-efficient\n",
    "2. **Centroid coordinates**: Approximate centers - good enough for hourly weather\n",
    "3. **Fuel type codes**: `WND` (wind), `SUN` (solar) - match EIA's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/regions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/regions.py\n",
    "# src/renewable/regions.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class RegionInfo(NamedTuple):\n",
    "    \"\"\"Region metadata for EIA and weather lookups.\"\"\"\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    timezone: str\n",
    "    # Some internal regions may not map cleanly to an EIA respondent.\n",
    "    # We keep them in REGIONS for weather/features, but EIA fetch requires this.\n",
    "    eia_respondent: Optional[str] = None\n",
    "\n",
    "\n",
    "REGIONS: dict[str, RegionInfo] = {\n",
    "    # Western Interconnection\n",
    "    \"CALI\": RegionInfo(\n",
    "        name=\"California ISO\",\n",
    "        lat=36.7,\n",
    "        lon=-119.4,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=\"CISO\",\n",
    "    ),\n",
    "    \"NW\": RegionInfo(\n",
    "        name=\"Northwest\",\n",
    "        lat=45.5,\n",
    "        lon=-122.0,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "    \"SW\": RegionInfo(\n",
    "        name=\"Southwest\",\n",
    "        lat=33.5,\n",
    "        lon=-112.0,\n",
    "        timezone=\"America/Phoenix\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "\n",
    "    # Texas Interconnection\n",
    "    \"ERCO\": RegionInfo(\n",
    "        name=\"ERCOT (Texas)\",\n",
    "        lat=31.0,\n",
    "        lon=-100.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"ERCO\",\n",
    "    ),\n",
    "\n",
    "    # Midwest\n",
    "    \"MISO\": RegionInfo(\n",
    "        name=\"Midcontinent ISO\",\n",
    "        lat=41.0,\n",
    "        lon=-93.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"MISO\",\n",
    "    ),\n",
    "    \"PJM\": RegionInfo(\n",
    "        name=\"PJM Interconnection\",\n",
    "        lat=39.0,\n",
    "        lon=-77.0,\n",
    "        timezone=\"America/New_York\",\n",
    "        eia_respondent=\"PJM\",\n",
    "    ),\n",
    "    \"SWPP\": RegionInfo(\n",
    "        name=\"Southwest Power Pool\",\n",
    "        lat=37.0,\n",
    "        lon=-97.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"SWPP\",\n",
    "    ),\n",
    "\n",
    "    # Internal/aggregate regions kept for non-EIA use (weather/features/etc.)\n",
    "    \"SE\": RegionInfo(name=\"Southeast\", lat=33.0, lon=-84.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"FLA\": RegionInfo(name=\"Florida\", lat=28.0, lon=-82.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"CAR\": RegionInfo(name=\"Carolinas\", lat=35.5, lon=-80.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"TEN\": RegionInfo(name=\"Tennessee Valley\", lat=35.5, lon=-86.0, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "\n",
    "    \"US48\": RegionInfo(name=\"Lower 48 States\", lat=39.8, lon=-98.5, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "}\n",
    "\n",
    "FUEL_TYPES = {\"WND\": \"Wind\", \"SUN\": \"Solar\"}\n",
    "\n",
    "\n",
    "def list_regions() -> list[str]:\n",
    "    return sorted(REGIONS.keys())\n",
    "\n",
    "\n",
    "def get_region_info(region_code: str) -> RegionInfo:\n",
    "    return REGIONS[region_code]\n",
    "\n",
    "\n",
    "def get_region_coords(region_code: str) -> tuple[float, float]:\n",
    "    r = REGIONS[region_code]\n",
    "    return (r.lat, r.lon)\n",
    "\n",
    "\n",
    "def get_eia_respondent(region_code: str) -> str:\n",
    "    \"\"\"Return the code EIA expects for facets[respondent][]. Fail loudly if missing.\"\"\"\n",
    "    info = REGIONS[region_code]\n",
    "    if not info.eia_respondent:\n",
    "        raise ValueError(\n",
    "            f\"Region '{region_code}' has no configured eia_respondent. \"\n",
    "            f\"Set REGIONS['{region_code}'].eia_respondent to a verified EIA respondent code \"\n",
    "            f\"before using it for EIA fetches.\"\n",
    "        )\n",
    "    return info.eia_respondent\n",
    "\n",
    "\n",
    "def validate_region(region_code: str) -> bool:\n",
    "    return region_code in REGIONS\n",
    "\n",
    "\n",
    "def validate_fuel_type(fuel_type: str) -> bool:\n",
    "    return fuel_type in FUEL_TYPES\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run - test region functions\n",
    "\n",
    "    print(\"=== Available Regions ===\")\n",
    "    print(f\"Total regions: {len(REGIONS)}\")\n",
    "    print(f\"Region codes: {list_regions()}\")\n",
    "\n",
    "    print(\"\\n=== Example: California ===\")\n",
    "    cali_info = get_region_info(\"CALI\")\n",
    "    print(f\"Name: {cali_info.name}\")\n",
    "    print(f\"Coordinates: ({cali_info.lat}, {cali_info.lon})\")\n",
    "    print(f\"Timezone: {cali_info.timezone}\")\n",
    "\n",
    "    print(\"\\n=== Weather API Coordinates ===\")\n",
    "    for region in [\"CALI\", \"ERCO\", \"MISO\"]:\n",
    "        lat, lon = get_region_coords(region)\n",
    "        print(f\"{region}: lat={lat}, lon={lon}\")\n",
    "\n",
    "    print(\"\\n=== Fuel Types ===\")\n",
    "    for code, name in FUEL_TYPES.items():\n",
    "        print(f\"{code}: {name}\")\n",
    "\n",
    "    print(\"\\n=== Validation ===\")\n",
    "    print(f\"validate_region('CALI'): {validate_region('CALI')}\")\n",
    "    print(f\"validate_region('INVALID'): {validate_region('INVALID')}\")\n",
    "    print(f\"validate_fuel_type('WND'): {validate_fuel_type('WND')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Region Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2: EIA Data Fetcher\n",
    "\n",
    "**File:** `src/renewable/eia_renewable.py`\n",
    "\n",
    "This module fetches **hourly wind and solar generation** from the EIA API.\n",
    "\n",
    "## Critical Concepts\n",
    "\n",
    "### StatsForecast Format\n",
    "StatsForecast expects data in a specific format:\n",
    "```\n",
    "unique_id | ds                  | y\n",
    "----------|---------------------|--------\n",
    "CALI_WND  | 2024-01-01 00:00:00 | 1234.5\n",
    "CALI_WND  | 2024-01-01 01:00:00 | 1456.7\n",
    "ERCO_WND  | 2024-01-01 00:00:00 | 2345.6\n",
    "```\n",
    "\n",
    "- `unique_id`: Identifies the time series (e.g., \"CALI_WND\" = California Wind)\n",
    "- `ds`: Datetime column (timezone-naive UTC)\n",
    "- `y`: Target value (generation in MWh)\n",
    "\n",
    "### API Rate Limiting\n",
    "- EIA API has rate limits (~5 requests/second)\n",
    "- We use controlled parallelism with delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/eia_renewable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/eia_renewable.py\n",
    "# src/renewable/eia_renewable.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode\n",
    "\n",
    "from src.renewable.regions import REGIONS, get_eia_respondent, validate_fuel_type, validate_region\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _sanitize_url(url: str) -> str:\n",
    "    parts = urlsplit(url)\n",
    "    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k.lower() != \"api_key\"]\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, urlencode(q), parts.fragment))\n",
    "\n",
    "\n",
    "def _load_env_once(*, debug: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load .env if present.\n",
    "    - Primary: find_dotenv(usecwd=True) (walk up from CWD)\n",
    "    - Fallback: repo_root/.env based on this file location\n",
    "    Returns the path loaded (or None).\n",
    "    \"\"\"\n",
    "    # 1) Try from current working directory upward\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "        if debug:\n",
    "            logger.info(\"Loaded .env via find_dotenv: %s\", dotenv_path)\n",
    "        return dotenv_path\n",
    "\n",
    "    # 2) Fallback: assume src-layout -> repo root is ../../ from this file\n",
    "    try:\n",
    "        repo_root = Path(__file__).resolve().parents[2]\n",
    "        fallback = repo_root / \".env\"\n",
    "        if fallback.exists():\n",
    "            load_dotenv(fallback, override=False)\n",
    "            if debug:\n",
    "                logger.info(\"Loaded .env via fallback: %s\", str(fallback))\n",
    "            return str(fallback)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"No .env found to load.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "class EIARenewableFetcher:\n",
    "    BASE_URL = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "    MAX_RECORDS_PER_REQUEST = 5000\n",
    "    RATE_LIMIT_DELAY = 0.2  # 5 requests/second max\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, *, debug_env: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize API key. Pulls from:\n",
    "        1) explicit api_key argument\n",
    "        2) environment variable EIA_API_KEY (optionally loaded from .env)\n",
    "        \"\"\"\n",
    "        loaded_env = _load_env_once(debug=debug_env)\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"EIA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"EIA API key required but not found.\\n\"\n",
    "                \"- Ensure .env contains EIA_API_KEY=...\\n\"\n",
    "                \"- Ensure your process CWD is under the repo (so find_dotenv can locate it), OR\\n\"\n",
    "                \"- Pass api_key=... explicitly.\\n\"\n",
    "                f\"Loaded .env path: {loaded_env}\"\n",
    "            )\n",
    "\n",
    "        # Debug without leaking the key\n",
    "        if debug_env:\n",
    "            masked = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            logger.info(\"EIA_API_KEY loaded (masked): %s\", masked)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_eia_response(payload: dict, *, request_url: Optional[str] = None) -> tuple[list[dict], dict]:\n",
    "        if not isinstance(payload, dict):\n",
    "            raise TypeError(f\"EIA payload is not a dict. type={type(payload)} url={request_url}\")\n",
    "\n",
    "        if \"error\" in payload and payload.get(\"response\") is None:\n",
    "            raise ValueError(f\"EIA returned error payload. url={request_url} error={payload.get('error')}\")\n",
    "\n",
    "        if \"response\" not in payload:\n",
    "            raise ValueError(\n",
    "                f\"EIA payload missing 'response'. url={request_url} keys={list(payload.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        response = payload.get(\"response\") or {}\n",
    "        if not isinstance(response, dict):\n",
    "            raise TypeError(f\"EIA payload['response'] is not a dict. type={type(response)} url={request_url}\")\n",
    "\n",
    "        if \"data\" not in response:\n",
    "            raise ValueError(\n",
    "                f\"EIA response missing 'data'. url={request_url} response_keys={list(response.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        records = response.get(\"data\") or []\n",
    "        if not isinstance(records, list):\n",
    "            raise TypeError(f\"EIA response['data'] is not a list. type={type(records)} url={request_url}\")\n",
    "\n",
    "        total = response.get(\"total\", None)\n",
    "        offset = response.get(\"offset\", None)\n",
    "\n",
    "        meta_obj = response.get(\"metadata\") or {}\n",
    "        if isinstance(meta_obj, dict):\n",
    "            if total is None and \"total\" in meta_obj:\n",
    "                total = meta_obj.get(\"total\")\n",
    "            if offset is None and \"offset\" in meta_obj:\n",
    "                offset = meta_obj.get(\"offset\")\n",
    "\n",
    "        try:\n",
    "            total = int(total) if total is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            offset = int(offset) if offset is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return records, {\"total\": total, \"offset\": offset}\n",
    "\n",
    "    def fetch_region(\n",
    "        self,\n",
    "        region: str,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "        diag: Optional[dict] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region):\n",
    "            raise ValueError(f\"Invalid region: {region}\")\n",
    "        if not validate_fuel_type(fuel_type):\n",
    "            raise ValueError(f\"Invalid fuel type: {fuel_type}\")\n",
    "\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        all_records: list[dict] = []\n",
    "        offset = 0\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"api_key\": self.api_key,\n",
    "                \"data[]\": \"value\",\n",
    "                \"facets[respondent][]\": respondent,\n",
    "                \"facets[fueltype][]\": fuel_type,\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"start\": f\"{start_date}T00\",\n",
    "                \"end\": f\"{end_date}T23\",\n",
    "                \"length\": self.MAX_RECORDS_PER_REQUEST,\n",
    "                \"offset\": offset,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"asc\",\n",
    "            }\n",
    "\n",
    "            resp = requests.get(self.BASE_URL, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "\n",
    "            records, meta = self._extract_eia_response(payload, request_url=resp.url)\n",
    "            page_count += 1\n",
    "            if total_hint is None:\n",
    "                total_hint = meta.get(\"total\")\n",
    "\n",
    "            returned = len(records)\n",
    "\n",
    "            if debug:\n",
    "                safe_url = _sanitize_url(resp.url)\n",
    "                print(\n",
    "                    f\"[PAGE] region={region} fuel={fuel_type} returned={returned} \"\n",
    "                    f\"offset={offset} total={meta.get('total')} url={safe_url}\"\n",
    "                )\n",
    "            if returned == 0 and offset == 0:\n",
    "                return pd.DataFrame(columns=[\"ds\", \"value\", \"region\", \"fuel_type\"])\n",
    "            if returned == 0:\n",
    "                break\n",
    "\n",
    "            all_records.extend(records)\n",
    "\n",
    "            if returned < self.MAX_RECORDS_PER_REQUEST:\n",
    "                break\n",
    "\n",
    "            offset += self.MAX_RECORDS_PER_REQUEST\n",
    "            time.sleep(self.RATE_LIMIT_DELAY)\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        missing_cols = [c for c in [\"period\", \"value\"] if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            sample_keys = sorted(set().union(*(r.keys() for r in all_records[:5]))) if all_records else []\n",
    "            raise ValueError(\n",
    "                f\"EIA records missing expected keys {missing_cols}. \"\n",
    "                f\"columns={df.columns.tolist()} sample_record_keys={sample_keys}\"\n",
    "            )\n",
    "\n",
    "        raw_rows = len(df)\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"period\"], utc=True, errors=\"coerce\").dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "        bad_ds = int(df[\"ds\"].isna().sum())\n",
    "        bad_val = int(df[\"value\"].isna().sum())\n",
    "\n",
    "        df[\"region\"] = region\n",
    "        df[\"fuel_type\"] = fuel_type\n",
    "\n",
    "        df = df.dropna(subset=[\"ds\", \"value\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if diag is not None:\n",
    "            diag.update({\n",
    "                \"region\": region,\n",
    "                \"fuel_type\": fuel_type,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"total_records\": total_hint,\n",
    "                \"pages\": page_count,\n",
    "                \"rows_parsed\": int(len(df)),\n",
    "                \"empty\": bool(len(df) == 0),\n",
    "            })\n",
    "\n",
    "        return df[[\"ds\", \"value\", \"region\", \"fuel_type\"]]\n",
    "\n",
    "    def fetch_all_regions(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        regions: Optional[list[str]] = None,\n",
    "        max_workers: int = 3,\n",
    "        diagnostics: Optional[list[dict]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if regions is None:\n",
    "            regions = [r for r in REGIONS.keys() if r != \"US48\"]\n",
    "\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "\n",
    "        def _run_one(region: str) -> tuple[str, pd.DataFrame, dict]:\n",
    "            d: dict = {}\n",
    "            df = self.fetch_region(region, fuel_type, start_date, end_date, diag=d)\n",
    "            return region, df, d\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(_run_one, region): region for region in regions}\n",
    "            for future in as_completed(futures):\n",
    "                region = futures[future]\n",
    "                try:\n",
    "                    _, df, d = future.result()\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append(d)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                        print(f\"[OK] {region}: {len(df)} rows\")\n",
    "                    else:\n",
    "                        print(f\"[EMPTY] {region}: 0 rows\")\n",
    "                except Exception as e:\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append({\n",
    "                            \"region\": region,\n",
    "                            \"fuel_type\": fuel_type,\n",
    "                            \"start_date\": start_date,\n",
    "                            \"end_date\": end_date,\n",
    "                            \"error\": str(e),\n",
    "                        })\n",
    "                    print(f\"[FAIL] {region}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame(columns=[\"unique_id\", \"ds\", \"y\"])\n",
    "\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined[\"unique_id\"] = combined[\"region\"] + \"_\" + combined[\"fuel_type\"]\n",
    "        combined = combined.rename(columns={\"value\": \"y\"})\n",
    "        return combined[[\"unique_id\", \"ds\", \"y\"]].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    def get_series_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.groupby(\"unique_id\").agg(\n",
    "            count=(\"y\", \"count\"),\n",
    "            min_value=(\"y\", \"min\"),\n",
    "            max_value=(\"y\", \"max\"),\n",
    "            mean_value=(\"y\", \"mean\"),\n",
    "            zero_count=(\"y\", lambda x: (x == 0).sum()),\n",
    "        ).reset_index()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 3: Weather Integration\n",
    "\n",
    "**File:** `src/renewable/open_meteo.py`\n",
    "\n",
    "Weather is **critical** for renewable forecasting:\n",
    "- **Wind generation** depends on wind speed (especially at hub height ~100m)\n",
    "- **Solar generation** depends on radiation and cloud cover\n",
    "\n",
    "## Key Concept: Preventing Leakage\n",
    "\n",
    "**Data leakage** occurs when training uses information that wouldn't be available at prediction time.\n",
    "\n",
    "```\n",
    "❌ WRONG: Using historical weather to predict future generation\n",
    "   - At prediction time, we don't have future actual weather!\n",
    "   \n",
    "✅ CORRECT: Use forecasted weather for predictions\n",
    "   - Training: historical weather aligned with historical generation\n",
    "   - Prediction: weather forecast for the prediction horizon\n",
    "```\n",
    "\n",
    "## Open-Meteo API\n",
    "\n",
    "Open-Meteo is **free** and requires no API key:\n",
    "- Historical API: Past weather data\n",
    "- Forecast API: Up to 16 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/open_meteo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/open_meteo.py\n",
    "# src/renewable/open_meteo.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from src.renewable.regions import get_region_coords, validate_region\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OpenMeteoEndpoints:\n",
    "    historical_url: str = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    forecast_url: str = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "\n",
    "class OpenMeteoRenewable:\n",
    "    \"\"\"\n",
    "    Fetch weather features for renewable energy forecasting.\n",
    "\n",
    "    Strict-by-default:\n",
    "    - If Open-Meteo doesn't return a requested variable, we raise.\n",
    "    - We do NOT fabricate values or silently \"fill\" missing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_VARS = [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"direct_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, timeout: int = 30, *, strict: bool = True):\n",
    "        self.timeout = timeout\n",
    "        self.strict = strict\n",
    "        self.endpoints = OpenMeteoEndpoints()\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=0.5,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def fetch_historical(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.historical_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][HIST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        return self._parse_response(resp.json(), variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "    def fetch_forecast(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        forecast_days = min((horizon_hours // 24) + 1, 16)\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"forecast_days\": forecast_days,\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.forecast_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][FCST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        df = self._parse_response(resp.json(), variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "        # Trim to requested horizon (ds is naive UTC)\n",
    "        if len(df) > 0:\n",
    "            cutoff = datetime.utcnow() + timedelta(hours=horizon_hours)\n",
    "            df = df[df[\"ds\"] <= cutoff].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_historical(lat, lon, start_date, end_date, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "    def fetch_all_regions_historical(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region(region, start_date, end_date, debug=debug)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Weather for {region}: {len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Weather for {region}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def _parse_response(\n",
    "        self,\n",
    "        data: dict,\n",
    "        variables: list[str],\n",
    "        *,\n",
    "        debug: bool,\n",
    "        request_url: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        hourly = data.get(\"hourly\")\n",
    "        if not isinstance(hourly, dict):\n",
    "            raise ValueError(f\"Open-Meteo response missing/invalid 'hourly'. url={request_url}\")\n",
    "\n",
    "        times = hourly.get(\"time\")\n",
    "        if not isinstance(times, list) or len(times) == 0:\n",
    "            raise ValueError(f\"Open-Meteo response has no hourly time grid. url={request_url}\")\n",
    "\n",
    "        # Build ds (naive UTC)\n",
    "        ds = pd.to_datetime(times, errors=\"coerce\", utc=True).tz_localize(None)\n",
    "        if ds.isna().any():\n",
    "            bad = int(ds.isna().sum())\n",
    "            raise ValueError(f\"Open-Meteo returned unparsable times. bad={bad} url={request_url}\")\n",
    "\n",
    "        df_data = {\"ds\": ds}\n",
    "\n",
    "        # Strict variable presence: raise if missing (no silent None padding)\n",
    "        missing_vars = [v for v in variables if v not in hourly]\n",
    "        if missing_vars and self.strict:\n",
    "            raise ValueError(f\"Open-Meteo missing requested vars={missing_vars}. url={request_url}\")\n",
    "\n",
    "        for var in variables:\n",
    "            values = hourly.get(var)\n",
    "            if values is None:\n",
    "                # If not strict, keep as all-NA but be explicit (not hidden)\n",
    "                df_data[var] = [None] * len(ds)\n",
    "                continue\n",
    "\n",
    "            if not isinstance(values, list):\n",
    "                raise ValueError(f\"Open-Meteo var '{var}' not a list. type={type(values)} url={request_url}\")\n",
    "\n",
    "            if len(values) != len(ds):\n",
    "                raise ValueError(\n",
    "                    f\"Open-Meteo length mismatch for '{var}': \"\n",
    "                    f\"len(values)={len(values)} len(time)={len(ds)} url={request_url}\"\n",
    "                )\n",
    "\n",
    "            df_data[var] = pd.to_numeric(values, errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame(df_data).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if debug:\n",
    "            dup = int(df[\"ds\"].duplicated().sum())\n",
    "            na_counts = {v: int(df[v].isna().sum()) for v in variables if v in df.columns}\n",
    "            print(f\"[OPENMETEO][PARSE] rows={len(df)} dup_ds={dup} na_counts(sample)={dict(list(na_counts.items())[:3])}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region_forecast(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_forecast(lat, lon, horizon_hours=horizon_hours, variables=variables, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fetch_all_regions_forecast(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region_forecast(\n",
    "                    region, horizon_hours=horizon_hours, variables=variables, debug=debug\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Forecast weather for {region}: {len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 4: Probabilistic Modeling\n",
    "\n",
    "**File:** `src/renewable/modeling.py`\n",
    "\n",
    "This is where the forecasting happens! We use **StatsForecast** for:\n",
    "\n",
    "1. **Multi-series forecasting**: Handle multiple regions/fuel types in one model\n",
    "2. **Probabilistic predictions**: Get prediction intervals, not just point forecasts\n",
    "3. **Weather exogenous**: Include weather features as predictors\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Prediction Intervals?\n",
    "\n",
    "Point forecasts are useful, but energy traders need **uncertainty quantification**:\n",
    "- **80% interval**: \"I'm 80% confident generation will be between X and Y\"\n",
    "- **95% interval**: Wider, for risk management\n",
    "\n",
    "### Zero-Value Safety (CRITICAL)\n",
    "\n",
    "**Solar panels generate ZERO at night!** This breaks MAPE:\n",
    "\n",
    "```\n",
    "MAPE = mean(|actual - predicted| / actual)\n",
    "\n",
    "When actual = 0:\n",
    "MAPE = |0 - pred| / 0 = undefined (division by zero!)\n",
    "```\n",
    "\n",
    "**Solution**: Always use RMSE and MAE for renewable forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/chapter2/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/chapter2/evaluation.py\n",
    "# file: src/chapter2/evaluation.py\n",
    "\"\"\"\n",
    "Chapter 2: Model Evaluation Metrics\n",
    "\n",
    "Computes forecasting metrics with explicit NaN handling (fail-loud principle).\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ForecastMetrics:\n",
    "    \"\"\"Compute and track forecasting evaluation metrics\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Root Mean Squared Error\n",
    "\n",
    "        Explicit NaN masking (fail-loud):\n",
    "        - Returns NaN if no valid predictions\n",
    "        - Masks NaN/inf values before computation\n",
    "        \"\"\"\n",
    "        valid_mask = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return np.nan\n",
    "\n",
    "        return np.sqrt(np.mean((y_pred[valid_mask] - y_true[valid_mask]) ** 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mean Absolute Error\n",
    "\n",
    "        Explicit NaN masking (fail-loud):\n",
    "        - Returns NaN if no valid predictions\n",
    "        - Masks NaN/inf values before computation\n",
    "        \"\"\"\n",
    "        valid_mask = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return np.nan\n",
    "\n",
    "        return np.mean(np.abs(y_pred[valid_mask] - y_true[valid_mask]))\n",
    "\n",
    "    @staticmethod\n",
    "    def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mean Absolute Percentage Error (%)\n",
    "\n",
    "        Explicit NaN masking (fail-loud):\n",
    "        - Returns NaN if no valid predictions\n",
    "        - Masks NaN/inf values and zero y_true before computation\n",
    "        \"\"\"\n",
    "        valid_mask = (\n",
    "            np.isfinite(y_pred) &\n",
    "            np.isfinite(y_true) &\n",
    "            (np.abs(y_true) > 1e-10)\n",
    "        )\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return np.nan\n",
    "\n",
    "        ape = np.abs((y_pred[valid_mask] - y_true[valid_mask]) / np.abs(y_true[valid_mask]))\n",
    "        return 100 * np.mean(ape)\n",
    "\n",
    "    @staticmethod\n",
    "    def mase(\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        season_length: int = 24\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Mean Absolute Scaled Error\n",
    "\n",
    "        Scales error relative to naive seasonal forecasting.\n",
    "\n",
    "        Explicit NaN masking (fail-loud):\n",
    "        - Returns NaN if insufficient training data\n",
    "        - Masks NaN/inf values before computation\n",
    "        \"\"\"\n",
    "        # Check minimum training data\n",
    "        if len(y_train) < season_length:\n",
    "            return np.nan\n",
    "\n",
    "        # Compute seasonal naive MAE\n",
    "        try:\n",
    "            mae_train = np.mean(np.abs(\n",
    "                y_train[season_length:] - y_train[:-season_length]\n",
    "            ))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "        if mae_train < 1e-10:\n",
    "            return np.nan\n",
    "\n",
    "        # Compute test MAE\n",
    "        valid_mask = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return np.nan\n",
    "\n",
    "        mae_test = np.mean(np.abs(y_pred[valid_mask] - y_true[valid_mask]))\n",
    "\n",
    "        return mae_test / mae_train\n",
    "\n",
    "    @staticmethod\n",
    "    def coverage(\n",
    "        y_true: np.ndarray,\n",
    "        lower: np.ndarray,\n",
    "        upper: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Prediction Interval Coverage (%)\n",
    "\n",
    "        Percentage of actual values within prediction interval.\n",
    "\n",
    "        Explicit NaN masking (fail-loud):\n",
    "        - Returns NaN if no valid predictions\n",
    "        - Counts valid (non-NaN) rows in denominator\n",
    "        \"\"\"\n",
    "        valid_mask = (\n",
    "            np.isfinite(y_true) &\n",
    "            np.isfinite(lower) &\n",
    "            np.isfinite(upper)\n",
    "        )\n",
    "\n",
    "        if valid_mask.sum() == 0:\n",
    "            return np.nan\n",
    "\n",
    "        covered = (y_true[valid_mask] >= lower[valid_mask]) & \\\n",
    "                  (y_true[valid_mask] <= upper[valid_mask])\n",
    "\n",
    "        return 100 * np.mean(covered)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_all(\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_train: Optional[np.ndarray] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute all metrics at once\n",
    "\n",
    "        Args:\n",
    "            y_true: Actual values\n",
    "            y_pred: Predictions\n",
    "            y_train: Training values (for MASE)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"rmse\": ForecastMetrics.rmse(y_true, y_pred),\n",
    "            \"mae\": ForecastMetrics.mae(y_true, y_pred),\n",
    "            \"mape\": ForecastMetrics.mape(y_true, y_pred),\n",
    "        }\n",
    "\n",
    "        if y_train is not None:\n",
    "            metrics[\"mase\"] = ForecastMetrics.mase(\n",
    "                y_true, y_pred, y_train, season_length=24\n",
    "            )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def compute_series_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_train: Optional[np.ndarray] = None,\n",
    "    valid_threshold: int = 1\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute metrics with explicit validation\n",
    "\n",
    "    Args:\n",
    "        y_true: Actual values\n",
    "        y_pred: Predictions\n",
    "        y_train: Training values (for MASE)\n",
    "        valid_threshold: Minimum valid predictions required\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Count valid predictions\n",
    "    valid_mask = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "    valid_count = valid_mask.sum()\n",
    "\n",
    "    if valid_count < valid_threshold:\n",
    "        return {\n",
    "            \"rmse\": np.nan,\n",
    "            \"mae\": np.nan,\n",
    "            \"mape\": np.nan,\n",
    "            \"mase\": np.nan,\n",
    "            \"valid_count\": valid_count,\n",
    "            \"error\": f\"Insufficient valid predictions: {valid_count} < {valid_threshold}\"\n",
    "        }\n",
    "\n",
    "    metrics = ForecastMetrics.compute_all(y_true, y_pred, y_train)\n",
    "    metrics[\"valid_count\"] = valid_count\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def aggregate_metrics(\n",
    "    results: pd.DataFrame,\n",
    "    by: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate metrics across splits and series\n",
    "\n",
    "    Args:\n",
    "        results: DataFrame with metric columns\n",
    "        by: Groupby column (\"model_name\", \"unique_id\", etc.)\n",
    "\n",
    "    Returns:\n",
    "        Aggregated metrics DataFrame\n",
    "    \"\"\"\n",
    "    metric_cols = [\"rmse\", \"mae\", \"mape\", \"mase\"]\n",
    "\n",
    "    if by is None:\n",
    "        # Overall aggregation\n",
    "        agg = results[metric_cols].agg([\n",
    "            (\"mean\", \"mean\"),\n",
    "            (\"std\", \"std\"),\n",
    "            (\"min\", \"min\"),\n",
    "            (\"max\", \"max\")\n",
    "        ])\n",
    "        return agg\n",
    "    else:\n",
    "        # Grouped aggregation\n",
    "        agg = results.groupby(by)[metric_cols].agg([\n",
    "            (\"mean\", \"mean\"),\n",
    "            (\"std\", \"std\"),\n",
    "            (\"count\", \"count\")\n",
    "        ])\n",
    "        return agg\n",
    "        return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/modeling.py\n",
    "# file: src/renewable/modeling.py\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "\n",
    "def _log_series_summary(df: pd.DataFrame, *, value_col: str = \"y\", label: str = \"series\") -> None:\n",
    "    if df.empty:\n",
    "        print(f\"[{label}] EMPTY\")\n",
    "        return\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ds\"] = pd.to_datetime(tmp[\"ds\"], errors=\"coerce\")\n",
    "\n",
    "    def _mode_delta_hours(g: pd.Series) -> float:\n",
    "        d = g.sort_values().diff().dropna()\n",
    "        if d.empty:\n",
    "            return float(\"nan\")\n",
    "        return float(d.dt.total_seconds().div(3600).mode().iloc[0])\n",
    "\n",
    "    g = tmp.groupby(\"unique_id\").agg(\n",
    "        rows=(value_col, \"count\"),\n",
    "        na_y=(value_col, lambda s: int(s.isna().sum())),\n",
    "        min_ds=(\"ds\", \"min\"),\n",
    "        max_ds=(\"ds\", \"max\"),\n",
    "        min_y=(value_col, \"min\"),\n",
    "        max_y=(value_col, \"max\"),\n",
    "        mean_y=(value_col, \"mean\"),\n",
    "        zero_y=(value_col, lambda s: int((s == 0).sum())),\n",
    "        mode_delta_hours=(\"ds\", _mode_delta_hours),\n",
    "    ).reset_index().sort_values(\"unique_id\")\n",
    "\n",
    "    print(f\"[{label}] series={g['unique_id'].nunique()} rows={len(tmp)}\")\n",
    "    print(g.head(20).to_string(index=False))\n",
    "\n",
    "def _missing_hour_blocks(ds: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp, int]]:\n",
    "    \"\"\"\n",
    "    Return contiguous blocks of missing hourly timestamps.\n",
    "    Each tuple: (block_start, block_end, n_hours)\n",
    "    \"\"\"\n",
    "    ds = pd.to_datetime(ds, errors=\"raise\").sort_values()\n",
    "    start, end = ds.iloc[0], ds.iloc[-1]\n",
    "    expected = pd.date_range(start, end, freq=\"h\")\n",
    "    missing = expected.difference(ds)\n",
    "\n",
    "    if missing.empty:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    block_start = missing[0]\n",
    "    prev = missing[0]\n",
    "    for t in missing[1:]:\n",
    "        if t - prev == pd.Timedelta(hours=1):\n",
    "            prev = t\n",
    "        else:\n",
    "            n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "            blocks.append((block_start, prev, n))\n",
    "            block_start = t\n",
    "            prev = t\n",
    "    n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "    blocks.append((block_start, prev, n))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def _hourly_grid_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for uid, g in df.groupby(\"unique_id\"):\n",
    "        g = g.sort_values(\"ds\")\n",
    "        start, end = g[\"ds\"].iloc[0], g[\"ds\"].iloc[-1]\n",
    "        expected = pd.date_range(start, end, freq=\"h\")\n",
    "        missing = expected.difference(g[\"ds\"])\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"unique_id\": uid,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"expected_hours\": int(len(expected)),\n",
    "                \"actual_hours\": int(len(g)),\n",
    "                \"missing_hours\": int(len(missing)),\n",
    "                \"missing_ratio\": float(len(missing) / max(len(expected), 1)),\n",
    "                \"n_missing_blocks\": int(len(blocks)),\n",
    "                \"largest_missing_block_hours\": int(max([b[2] for b in blocks], default=0)),\n",
    "                \"first_missing_block_start\": blocks[0][0] if blocks else pd.NaT,\n",
    "                \"first_missing_block_end\": blocks[0][1] if blocks else pd.NaT,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).sort_values([\"missing_ratio\", \"missing_hours\"], ascending=False)\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    label: str,\n",
    "    policy: str = \"raise\",  # \"raise\" | \"drop_incomplete_series\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce hourly continuity without imputation.\n",
    "    - raise: fail loud with detailed report\n",
    "    - drop_incomplete_series: drop series that have missing hours (log what was dropped)\n",
    "    \"\"\"\n",
    "    rep = _hourly_grid_report(df)\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "\n",
    "    if worst[\"missing_hours\"] == 0:\n",
    "        return df\n",
    "\n",
    "    print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "\n",
    "    if policy == \"drop_incomplete_series\":\n",
    "        bad_uids = rep.loc[rep[\"missing_hours\"] > 0, \"unique_id\"].tolist()\n",
    "        kept = df.loc[~df[\"unique_id\"].isin(bad_uids)].copy()\n",
    "        print(f\"[{label}][GRID] policy=drop_incomplete_series dropped={bad_uids} kept_series={kept['unique_id'].nunique()}\")\n",
    "        if kept.empty:\n",
    "            raise RuntimeError(f\"[{label}][GRID] all series dropped due to missing hours\")\n",
    "        return kept\n",
    "\n",
    "    # default: raise\n",
    "    worst_uid = worst[\"unique_id\"]\n",
    "    g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "    blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "    sample_blocks = blocks[:3]\n",
    "    raise RuntimeError(\n",
    "        f\"[{label}][GRID] Missing hours detected (no imputation). \"\n",
    "        f\"worst_unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "        f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={sample_blocks}\"\n",
    "    )\n",
    "\n",
    "def _validate_hourly_grid_fail_loud(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_missing_ratio: float = 0.0,\n",
    "    label: str = \"generation\",\n",
    ") -> None:\n",
    "    # Keep your original basic checks:\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"[{label}] empty dataframe\")\n",
    "\n",
    "    bad = df[\"ds\"].isna().sum()\n",
    "    if bad:\n",
    "        raise RuntimeError(f\"[{label}] ds has NaT values bad={int(bad)}\")\n",
    "\n",
    "    dup = df.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        raise RuntimeError(f\"[{label}] duplicate (unique_id, ds) rows dup={int(dup)}\")\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "    if worst[\"missing_ratio\"] > max_missing_ratio:\n",
    "        print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "        worst_uid = worst[\"unique_id\"]\n",
    "        g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Missing hours detected (no imputation allowed). \"\n",
    "            f\"unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "            f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "\n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "def _infer_model_columns(cv_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Infer StatsForecast model prediction columns from a cross_validation dataframe.\n",
    "\n",
    "    We treat as \"model columns\" those that:\n",
    "      - are not core columns (unique_id, ds, cutoff, y)\n",
    "      - are not interval columns like '<model>-lo-80' or '<model>-hi-95'\n",
    "    \"\"\"\n",
    "    core = {\"unique_id\", \"ds\", \"cutoff\", \"y\"}\n",
    "    cols = [c for c in cv_df.columns if c not in core]\n",
    "\n",
    "    model_cols: set[str] = set()\n",
    "    interval_pat = re.compile(r\"-(lo|hi)-\\d+$\")\n",
    "    for c in cols:\n",
    "        if interval_pat.search(c):\n",
    "            continue\n",
    "        model_cols.add(c)\n",
    "\n",
    "    return sorted(model_cols)\n",
    "\n",
    "\n",
    "def compute_leaderboard(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    confidence_levels: tuple[int, int] = (80, 95),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an aggregated leaderboard from StatsForecast cross_validation output.\n",
    "\n",
    "    Returns columns:\n",
    "      - model, rmse, mae, mape, valid_rows\n",
    "      - coverage_<level> if interval columns exist\n",
    "    \"\"\"\n",
    "    required = {\"y\", \"unique_id\", \"ds\", \"cutoff\"}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"[leaderboard] cv_df missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    model_cols = _infer_model_columns(cv_df)\n",
    "    if not model_cols:\n",
    "        raise RuntimeError(\n",
    "            f\"[leaderboard] Could not infer any model prediction columns. \"\n",
    "            f\"cv_df columns={cv_df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    y_true = cv_df[\"y\"].to_numpy()\n",
    "\n",
    "    for m in model_cols:\n",
    "        if m not in cv_df.columns:\n",
    "            continue\n",
    "\n",
    "        y_pred = cv_df[m].to_numpy()\n",
    "        valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        valid_rows = int(valid_mask.sum())\n",
    "\n",
    "        metrics = {\n",
    "            \"model\": m,\n",
    "            \"rmse\": float(ForecastMetrics.rmse(y_true, y_pred)),\n",
    "            \"mae\": float(ForecastMetrics.mae(y_true, y_pred)),\n",
    "            \"mape\": float(ForecastMetrics.mape(y_true, y_pred)),\n",
    "            \"valid_rows\": valid_rows,\n",
    "        }\n",
    "\n",
    "        # Coverage if interval columns exist\n",
    "        for lvl in confidence_levels:\n",
    "            lo_col = f\"{m}-lo-{lvl}\"\n",
    "            hi_col = f\"{m}-hi-{lvl}\"\n",
    "            if lo_col in cv_df.columns and hi_col in cv_df.columns:\n",
    "                cov = ForecastMetrics.coverage(\n",
    "                    y_true,\n",
    "                    cv_df[lo_col].to_numpy(),\n",
    "                    cv_df[hi_col].to_numpy(),\n",
    "                )\n",
    "                metrics[f\"coverage_{lvl}\"] = float(cov)\n",
    "\n",
    "        rows.append(metrics)\n",
    "\n",
    "    lb = pd.DataFrame(rows)\n",
    "    if lb.empty:\n",
    "        raise RuntimeError(\"[leaderboard] computed empty leaderboard (no usable model columns).\")\n",
    "\n",
    "    # Fail-loud sorting: rmse NaNs should sort last\n",
    "    lb = lb.sort_values([\"rmse\"], ascending=True, na_position=\"last\").reset_index(drop=True)\n",
    "    return lb\n",
    "\n",
    "\n",
    "def compute_baseline_metrics(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    threshold_k: float = 2.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for drift detection from CV output.\n",
    "\n",
    "    We compute RMSE/MAE per (unique_id, cutoff) window, then aggregate:\n",
    "      rmse_mean, rmse_std, drift_threshold_rmse = mean + k*std\n",
    "\n",
    "    No imputation/filling: metrics are computed only from finite values.\n",
    "    \"\"\"\n",
    "    required = {\"unique_id\", \"cutoff\", \"y\", model_name}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"[baseline] cv_df missing required columns for model '{model_name}': {sorted(missing)}\"\n",
    "        )\n",
    "\n",
    "    # Compute per-window metrics (unique_id, cutoff)\n",
    "    def _window_metrics(g: pd.DataFrame) -> pd.Series:\n",
    "        yt = g[\"y\"].to_numpy()\n",
    "        yp = g[model_name].to_numpy()\n",
    "        valid = np.isfinite(yt) & np.isfinite(yp)\n",
    "        if valid.sum() == 0:\n",
    "            return pd.Series({\"rmse\": np.nan, \"mae\": np.nan, \"valid_rows\": 0})\n",
    "        return pd.Series({\n",
    "            \"rmse\": ForecastMetrics.rmse(yt, yp),\n",
    "            \"mae\": ForecastMetrics.mae(yt, yp),\n",
    "            \"valid_rows\": int(valid.sum()),\n",
    "        })\n",
    "\n",
    "    per_window = (\n",
    "        cv_df.groupby([\"unique_id\", \"cutoff\"], sort=False, dropna=False)\n",
    "        .apply(_window_metrics)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Fail loud if baseline is entirely NaN\n",
    "    if per_window[\"rmse\"].notna().sum() == 0:\n",
    "        sample_cols = [\"unique_id\", \"cutoff\", \"y\", model_name]\n",
    "        raise RuntimeError(\n",
    "            \"[baseline] All per-window RMSE are NaN. \"\n",
    "            \"This usually means predictions or y are non-finite everywhere. \"\n",
    "            f\"Sample:\\n{cv_df[sample_cols].head(20).to_string(index=False)}\"\n",
    "        )\n",
    "\n",
    "    rmse_mean = float(per_window[\"rmse\"].mean(skipna=True))\n",
    "    rmse_std = float(per_window[\"rmse\"].std(skipna=True, ddof=0))\n",
    "    mae_mean = float(per_window[\"mae\"].mean(skipna=True))\n",
    "    mae_std = float(per_window[\"mae\"].std(skipna=True, ddof=0))\n",
    "\n",
    "    baseline = {\n",
    "        \"model\": model_name,\n",
    "        \"rmse_mean\": rmse_mean,\n",
    "        \"rmse_std\": rmse_std,\n",
    "        \"mae_mean\": mae_mean,\n",
    "        \"mae_std\": mae_std,\n",
    "        \"drift_threshold_rmse\": float(rmse_mean + threshold_k * rmse_std),\n",
    "        \"drift_threshold_mae\": float(mae_mean + threshold_k * mae_std),\n",
    "        \"n_series\": int(per_window[\"unique_id\"].nunique()),\n",
    "        \"n_windows\": int(per_window[\"cutoff\"].nunique()),\n",
    "        \"per_window_rows\": int(len(per_window)),\n",
    "    }\n",
    "\n",
    "    # Optional per-series baseline (useful later if you want drift per series)\n",
    "    per_series = (\n",
    "        per_window.groupby(\"unique_id\")[[\"rmse\", \"mae\"]]\n",
    "        .agg(rmse_mean=(\"rmse\", \"mean\"), rmse_std=(\"rmse\", lambda s: s.std(ddof=0)),\n",
    "             mae_mean=(\"mae\", \"mean\"), mae_std=(\"mae\", lambda s: s.std(ddof=0)))\n",
    "        .reset_index()\n",
    "    )\n",
    "    per_series[\"drift_threshold_rmse\"] = per_series[\"rmse_mean\"] + threshold_k * per_series[\"rmse_std\"]\n",
    "    per_series[\"drift_threshold_mae\"] = per_series[\"mae_mean\"] + threshold_k * per_series[\"mae_std\"]\n",
    "    baseline[\"per_series\"] = per_series.to_dict(orient=\"records\")\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "\n",
    "\n",
    "class RenewableForecastModel:\n",
    "    def __init__(self, horizon: int = 24, confidence_levels: tuple[int, int] = (80, 95)):\n",
    "        self.horizon = horizon\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.sf = None\n",
    "        self._train_df = None  # contains y + exog columns\n",
    "        self._exog_cols: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def prepare_training_df(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "        req = {\"unique_id\", \"ds\", \"y\"}\n",
    "        if not req.issubset(df.columns):\n",
    "            raise ValueError(f\"generation df missing cols={sorted(req - set(df.columns))}\")\n",
    "\n",
    "        work = df.copy()\n",
    "        work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"raise\")\n",
    "        work = work.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        # Fail-loud on null y (truthful: indicates missing measurements, not missing timestamps)\n",
    "        y_null = work[\"y\"].isna()\n",
    "        if y_null.any():\n",
    "            sample = work.loc[y_null, [\"unique_id\", \"ds\", \"y\"]].head(25)\n",
    "            raise RuntimeError(\n",
    "                f\"[generation][Y] Found null y values (no imputation). rows={int(y_null.sum())}. \"\n",
    "                f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "            )\n",
    "\n",
    "        # Hourly grid enforcement (no imputation).\n",
    "        # Default policy remains strict. To run CV while keeping the defect visible:\n",
    "        #   set policy=\"drop_incomplete_series\" here temporarily.\n",
    "        work = _enforce_hourly_grid(work, label=\"generation\", policy=\"drop_incomplete_series\")\n",
    "\n",
    "\n",
    "        # Deterministic time features\n",
    "        work = _add_time_features(work)\n",
    "\n",
    "        # Weather merge (FAIL LOUD on missing)\n",
    "        if weather_df is not None and not weather_df.empty:\n",
    "            if not {\"ds\", \"region\"}.issubset(weather_df.columns):\n",
    "                raise ValueError(\"weather_df must have columns ['ds','region', ...]\")\n",
    "\n",
    "            work[\"region\"] = work[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "            wcols = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "            if not wcols:\n",
    "                raise ValueError(\"weather_df has none of expected WEATHER_VARS\")\n",
    "\n",
    "            merged = work.merge(\n",
    "                weather_df[[\"ds\", \"region\"] + wcols],\n",
    "                on=[\"ds\", \"region\"],\n",
    "                how=\"left\",\n",
    "                validate=\"many_to_one\",\n",
    "            )\n",
    "\n",
    "            missing_any = merged[wcols].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = merged.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + wcols].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[weather][ALIGN] Missing weather after merge rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "            work = merged.drop(columns=[\"region\"])\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"] + wcols\n",
    "        else:\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "        return work\n",
    "\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame] = None) -> None:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, SeasonalNaive, AutoETS, MSTL\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "\n",
    "        self.sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "        self._train_df = train_df\n",
    "        self.fitted = True\n",
    "\n",
    "        print(f\"[fit] rows={len(train_df)} series={train_df['unique_id'].nunique()} exog_cols={self._exog_cols}\")\n",
    "\n",
    "    def build_future_X_df(self, future_weather: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build future X_df for forecast horizon using forecast weather.\n",
    "        Must include: unique_id, ds, and exactly the exog columns used in training.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        if future_weather is None or future_weather.empty:\n",
    "            raise RuntimeError(\"future_weather required to forecast with regressors (no fabrication).\")\n",
    "\n",
    "        if not {\"ds\", \"region\"}.issubset(future_weather.columns):\n",
    "            raise ValueError(\"future_weather must have columns ['ds','region', ...]\")\n",
    "\n",
    "        # Create the future ds grid per series\n",
    "        last_ds = self._train_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "        frames = []\n",
    "        for uid, end in last_ds.items():\n",
    "            future_ds = pd.date_range(end + pd.Timedelta(hours=1), periods=self.horizon, freq=\"h\")\n",
    "            frames.append(pd.DataFrame({\"unique_id\": uid, \"ds\": future_ds}))\n",
    "        X = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        X = _add_time_features(X)\n",
    "        X[\"region\"] = X[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "        wcols = [c for c in WEATHER_VARS if c in future_weather.columns]\n",
    "        X = X.merge(\n",
    "            future_weather[[\"ds\", \"region\"] + wcols],\n",
    "            on=[\"ds\", \"region\"],\n",
    "            how=\"left\",\n",
    "            validate=\"many_to_one\",\n",
    "        )\n",
    "\n",
    "        # Fail loud on missing future regressors\n",
    "        needed = [c for c in self._exog_cols if c not in [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]]  # weather cols\n",
    "        if needed:\n",
    "            missing_any = X[needed].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = X.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + needed].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[future_weather][ALIGN] Missing future weather rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "        X = X.drop(columns=[\"region\"])\n",
    "        keep = [\"unique_id\", \"ds\"] + self._exog_cols\n",
    "        return X[keep].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    def predict(self, future_weather: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        X_df = self.build_future_X_df(future_weather)\n",
    "\n",
    "        # IMPORTANT: If you fit models using exogenous regressors, you must supply X_df at forecast time.\n",
    "        fcst = self.sf.forecast(\n",
    "            h=self.horizon,\n",
    "            df=self._train_df,\n",
    "            X_df=X_df,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        return fcst\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        weather_df: Optional[pd.DataFrame] = None,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, SeasonalNaive, AutoETS, MSTL\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "        sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "\n",
    "        print(\n",
    "            f\"[cv] windows={n_windows} step={step_size} h={self.horizon} \"\n",
    "            f\"rows={len(train_df)} series={train_df['unique_id'].nunique()}\"\n",
    "        )\n",
    "\n",
    "        cv = sf.cross_validation(\n",
    "            df=train_df,\n",
    "            h=self.horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        leaderboard = compute_leaderboard(cv, confidence_levels=self.confidence_levels)\n",
    "        return cv, leaderboard\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # REAL EXAMPLE: multi-series WND with strict gates and CV\n",
    "\n",
    "    from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "    from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "\n",
    "    regions = [\"CALI\", \"ERCO\", \"MISO\"]\n",
    "    fuel = \"WND\"\n",
    "    start_date = \"2024-11-01\"\n",
    "    end_date = \"2024-12-15\"\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "    gen = fetcher.fetch_all_regions(fuel, start_date, end_date, regions=regions)\n",
    "    _log_series_summary(gen, label=\"generation_raw\")\n",
    "\n",
    "    weather_api = OpenMeteoRenewable(strict=True)\n",
    "    wx_hist = weather_api.fetch_all_regions_historical(regions, start_date, end_date, debug=True)\n",
    "\n",
    "    model = RenewableForecastModel(horizon=24, confidence_levels=(80, 95))\n",
    "\n",
    "    # CV (historical): regressors live in df, no filling allowed\n",
    "    cv = model.cross_validate(gen, weather_df=wx_hist, n_windows=3, step_size=168)\n",
    "    print(cv.head().to_string(index=False))\n",
    "\n",
    "    # Optional: fit + forecast next 24h using forecast weather (no leakage)\n",
    "    # wx_future = weather_api.fetch_all_regions_forecast(regions, horizon_hours=48, debug=True)\n",
    "    # model.fit(gen, weather_df=wx_hist)\n",
    "    # fcst = model.predict(future_weather=wx_future)\n",
    "    # print(fcst.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/db.py\n",
    "# file: src/renewable/db.py\n",
    "\"\"\"Database schema and operations for renewable forecasting.\n",
    "\n",
    "Extends the Chapter 4 monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    \"\"\"Connect to SQLite database with optimized settings.\"\"\"\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def init_renewable_db(db_path: str) -> None:\n",
    "    \"\"\"Initialize renewable forecasting database schema.\n",
    "\n",
    "    Creates tables:\n",
    "    - renewable_forecasts: Forecasts with dual intervals\n",
    "    - renewable_scores: Evaluation metrics with coverage\n",
    "    - weather_features: Weather data by region\n",
    "    - drift_alerts: Drift detection history\n",
    "    - baseline_metrics: Backtest baselines for drift thresholds\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Forecasts with dual prediction intervals\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_forecasts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        yhat_lo_80 REAL,\n",
    "        yhat_hi_80 REAL,\n",
    "        yhat_lo_95 REAL,\n",
    "        yhat_hi_95 REAL,\n",
    "        UNIQUE (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Index for efficient queries\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_region_ds\n",
    "    ON renewable_forecasts (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_fuel_ds\n",
    "    ON renewable_forecasts (fuel_type, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Evaluation scores with dual coverage\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_scores (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        scored_at TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mae REAL,\n",
    "        coverage_80 REAL,\n",
    "        coverage_95 REAL,\n",
    "        valid_rows INTEGER,\n",
    "        UNIQUE (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Weather features by region\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_features (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        region TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        temperature_2m REAL,\n",
    "        wind_speed_10m REAL,\n",
    "        wind_speed_100m REAL,\n",
    "        wind_direction_10m REAL,\n",
    "        direct_radiation REAL,\n",
    "        diffuse_radiation REAL,\n",
    "        cloud_cover REAL,\n",
    "        is_forecast INTEGER DEFAULT 0,\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE (region, ds, is_forecast)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_weather_region_ds\n",
    "    ON weather_features (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Drift detection alerts\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS drift_alerts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        alert_at TEXT NOT NULL,\n",
    "        run_id TEXT,\n",
    "        unique_id TEXT,\n",
    "        region TEXT,\n",
    "        fuel_type TEXT,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        current_rmse REAL,\n",
    "        threshold_rmse REAL,\n",
    "        message TEXT,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_drift_alerts_time\n",
    "    ON drift_alerts (alert_at);\n",
    "    \"\"\")\n",
    "\n",
    "    # Baseline metrics for drift detection\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS baseline_metrics (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        rmse_mean REAL NOT NULL,\n",
    "        rmse_std REAL NOT NULL,\n",
    "        mae_mean REAL,\n",
    "        mae_std REAL,\n",
    "        drift_threshold_rmse REAL NOT NULL,\n",
    "        drift_threshold_mae REAL,\n",
    "        n_windows INTEGER,\n",
    "        metadata_json TEXT,\n",
    "        UNIQUE (unique_id, model)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_forecasts(\n",
    "    db_path: str,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    run_id: str,\n",
    "    model: str = \"MSTL_ARIMA\",\n",
    ") -> int:\n",
    "    \"\"\"Save forecasts to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        forecasts_df: DataFrame with [unique_id, ds, yhat, yhat_lo_80, ...]\n",
    "        run_id: Pipeline run identifier\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    created_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in forecasts_df.iterrows():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        parts = unique_id.split(\"_\")\n",
    "        region = parts[0] if len(parts) > 0 else \"\"\n",
    "        fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        rows.append((\n",
    "            run_id,\n",
    "            created_at,\n",
    "            unique_id,\n",
    "            region,\n",
    "            fuel_type,\n",
    "            str(row[\"ds\"]),\n",
    "            model,\n",
    "            row.get(\"yhat\"),\n",
    "            row.get(\"yhat_lo_80\"),\n",
    "            row.get(\"yhat_hi_80\"),\n",
    "            row.get(\"yhat_lo_95\"),\n",
    "            row.get(\"yhat_hi_95\"),\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO renewable_forecasts\n",
    "        (run_id, created_at, unique_id, region, fuel_type, ds, model,\n",
    "         yhat, yhat_lo_80, yhat_hi_80, yhat_lo_95, yhat_hi_95)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_weather(\n",
    "    db_path: str,\n",
    "    weather_df: pd.DataFrame,\n",
    "    is_forecast: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"Save weather features to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        weather_df: DataFrame with [ds, region, weather_vars...]\n",
    "        is_forecast: True if this is forecast weather data\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temperature_2m\", \"wind_speed_10m\", \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\", \"direct_radiation\", \"diffuse_radiation\", \"cloud_cover\"\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in weather_df.iterrows():\n",
    "        values = [row.get(col) for col in weather_cols]\n",
    "        rows.append((\n",
    "            row[\"region\"],\n",
    "            str(row[\"ds\"]),\n",
    "            *values,\n",
    "            1 if is_forecast else 0,\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(f\"\"\"\n",
    "        INSERT OR REPLACE INTO weather_features\n",
    "        (region, ds, {', '.join(weather_cols)}, is_forecast)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_drift_alert(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    unique_id: str,\n",
    "    current_rmse: float,\n",
    "    threshold_rmse: float,\n",
    "    severity: str = \"warning\",\n",
    "    metadata: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Save drift detection alert.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        run_id: Pipeline run identifier\n",
    "        unique_id: Series identifier\n",
    "        current_rmse: Current RMSE value\n",
    "        threshold_rmse: Drift threshold\n",
    "        severity: Alert severity (info, warning, critical)\n",
    "        metadata: Additional metadata\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    parts = unique_id.split(\"_\")\n",
    "    region = parts[0] if len(parts) > 0 else \"\"\n",
    "    fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    alert_type = \"drift_detected\" if current_rmse > threshold_rmse else \"drift_check\"\n",
    "    message = (\n",
    "        f\"RMSE {current_rmse:.1f} {'>' if current_rmse > threshold_rmse else '<='} \"\n",
    "        f\"threshold {threshold_rmse:.1f}\"\n",
    "    )\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO drift_alerts\n",
    "        (alert_at, run_id, unique_id, region, fuel_type, alert_type, severity,\n",
    "         current_rmse, threshold_rmse, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        run_id,\n",
    "        unique_id,\n",
    "        region,\n",
    "        fuel_type,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        current_rmse,\n",
    "        threshold_rmse,\n",
    "        message,\n",
    "        json.dumps(metadata) if metadata else None,\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_baseline(\n",
    "    db_path: str,\n",
    "    unique_id: str,\n",
    "    model: str,\n",
    "    baseline: dict,\n",
    ") -> None:\n",
    "    \"\"\"Save baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        unique_id: Series identifier\n",
    "        model: Model name\n",
    "        baseline: Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO baseline_metrics\n",
    "        (created_at, unique_id, model, rmse_mean, rmse_std, mae_mean, mae_std,\n",
    "         drift_threshold_rmse, drift_threshold_mae, n_windows, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        unique_id,\n",
    "        model,\n",
    "        baseline.get(\"rmse_mean\"),\n",
    "        baseline.get(\"rmse_std\"),\n",
    "        baseline.get(\"mae_mean\"),\n",
    "        baseline.get(\"mae_std\"),\n",
    "        baseline.get(\"drift_threshold_rmse\"),\n",
    "        baseline.get(\"drift_threshold_mae\"),\n",
    "        baseline.get(\"n_windows\"),\n",
    "        json.dumps(baseline),\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def get_recent_forecasts(\n",
    "    db_path: str,\n",
    "    region: Optional[str] = None,\n",
    "    fuel_type: Optional[str] = None,\n",
    "    hours: int = 48,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent forecasts from database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        region: Filter by region (optional)\n",
    "        fuel_type: Filter by fuel type (optional)\n",
    "        hours: Hours of history to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with forecasts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM renewable_forecasts\n",
    "        WHERE datetime(created_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if region:\n",
    "        query += \" AND region = ?\"\n",
    "        params.append(region)\n",
    "\n",
    "    if fuel_type:\n",
    "        query += \" AND fuel_type = ?\"\n",
    "        params.append(fuel_type)\n",
    "\n",
    "    query += \" ORDER BY ds DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_drift_alerts(\n",
    "    db_path: str,\n",
    "    hours: int = 24,\n",
    "    severity: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent drift alerts.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        hours: Hours of history\n",
    "        severity: Filter by severity (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with alerts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM drift_alerts\n",
    "        WHERE datetime(alert_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if severity:\n",
    "        query += \" AND severity = ?\"\n",
    "        params.append(severity)\n",
    "\n",
    "    query += \" ORDER BY alert_at DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test database initialization\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        db_path = f\"{tmpdir}/test_renewable.db\"\n",
    "\n",
    "        print(\"Initializing database...\")\n",
    "        init_renewable_db(db_path)\n",
    "\n",
    "        print(\"Database initialized successfully!\")\n",
    "\n",
    "        # Test connection\n",
    "        con = connect(db_path)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"Tables created: {[t[0] for t in tables]}\")\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 6: Pipeline Tasks\n",
    "\n",
    "**File:** `src/renewable/tasks.py`\n",
    "\n",
    "This module orchestrates the complete pipeline:\n",
    "\n",
    "1. **Fetch generation data** from EIA\n",
    "2. **Fetch weather data** from Open-Meteo\n",
    "3. **Train models** with cross-validation\n",
    "4. **Generate forecasts** with prediction intervals\n",
    "5. **Compute drift metrics** vs baseline\n",
    "\n",
    "## Key Feature: Adaptive CV\n",
    "\n",
    "Cross-validation requires sufficient data:\n",
    "```\n",
    "Minimum rows = horizon + (n_windows × step_size)\n",
    "```\n",
    "\n",
    "For short series, we **adapt** the CV settings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/tasks.py\n",
    "# file: src\\renewable\\tasks.py\n",
    "\"\"\"Renewable energy forecasting pipeline tasks.\n",
    "\n",
    "Idempotent tasks for:\n",
    "- Fetching EIA renewable generation data\n",
    "- Fetching weather data from Open-Meteo\n",
    "- Training probabilistic models\n",
    "- Generating forecasts with intervals\n",
    "- Computing drift metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "from src.renewable.modeling import (\n",
    "    RenewableForecastModel,\n",
    "    _log_series_summary,\n",
    "    compute_baseline_metrics,\n",
    ")\n",
    "from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "from src.renewable.regions import REGIONS, list_regions\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RenewablePipelineConfig:\n",
    "    \"\"\"Configuration for renewable forecasting pipeline.\"\"\"\n",
    "\n",
    "    # Data parameters\n",
    "    regions: list[str] = field(default_factory=lambda: [\"CALI\", \"ERCO\", \"MISO\", \"PJM\", \"SWPP\"])\n",
    "    fuel_types: list[str] = field(default_factory=lambda: [\"WND\", \"SUN\"])\n",
    "    start_date: str = \"\"  # Set dynamically\n",
    "    end_date: str = \"\"  # Set dynamically\n",
    "    lookback_days: int = 30\n",
    "\n",
    "    # Forecast parameters\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "\n",
    "    # CV parameters\n",
    "    cv_windows: int = 5\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "    # Output paths\n",
    "    data_dir: str = \"data/renewable\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Set default dates if not provided\n",
    "        if not self.end_date:\n",
    "            self.end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        if not self.start_date:\n",
    "            end = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "            start = end - timedelta(days=self.lookback_days)\n",
    "            self.start_date = start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def generation_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"generation.parquet\"\n",
    "\n",
    "    def weather_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"weather.parquet\"\n",
    "\n",
    "    def forecasts_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"forecasts.parquet\"\n",
    "\n",
    "    def baseline_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"baseline.json\"\n",
    "\n",
    "\n",
    "def fetch_renewable_data(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 1: Fetch EIA generation data for all regions and fuel types.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [unique_id, ds, y]\n",
    "    \"\"\"\n",
    "    output_path = config.generation_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_generation_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        _log_series_summary(df, value_col=\"y\", label=f\"generation_data_{source}\")\n",
    "\n",
    "        expected_series = {\n",
    "            f\"{region}_{fuel}\" for region in config.regions for fuel in config.fuel_types\n",
    "        }\n",
    "        present_series = set(df[\"unique_id\"]) if \"unique_id\" in df.columns else set()\n",
    "        missing_series = sorted(expected_series - present_series)\n",
    "        if missing_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Missing expected series (%s): %s\",\n",
    "                source,\n",
    "                missing_series,\n",
    "            )\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_generation] No generation data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"unique_id\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"unique_id\")\n",
    "        )\n",
    "        max_series_log = 25\n",
    "        if len(coverage) > max_series_log:\n",
    "            logger.info(\n",
    "                \"[fetch_generation] Coverage (%s, first %s series):\\n%s\",\n",
    "                source,\n",
    "                max_series_log,\n",
    "                coverage.head(max_series_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_generation] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_generation] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached coverage to surface missing series without refetching.\n",
    "        _log_generation_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_generation] Fetching {config.fuel_types} for {config.regions}\")\n",
    "\n",
    "    fetcher = EIARenewableFetcher()\n",
    "    all_dfs = []\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        df = fetcher.fetch_all_regions(\n",
    "            fuel_type=fuel_type,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            regions=config.regions,\n",
    "            diagnostics=fetch_diagnostics,\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined = combined.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh coverage to highlight gaps or unexpected negatives.\n",
    "    _log_generation_summary(combined, source=\"fresh\")\n",
    "\n",
    "    if fetch_diagnostics:\n",
    "        empty_series = [\n",
    "            entry\n",
    "            for entry in fetch_diagnostics\n",
    "            if entry.get(\"empty\")\n",
    "        ]\n",
    "        for entry in empty_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Empty series detail: region=%s fuel=%s total=%s pages=%s\",\n",
    "                entry.get(\"region\"),\n",
    "                entry.get(\"fuel_type\"),\n",
    "                entry.get(\"total_records\"),\n",
    "                entry.get(\"pages\"),\n",
    "            )\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_generation] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def fetch_renewable_weather(\n",
    "    config: RenewablePipelineConfig,\n",
    "    include_forecast: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 2: Fetch weather data for all regions.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        include_forecast: Include forecast weather for predictions\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [ds, region, weather_vars...]\n",
    "    \"\"\"\n",
    "    output_path = config.weather_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_weather_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_weather] No weather data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"region\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"region\")\n",
    "        )\n",
    "        max_region_log = 25\n",
    "        if len(coverage) > max_region_log:\n",
    "            logger.info(\n",
    "                \"[fetch_weather] Coverage (%s, first %s regions):\\n%s\",\n",
    "                source,\n",
    "                max_region_log,\n",
    "                coverage.head(max_region_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_weather] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "        missing_cols = [\n",
    "            col for col in OpenMeteoRenewable.WEATHER_VARS if col not in df.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing expected weather columns (%s): %s\",\n",
    "                source,\n",
    "                missing_cols,\n",
    "            )\n",
    "\n",
    "        missing_values = {\n",
    "            col: int(df[col].isna().sum())\n",
    "            for col in OpenMeteoRenewable.WEATHER_VARS\n",
    "            if col in df.columns and df[col].isna().any()\n",
    "        }\n",
    "        if missing_values:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing weather values (%s): %s\",\n",
    "                source,\n",
    "                missing_values,\n",
    "            )\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_weather] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached weather coverage to surface missing regions/columns.\n",
    "        _log_weather_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_weather] Fetching weather for {config.regions}\")\n",
    "\n",
    "    weather = OpenMeteoRenewable()\n",
    "\n",
    "    # Historical weather\n",
    "    hist_df = weather.fetch_all_regions_historical(\n",
    "        regions=config.regions,\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "    )\n",
    "\n",
    "    # Forecast weather (for prediction, prevents leakage)\n",
    "    if include_forecast:\n",
    "        fcst_df = weather.fetch_all_regions_forecast(\n",
    "            regions=config.regions,\n",
    "            horizon_hours=config.horizon + 24,  # Buffer\n",
    "        )\n",
    "\n",
    "        # Combine, preferring forecast for overlapping times\n",
    "        combined = pd.concat([hist_df, fcst_df], ignore_index=True)\n",
    "        combined = combined.drop_duplicates(subset=[\"ds\", \"region\"], keep=\"last\")\n",
    "    else:\n",
    "        combined = hist_df\n",
    "\n",
    "    combined = combined.sort_values([\"region\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh weather coverage and missing values before saving.\n",
    "    _log_weather_summary(combined, source=\"fresh\")\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_weather] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def train_renewable_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Task 3: Train models and compute baseline metrics via cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cv_results, leaderboard, baseline_metrics)\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_models] Training on {len(generation_df)} rows\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Compute adaptive CV settings based on shortest series\n",
    "    min_series_len = generation_df.groupby(\"unique_id\").size().min()\n",
    "\n",
    "    # CV needs: horizon + (n_windows * step_size) rows minimum\n",
    "    # Solve for n_windows: n_windows = (min_series_len - horizon) / step_size\n",
    "    available_for_cv = min_series_len - config.horizon\n",
    "\n",
    "    # Adjust step_size and n_windows to fit data\n",
    "    step_size = min(config.cv_step_size, max(24, available_for_cv // 3))\n",
    "    n_windows = min(config.cv_windows, max(2, available_for_cv // step_size))\n",
    "\n",
    "    logger.info(\n",
    "        f\"[train_models] Adaptive CV: {n_windows} windows, \"\n",
    "        f\"step={step_size}h (min_series={min_series_len} rows)\"\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results, leaderboard = model.cross_validate(\n",
    "        df=generation_df,\n",
    "        weather_df=weather_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    baseline = compute_baseline_metrics(cv_results, model_name=best_model)\n",
    "\n",
    "\n",
    "    logger.info(f\"[train_models] Best model: {best_model}, RMSE: {baseline['rmse_mean']:.1f}\")\n",
    "\n",
    "    return cv_results, leaderboard, baseline\n",
    "\n",
    "\n",
    "def generate_renewable_forecasts(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 4: Generate forecasts with prediction intervals.\"\"\"\n",
    "    output_path = config.forecasts_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[generate_forecasts] Generating {config.horizon}h forecasts\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Fit uses only historical generation timestamps, weather merge will fail-loud if missing.\n",
    "    model.fit(generation_df, weather_df)\n",
    "\n",
    "    # Future weather must cover the horizon after the latest generation timestamp.\n",
    "    last_gen_ds = generation_df[\"ds\"].max()\n",
    "    future_weather = weather_df[weather_df[\"ds\"] > last_gen_ds].copy()\n",
    "\n",
    "    if future_weather.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[generate_forecasts] No future weather rows found after last generation timestamp. \"\n",
    "            f\"last_gen_ds={last_gen_ds}\"\n",
    "        )\n",
    "\n",
    "    forecasts = model.predict(future_weather=future_weather)\n",
    "\n",
    "    forecasts.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[generate_forecasts] Saved: {output_path} ({len(forecasts)} rows)\")\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "\n",
    "def compute_renewable_drift(\n",
    "    predictions: pd.DataFrame,\n",
    "    actuals: pd.DataFrame,\n",
    "    baseline_metrics: dict,\n",
    ") -> dict:\n",
    "    \"\"\"Task 5: Detect drift by comparing current metrics to baseline.\n",
    "\n",
    "    Drift is flagged when current RMSE > baseline_mean + 2*baseline_std\n",
    "\n",
    "    Args:\n",
    "        predictions: Forecast DataFrame with [unique_id, ds, yhat]\n",
    "        actuals: Actual values DataFrame with [unique_id, ds, y]\n",
    "        baseline_metrics: Baseline from cross-validation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with drift status and details\n",
    "    \"\"\"\n",
    "    from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "    # Merge predictions with actuals\n",
    "    merged = predictions.merge(\n",
    "        actuals[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        return {\n",
    "            \"status\": \"no_data\",\n",
    "            \"message\": \"No overlapping data between predictions and actuals\",\n",
    "        }\n",
    "\n",
    "    # Compute current metrics\n",
    "    y_true = merged[\"y\"].values\n",
    "    y_pred = merged[\"yhat\"].values\n",
    "\n",
    "    current_rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "    current_mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "\n",
    "    # Check against threshold\n",
    "    threshold = baseline_metrics.get(\"drift_threshold_rmse\", float(\"inf\"))\n",
    "    is_drifting = current_rmse > threshold\n",
    "\n",
    "    result = {\n",
    "        \"status\": \"drift_detected\" if is_drifting else \"stable\",\n",
    "        \"current_rmse\": float(current_rmse),\n",
    "        \"current_mae\": float(current_mae),\n",
    "        \"baseline_rmse\": float(baseline_metrics.get(\"rmse_mean\", 0)),\n",
    "        \"drift_threshold\": float(threshold),\n",
    "        \"threshold_exceeded_by\": float(max(0, current_rmse - threshold)),\n",
    "        \"n_predictions\": len(merged),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    if is_drifting:\n",
    "        logger.warning(\n",
    "            f\"[drift] DRIFT DETECTED: RMSE={current_rmse:.1f} > threshold={threshold:.1f}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"[drift] Stable: RMSE={current_rmse:.1f} <= threshold={threshold:.1f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Run the complete renewable forecasting pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetch generation data\n",
    "    2. Fetch weather data\n",
    "    3. Train models (CV)\n",
    "    4. Generate forecasts\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pipeline results\n",
    "    \"\"\"\n",
    "    logger.info(f\"[pipeline] Starting: {config.start_date} to {config.end_date}\")\n",
    "    logger.info(f\"[pipeline] Regions: {config.regions}\")\n",
    "    logger.info(f\"[pipeline] Fuel types: {config.fuel_types}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Fetch generation\n",
    "    generation_df = fetch_renewable_data(config, fetch_diagnostics=fetch_diagnostics)\n",
    "    results[\"generation_rows\"] = len(generation_df)\n",
    "    results[\"series_count\"] = generation_df[\"unique_id\"].nunique()\n",
    "\n",
    "    # Step 2: Fetch weather\n",
    "    weather_df = fetch_renewable_weather(config)\n",
    "    results[\"weather_rows\"] = len(weather_df)\n",
    "\n",
    "    # Step 3: Train and validate\n",
    "    cv_results, leaderboard, baseline = train_renewable_models(\n",
    "        config, generation_df, weather_df\n",
    "    )\n",
    "    results[\"best_model\"] = leaderboard.iloc[0][\"model\"]\n",
    "    results[\"best_rmse\"] = float(leaderboard.iloc[0][\"rmse\"])\n",
    "    results[\"baseline\"] = baseline\n",
    "\n",
    "    # Step 4: Generate forecasts\n",
    "    forecasts = generate_renewable_forecasts(config, generation_df, weather_df)\n",
    "    results[\"forecast_rows\"] = len(forecasts)\n",
    "\n",
    "    if fetch_diagnostics is not None:\n",
    "        results[\"fetch_diagnostics\"] = fetch_diagnostics\n",
    "\n",
    "    logger.info(f\"[pipeline] Complete. Best model: {results['best_model']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entry point for renewable pipeline.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Renewable Energy Forecasting Pipeline\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--regions\",\n",
    "        type=str,\n",
    "        default=\"CALI,ERCO,MISO\",\n",
    "        help=\"Comma-separated region codes (default: CALI,ERCO,MISO)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuel\",\n",
    "        type=str,\n",
    "        default=\"WND,SUN\",\n",
    "        help=\"Comma-separated fuel types (default: WND,SUN)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--days\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"Lookback days (default: 30)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--horizon\",\n",
    "        type=int,\n",
    "        default=24,\n",
    "        help=\"Forecast horizon in hours (default: 24)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing data files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"data/renewable\",\n",
    "        help=\"Output directory (default: data/renewable)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Build config\n",
    "    config = RenewablePipelineConfig(\n",
    "        regions=args.regions.split(\",\"),\n",
    "        fuel_types=args.fuel.split(\",\"),\n",
    "        lookback_days=args.days,\n",
    "        horizon=args.horizon,\n",
    "        overwrite=args.overwrite,\n",
    "        data_dir=args.data_dir,\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    results = run_full_pipeline(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Series count: {results['series_count']}\")\n",
    "    print(f\"  Generation rows: {results['generation_rows']}\")\n",
    "    print(f\"  Weather rows: {results['weather_rows']}\")\n",
    "    print(f\"  Forecast rows: {results['forecast_rows']}\")\n",
    "    print(f\"  Best model: {results['best_model']}\")\n",
    "    print(f\"  Best RMSE: {results['best_rmse']:.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/validation.py\n",
    "# file: src/renewable/validation.py\n",
    "\"\"\"Validation utilities for renewable generation data.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ValidationReport:\n",
    "    ok: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "def validate_generation_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lag_hours: int = 3,\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    expected_series: Optional[Iterable[str]] = None,\n",
    ") -> ValidationReport:\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    missing_cols = required - set(df.columns)\n",
    "    if missing_cols:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Missing required columns\",\n",
    "            {\"missing_cols\": sorted(missing_cols)},\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        return ValidationReport(False, \"Generation data is empty\", {})\n",
    "\n",
    "    work = df.copy()\n",
    "\n",
    "    work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"coerce\", utc=True)\n",
    "    if work[\"ds\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable ds values found\",\n",
    "            {\"bad_ds\": int(work[\"ds\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    work[\"y\"] = pd.to_numeric(work[\"y\"], errors=\"coerce\")\n",
    "    if work[\"y\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable y values found\",\n",
    "            {\"bad_y\": int(work[\"y\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    if (work[\"y\"] < 0).any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Negative generation values found\",\n",
    "            {\"neg_y\": int((work[\"y\"] < 0).sum())},\n",
    "        )\n",
    "\n",
    "    dup = work.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Duplicate (unique_id, ds) rows found\",\n",
    "            {\"duplicates\": int(dup)},\n",
    "        )\n",
    "\n",
    "    if expected_series:\n",
    "        expected = sorted(set(expected_series))\n",
    "        present = sorted(set(work[\"unique_id\"]))\n",
    "        missing_series = sorted(set(expected) - set(present))\n",
    "        if missing_series:\n",
    "            return ValidationReport(\n",
    "                False,\n",
    "                \"Missing expected series\",\n",
    "                {\"missing_series\": missing_series, \"present_series\": present},\n",
    "            )\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"H\")\n",
    "    max_ds = work[\"ds\"].max()\n",
    "    lag_hours = (now_utc - max_ds).total_seconds() / 3600.0\n",
    "    if lag_hours > max_lag_hours:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Data not fresh enough\",\n",
    "            {\n",
    "                \"now_utc\": now_utc.isoformat(),\n",
    "                \"max_ds\": max_ds.isoformat(),\n",
    "                \"lag_hours\": lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    series_max = work.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    series_lag = (now_utc - series_max).dt.total_seconds() / 3600.0\n",
    "    stale = series_lag[series_lag > max_lag_hours].sort_values(ascending=False)\n",
    "    if not stale.empty:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Stale series found\",\n",
    "            {\n",
    "                \"stale_series\": stale.head(10).to_dict(),\n",
    "                \"max_lag_hours\": max_lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    missing_ratios = {}\n",
    "    for uid, group in work.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")\n",
    "        start = group[\"ds\"].iloc[0]\n",
    "        end = group[\"ds\"].iloc[-1]\n",
    "        expected = int(((end - start) / pd.Timedelta(hours=1)) + 1)\n",
    "        actual = len(group)\n",
    "        missing = max(expected - actual, 0)\n",
    "        missing_ratios[uid] = missing / max(expected, 1)\n",
    "\n",
    "    worst_uid = max(missing_ratios, key=missing_ratios.get)\n",
    "    worst_ratio = missing_ratios[worst_uid]\n",
    "    if worst_ratio > max_missing_ratio:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Too many missing hourly points\",\n",
    "            {\"worst_uid\": worst_uid, \"worst_missing_ratio\": worst_ratio},\n",
    "        )\n",
    "\n",
    "    return ValidationReport(\n",
    "        True,\n",
    "        \"OK\",\n",
    "        {\n",
    "            \"row_count\": len(work),\n",
    "            \"series_count\": int(work[\"unique_id\"].nunique()),\n",
    "            \"max_ds\": max_ds.isoformat(),\n",
    "            \"lag_hours\": lag_hours,\n",
    "            \"worst_missing_ratio\": worst_ratio,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8: Dashboard\n",
    "\n",
    "**File:** `src/renewable/dashboard.py`\n",
    "\n",
    "The Streamlit dashboard provides:\n",
    "- **Forecast visualization** with prediction intervals\n",
    "- **Drift monitoring** and alerts\n",
    "- **Coverage analysis** (nominal vs empirical)\n",
    "- **Weather features** by region\n",
    "\n",
    "## Running the Dashboard\n",
    "\n",
    "```bash\n",
    "streamlit run src/renewable/dashboard.py\n",
    "```\n",
    "\n",
    "The dashboard will:\n",
    "1. Load forecasts from `data/renewable/forecasts.parquet`\n",
    "2. Display interactive charts with Plotly\n",
    "3. Show drift alerts from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dashboard.py\n",
    "# file: src/renewable/dashboard.py\n",
    "\"\"\"Streamlit dashboard for renewable energy forecasting.\n",
    "\n",
    "Provides:\n",
    "- Forecast visualization with prediction intervals\n",
    "- Drift monitoring and alerts\n",
    "- Coverage analysis (nominal vs empirical)\n",
    "- Weather features by region\n",
    "\n",
    "Run with:\n",
    "    streamlit run src/renewable/dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from src.renewable.db import (\n",
    "    connect,\n",
    "    get_drift_alerts,\n",
    "    get_recent_forecasts,\n",
    "    init_renewable_db,\n",
    ")\n",
    "from src.renewable.regions import FUEL_TYPES, REGIONS\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Renewable Forecast Dashboard\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main dashboard application.\"\"\"\n",
    "    st.title(\"⚡ Renewable Energy Forecast Dashboard\")\n",
    "    st.markdown(\"Next-24h wind/solar generation forecasts with drift monitoring\")\n",
    "\n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "\n",
    "        db_path = st.text_input(\n",
    "            \"Database Path\",\n",
    "            value=\"data/renewable/renewable.db\",\n",
    "        )\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not Path(db_path).exists():\n",
    "            init_renewable_db(db_path)\n",
    "            st.info(\"Database initialized\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Region filter\n",
    "        all_regions = list(REGIONS.keys())\n",
    "        selected_regions = st.multiselect(\n",
    "            \"Regions\",\n",
    "            options=all_regions,\n",
    "            default=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        )\n",
    "\n",
    "        # Fuel type filter\n",
    "        fuel_type = st.selectbox(\n",
    "            \"Fuel Type\",\n",
    "            options=[\"WND\", \"SUN\", \"Both\"],\n",
    "            index=0,\n",
    "        )\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Actions\n",
    "        show_debug = st.checkbox(\"Show Debug\", value=False)\n",
    "        if st.button(\"🔄 Refresh Data\", width=\"stretch\"):\n",
    "            st.rerun()\n",
    "\n",
    "        if st.button(\"📊 Run Pipeline\", width=\"stretch\"):\n",
    "            run_pipeline_from_dashboard(db_path, selected_regions, fuel_type)\n",
    "\n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\n",
    "        \"📈 Forecasts\",\n",
    "        \"⚠️ Drift Monitor\",\n",
    "        \"📊 Coverage\",\n",
    "        \"🌤️ Weather\",\n",
    "    ])\n",
    "\n",
    "    with tab1:\n",
    "        render_forecasts_tab(db_path, selected_regions, fuel_type, show_debug=show_debug)\n",
    "\n",
    "    with tab2:\n",
    "        render_drift_tab(db_path)\n",
    "\n",
    "    with tab3:\n",
    "        render_coverage_tab(db_path)\n",
    "\n",
    "    with tab4:\n",
    "        render_weather_tab(db_path, selected_regions)\n",
    "\n",
    "\n",
    "def render_forecasts_tab(db_path: str, regions: list, fuel_type: str, *, show_debug: bool = False):\n",
    "    \"\"\"Render forecast visualization with prediction intervals.\"\"\"\n",
    "    st.subheader(\"Generation Forecasts\")\n",
    "\n",
    "    forecasts_df = pd.DataFrame()\n",
    "    data_source = \"none\"\n",
    "    derived_columns: list[str] = []\n",
    "\n",
    "    # Try to load from parquet file first (pipeline output)\n",
    "    parquet_path = Path(\"data/renewable/forecasts.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            forecasts_df = pd.read_parquet(parquet_path)\n",
    "            data_source = f\"parquet:{parquet_path}\"\n",
    "            # Add region/fuel_type columns if missing\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                parts = forecasts_df[\"unique_id\"].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "                if \"region\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"region\"] = parts[0]\n",
    "                    derived_columns.append(\"region\")\n",
    "                if \"fuel_type\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"fuel_type\"] = parts[1] if parts.shape[1] > 1 else pd.NA\n",
    "                    derived_columns.append(\"fuel_type\")\n",
    "            st.success(f\"Loaded {len(forecasts_df)} forecasts from pipeline\")\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load parquet: {e}\")\n",
    "\n",
    "    # Fall back to database\n",
    "    if forecasts_df.empty:\n",
    "        try:\n",
    "            forecasts_df = get_recent_forecasts(db_path, hours=72)\n",
    "            data_source = f\"db:{db_path}\"\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load from database: {e}\")\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        # Show demo data\n",
    "        st.info(\"No forecasts found. Showing demo data.\")\n",
    "        forecasts_df = generate_demo_forecasts(regions, fuel_type)\n",
    "        data_source = \"demo\"\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Forecast Data\", expanded=False):\n",
    "            st.markdown(\"**Source**\")\n",
    "            st.code(data_source)\n",
    "            st.markdown(\"**Columns**\")\n",
    "            st.code(\", \".join(forecasts_df.columns.tolist()))\n",
    "\n",
    "            st.markdown(\"**Counts (pre-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "\n",
    "            if derived_columns:\n",
    "                st.markdown(\"**Derived Columns**\")\n",
    "                st.write(derived_columns)\n",
    "\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id sample**\")\n",
    "                st.write(forecasts_df[\"unique_id\"].dropna().astype(str).head(10).tolist())\n",
    "\n",
    "            if \"fuel_type\" in forecasts_df.columns:\n",
    "                st.markdown(\"**fuel_type counts**\")\n",
    "                st.dataframe(forecasts_df[\"fuel_type\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "                unknown = sorted(\n",
    "                    {str(v) for v in forecasts_df[\"fuel_type\"].dropna().unique()}\n",
    "                    - set(FUEL_TYPES.keys())\n",
    "                )\n",
    "                if unknown:\n",
    "                    st.warning(f\"Unknown fuel_type values: {unknown}\")\n",
    "\n",
    "            if \"region\" in forecasts_df.columns:\n",
    "                st.markdown(\"**region counts**\")\n",
    "                st.dataframe(forecasts_df[\"region\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "    # Filter by selections\n",
    "    if fuel_type != \"Both\":\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"fuel_type\"] == fuel_type]\n",
    "\n",
    "    if regions:\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"region\"].isin(regions)]\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Filter Result\", expanded=False):\n",
    "            st.markdown(\"**Applied Filters**\")\n",
    "            st.write({\"fuel_type\": fuel_type, \"regions\": regions})\n",
    "            st.markdown(\"**Counts (post-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id after filter**\")\n",
    "                st.write(sorted(forecasts_df[\"unique_id\"].dropna().astype(str).unique().tolist()))\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        st.warning(\"No data matching filters\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    series_options = forecasts_df[\"unique_id\"].unique().tolist()\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=series_options,\n",
    "        index=0 if series_options else None,\n",
    "    )\n",
    "\n",
    "    if selected_series:\n",
    "        series_data = forecasts_df[forecasts_df[\"unique_id\"] == selected_series].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Create forecast plot with intervals\n",
    "        fig = create_forecast_plot(series_data, selected_series)\n",
    "        st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "        # Show data table\n",
    "        with st.expander(\"View Data\"):\n",
    "            st.dataframe(\n",
    "                series_data[[\"ds\", \"yhat\", \"yhat_lo_80\", \"yhat_hi_80\", \"yhat_lo_95\", \"yhat_hi_95\"]],\n",
    "                width=\"stretch\",\n",
    "            )\n",
    "\n",
    "\n",
    "def create_forecast_plot(df: pd.DataFrame, title: str) -> go.Figure:\n",
    "    \"\"\"Create Plotly figure with forecast and prediction intervals.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ensure datetime\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "    # 95% interval (outer, lighter)\n",
    "    if \"yhat_lo_95\" in df.columns and \"yhat_hi_95\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_95\"], df[\"yhat_lo_95\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"95% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # 80% interval (inner, darker)\n",
    "    if \"yhat_lo_80\" in df.columns and \"yhat_hi_80\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_80\"], df[\"yhat_lo_80\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.4)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"80% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # Point forecast\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"ds\"],\n",
    "        y=df[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"#1f77b4\", width=2),\n",
    "    ))\n",
    "\n",
    "    # Actuals if available\n",
    "    if \"y\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Actual\",\n",
    "            marker=dict(color=\"#2ca02c\", size=6),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast: {title}\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Generation (MWh)\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "        height=450,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def render_drift_tab(db_path: str):\n",
    "    \"\"\"Render drift monitoring and alerts.\"\"\"\n",
    "    st.subheader(\"Drift Detection\")\n",
    "\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Try to load alerts\n",
    "    try:\n",
    "        alerts_df = get_drift_alerts(db_path, hours=48)\n",
    "    except Exception:\n",
    "        alerts_df = pd.DataFrame()\n",
    "\n",
    "    # Summary metrics\n",
    "    with col1:\n",
    "        critical = len(alerts_df[alerts_df[\"severity\"] == \"critical\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\n",
    "            \"Critical Alerts\",\n",
    "            critical,\n",
    "            delta=None,\n",
    "            delta_color=\"inverse\" if critical > 0 else \"off\",\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "        warning = len(alerts_df[alerts_df[\"severity\"] == \"warning\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Warnings\", warning)\n",
    "\n",
    "    with col3:\n",
    "        stable = len(alerts_df[alerts_df[\"alert_type\"] == \"drift_check\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Stable Checks\", stable)\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    if alerts_df.empty:\n",
    "        st.info(\"No drift alerts in the last 48 hours. System is stable.\")\n",
    "\n",
    "        # Show demo drift status\n",
    "        st.markdown(\"### Demo Drift Status\")\n",
    "        demo_drift = pd.DataFrame({\n",
    "            \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "            \"Current RMSE\": [125.3, 98.7, 156.2, 45.1, 67.8],\n",
    "            \"Threshold\": [150.0, 120.0, 180.0, 60.0, 80.0],\n",
    "            \"Status\": [\"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\"],\n",
    "        })\n",
    "        st.dataframe(demo_drift, width=\"stretch\")\n",
    "    else:\n",
    "        # Show alerts table\n",
    "        st.dataframe(\n",
    "            alerts_df[[\"alert_at\", \"unique_id\", \"severity\", \"current_rmse\", \"threshold_rmse\", \"message\"]],\n",
    "            width=\"stretch\",\n",
    "        )\n",
    "\n",
    "        # Drift timeline\n",
    "        if len(alerts_df) > 1:\n",
    "            alerts_df[\"alert_at\"] = pd.to_datetime(alerts_df[\"alert_at\"])\n",
    "            fig = px.scatter(\n",
    "                alerts_df,\n",
    "                x=\"alert_at\",\n",
    "                y=\"current_rmse\",\n",
    "                color=\"severity\",\n",
    "                size=\"current_rmse\",\n",
    "                hover_data=[\"unique_id\", \"message\"],\n",
    "                title=\"Drift Timeline\",\n",
    "            )\n",
    "            fig.add_hline(\n",
    "                y=alerts_df[\"threshold_rmse\"].mean(),\n",
    "                line_dash=\"dash\",\n",
    "                annotation_text=\"Avg Threshold\",\n",
    "            )\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_coverage_tab(db_path: str):\n",
    "    \"\"\"Render coverage analysis comparing nominal vs empirical.\"\"\"\n",
    "    st.subheader(\"Prediction Interval Coverage\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    **Coverage** measures how often actual values fall within prediction intervals.\n",
    "    - **Nominal**: The expected coverage (80% or 95%)\n",
    "    - **Empirical**: The actual observed coverage\n",
    "    - **Gap**: Difference indicates calibration quality\n",
    "    \"\"\")\n",
    "\n",
    "    # Demo coverage data\n",
    "    coverage_data = pd.DataFrame({\n",
    "        \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"SWPP_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "        \"Nominal 80%\": [80, 80, 80, 80, 80, 80],\n",
    "        \"Empirical 80%\": [78.5, 82.1, 76.3, 79.8, 81.2, 77.9],\n",
    "        \"Nominal 95%\": [95, 95, 95, 95, 95, 95],\n",
    "        \"Empirical 95%\": [93.2, 96.1, 91.5, 94.8, 95.7, 92.3],\n",
    "    })\n",
    "\n",
    "    coverage_data[\"Gap 80%\"] = coverage_data[\"Empirical 80%\"] - coverage_data[\"Nominal 80%\"]\n",
    "    coverage_data[\"Gap 95%\"] = coverage_data[\"Empirical 95%\"] - coverage_data[\"Nominal 95%\"]\n",
    "\n",
    "    # Summary\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        avg_80 = coverage_data[\"Empirical 80%\"].mean()\n",
    "        st.metric(\"Avg 80% Coverage\", f\"{avg_80:.1f}%\", f\"{avg_80 - 80:.1f}%\")\n",
    "\n",
    "    with col2:\n",
    "        avg_95 = coverage_data[\"Empirical 95%\"].mean()\n",
    "        st.metric(\"Avg 95% Coverage\", f\"{avg_95:.1f}%\", f\"{avg_95 - 95:.1f}%\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Coverage comparison chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"80% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 80%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.7)\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"95% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 95%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.4)\",\n",
    "    ))\n",
    "\n",
    "    # Nominal lines\n",
    "    fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"80% Nominal\")\n",
    "    fig.add_hline(y=95, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"95% Nominal\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coverage by Series\",\n",
    "        xaxis_title=\"Series\",\n",
    "        yaxis_title=\"Coverage (%)\",\n",
    "        barmode=\"group\",\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Detailed table\n",
    "    with st.expander(\"View Coverage Data\"):\n",
    "        st.dataframe(coverage_data, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_weather_tab(db_path: str, regions: list):\n",
    "    \"\"\"Render weather features visualization.\"\"\"\n",
    "    st.subheader(\"Weather Features\")\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "    # Prefer real pipeline output; no demo fallback.\n",
    "    parquet_path = Path(\"data/renewable/weather.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            weather_df = pd.read_parquet(parquet_path)\n",
    "            st.success(f\"Loaded {len(weather_df)} weather rows from pipeline\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather parquet: {exc}\")\n",
    "\n",
    "    if weather_df.empty and Path(db_path).exists():\n",
    "        try:\n",
    "            with connect(db_path) as con:\n",
    "                weather_df = pd.read_sql_query(\n",
    "                    \"SELECT * FROM weather_features ORDER BY ds ASC\",\n",
    "                    con,\n",
    "                )\n",
    "            if not weather_df.empty:\n",
    "                st.success(f\"Loaded {len(weather_df)} weather rows from database\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather data from database: {exc}\")\n",
    "\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data available. Run the pipeline to populate weather features.\")\n",
    "        return\n",
    "\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"coerce\")\n",
    "    if regions:\n",
    "        weather_df = weather_df[weather_df[\"region\"].isin(regions)]\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data matching selected regions.\")\n",
    "        return\n",
    "\n",
    "    # Variable selector\n",
    "    weather_vars = [\n",
    "        col for col in [\"wind_speed_10m\", \"wind_speed_100m\", \"direct_radiation\", \"cloud_cover\"]\n",
    "        if col in weather_df.columns\n",
    "    ]\n",
    "    if not weather_vars:\n",
    "        st.warning(\"Weather data missing expected variables.\")\n",
    "        return\n",
    "    selected_var = st.selectbox(\"Weather Variable\", options=weather_vars)\n",
    "\n",
    "    # Plot\n",
    "    fig = px.line(\n",
    "        weather_df,\n",
    "        x=\"ds\",\n",
    "        y=selected_var,\n",
    "        color=\"region\",\n",
    "        title=f\"{selected_var} by Region\",\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Summary stats\n",
    "    st.markdown(\"### Current Conditions\")\n",
    "\n",
    "    cols = st.columns(len(regions[:4]))\n",
    "    for i, region in enumerate(regions[:4]):\n",
    "        if i < len(cols):\n",
    "            with cols[i]:\n",
    "                region_data = weather_df[weather_df[\"region\"] == region].iloc[-1] if len(weather_df[weather_df[\"region\"] == region]) > 0 else {}\n",
    "                st.metric(\n",
    "                    region,\n",
    "                    f\"{region_data.get('wind_speed_10m', 0):.1f} m/s\",\n",
    "                    help=\"Wind speed at 10m\",\n",
    "                )\n",
    "\n",
    "\n",
    "def generate_demo_forecasts(regions: list, fuel_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate demo forecast data for display.\"\"\"\n",
    "    data = []\n",
    "    base_time = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    fuel_types = [fuel_type] if fuel_type != \"Both\" else [\"WND\", \"SUN\"]\n",
    "\n",
    "    for region in regions[:3]:\n",
    "        for ft in fuel_types:\n",
    "            unique_id = f\"{region}_{ft}\"\n",
    "            base_value = 500 if ft == \"WND\" else 300\n",
    "\n",
    "            for h in range(24):\n",
    "                ds = base_time + timedelta(hours=h)\n",
    "\n",
    "                # Add daily pattern\n",
    "                if ft == \"SUN\":\n",
    "                    hour_factor = max(0, np.sin((ds.hour - 6) * np.pi / 12)) if 6 < ds.hour < 18 else 0\n",
    "                    yhat = base_value * hour_factor + np.random.normal(0, 20)\n",
    "                else:\n",
    "                    yhat = base_value + np.sin(ds.hour * np.pi / 12) * 100 + np.random.normal(0, 30)\n",
    "\n",
    "                yhat = max(0, yhat)\n",
    "\n",
    "                data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"region\": region,\n",
    "                    \"fuel_type\": ft,\n",
    "                    \"ds\": ds,\n",
    "                    \"yhat\": yhat,\n",
    "                    \"yhat_lo_80\": yhat * 0.85,\n",
    "                    \"yhat_hi_80\": yhat * 1.15,\n",
    "                    \"yhat_lo_95\": yhat * 0.75,\n",
    "                    \"yhat_hi_95\": yhat * 1.25,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def run_pipeline_from_dashboard(db_path: str, regions: list, fuel_type: str):\n",
    "    \"\"\"Run the forecasting pipeline from the dashboard.\"\"\"\n",
    "    st.info(\"Running pipeline... (This would trigger the actual pipeline)\")\n",
    "\n",
    "    # In production, this would call:\n",
    "    # from src.renewable.tasks import run_full_pipeline, RenewablePipelineConfig\n",
    "    # config = RenewablePipelineConfig(regions=regions, fuel_types=[fuel_type])\n",
    "    # results = run_full_pipeline(config)\n",
    "\n",
    "    st.success(\"Pipeline completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Built\n",
    "\n",
    "| Module | Purpose | Key Concept |\n",
    "|--------|---------|-------------|\n",
    "| `regions.py` | Region definitions | EIA codes + coordinates |\n",
    "| `eia_renewable.py` | Data fetching | StatsForecast format |\n",
    "| `open_meteo.py` | Weather integration | Leakage prevention |\n",
    "| `modeling.py` | Forecasting | Probabilistic intervals |\n",
    "| `db.py` | Persistence | SQLite with WAL |\n",
    "| `tasks.py` | Pipeline orchestration | Adaptive CV |\n",
    "| `dashboard.py` | Visualization | Streamlit + Plotly |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` enables multi-series modeling\n",
    "2. **No MAPE for renewables**: Solar has zeros - use RMSE/MAE instead\n",
    "3. **Weather leakage**: Use forecast weather for predictions, not historical\n",
    "4. **Drift detection**: threshold = baseline_mean + 2 × baseline_std\n",
    "5. **Adaptive CV**: Adjust window count for short time series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
