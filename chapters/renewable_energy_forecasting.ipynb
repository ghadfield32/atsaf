{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos:\n",
    "- update preprocessing data so it's organized and in the data pull rather than in the preprocessing\n",
    "- add in eda module to explore data and show decisions made based on that\n",
    "- go over the full explanation of why we would choose this data, why we would choose this model, \n",
    "    - ensure to include decision making and thought process not just end results, \n",
    "    - archive the notebooks, \n",
    "    - update the readme, \n",
    "    - ensure this is software that is automated, \n",
    "    - add in mermaid graph to readme and linkedin post\n",
    "    - add in start up instructions to readme\n",
    "    - finally post on linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable Energy Forecasting Pipeline\n",
    "\n",
    "This notebook walks through building a **next-24h renewable generation forecast system** with:\n",
    "\n",
    "- **EIA data integration** - Hourly wind/solar generation for US regions\n",
    "- **Weather features** - Open-Meteo integration (wind speed, solar radiation)\n",
    "- **Probabilistic forecasting** - Dual prediction intervals (80%, 95%)\n",
    "- **Drift monitoring** - Automatic detection of model degradation\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "```\n",
    "┌─────────────┐      ┌──────────────┐      ┌─────────────┐\n",
    "│  EIA API    │──┬──▶│ Data         │──┬──▶│ StatsForecast│\n",
    "│ (WND/SUN)   │  │   │ Pipeline     │  │   │ Models       │\n",
    "└─────────────┘  │   └──────────────┘  │   └─────────────┘\n",
    "                 │                     │           │\n",
    "┌─────────────┐  │   ┌──────────────┐  │   ┌─────▼──────┐\n",
    "│ Open-Meteo  │──┘   │ Validation   │  │   │Probabilistic│\n",
    "│ Weather API │      │ & Quality    │  │   │Forecasts    │\n",
    "└─────────────┘      │ Gates        │  │   │(80%, 95% CI)│\n",
    "                     └──────────────┘  │   └────────────┘\n",
    "                                       │           │\n",
    "                                       │   ┌───────▼─────┐\n",
    "                                       └──▶│  Artifacts  │\n",
    "                                           │  Commit &   │\n",
    "                                           │  Dashboard  │\n",
    "                                           └─────────────┘\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` - where `unique_id` = `{region}_{fuel_type}`\n",
    "2. **Zero-value handling**: Solar generates 0 at night - we use RMSE/MAE, NOT MAPE\n",
    "3. **Leakage prevention**: Use **forecasted** weather for predictions, not historical\n",
    "4. **Drift detection**: Threshold = mean + 2*std from backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure we have the project root in our path and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to project root: c:\\docker_projects\\atsaf we are currently at c:\\docker_projects\\atsaf\n",
      "Project root: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\docker_projects\\atsaf\"\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "if os.getcwd() != str(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to project root: {project_root} we are currently at {os.getcwd()}\")\n",
    "\n",
    "# Configure logging for visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 1: Region Definitions\n",
    "\n",
    "**File:** `src/renewable/regions.py`\n",
    "\n",
    "This module maps **EIA balancing authority regions** to their geographic coordinates. Why do we need coordinates?\n",
    "\n",
    "- **Weather API lookup**: Open-Meteo requires latitude/longitude\n",
    "- **Regional analysis**: Compare forecast accuracy across regions\n",
    "- **Timezone handling**: Each region has a primary timezone\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "1. **NamedTuple for RegionInfo**: Immutable, type-safe, and memory-efficient\n",
    "2. **Centroid coordinates**: Approximate centers - good enough for hourly weather\n",
    "3. **Fuel type codes**: `WND` (wind), `SUN` (solar) - match EIA's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/regions.py\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/regions.py\n",
    "# src/renewable/regions.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class RegionInfo(NamedTuple):\n",
    "    \"\"\"Region metadata for EIA and weather lookups.\"\"\"\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    timezone: str\n",
    "    # Some internal regions may not map cleanly to an EIA respondent.\n",
    "    # We keep them in REGIONS for weather/features, but EIA fetch requires this.\n",
    "    eia_respondent: Optional[str] = None\n",
    "\n",
    "\n",
    "REGIONS: dict[str, RegionInfo] = {\n",
    "    # Western Interconnection\n",
    "    \"CALI\": RegionInfo(\n",
    "        name=\"California ISO\",\n",
    "        lat=36.7,\n",
    "        lon=-119.4,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=\"CISO\",\n",
    "    ),\n",
    "    \"NW\": RegionInfo(\n",
    "        name=\"Northwest\",\n",
    "        lat=45.5,\n",
    "        lon=-122.0,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "    \"SW\": RegionInfo(\n",
    "        name=\"Southwest\",\n",
    "        lat=33.5,\n",
    "        lon=-112.0,\n",
    "        timezone=\"America/Phoenix\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "\n",
    "    # Texas Interconnection\n",
    "    \"ERCO\": RegionInfo(\n",
    "        name=\"ERCOT (Texas)\",\n",
    "        lat=31.0,\n",
    "        lon=-100.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"ERCO\",\n",
    "    ),\n",
    "\n",
    "    # Midwest\n",
    "    \"MISO\": RegionInfo(\n",
    "        name=\"Midcontinent ISO\",\n",
    "        lat=41.0,\n",
    "        lon=-93.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"MISO\",\n",
    "    ),\n",
    "    \"PJM\": RegionInfo(\n",
    "        name=\"PJM Interconnection\",\n",
    "        lat=39.0,\n",
    "        lon=-77.0,\n",
    "        timezone=\"America/New_York\",\n",
    "        eia_respondent=\"PJM\",\n",
    "    ),\n",
    "    \"SWPP\": RegionInfo(\n",
    "        name=\"Southwest Power Pool\",\n",
    "        lat=37.0,\n",
    "        lon=-97.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"SWPP\",\n",
    "    ),\n",
    "\n",
    "    # Internal/aggregate regions kept for non-EIA use (weather/features/etc.)\n",
    "    \"SE\": RegionInfo(name=\"Southeast\", lat=33.0, lon=-84.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"FLA\": RegionInfo(name=\"Florida\", lat=28.0, lon=-82.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"CAR\": RegionInfo(name=\"Carolinas\", lat=35.5, lon=-80.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"TEN\": RegionInfo(name=\"Tennessee Valley\", lat=35.5, lon=-86.0, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "\n",
    "    \"US48\": RegionInfo(name=\"Lower 48 States\", lat=39.8, lon=-98.5, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "}\n",
    "\n",
    "FUEL_TYPES = {\"WND\": \"Wind\", \"SUN\": \"Solar\"}\n",
    "\n",
    "\n",
    "def list_regions() -> list[str]:\n",
    "    return sorted(REGIONS.keys())\n",
    "\n",
    "\n",
    "def get_region_info(region_code: str) -> RegionInfo:\n",
    "    return REGIONS[region_code]\n",
    "\n",
    "\n",
    "def get_region_coords(region_code: str) -> tuple[float, float]:\n",
    "    r = REGIONS[region_code]\n",
    "    return (r.lat, r.lon)\n",
    "\n",
    "\n",
    "def get_eia_respondent(region_code: str) -> str:\n",
    "    \"\"\"Return the code EIA expects for facets[respondent][]. Fail loudly if missing.\"\"\"\n",
    "    info = REGIONS[region_code]\n",
    "    if not info.eia_respondent:\n",
    "        raise ValueError(\n",
    "            f\"Region '{region_code}' has no configured eia_respondent. \"\n",
    "            f\"Set REGIONS['{region_code}'].eia_respondent to a verified EIA respondent code \"\n",
    "            f\"before using it for EIA fetches.\"\n",
    "        )\n",
    "    return info.eia_respondent\n",
    "\n",
    "\n",
    "def validate_region(region_code: str) -> bool:\n",
    "    return region_code in REGIONS\n",
    "\n",
    "\n",
    "def validate_fuel_type(fuel_type: str) -> bool:\n",
    "    return fuel_type in FUEL_TYPES\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run - test region functions\n",
    "\n",
    "    print(\"=== Available Regions ===\")\n",
    "    print(f\"Total regions: {len(REGIONS)}\")\n",
    "    print(f\"Region codes: {list_regions()}\")\n",
    "\n",
    "    print(\"\\n=== Example: California ===\")\n",
    "    cali_info = get_region_info(\"CALI\")\n",
    "    print(f\"Name: {cali_info.name}\")\n",
    "    print(f\"Coordinates: ({cali_info.lat}, {cali_info.lon})\")\n",
    "    print(f\"Timezone: {cali_info.timezone}\")\n",
    "\n",
    "    print(\"\\n=== Weather API Coordinates ===\")\n",
    "    for region in [\"CALI\", \"ERCO\", \"MISO\"]:\n",
    "        lat, lon = get_region_coords(region)\n",
    "        print(f\"{region}: lat={lat}, lon={lon}\")\n",
    "\n",
    "    print(\"\\n=== Fuel Types ===\")\n",
    "    for code, name in FUEL_TYPES.items():\n",
    "        print(f\"{code}: {name}\")\n",
    "\n",
    "    print(\"\\n=== Validation ===\")\n",
    "    print(f\"validate_region('CALI'): {validate_region('CALI')}\")\n",
    "    print(f\"validate_region('INVALID'): {validate_region('INVALID')}\")\n",
    "    print(f\"validate_fuel_type('WND'): {validate_fuel_type('WND')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Region Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2: EIA Data Fetcher\n",
    "\n",
    "**File:** `src/renewable/eia_renewable.py`\n",
    "\n",
    "This module fetches **hourly wind and solar generation** from the EIA API.\n",
    "\n",
    "## Critical Concepts\n",
    "\n",
    "### StatsForecast Format\n",
    "StatsForecast expects data in a specific format:\n",
    "```\n",
    "unique_id | ds                  | y\n",
    "----------|---------------------|--------\n",
    "CALI_WND  | 2024-01-01 00:00:00 | 1234.5\n",
    "CALI_WND  | 2024-01-01 01:00:00 | 1456.7\n",
    "ERCO_WND  | 2024-01-01 00:00:00 | 2345.6\n",
    "```\n",
    "\n",
    "- `unique_id`: Identifies the time series (e.g., \"CALI_WND\" = California Wind)\n",
    "- `ds`: Datetime column (timezone-naive UTC)\n",
    "- `y`: Target value (generation in MWh)\n",
    "\n",
    "### API Rate Limiting\n",
    "- EIA API has rate limits (~5 requests/second)\n",
    "- We use controlled parallelism with delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_dotenv, load_dotenv\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrenewable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (REGIONS, get_eia_respondent,\n\u001b[32m     20\u001b[39m                                    validate_fuel_type, validate_region)\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sanitize_url\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/eia_renewable.py\n",
    "# src/renewable/eia_renewable.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.parse import parse_qsl, urlencode, urlsplit, urlunsplit\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from src.renewable.regions import (REGIONS, get_eia_respondent,\n",
    "                                   validate_fuel_type, validate_region)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _sanitize_url(url: str) -> str:\n",
    "    parts = urlsplit(url)\n",
    "    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k.lower() != \"api_key\"]\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, urlencode(q), parts.fragment))\n",
    "\n",
    "\n",
    "def _load_env_once(*, debug: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load .env if present.\n",
    "    - Primary: find_dotenv(usecwd=True) (walk up from CWD)\n",
    "    - Fallback: repo_root/.env based on this file location\n",
    "    Returns the path loaded (or None).\n",
    "    \"\"\"\n",
    "    # 1) Try from current working directory upward\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "        if debug:\n",
    "            logger.info(\"Loaded .env via find_dotenv: %s\", dotenv_path)\n",
    "        return dotenv_path\n",
    "\n",
    "    # 2) Fallback: assume src-layout -> repo root is ../../ from this file\n",
    "    try:\n",
    "        repo_root = Path(__file__).resolve().parents[2]\n",
    "        fallback = repo_root / \".env\"\n",
    "        if fallback.exists():\n",
    "            load_dotenv(fallback, override=False)\n",
    "            if debug:\n",
    "                logger.info(\"Loaded .env via fallback: %s\", str(fallback))\n",
    "            return str(fallback)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"No .env found to load.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "class EIARenewableFetcher:\n",
    "    BASE_URL = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "    MAX_RECORDS_PER_REQUEST = 5000\n",
    "    RATE_LIMIT_DELAY = 0.2  # 5 requests/second max\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, *, timeout: int = 60, debug_env: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize API key and configuration.\n",
    "\n",
    "        Args:\n",
    "            api_key: EIA API key (or reads from EIA_API_KEY env var)\n",
    "            timeout: Request timeout in seconds (default: 60)\n",
    "            debug_env: Enable debug logging for environment loading\n",
    "        \"\"\"\n",
    "        loaded_env = _load_env_once(debug=debug_env)\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"EIA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"EIA API key required but not found.\\n\"\n",
    "                \"- Ensure .env contains EIA_API_KEY=...\\n\"\n",
    "                \"- Ensure your process CWD is under the repo (so find_dotenv can locate it), OR\\n\"\n",
    "                \"- Pass api_key=... explicitly.\\n\"\n",
    "                f\"Loaded .env path: {loaded_env}\"\n",
    "            )\n",
    "\n",
    "        self.timeout = timeout\n",
    "        self.session = self._create_session()  # Add retry-enabled session\n",
    "\n",
    "        # Debug without leaking the key\n",
    "        if debug_env:\n",
    "            masked = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            logger.info(\"EIA_API_KEY loaded (masked): %s\", masked)\n",
    "            logger.info(\"Request timeout: %d seconds\", self.timeout)\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create requests Session with retry logic for transient errors.\"\"\"\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # Retry on server errors and rate limits\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_eia_response(payload: dict, *, request_url: Optional[str] = None) -> tuple[list[dict], dict]:\n",
    "        if not isinstance(payload, dict):\n",
    "            raise TypeError(f\"EIA payload is not a dict. type={type(payload)} url={request_url}\")\n",
    "\n",
    "        if \"error\" in payload and payload.get(\"response\") is None:\n",
    "            raise ValueError(f\"EIA returned error payload. url={request_url} error={payload.get('error')}\")\n",
    "\n",
    "        if \"response\" not in payload:\n",
    "            raise ValueError(\n",
    "                f\"EIA payload missing 'response'. url={request_url} keys={list(payload.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        response = payload.get(\"response\") or {}\n",
    "        if not isinstance(response, dict):\n",
    "            raise TypeError(f\"EIA payload['response'] is not a dict. type={type(response)} url={request_url}\")\n",
    "\n",
    "        if \"data\" not in response:\n",
    "            raise ValueError(\n",
    "                f\"EIA response missing 'data'. url={request_url} response_keys={list(response.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        records = response.get(\"data\") or []\n",
    "        if not isinstance(records, list):\n",
    "            raise TypeError(f\"EIA response['data'] is not a list. type={type(records)} url={request_url}\")\n",
    "\n",
    "        total = response.get(\"total\", None)\n",
    "        offset = response.get(\"offset\", None)\n",
    "\n",
    "        meta_obj = response.get(\"metadata\") or {}\n",
    "        if isinstance(meta_obj, dict):\n",
    "            if total is None and \"total\" in meta_obj:\n",
    "                total = meta_obj.get(\"total\")\n",
    "            if offset is None and \"offset\" in meta_obj:\n",
    "                offset = meta_obj.get(\"offset\")\n",
    "\n",
    "        try:\n",
    "            total = int(total) if total is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            offset = int(offset) if offset is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return records, {\"total\": total, \"offset\": offset}\n",
    "\n",
    "    def fetch_region(\n",
    "        self,\n",
    "        region: str,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "        diag: Optional[dict] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region):\n",
    "            raise ValueError(f\"Invalid region: {region}\")\n",
    "        if not validate_fuel_type(fuel_type):\n",
    "            raise ValueError(f\"Invalid fuel type: {fuel_type}\")\n",
    "\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        all_records: list[dict] = []\n",
    "        offset = 0\n",
    "\n",
    "        # ✅ FIX: initialize loop diagnostics counters\n",
    "        page_count = 0\n",
    "        total_hint: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"api_key\": self.api_key,\n",
    "                \"data[]\": \"value\",\n",
    "                \"facets[respondent][]\": respondent,\n",
    "                \"facets[fueltype][]\": fuel_type,\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"start\": f\"{start_date}T00\",\n",
    "                \"end\": f\"{end_date}T23\",\n",
    "                \"length\": self.MAX_RECORDS_PER_REQUEST,\n",
    "                \"offset\": offset,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"asc\",\n",
    "            }\n",
    "\n",
    "            resp = self.session.get(self.BASE_URL, params=params, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "\n",
    "            records, meta = self._extract_eia_response(payload, request_url=resp.url)\n",
    "\n",
    "            page_count += 1\n",
    "            if total_hint is None:\n",
    "                total_hint = meta.get(\"total\")\n",
    "\n",
    "            returned = len(records)\n",
    "\n",
    "            if debug:\n",
    "                safe_url = _sanitize_url(resp.url)\n",
    "                print(\n",
    "                    f\"[PAGE] region={region} fuel={fuel_type} returned={returned} \"\n",
    "                    f\"offset={offset} total={meta.get('total')} url={safe_url}\"\n",
    "                )\n",
    "\n",
    "            # Empty on first page: legitimate empty series for that window\n",
    "            if returned == 0 and offset == 0:\n",
    "                if diag is not None:\n",
    "                    diag.update({\n",
    "                        \"region\": region,\n",
    "                        \"fuel_type\": fuel_type,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date,\n",
    "                        \"total_records\": total_hint,\n",
    "                        \"pages\": page_count,\n",
    "                        \"rows_parsed\": 0,\n",
    "                        \"empty\": True,\n",
    "                    })\n",
    "                return pd.DataFrame(columns=[\"ds\", \"value\", \"region\", \"fuel_type\"])\n",
    "\n",
    "            if returned == 0:\n",
    "                break\n",
    "\n",
    "            all_records.extend(records)\n",
    "\n",
    "            if returned < self.MAX_RECORDS_PER_REQUEST:\n",
    "                break\n",
    "\n",
    "            offset += self.MAX_RECORDS_PER_REQUEST\n",
    "            time.sleep(self.RATE_LIMIT_DELAY)\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        missing_cols = [c for c in [\"period\", \"value\"] if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            sample_keys = sorted(set().union(*(r.keys() for r in all_records[:5]))) if all_records else []\n",
    "            raise ValueError(\n",
    "                f\"EIA records missing expected keys {missing_cols}. \"\n",
    "                f\"columns={df.columns.tolist()} sample_record_keys={sample_keys}\"\n",
    "            )\n",
    "\n",
    "        # EIA returns timestamps in UTC format WITHOUT timezone marker (e.g., \"2026-01-21T00\")\n",
    "        # Simply parse and treat as UTC (no conversion needed)\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"period\"], utc=True, errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"region\"] = region\n",
    "        df[\"fuel_type\"] = fuel_type\n",
    "\n",
    "        df = df.dropna(subset=[\"ds\", \"value\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        # Log negative values for investigation (but don't clamp - let dataset builder handle)\n",
    "        neg_mask = df[\"value\"] < 0\n",
    "        if neg_mask.any():\n",
    "            neg_count = int(neg_mask.sum())\n",
    "            neg_min = float(df.loc[neg_mask, \"value\"].min())\n",
    "            neg_max = float(df.loc[neg_mask, \"value\"].max())\n",
    "            neg_pct = 100 * neg_count / max(len(df), 1)\n",
    "            logger.warning(\n",
    "                \"[fetch_region][NEGATIVE] region=%s fuel=%s count=%d (%.1f%%) range=[%.2f, %.2f]\",\n",
    "                region, fuel_type, neg_count, neg_pct, neg_min, neg_max,\n",
    "            )\n",
    "            # Log sample for debugging\n",
    "            neg_sample = df.loc[neg_mask, [\"ds\", \"value\"]].head(5)\n",
    "            for _, row in neg_sample.iterrows():\n",
    "                logger.debug(\"  ds=%s value=%.2f\", row[\"ds\"], row[\"value\"])\n",
    "\n",
    "            # NOTE: Keeping negative values in raw data for transparency\n",
    "            # Dataset builder will handle negatives according to configured policy\n",
    "\n",
    "        if diag is not None:\n",
    "            diag.update({\n",
    "                \"region\": region,\n",
    "                \"fuel_type\": fuel_type,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"total_records\": total_hint,\n",
    "                \"pages\": page_count,\n",
    "                \"rows_parsed\": int(len(df)),\n",
    "                \"empty\": bool(len(df) == 0),\n",
    "            })\n",
    "\n",
    "        return df[[\"ds\", \"value\", \"region\", \"fuel_type\"]]\n",
    "\n",
    "    def fetch_all_regions(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        regions: Optional[list[str]] = None,\n",
    "        max_workers: int = 3,\n",
    "        diagnostics: Optional[list[dict]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Fetch generation data for all regions for a given fuel type.\n",
    "\n",
    "        Args:\n",
    "            fuel_type: Fuel type code (WND, SUN, etc.)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            regions: List of region codes (defaults to all non-US48 regions)\n",
    "            max_workers: Number of parallel workers\n",
    "            diagnostics: Optional list to collect diagnostic info\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns [unique_id, ds, y]\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If no regions could be fetched (complete failure)\n",
    "        \"\"\"\n",
    "        if regions is None:\n",
    "            regions = [r for r in REGIONS.keys() if r != \"US48\"]\n",
    "\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        failed_regions: list[tuple[str, str]] = []  # (region, error_msg)\n",
    "\n",
    "        def _run_one(region: str) -> tuple[str, pd.DataFrame, dict]:\n",
    "            d: dict = {}\n",
    "            df = self.fetch_region(region, fuel_type, start_date, end_date, diag=d)\n",
    "            return region, df, d\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(_run_one, region): region for region in regions}\n",
    "            for future in as_completed(futures):\n",
    "                region = futures[future]\n",
    "                try:\n",
    "                    _, df, d = future.result()\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append(d)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                        print(f\"[OK] {region}: {len(df)} rows\")\n",
    "                    else:\n",
    "                        print(f\"[EMPTY] {region}: 0 rows\")\n",
    "                        failed_regions.append((region, \"Empty response (0 rows)\"))\n",
    "                except Exception as e:\n",
    "                    failed_regions.append((region, str(e)))\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append({\n",
    "                            \"region\": region,\n",
    "                            \"fuel_type\": fuel_type,\n",
    "                            \"start_date\": start_date,\n",
    "                            \"end_date\": end_date,\n",
    "                            \"error\": str(e),\n",
    "                        })\n",
    "                    print(f\"[FAIL] {region}: {e}\")\n",
    "\n",
    "        # Explicit validation: require at least one successful region\n",
    "        if not all_dfs:\n",
    "            error_details = \"; \".join([f\"{r[0]}({r[1][:80]})\" for r in failed_regions])\n",
    "            raise RuntimeError(\n",
    "                f\"[EIA][FETCH] Failed to fetch {fuel_type} data for ALL regions. \"\n",
    "                f\"Failures: {error_details}. \"\n",
    "                f\"Check EIA API availability, API key validity, network connectivity, \"\n",
    "                f\"and consider increasing timeout or reducing concurrency.\"\n",
    "            )\n",
    "\n",
    "        # Warn if partial failure (some regions succeeded, some failed)\n",
    "        if failed_regions:\n",
    "            failed_count = len(failed_regions)\n",
    "            total_count = len(regions)\n",
    "            print(f\"[WARNING] Partial {fuel_type} fetch: {failed_count}/{total_count} regions failed\")\n",
    "            for region, error_msg in failed_regions:\n",
    "                # Print first 100 chars of error\n",
    "                print(f\"  - {region}: {error_msg[:100]}\")\n",
    "\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined[\"unique_id\"] = combined[\"region\"] + \"_\" + combined[\"fuel_type\"]\n",
    "        combined = combined.rename(columns={\"value\": \"y\"})\n",
    "\n",
    "        result = combined[[\"unique_id\", \"ds\", \"y\"]].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        print(f\"[SUMMARY] {fuel_type} data: {result['unique_id'].nunique()} series, {len(result)} total rows\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_series_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.groupby(\"unique_id\").agg(\n",
    "            count=(\"y\", \"count\"),\n",
    "            min_value=(\"y\", \"min\"),\n",
    "            max_value=(\"y\", \"max\"),\n",
    "            mean_value=(\"y\", \"mean\"),\n",
    "            zero_count=(\"y\", lambda x: (x == 0).sum()),\n",
    "        ).reset_index()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 3: Weather Integration\n",
    "\n",
    "**File:** `src/renewable/open_meteo.py`\n",
    "\n",
    "Weather is **critical** for renewable forecasting:\n",
    "- **Wind generation** depends on wind speed (especially at hub height ~100m)\n",
    "- **Solar generation** depends on radiation and cloud cover\n",
    "\n",
    "## Key Concept: Preventing Leakage\n",
    "\n",
    "**Data leakage** occurs when training uses information that wouldn't be available at prediction time.\n",
    "\n",
    "```\n",
    "❌ WRONG: Using historical weather to predict future generation\n",
    "   - At prediction time, we don't have future actual weather!\n",
    "   \n",
    "✅ CORRECT: Use forecasted weather for predictions\n",
    "   - Training: historical weather aligned with historical generation\n",
    "   - Prediction: weather forecast for the prediction horizon\n",
    "```\n",
    "\n",
    "## Open-Meteo API\n",
    "\n",
    "Open-Meteo is **free** and requires no API key:\n",
    "- Historical API: Past weather data\n",
    "- Forecast API: Up to 16 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPAdapter\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrenewable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_region_coords, validate_region\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m(frozen=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOpenMeteoEndpoints\u001b[39;00m:\n\u001b[32m     19\u001b[39m     historical_url: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mhttps://archive-api.open-meteo.com/v1/archive\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/open_meteo.py\n",
    "# src/renewable/open_meteo.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from src.renewable.regions import get_region_coords, validate_region\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OpenMeteoEndpoints:\n",
    "    historical_url: str = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    forecast_url: str = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "\n",
    "class OpenMeteoRenewable:\n",
    "    \"\"\"\n",
    "    Fetch weather features for renewable energy forecasting.\n",
    "\n",
    "    Strict-by-default:\n",
    "    - If Open-Meteo doesn't return a requested variable, we raise.\n",
    "    - We do NOT fabricate values or silently \"fill\" missing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_VARS = [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"direct_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, timeout: int = 60, *, strict: bool = True):\n",
    "        self.timeout = timeout\n",
    "        self.strict = strict\n",
    "        self.endpoints = OpenMeteoEndpoints()\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def fetch_historical(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.historical_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][HIST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            # Log actual response content for debugging\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][HIST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        return self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "    def fetch_forecast(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        forecast_days = min((horizon_hours // 24) + 1, 16)\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"forecast_days\": forecast_days,\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.forecast_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][FCST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][FCST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        df = self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "        # Trim to requested horizon (ds is naive UTC)\n",
    "        if len(df) > 0:\n",
    "            cutoff = datetime.utcnow() + timedelta(hours=horizon_hours)\n",
    "            df = df[df[\"ds\"] <= cutoff].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_historical(lat, lon, start_date, end_date, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "    def fetch_all_regions_historical(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region(region, start_date, end_date, debug=debug)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def _parse_response(\n",
    "        self,\n",
    "        data: dict,\n",
    "        variables: list[str],\n",
    "        *,\n",
    "        debug: bool,\n",
    "        request_url: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        hourly = data.get(\"hourly\")\n",
    "        if not isinstance(hourly, dict):\n",
    "            raise ValueError(f\"Open-Meteo response missing/invalid 'hourly'. url={request_url}\")\n",
    "\n",
    "        times = hourly.get(\"time\")\n",
    "        if not isinstance(times, list) or len(times) == 0:\n",
    "            raise ValueError(f\"Open-Meteo response has no hourly time grid. url={request_url}\")\n",
    "\n",
    "        # Build ds (naive UTC)\n",
    "        ds = pd.to_datetime(times, errors=\"coerce\", utc=True).tz_localize(None)\n",
    "        if ds.isna().any():\n",
    "            bad = int(ds.isna().sum())\n",
    "            raise ValueError(f\"Open-Meteo returned unparsable times. bad={bad} url={request_url}\")\n",
    "\n",
    "        df_data = {\"ds\": ds}\n",
    "\n",
    "        # Strict variable presence: raise if missing (no silent None padding)\n",
    "        missing_vars = [v for v in variables if v not in hourly]\n",
    "        if missing_vars and self.strict:\n",
    "            raise ValueError(f\"Open-Meteo missing requested vars={missing_vars}. url={request_url}\")\n",
    "\n",
    "        for var in variables:\n",
    "            values = hourly.get(var)\n",
    "            if values is None:\n",
    "                # If not strict, keep as all-NA but be explicit (not hidden)\n",
    "                df_data[var] = [None] * len(ds)\n",
    "                continue\n",
    "\n",
    "            if not isinstance(values, list):\n",
    "                raise ValueError(f\"Open-Meteo var '{var}' not a list. type={type(values)} url={request_url}\")\n",
    "\n",
    "            if len(values) != len(ds):\n",
    "                raise ValueError(\n",
    "                    f\"Open-Meteo length mismatch for '{var}': \"\n",
    "                    f\"len(values)={len(values)} len(time)={len(ds)} url={request_url}\"\n",
    "                )\n",
    "\n",
    "            df_data[var] = pd.to_numeric(values, errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame(df_data).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if debug:\n",
    "            dup = int(df[\"ds\"].duplicated().sum())\n",
    "            na_counts = {v: int(df[v].isna().sum()) for v in variables if v in df.columns}\n",
    "            print(f\"[OPENMETEO][PARSE] rows={len(df)} dup_ds={dup} na_counts(sample)={dict(list(na_counts.items())[:3])}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region_forecast(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_forecast(lat, lon, horizon_hours=horizon_hours, variables=variables, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fetch_all_regions_forecast(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region_forecast(\n",
    "                    region, horizon_hours=horizon_hours, variables=variables, debug=debug\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Forecast weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/eda.py\n",
    "# file: src/renewable/eda.py\n",
    "\"\"\"\n",
    "Enhanced Exploratory Data Analysis for Renewable Energy Forecasting\n",
    "\n",
    "This module provides decision-driven EDA with emphasis on:\n",
    "1. Understanding WHY negative values exist (not just detecting them)\n",
    "2. Providing actionable recommendations based on findings\n",
    "3. Validating physical constraints for renewable energy data\n",
    "\n",
    "Key Principle: Renewable energy generation CANNOT be negative.\n",
    "- Solar panels produce 0-X power, never negative\n",
    "- Wind turbines produce 0-X power, never negative\n",
    "- Negative values in data are ALWAYS data quality issues that need investigation\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "\n",
    "class NegativeValueInvestigation:\n",
    "    \"\"\"\n",
    "    Deep investigation into why negative values exist in renewable generation data.\n",
    "    \n",
    "    EIA Data Context:\n",
    "    - EIA reports \"net generation\" which is gross generation minus station use\n",
    "    - Station auxiliary loads (cooling, controls, etc.) can exceed generation during:\n",
    "      * Low-wind periods for wind farms\n",
    "      * Night/cloudy periods for solar (if inverters consume standby power)\n",
    "      * Startup/shutdown events\n",
    "    \n",
    "    This is VALID data but represents net metering, not physical impossibility.\n",
    "    However, for FORECASTING purposes, we typically want to predict gross generation\n",
    "    or at minimum clamp to zero since negative \"production\" isn't meaningful for\n",
    "    grid planning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns [unique_id, ds, y] where y is generation\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.df['ds'] = pd.to_datetime(self.df['ds'])\n",
    "        self.df['hour'] = self.df['ds'].dt.hour\n",
    "        self.df['dow'] = self.df['ds'].dt.dayofweek\n",
    "        self.df['date'] = self.df['ds'].dt.date\n",
    "        \n",
    "    def get_negative_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get high-level summary of negative values.\"\"\"\n",
    "        neg_mask = self.df['y'] < 0\n",
    "        \n",
    "        summary = {\n",
    "            'total_rows': len(self.df),\n",
    "            'negative_count': int(neg_mask.sum()),\n",
    "            'negative_ratio': float(neg_mask.sum() / len(self.df)) if len(self.df) > 0 else 0,\n",
    "            'affected_series': self.df.loc[neg_mask, 'unique_id'].unique().tolist(),\n",
    "            'min_value': float(self.df['y'].min()),\n",
    "            'max_negative': float(self.df.loc[neg_mask, 'y'].max()) if neg_mask.any() else None,\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def analyze_negative_patterns(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Investigate WHEN and WHERE negatives occur to understand root cause.\n",
    "        \n",
    "        Key questions:\n",
    "        1. Are negatives concentrated in specific hours? (auxiliary load pattern)\n",
    "        2. Are negatives concentrated in specific series? (regional data issue)\n",
    "        3. What's the magnitude? (small negatives = metering noise, large = real issue)\n",
    "        \"\"\"\n",
    "        neg_df = self.df[self.df['y'] < 0].copy()\n",
    "        \n",
    "        if len(neg_df) == 0:\n",
    "            return {'status': 'no_negatives_found'}\n",
    "        \n",
    "        analysis = {\n",
    "            'by_series': {},\n",
    "            'by_hour': {},\n",
    "            'by_dow': {},\n",
    "            'magnitude_analysis': {},\n",
    "            'temporal_clustering': {},\n",
    "        }\n",
    "        \n",
    "        # 1. Analyze by series\n",
    "        for uid in neg_df['unique_id'].unique():\n",
    "            series_neg = neg_df[neg_df['unique_id'] == uid]\n",
    "            series_total = self.df[self.df['unique_id'] == uid]\n",
    "            \n",
    "            fuel_type = uid.split('_')[1] if '_' in uid else 'UNKNOWN'\n",
    "            \n",
    "            analysis['by_series'][uid] = {\n",
    "                'count': int(len(series_neg)),\n",
    "                'ratio': float(len(series_neg) / len(series_total)),\n",
    "                'fuel_type': fuel_type,\n",
    "                'min_value': float(series_neg['y'].min()),\n",
    "                'max_value': float(series_neg['y'].max()),\n",
    "                'mean_value': float(series_neg['y'].mean()),\n",
    "                'std_value': float(series_neg['y'].std()),\n",
    "            }\n",
    "        \n",
    "        # 2. Analyze by hour (Are negatives at night for solar? Low-wind hours for wind?)\n",
    "        hour_counts = neg_df.groupby('hour').size()\n",
    "        total_by_hour = self.df.groupby('hour').size()\n",
    "        neg_ratio_by_hour = (hour_counts / total_by_hour).fillna(0)\n",
    "        \n",
    "        analysis['by_hour'] = {\n",
    "            'counts': hour_counts.to_dict(),\n",
    "            'ratios': neg_ratio_by_hour.to_dict(),\n",
    "            'peak_negative_hour': int(neg_ratio_by_hour.idxmax()) if len(neg_ratio_by_hour) > 0 else None,\n",
    "        }\n",
    "        \n",
    "        # 3. Magnitude analysis - categorize severity\n",
    "        neg_values = neg_df['y'].values\n",
    "        analysis['magnitude_analysis'] = {\n",
    "            'tiny_negatives_count': int((neg_values > -10).sum()),  # Likely metering noise\n",
    "            'small_negatives_count': int(((neg_values <= -10) & (neg_values > -100)).sum()),\n",
    "            'medium_negatives_count': int(((neg_values <= -100) & (neg_values > -1000)).sum()),\n",
    "            'large_negatives_count': int((neg_values <= -1000).sum()),  # Significant issue\n",
    "            'percentiles': {\n",
    "                'p5': float(np.percentile(neg_values, 5)),\n",
    "                'p25': float(np.percentile(neg_values, 25)),\n",
    "                'p50': float(np.percentile(neg_values, 50)),\n",
    "                'p75': float(np.percentile(neg_values, 75)),\n",
    "                'p95': float(np.percentile(neg_values, 95)),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 4. Check for temporal clustering (consecutive hours of negatives)\n",
    "        for uid in neg_df['unique_id'].unique():\n",
    "            series_df = self.df[self.df['unique_id'] == uid].sort_values('ds')\n",
    "            series_df['is_negative'] = series_df['y'] < 0\n",
    "            \n",
    "            # Find consecutive negative runs\n",
    "            series_df['neg_block'] = (series_df['is_negative'] != series_df['is_negative'].shift()).cumsum()\n",
    "            neg_blocks = series_df[series_df['is_negative']].groupby('neg_block').agg(\n",
    "                start=('ds', 'min'),\n",
    "                end=('ds', 'max'),\n",
    "                duration_hours=('ds', 'count'),\n",
    "                min_value=('y', 'min'),\n",
    "            ).reset_index(drop=True)\n",
    "            \n",
    "            if len(neg_blocks) > 0:\n",
    "                analysis['temporal_clustering'][uid] = {\n",
    "                    'num_blocks': len(neg_blocks),\n",
    "                    'avg_block_duration_hours': float(neg_blocks['duration_hours'].mean()),\n",
    "                    'max_block_duration_hours': int(neg_blocks['duration_hours'].max()),\n",
    "                    'longest_block_start': str(neg_blocks.loc[neg_blocks['duration_hours'].idxmax(), 'start']),\n",
    "                }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def determine_root_cause(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Based on patterns, determine the likely root cause and recommend action.\n",
    "        \n",
    "        Possible causes:\n",
    "        1. NET GENERATION DATA: EIA reports net = gross - auxiliary. This is valid.\n",
    "        2. METERING NOISE: Tiny negatives (-1 to -10 MWh) are measurement error.\n",
    "        3. DATA REPORTING ERROR: Large sporadic negatives are likely errors.\n",
    "        4. SYSTEMATIC ISSUE: Negatives always at same time = station use pattern.\n",
    "        \"\"\"\n",
    "        patterns = self.analyze_negative_patterns()\n",
    "        \n",
    "        if patterns.get('status') == 'no_negatives_found':\n",
    "            return {\n",
    "                'root_cause': 'NONE',\n",
    "                'confidence': 'HIGH',\n",
    "                'recommendation': 'No action needed - data is clean',\n",
    "                'preprocessing_policy': 'pass_through',\n",
    "            }\n",
    "        \n",
    "        magnitude = patterns.get('magnitude_analysis', {})\n",
    "        total_neg = sum([\n",
    "            magnitude.get('tiny_negatives_count', 0),\n",
    "            magnitude.get('small_negatives_count', 0),\n",
    "            magnitude.get('medium_negatives_count', 0),\n",
    "            magnitude.get('large_negatives_count', 0),\n",
    "        ])\n",
    "        \n",
    "        # Determine root cause based on patterns\n",
    "        root_cause_analysis = {\n",
    "            'factors': [],\n",
    "            'root_cause': 'UNKNOWN',\n",
    "            'confidence': 'LOW',\n",
    "            'recommendation': '',\n",
    "            'preprocessing_policy': 'clamp_to_zero',\n",
    "        }\n",
    "        \n",
    "        # Check if mostly tiny negatives (metering noise)\n",
    "        if magnitude.get('tiny_negatives_count', 0) / max(total_neg, 1) > 0.9:\n",
    "            root_cause_analysis['factors'].append('90%+ negatives are tiny (<10 MWh)')\n",
    "            root_cause_analysis['root_cause'] = 'METERING_NOISE'\n",
    "            root_cause_analysis['confidence'] = 'HIGH'\n",
    "            root_cause_analysis['recommendation'] = (\n",
    "                'Tiny negatives are measurement noise. Safe to clamp to 0.'\n",
    "            )\n",
    "            root_cause_analysis['preprocessing_policy'] = 'clamp_to_zero'\n",
    "            \n",
    "        # Check if negatives are systematic (same hours)\n",
    "        elif patterns.get('by_hour', {}).get('peak_negative_hour') is not None:\n",
    "            hour_ratios = patterns.get('by_hour', {}).get('ratios', {})\n",
    "            max_ratio = max(hour_ratios.values()) if hour_ratios else 0\n",
    "            \n",
    "            if max_ratio > 0.3:  # >30% of negatives in one hour\n",
    "                root_cause_analysis['factors'].append(f'Negatives concentrated at specific hours')\n",
    "                root_cause_analysis['root_cause'] = 'NET_GENERATION_AUXILIARY_LOAD'\n",
    "                root_cause_analysis['confidence'] = 'MEDIUM'\n",
    "                root_cause_analysis['recommendation'] = (\n",
    "                    'Negatives likely represent station auxiliary loads exceeding generation. '\n",
    "                    'This is valid net generation data. For forecasting, clamp to 0 since '\n",
    "                    'we want to predict usable power output.'\n",
    "                )\n",
    "                root_cause_analysis['preprocessing_policy'] = 'clamp_to_zero'\n",
    "        \n",
    "        # Check for large sporadic negatives (data errors)\n",
    "        if magnitude.get('large_negatives_count', 0) > 0:\n",
    "            root_cause_analysis['factors'].append(f\"{magnitude.get('large_negatives_count')} large negatives (<-1000 MWh)\")\n",
    "            \n",
    "            # If ONLY large negatives and they're sporadic, likely errors\n",
    "            if magnitude.get('tiny_negatives_count', 0) == 0 and magnitude.get('small_negatives_count', 0) == 0:\n",
    "                root_cause_analysis['root_cause'] = 'DATA_REPORTING_ERROR'\n",
    "                root_cause_analysis['confidence'] = 'MEDIUM'\n",
    "                root_cause_analysis['recommendation'] = (\n",
    "                    'Large sporadic negatives are likely data reporting errors. '\n",
    "                    'Recommend clamping to 0 or investigating with EIA.'\n",
    "                )\n",
    "        \n",
    "        return root_cause_analysis\n",
    "    \n",
    "    def generate_report(self, output_dir: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive negative value investigation report.\"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        summary = self.get_negative_summary()\n",
    "        patterns = self.analyze_negative_patterns()\n",
    "        root_cause = self.determine_root_cause()\n",
    "        \n",
    "        report = {\n",
    "            'summary': summary,\n",
    "            'patterns': patterns,\n",
    "            'root_cause_analysis': root_cause,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Save JSON report\n",
    "        report_file = output_dir / 'negative_investigation.json'\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        # Generate visualizations if negatives exist\n",
    "        if summary['negative_count'] > 0:\n",
    "            self._plot_negative_analysis(patterns, output_dir)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _plot_negative_analysis(self, patterns: Dict, output_dir: Path) -> None:\n",
    "        \"\"\"Create diagnostic plots for negative value analysis.\"\"\"\n",
    "        neg_df = self.df[self.df['y'] < 0]\n",
    "        \n",
    "        if len(neg_df) == 0:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Distribution of negative values\n",
    "        ax = axes[0, 0]\n",
    "        neg_values = neg_df['y'].values\n",
    "        ax.hist(neg_values, bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=np.median(neg_values), color='blue', linestyle='--', \n",
    "                   label=f'Median: {np.median(neg_values):.1f}')\n",
    "        ax.set_xlabel('Generation (MWh)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Distribution of Negative Values')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 2. Negative ratio by hour\n",
    "        ax = axes[0, 1]\n",
    "        hour_ratios = patterns.get('by_hour', {}).get('ratios', {})\n",
    "        if hour_ratios:\n",
    "            hours = sorted(hour_ratios.keys())\n",
    "            ratios = [hour_ratios[h] for h in hours]\n",
    "            ax.bar(hours, ratios, color='orange', alpha=0.7)\n",
    "            ax.set_xlabel('Hour of Day')\n",
    "            ax.set_ylabel('Negative Ratio')\n",
    "            ax.set_title('When Do Negatives Occur? (by Hour)')\n",
    "            ax.set_xticks(range(0, 24, 2))\n",
    "        \n",
    "        # 3. Negative values by series\n",
    "        ax = axes[1, 0]\n",
    "        series_data = patterns.get('by_series', {})\n",
    "        if series_data:\n",
    "            series_names = list(series_data.keys())\n",
    "            series_counts = [series_data[s]['count'] for s in series_names]\n",
    "            colors = ['red' if 'SUN' in s else 'blue' for s in series_names]\n",
    "            ax.barh(series_names, series_counts, color=colors, alpha=0.7)\n",
    "            ax.set_xlabel('Negative Count')\n",
    "            ax.set_title('Negatives by Series (Blue=Wind, Red=Solar)')\n",
    "        \n",
    "        # 4. Time series with negatives highlighted\n",
    "        ax = axes[1, 1]\n",
    "        # Plot first affected series\n",
    "        affected = patterns.get('by_series', {})\n",
    "        if affected:\n",
    "            first_series = list(affected.keys())[0]\n",
    "            series_df = self.df[self.df['unique_id'] == first_series].sort_values('ds')\n",
    "            ax.plot(series_df['ds'], series_df['y'], 'b-', alpha=0.5, label='Generation')\n",
    "            neg_mask = series_df['y'] < 0\n",
    "            ax.scatter(series_df.loc[neg_mask, 'ds'], series_df.loc[neg_mask, 'y'], \n",
    "                      c='red', s=20, label='Negative values', zorder=5)\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Generation (MWh)')\n",
    "            ax.set_title(f'Time Series: {first_series}')\n",
    "            ax.legend()\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'negative_investigation.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def run_full_eda(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run comprehensive EDA with emphasis on understanding data quality issues.\n",
    "    \n",
    "    This function produces actionable insights, not just statistics.\n",
    "    \n",
    "    Args:\n",
    "        generation_df: DataFrame with columns [unique_id, ds, y]\n",
    "        weather_df: DataFrame with columns [ds, region, weather_vars...]\n",
    "        output_dir: Directory to save all outputs\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with all EDA results and recommendations\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_dir = output_dir / timestamp\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"RENEWABLE ENERGY EDA - Comprehensive Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {\n",
    "        'timestamp': timestamp,\n",
    "        'output_dir': str(report_dir),\n",
    "        'data_summary': {},\n",
    "        'negative_investigation': {},\n",
    "        'seasonality': {},\n",
    "        'zero_inflation': {},\n",
    "        'weather_alignment': {},\n",
    "        'recommendations': {},\n",
    "    }\n",
    "    \n",
    "    # 1. Data Summary\n",
    "    print(\"\\n[1/5] Data Summary...\")\n",
    "    results['data_summary'] = {\n",
    "        'generation_rows': len(generation_df),\n",
    "        'generation_series': generation_df['unique_id'].nunique(),\n",
    "        'series_list': generation_df['unique_id'].unique().tolist(),\n",
    "        'date_range': {\n",
    "            'start': str(generation_df['ds'].min()),\n",
    "            'end': str(generation_df['ds'].max()),\n",
    "        },\n",
    "        'weather_rows': len(weather_df),\n",
    "        'weather_regions': weather_df['region'].nunique() if 'region' in weather_df.columns else 0,\n",
    "    }\n",
    "    print(f\"   Generation: {results['data_summary']['generation_rows']:,} rows, \"\n",
    "          f\"{results['data_summary']['generation_series']} series\")\n",
    "    \n",
    "    # 2. CRITICAL: Negative Value Investigation\n",
    "    print(\"\\n[2/5] Negative Value Investigation (CRITICAL)...\")\n",
    "    neg_investigator = NegativeValueInvestigation(generation_df)\n",
    "    results['negative_investigation'] = neg_investigator.generate_report(\n",
    "        report_dir / 'negative_values'\n",
    "    )\n",
    "    \n",
    "    neg_summary = results['negative_investigation']['summary']\n",
    "    root_cause = results['negative_investigation']['root_cause_analysis']\n",
    "    \n",
    "    if neg_summary['negative_count'] > 0:\n",
    "        print(f\"   [WARNING] Found {neg_summary['negative_count']} negative values \"\n",
    "              f\"({neg_summary['negative_ratio']:.2%})\")\n",
    "        print(f\"   [WARNING] Affected series: {neg_summary['affected_series']}\")\n",
    "        print(f\"   [ANALYSIS] Root cause: {root_cause['root_cause']} \"\n",
    "              f\"(confidence: {root_cause['confidence']})\")\n",
    "        print(f\"   [RECOMMENDATION] {root_cause['recommendation']}\")\n",
    "    else:\n",
    "        print(\"   [OK] No negative values found\")\n",
    "    \n",
    "    # 3. Seasonality Analysis\n",
    "    print(\"\\n[3/5] Seasonality Analysis...\")\n",
    "    seasonality_dir = report_dir / 'seasonality'\n",
    "    seasonality_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    seasonality_results = _analyze_seasonality(generation_df, seasonality_dir)\n",
    "    results['seasonality'] = seasonality_results\n",
    "    print(f\"   [OK] Analyzed {len(seasonality_results.get('series_analyzed', []))} series\")\n",
    "    \n",
    "    # 4. Zero-Inflation Analysis\n",
    "    print(\"\\n[4/5] Zero-Inflation Analysis...\")\n",
    "    zero_dir = report_dir / 'zero_inflation'\n",
    "    zero_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    zero_results = _analyze_zero_inflation(generation_df, zero_dir)\n",
    "    results['zero_inflation'] = zero_results\n",
    "    \n",
    "    solar_series = [uid for uid in generation_df['unique_id'].unique() if 'SUN' in uid]\n",
    "    if solar_series:\n",
    "        avg_zero = sum(\n",
    "            zero_results['series_zero_ratios'].get(uid, {}).get('zero_ratio', 0)\n",
    "            for uid in solar_series\n",
    "        ) / len(solar_series)\n",
    "        print(f\"   [OK] Solar zero ratio: {avg_zero:.1%} (zeros at night expected)\")\n",
    "    \n",
    "    # 5. Weather Alignment\n",
    "    print(\"\\n[5/5] Weather Alignment...\")\n",
    "    weather_dir = report_dir / 'weather_alignment'\n",
    "    weather_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    weather_results = _analyze_weather_alignment(generation_df, weather_df, weather_dir)\n",
    "    results['weather_alignment'] = weather_results\n",
    "    print(f\"   [OK] Merge success rate: {weather_results['merge_success_ratio']:.1%}\")\n",
    "    \n",
    "    # Generate Final Recommendations\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    recommendations = {\n",
    "        'preprocessing': {},\n",
    "        'modeling': {},\n",
    "        'evaluation': {},\n",
    "    }\n",
    "    \n",
    "    # Preprocessing recommendations based on negative analysis\n",
    "    if neg_summary['negative_count'] > 0:\n",
    "        policy = root_cause.get('preprocessing_policy', 'clamp_to_zero')\n",
    "        recommendations['preprocessing']['negative_handling'] = {\n",
    "            'policy': policy,\n",
    "            'reason': root_cause['recommendation'],\n",
    "            'affected_series': neg_summary['affected_series'],\n",
    "        }\n",
    "        print(f\"\\n[PREPROCESSING] Negative Handling: {policy}\")\n",
    "        print(f\"   Reason: {root_cause['recommendation']}\")\n",
    "    else:\n",
    "        recommendations['preprocessing']['negative_handling'] = {\n",
    "            'policy': 'none_needed',\n",
    "            'reason': 'No negative values in raw data',\n",
    "        }\n",
    "    \n",
    "    # Modeling recommendations\n",
    "    recommendations['modeling'] = {\n",
    "        'seasonality': 'Use MSTL with season_length=[24, 168] (daily + weekly)',\n",
    "        'forecast_constraints': 'ALWAYS clip forecasts and intervals to [0, ∞)',\n",
    "        'reason': 'Physical constraint: renewable generation cannot be negative',\n",
    "    }\n",
    "    print(f\"\\n[MODELING] Forecast Constraints: Clip to [0, ∞)\")\n",
    "    print(f\"   Reason: Physical constraint - renewable generation cannot be negative\")\n",
    "    \n",
    "    # Evaluation recommendations\n",
    "    recommendations['evaluation'] = {\n",
    "        'metrics': ['RMSE', 'MAE'],\n",
    "        'avoid': 'MAPE (undefined when y=0)',\n",
    "        'reason': f\"Solar has {avg_zero:.1%} zeros (nighttime)\" if solar_series else \"Standard metrics\",\n",
    "    }\n",
    "    print(f\"\\n[EVALUATION] Use RMSE/MAE, avoid MAPE\")\n",
    "    \n",
    "    results['recommendations'] = recommendations\n",
    "    \n",
    "    # Save full report\n",
    "    report_file = report_dir / 'eda_report.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[SUCCESS] EDA complete. Report saved to: {report_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _analyze_seasonality(df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze seasonal patterns in generation data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    \n",
    "    results = {\n",
    "        'series_analyzed': [],\n",
    "        'hourly_patterns': {},\n",
    "    }\n",
    "    \n",
    "    for uid in df['unique_id'].unique()[:3]:  # Analyze first 3 series\n",
    "        series_data = df[df['unique_id'] == uid].copy()\n",
    "        series_data['hour'] = series_data['ds'].dt.hour\n",
    "        \n",
    "        hourly_profile = series_data.groupby('hour')['y'].agg(['mean', 'std']).reset_index()\n",
    "        results['series_analyzed'].append(uid)\n",
    "        results['hourly_patterns'][uid] = hourly_profile.to_dict(orient='records')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, len(results['series_analyzed']), \n",
    "                            figsize=(5 * len(results['series_analyzed']), 4))\n",
    "    if len(results['series_analyzed']) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, uid in enumerate(results['series_analyzed']):\n",
    "        series_data = df[df['unique_id'] == uid].copy()\n",
    "        series_data['hour'] = series_data['ds'].dt.hour\n",
    "        hourly_mean = series_data.groupby('hour')['y'].mean()\n",
    "        hourly_std = series_data.groupby('hour')['y'].std()\n",
    "        \n",
    "        axes[idx].plot(hourly_mean.index, hourly_mean.values, marker='o')\n",
    "        axes[idx].fill_between(hourly_mean.index, \n",
    "                               hourly_mean - hourly_std, \n",
    "                               hourly_mean + hourly_std, alpha=0.3)\n",
    "        axes[idx].set_xlabel('Hour of Day')\n",
    "        axes[idx].set_ylabel('Generation (MWh)')\n",
    "        axes[idx].set_title(f'{uid} - Hourly Profile')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'hourly_profiles.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _analyze_zero_inflation(df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze zero values in generation data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "    \n",
    "    results = {\n",
    "        'series_zero_ratios': {},\n",
    "    }\n",
    "    \n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_data = df[df['unique_id'] == uid]\n",
    "        zero_count = (series_data['y'] == 0).sum()\n",
    "        total_count = len(series_data)\n",
    "        \n",
    "        results['series_zero_ratios'][uid] = {\n",
    "            'zero_count': int(zero_count),\n",
    "            'total_count': int(total_count),\n",
    "            'zero_ratio': float(zero_count / total_count) if total_count > 0 else 0,\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    solar_series = [uid for uid in df['unique_id'].unique() if 'SUN' in uid]\n",
    "    wind_series = [uid for uid in df['unique_id'].unique() if 'WND' in uid]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Solar zeros by hour\n",
    "    if solar_series:\n",
    "        solar_df = df[df['unique_id'].isin(solar_series)]\n",
    "        solar_zero_by_hour = solar_df.groupby('hour').apply(\n",
    "            lambda x: (x['y'] == 0).mean()\n",
    "        )\n",
    "        axes[0].bar(solar_zero_by_hour.index, solar_zero_by_hour.values, \n",
    "                   color='orange', alpha=0.7)\n",
    "        axes[0].set_xlabel('Hour of Day')\n",
    "        axes[0].set_ylabel('Zero Ratio')\n",
    "        axes[0].set_title('Solar: Zero Ratio by Hour (Night = Expected)')\n",
    "        axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Wind zeros by hour\n",
    "    if wind_series:\n",
    "        wind_df = df[df['unique_id'].isin(wind_series)]\n",
    "        wind_zero_by_hour = wind_df.groupby('hour').apply(\n",
    "            lambda x: (x['y'] == 0).mean()\n",
    "        )\n",
    "        axes[1].bar(wind_zero_by_hour.index, wind_zero_by_hour.values,\n",
    "                   color='blue', alpha=0.7)\n",
    "        axes[1].set_xlabel('Hour of Day')\n",
    "        axes[1].set_ylabel('Zero Ratio')\n",
    "        axes[1].set_title('Wind: Zero Ratio by Hour')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'zero_inflation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _analyze_weather_alignment(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze weather-generation correlation.\"\"\"\n",
    "    generation_df = generation_df.copy()\n",
    "    weather_df = weather_df.copy()\n",
    "    \n",
    "    generation_df['ds'] = pd.to_datetime(generation_df['ds'])\n",
    "    weather_df['ds'] = pd.to_datetime(weather_df['ds'])\n",
    "    generation_df['region'] = generation_df['unique_id'].str.split('_').str[0]\n",
    "    \n",
    "    merged = generation_df.merge(weather_df, on=['ds', 'region'], how='left')\n",
    "    \n",
    "    weather_vars = [c for c in weather_df.columns \n",
    "                   if c not in ['ds', 'region'] and c in merged.columns]\n",
    "    \n",
    "    results = {\n",
    "        'merge_success_ratio': float(\n",
    "            merged[weather_vars[0]].notna().mean() if weather_vars else 0\n",
    "        ),\n",
    "        'correlation_by_fuel': {},\n",
    "    }\n",
    "    \n",
    "    # Calculate correlations\n",
    "    wind_series = merged[merged['unique_id'].str.contains('WND')]\n",
    "    solar_series = merged[merged['unique_id'].str.contains('SUN')]\n",
    "    \n",
    "    if len(wind_series) > 0:\n",
    "        wind_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in wind_series.columns:\n",
    "                corr = wind_series[['y', var]].corr().iloc[0, 1]\n",
    "                wind_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['WND'] = wind_corr\n",
    "    \n",
    "    if len(solar_series) > 0:\n",
    "        solar_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in solar_series.columns:\n",
    "                corr = solar_series[['y', var]].corr().iloc[0, 1]\n",
    "                solar_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['SUN'] = solar_corr\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_dir / 'weather_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Run EDA on renewable energy data.\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "    \n",
    "    if not generation_path.exists() or not weather_path.exists():\n",
    "        print(\"Data files not found. Run pipeline first.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "    \n",
    "    output_dir = Path(\"reports/renewable/eda\")\n",
    "    \n",
    "    results = run_full_eda(generation_df, weather_df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder based on EDA from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dataset_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dataset_builder.py\n",
    "# file: src/renewable/dataset_builder.py\n",
    "\"\"\"\n",
    "Dataset Builder for Renewable Energy Forecasting\n",
    "\n",
    "This module transforms raw EIA/weather data into modeling-ready datasets with:\n",
    "1. Transparent preprocessing based on EDA findings\n",
    "2. Physical constraint enforcement (non-negativity)\n",
    "3. Comprehensive diagnostics\n",
    "\n",
    "KEY PRINCIPLE:\n",
    "Renewable energy generation CANNOT be negative. This is a physical law.\n",
    "- Solar panels: 0 to max capacity\n",
    "- Wind turbines: 0 to max capacity\n",
    "\n",
    "Any negative values in raw data are data quality issues (metering, net generation\n",
    "accounting, etc.) and should be handled transparently.\n",
    "\n",
    "Preprocessing Policies:\n",
    "- clamp_to_zero: Set negative values to 0 (recommended for most cases)\n",
    "- investigate: Fail with detailed diagnostics (for initial data exploration)\n",
    "- pass_through: No modification (only if you understand why negatives exist)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Weather variables from Open-Meteo\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "# Time features for modeling\n",
    "TIME_FEATURES = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NegativeValueReport:\n",
    "    \"\"\"Report on negative values found and handled.\"\"\"\n",
    "    total_negative_count: int\n",
    "    total_rows: int\n",
    "    negative_ratio: float\n",
    "    by_series: Dict[str, Dict[str, Any]]\n",
    "    action_taken: str\n",
    "    samples: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingReport:\n",
    "    \"\"\"Complete report of all preprocessing steps.\"\"\"\n",
    "    timestamp: str\n",
    "    \n",
    "    # Input stats\n",
    "    input_rows: int\n",
    "    input_series: int\n",
    "    input_date_range: Dict[str, str]\n",
    "    \n",
    "    # Negative handling\n",
    "    negative_report: NegativeValueReport\n",
    "    \n",
    "    # Missing data\n",
    "    missing_hours_dropped: int\n",
    "    series_dropped_incomplete: List[str]\n",
    "    \n",
    "    # Weather alignment\n",
    "    weather_coverage: float\n",
    "    weather_vars_used: List[str]\n",
    "    \n",
    "    # Output stats\n",
    "    output_rows: int\n",
    "    output_series: int\n",
    "    output_features: List[str]\n",
    "    \n",
    "    # Configuration used\n",
    "    config: Dict[str, Any]\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add cyclical time features (hour, day of week).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "    \n",
    "    # Cyclical encoding (sin/cos transform)\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "    \n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "\n",
    "def _handle_negative_values(\n",
    "    df: pd.DataFrame,\n",
    "    policy: str,\n",
    ") -> Tuple[pd.DataFrame, NegativeValueReport]:\n",
    "    \"\"\"\n",
    "    Handle negative values in generation data.\n",
    "    \n",
    "    Physical Reality:\n",
    "    - Renewable energy generation CANNOT be negative\n",
    "    - Negative values are ALWAYS data quality issues:\n",
    "      * Net generation accounting (gross - auxiliary)\n",
    "      * Metering errors\n",
    "      * Data reporting issues\n",
    "    \n",
    "    Policies:\n",
    "    - clamp_to_zero: Set negatives to 0 (RECOMMENDED for forecasting)\n",
    "    - investigate: Raise error with diagnostics (for initial exploration)\n",
    "    - pass_through: No modification (not recommended)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with [unique_id, ds, y]\n",
    "        policy: How to handle negatives\n",
    "        \n",
    "    Returns:\n",
    "        (processed_df, report)\n",
    "    \"\"\"\n",
    "    neg_mask = df['y'] < 0\n",
    "    neg_count = int(neg_mask.sum())\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Build report\n",
    "    by_series = {}\n",
    "    samples = []\n",
    "    \n",
    "    if neg_count > 0:\n",
    "        for uid in df.loc[neg_mask, 'unique_id'].unique():\n",
    "            series_mask = (df['unique_id'] == uid) & neg_mask\n",
    "            series_neg = df.loc[series_mask]\n",
    "            series_total = len(df[df['unique_id'] == uid])\n",
    "            \n",
    "            by_series[uid] = {\n",
    "                'count': int(series_mask.sum()),\n",
    "                'ratio': float(series_mask.sum() / series_total),\n",
    "                'min_value': float(series_neg['y'].min()),\n",
    "                'max_value': float(series_neg['y'].max()),\n",
    "                'mean_value': float(series_neg['y'].mean()),\n",
    "            }\n",
    "            \n",
    "            # Collect samples for reporting\n",
    "            for _, row in series_neg.head(5).iterrows():\n",
    "                samples.append({\n",
    "                    'unique_id': row['unique_id'],\n",
    "                    'ds': str(row['ds']),\n",
    "                    'y': float(row['y']),\n",
    "                })\n",
    "    \n",
    "    report = NegativeValueReport(\n",
    "        total_negative_count=neg_count,\n",
    "        total_rows=total_rows,\n",
    "        negative_ratio=float(neg_count / total_rows) if total_rows > 0 else 0,\n",
    "        by_series=by_series,\n",
    "        action_taken=policy,\n",
    "        samples=samples,\n",
    "    )\n",
    "    \n",
    "    # Apply policy\n",
    "    if neg_count == 0:\n",
    "        report.action_taken = 'none_needed'\n",
    "        return df.copy(), report\n",
    "    \n",
    "    if policy == 'investigate':\n",
    "        # Detailed error for investigation\n",
    "        error_msg = (\n",
    "            f\"\\n{'='*80}\\n\"\n",
    "            f\"NEGATIVE VALUES DETECTED - Investigation Required\\n\"\n",
    "            f\"{'='*80}\\n\"\n",
    "            f\"Total negatives: {neg_count} ({report.negative_ratio:.2%})\\n\"\n",
    "            f\"Affected series: {list(by_series.keys())}\\n\\n\"\n",
    "            f\"Sample negative values:\\n\"\n",
    "        )\n",
    "        for s in samples[:10]:\n",
    "            error_msg += f\"  {s['unique_id']} @ {s['ds']}: {s['y']:.2f} MWh\\n\"\n",
    "        error_msg += (\n",
    "            f\"\\n{'='*80}\\n\"\n",
    "            f\"RECOMMENDATION: Renewable generation cannot be negative.\\n\"\n",
    "            f\"These are likely:\\n\"\n",
    "            f\"  1. Net generation values (gross - auxiliary load)\\n\"\n",
    "            f\"  2. Metering/reporting errors\\n\"\n",
    "            f\"\\nFor forecasting, use: negative_policy='clamp_to_zero'\\n\"\n",
    "            f\"{'='*80}\"\n",
    "        )\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    elif policy == 'clamp_to_zero':\n",
    "        # Clamp to zero - the physically correct approach for forecasting\n",
    "        out = df.copy()\n",
    "        out['y'] = out['y'].clip(lower=0)\n",
    "        \n",
    "        logger.warning(\n",
    "            f\"[PREPROCESSING] Clamped {neg_count} negative values to 0 \"\n",
    "            f\"({report.negative_ratio:.2%} of data)\"\n",
    "        )\n",
    "        for uid, info in by_series.items():\n",
    "            logger.warning(\n",
    "                f\"  {uid}: {info['count']} negatives clamped \"\n",
    "                f\"(range: [{info['min_value']:.1f}, {info['max_value']:.1f}])\"\n",
    "            )\n",
    "        \n",
    "        report.action_taken = 'clamped_to_zero'\n",
    "        return out, report\n",
    "    \n",
    "    elif policy == 'pass_through':\n",
    "        logger.warning(\n",
    "            f\"[PREPROCESSING] pass_through policy: {neg_count} negative values NOT modified. \"\n",
    "            f\"This may cause issues with forecasting.\"\n",
    "        )\n",
    "        report.action_taken = 'passed_through'\n",
    "        return df.copy(), report\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown negative_policy: {policy}\")\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    max_missing_ratio: float = 0.02,\n",
    ") -> Tuple[pd.DataFrame, List[str], int]:\n",
    "    \"\"\"\n",
    "    Enforce complete hourly grid (no gaps).\n",
    "    \n",
    "    For time series forecasting, we need continuous hourly data.\n",
    "    Series with too many gaps are dropped (no imputation - we don't fabricate data).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with [unique_id, ds, y]\n",
    "        max_missing_ratio: Maximum allowed ratio of missing hours\n",
    "        \n",
    "    Returns:\n",
    "        (filtered_df, dropped_series, total_missing_hours)\n",
    "    \"\"\"\n",
    "    dropped_series = []\n",
    "    total_missing = 0\n",
    "    \n",
    "    keep_rows = []\n",
    "    \n",
    "    for uid, group in df.groupby('unique_id'):\n",
    "        group = group.sort_values('ds')\n",
    "        start = group['ds'].min()\n",
    "        end = group['ds'].max()\n",
    "        \n",
    "        expected_hours = pd.date_range(start, end, freq='h')\n",
    "        actual_hours = len(group)\n",
    "        expected_count = len(expected_hours)\n",
    "        \n",
    "        missing_count = expected_count - actual_hours\n",
    "        missing_ratio = missing_count / expected_count if expected_count > 0 else 0\n",
    "        \n",
    "        total_missing += missing_count\n",
    "        \n",
    "        if missing_ratio > max_missing_ratio:\n",
    "            dropped_series.append(uid)\n",
    "            logger.warning(\n",
    "                f\"[GRID] Dropping {uid}: missing {missing_count} hours \"\n",
    "                f\"({missing_ratio:.1%} > {max_missing_ratio:.1%} threshold)\"\n",
    "            )\n",
    "        else:\n",
    "            keep_rows.append(group)\n",
    "    \n",
    "    if not keep_rows:\n",
    "        raise RuntimeError(\n",
    "            f\"All series dropped due to missing hours. \"\n",
    "            f\"Dropped: {dropped_series}. Consider increasing max_missing_ratio.\"\n",
    "        )\n",
    "    \n",
    "    filtered = pd.concat(keep_rows, ignore_index=True)\n",
    "    return filtered, dropped_series, total_missing\n",
    "\n",
    "\n",
    "def _align_weather(\n",
    "    df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    ") -> Tuple[pd.DataFrame, float, List[str]]:\n",
    "    \"\"\"\n",
    "    Align weather data to generation timestamps.\n",
    "    \n",
    "    Args:\n",
    "        df: Generation DataFrame (must have 'region' column or unique_id with region prefix)\n",
    "        weather_df: Weather DataFrame with [ds, region, weather_vars...]\n",
    "        \n",
    "    Returns:\n",
    "        (merged_df, coverage_ratio, weather_vars_used)\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "    \n",
    "    # Extract region from unique_id if not present\n",
    "    if 'region' not in work.columns:\n",
    "        work['region'] = work['unique_id'].str.split('_').str[0]\n",
    "    \n",
    "    # Find available weather variables\n",
    "    available_vars = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "    if not available_vars:\n",
    "        raise ValueError(\n",
    "            f\"No weather variables found in weather_df. \"\n",
    "            f\"Expected: {WEATHER_VARS}, Got: {weather_df.columns.tolist()}\"\n",
    "        )\n",
    "    \n",
    "    # Merge\n",
    "    merged = work.merge(\n",
    "        weather_df[['ds', 'region'] + available_vars],\n",
    "        on=['ds', 'region'],\n",
    "        how='left',\n",
    "        validate='many_to_one',\n",
    "    )\n",
    "    \n",
    "    # Check coverage\n",
    "    missing_weather = merged[available_vars].isna().any(axis=1)\n",
    "    coverage = 1 - (missing_weather.sum() / len(merged))\n",
    "    \n",
    "    if missing_weather.any():\n",
    "        missing_count = int(missing_weather.sum())\n",
    "        logger.warning(\n",
    "            f\"[WEATHER] {missing_count} rows ({1-coverage:.1%}) missing weather data\"\n",
    "        )\n",
    "        \n",
    "        # Drop rows with missing weather (no fabrication)\n",
    "        merged = merged[~missing_weather].reset_index(drop=True)\n",
    "        logger.warning(f\"[WEATHER] Dropped {missing_count} rows with missing weather\")\n",
    "    \n",
    "    # Drop region column (not needed for modeling)\n",
    "    merged = merged.drop(columns=['region'])\n",
    "    \n",
    "    return merged, coverage, available_vars\n",
    "\n",
    "\n",
    "def build_modeling_dataset(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    *,\n",
    "    negative_policy: str = 'clamp_to_zero',\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    output_dir: Optional[Path] = None,\n",
    ") -> Tuple[pd.DataFrame, PreprocessingReport]:\n",
    "    \"\"\"\n",
    "    Build modeling-ready dataset from raw data.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Validate inputs\n",
    "    2. Handle negative values (based on policy)\n",
    "    3. Enforce hourly grid (drop incomplete series)\n",
    "    4. Add time features\n",
    "    5. Align weather data\n",
    "    \n",
    "    Args:\n",
    "        generation_df: Raw generation data [unique_id, ds, y]\n",
    "        weather_df: Raw weather data [ds, region, weather_vars...]\n",
    "        negative_policy: How to handle negative values:\n",
    "            - 'clamp_to_zero': Set to 0 (RECOMMENDED)\n",
    "            - 'investigate': Fail with diagnostics\n",
    "            - 'pass_through': No modification\n",
    "        max_missing_ratio: Max ratio of missing hours before dropping series\n",
    "        output_dir: Optional directory for detailed reports\n",
    "        \n",
    "    Returns:\n",
    "        (modeling_df, preprocessing_report)\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"DATASET BUILDER - Building Modeling Dataset\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Validate inputs\n",
    "    required_gen = {'unique_id', 'ds', 'y'}\n",
    "    if not required_gen.issubset(generation_df.columns):\n",
    "        missing = required_gen - set(generation_df.columns)\n",
    "        raise ValueError(f\"generation_df missing columns: {missing}\")\n",
    "    \n",
    "    if generation_df.empty:\n",
    "        raise ValueError(\"generation_df is empty\")\n",
    "    \n",
    "    required_weather = {'ds', 'region'}\n",
    "    if not required_weather.issubset(weather_df.columns):\n",
    "        missing = required_weather - set(weather_df.columns)\n",
    "        raise ValueError(f\"weather_df missing columns: {missing}\")\n",
    "    \n",
    "    # Ensure datetime\n",
    "    work = generation_df.copy()\n",
    "    work['ds'] = pd.to_datetime(work['ds'])\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df['ds'] = pd.to_datetime(weather_df['ds'])\n",
    "    \n",
    "    input_rows = len(work)\n",
    "    input_series = work['unique_id'].nunique()\n",
    "    input_date_range = {\n",
    "        'start': str(work['ds'].min()),\n",
    "        'end': str(work['ds'].max()),\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Input: {input_rows:,} rows, {input_series} series\")\n",
    "    logger.info(f\"Date range: {input_date_range['start']} to {input_date_range['end']}\")\n",
    "    \n",
    "    # Step 1: Handle negative values\n",
    "    logger.info(f\"\\n[1/4] Handling negative values (policy={negative_policy})...\")\n",
    "    work, neg_report = _handle_negative_values(work, policy=negative_policy)\n",
    "    \n",
    "    # Step 2: Enforce hourly grid\n",
    "    logger.info(f\"\\n[2/4] Enforcing hourly grid (max_missing={max_missing_ratio:.1%})...\")\n",
    "    work, dropped_series, missing_hours = _enforce_hourly_grid(\n",
    "        work, max_missing_ratio=max_missing_ratio\n",
    "    )\n",
    "    \n",
    "    # Step 3: Add time features\n",
    "    logger.info(\"\\n[3/4] Adding time features...\")\n",
    "    work = _add_time_features(work)\n",
    "    logger.info(f\"   Added: {TIME_FEATURES}\")\n",
    "    \n",
    "    # Step 4: Align weather\n",
    "    logger.info(\"\\n[4/4] Aligning weather data...\")\n",
    "    work, weather_coverage, weather_vars = _align_weather(work, weather_df)\n",
    "    logger.info(f\"   Coverage: {weather_coverage:.1%}\")\n",
    "    logger.info(f\"   Variables: {weather_vars}\")\n",
    "    \n",
    "    # Sort and finalize\n",
    "    work = work.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    \n",
    "    output_rows = len(work)\n",
    "    output_series = work['unique_id'].nunique()\n",
    "    output_features = ['unique_id', 'ds', 'y'] + TIME_FEATURES + weather_vars\n",
    "    \n",
    "    # Build report\n",
    "    report = PreprocessingReport(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        input_rows=input_rows,\n",
    "        input_series=input_series,\n",
    "        input_date_range=input_date_range,\n",
    "        negative_report=neg_report,\n",
    "        missing_hours_dropped=missing_hours,\n",
    "        series_dropped_incomplete=dropped_series,\n",
    "        weather_coverage=weather_coverage,\n",
    "        weather_vars_used=weather_vars,\n",
    "        output_rows=output_rows,\n",
    "        output_series=output_series,\n",
    "        output_features=output_features,\n",
    "        config={\n",
    "            'negative_policy': negative_policy,\n",
    "            'max_missing_ratio': max_missing_ratio,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Save report if output_dir provided\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        report_dict = asdict(report)\n",
    "        report_dict['negative_report'] = asdict(report.negative_report)\n",
    "        \n",
    "        report_file = output_dir / 'preprocessing_report.json'\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report_dict, f, indent=2, default=str)\n",
    "        logger.info(f\"\\n[REPORT] Saved to: {report_file}\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 60)\n",
    "    logger.info(\"DATASET BUILDER - Complete\")\n",
    "    logger.info(f\"Output: {output_rows:,} rows, {output_series} series\")\n",
    "    logger.info(f\"Dropped: {input_rows - output_rows:,} rows\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return work, report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Test dataset builder with real data.\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    \n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "    \n",
    "    if not generation_path.exists() or not weather_path.exists():\n",
    "        print(\"Data files not found. Run pipeline first.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "    \n",
    "    # First investigate negatives\n",
    "    print(\"\\n[TEST 1] Investigating negatives...\")\n",
    "    try:\n",
    "        _, _ = build_modeling_dataset(\n",
    "            generation_df, weather_df,\n",
    "            negative_policy='investigate',\n",
    "            output_dir=Path(\"data/renewable/test_investigate\")\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "    \n",
    "    # Then build with clamp\n",
    "    print(\"\\n[TEST 2] Building with clamp_to_zero...\")\n",
    "    modeling_df, report = build_modeling_dataset(\n",
    "        generation_df, weather_df,\n",
    "        negative_policy='clamp_to_zero',\n",
    "        output_dir=Path(\"data/renewable/preprocessing\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {modeling_df.shape}\")\n",
    "    print(f\"Columns: {modeling_df.columns.tolist()}\")\n",
    "    print(f\"\\nSample:\")\n",
    "    print(modeling_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 4: Probabilistic Modeling\n",
    "\n",
    "**File:** `src/renewable/modeling.py`\n",
    "\n",
    "This is where the forecasting happens! We use **StatsForecast** for:\n",
    "\n",
    "1. **Multi-series forecasting**: Handle multiple regions/fuel types in one model\n",
    "2. **Probabilistic predictions**: Get prediction intervals, not just point forecasts\n",
    "3. **Weather exogenous**: Include weather features as predictors\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Prediction Intervals?\n",
    "\n",
    "Point forecasts are useful, but energy traders need **uncertainty quantification**:\n",
    "- **80% interval**: \"I'm 80% confident generation will be between X and Y\"\n",
    "- **95% interval**: Wider, for risk management\n",
    "\n",
    "### Zero-Value Safety (CRITICAL)\n",
    "\n",
    "**Solar panels generate ZERO at night!** This breaks MAPE:\n",
    "\n",
    "```\n",
    "MAPE = mean(|actual - predicted| / actual)\n",
    "\n",
    "When actual = 0:\n",
    "MAPE = |0 - pred| / 0 = undefined (division by zero!)\n",
    "```\n",
    "\n",
    "**Solution**: Always use RMSE and MAE for renewable forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/renewable/validation.py\n",
    "# file: src/renewable/validation.py\n",
    "\"\"\"Validation utilities for renewable generation data.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ValidationReport:\n",
    "    ok: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "def validate_generation_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lag_hours: int = 3,\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    expected_series: Optional[Iterable[str]] = None,\n",
    ") -> ValidationReport:\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    missing_cols = required - set(df.columns)\n",
    "    if missing_cols:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Missing required columns\",\n",
    "            {\"missing_cols\": sorted(missing_cols)},\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        return ValidationReport(False, \"Generation data is empty\", {})\n",
    "\n",
    "    work = df.copy()\n",
    "\n",
    "    work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"coerce\", utc=True)\n",
    "    if work[\"ds\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable ds values found\",\n",
    "            {\"bad_ds\": int(work[\"ds\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    work[\"y\"] = pd.to_numeric(work[\"y\"], errors=\"coerce\")\n",
    "    if work[\"y\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable y values found\",\n",
    "            {\"bad_y\": int(work[\"y\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    # Check for negative values and log warning (but allow to pass)\n",
    "    # Dataset builder will handle negatives according to configured policy\n",
    "    if (work[\"y\"] < 0).any():\n",
    "        import logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        neg_mask = work[\"y\"] < 0\n",
    "        neg_count = int(neg_mask.sum())\n",
    "        by_series = (\n",
    "            work[neg_mask]\n",
    "            .groupby(\"unique_id\")\n",
    "            .agg(count=(\"y\", \"count\"), min_y=(\"y\", \"min\"), max_y=(\"y\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        logger.warning(\n",
    "            \"[validation][NEGATIVE] Found %d negative values (%.1f%%) across %d series\",\n",
    "            neg_count,\n",
    "            100 * neg_count / len(work),\n",
    "            len(by_series)\n",
    "        )\n",
    "\n",
    "        for _, row in by_series.iterrows():\n",
    "            logger.warning(\n",
    "                \"  Series %s: %d negative values, range=[%.1f, %.1f]\",\n",
    "                row[\"unique_id\"], row[\"count\"], row[\"min_y\"], row[\"max_y\"]\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            \"[validation][NEGATIVE] Negatives will be handled by dataset builder \"\n",
    "            \"according to configured negative_policy\"\n",
    "        )\n",
    "\n",
    "        # Continue validation instead of failing\n",
    "        # (Dataset builder will handle negatives per policy)\n",
    "\n",
    "    dup = work.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Duplicate (unique_id, ds) rows found\",\n",
    "            {\"duplicates\": int(dup)},\n",
    "        )\n",
    "\n",
    "    if expected_series:\n",
    "        expected = sorted(set(expected_series))\n",
    "        present = sorted(set(work[\"unique_id\"]))\n",
    "        missing_series = sorted(set(expected) - set(present))\n",
    "        if missing_series:\n",
    "            return ValidationReport(\n",
    "                False,\n",
    "                \"Missing expected series\",\n",
    "                {\"missing_series\": missing_series, \"present_series\": present},\n",
    "            )\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "    max_ds = work[\"ds\"].max()\n",
    "    lag_hours = (now_utc - max_ds).total_seconds() / 3600.0\n",
    "    if lag_hours > max_lag_hours:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Data not fresh enough\",\n",
    "            {\n",
    "                \"now_utc\": now_utc.isoformat(),\n",
    "                \"max_ds\": max_ds.isoformat(),\n",
    "                \"lag_hours\": lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    series_max = work.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    series_lag = (now_utc - series_max).dt.total_seconds() / 3600.0\n",
    "    stale = series_lag[series_lag > max_lag_hours].sort_values(ascending=False)\n",
    "    if not stale.empty:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Stale series found\",\n",
    "            {\n",
    "                \"stale_series\": stale.head(10).to_dict(),\n",
    "                \"max_lag_hours\": max_lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    missing_ratios = {}\n",
    "    for uid, group in work.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")\n",
    "        start = group[\"ds\"].iloc[0]\n",
    "        end = group[\"ds\"].iloc[-1]\n",
    "        expected = int(((end - start) / pd.Timedelta(hours=1)) + 1)\n",
    "        actual = len(group)\n",
    "        missing = max(expected - actual, 0)\n",
    "        missing_ratios[uid] = missing / max(expected, 1)\n",
    "\n",
    "    worst_uid = max(missing_ratios, key=missing_ratios.get)\n",
    "    worst_ratio = missing_ratios[worst_uid]\n",
    "    if worst_ratio > max_missing_ratio:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Too many missing hourly points\",\n",
    "            {\"worst_uid\": worst_uid, \"worst_missing_ratio\": worst_ratio},\n",
    "        )\n",
    "\n",
    "    return ValidationReport(\n",
    "        True,\n",
    "        \"OK\",\n",
    "        {\n",
    "            \"row_count\": len(work),\n",
    "            \"series_count\": int(work[\"unique_id\"].nunique()),\n",
    "            \"max_ds\": max_ds.isoformat(),\n",
    "            \"lag_hours\": lag_hours,\n",
    "            \"worst_missing_ratio\": worst_ratio,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/modeling.py\n",
    "# file: src/renewable/modeling.py\n",
    "\"\"\"\n",
    "Renewable Energy Forecasting Models\n",
    "\n",
    "This module provides probabilistic forecasting with PHYSICAL CONSTRAINTS.\n",
    "\n",
    "KEY PRINCIPLE:\n",
    "Statistical models (ARIMA, ETS, etc.) can produce negative forecasts and\n",
    "prediction intervals because they assume Gaussian errors. However:\n",
    "\n",
    "  RENEWABLE ENERGY GENERATION CANNOT BE NEGATIVE.\n",
    "\n",
    "This module enforces this physical constraint by clipping ALL forecasts\n",
    "and prediction intervals to [0, ∞). This is NOT \"defensive coding\" - it's\n",
    "applying domain knowledge about physical reality.\n",
    "\n",
    "Model Architecture:\n",
    "1. StatsForecast for multi-series probabilistic forecasting\n",
    "2. Post-processing to enforce [0, ∞) constraint\n",
    "3. Calibration check for prediction intervals\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "TIME_FEATURES = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    \"\"\"Configuration for forecasting.\"\"\"\n",
    "    horizon: int = 24\n",
    "    confidence_levels: Tuple[int, int] = (80, 95)\n",
    "    \n",
    "    # Physical constraints\n",
    "    enforce_non_negative: bool = True  # ALWAYS True for renewable energy\n",
    "    \n",
    "    # CV settings\n",
    "    cv_windows: int = 3\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "\n",
    "def enforce_physical_constraints(\n",
    "    df: pd.DataFrame,\n",
    "    min_value: float = 0.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce physical constraints on forecasts.\n",
    "    \n",
    "    For renewable energy:\n",
    "    - Generation cannot be negative\n",
    "    - All forecast columns (point and intervals) are clipped to [0, ∞)\n",
    "    \n",
    "    This is applying physical reality:\n",
    "    - A solar panel cannot generate negative power\n",
    "    - A wind turbine cannot generate negative power\n",
    "    \n",
    "    Args:\n",
    "        df: Forecast DataFrame with columns like yhat, yhat_lo_80, yhat_hi_80, etc.\n",
    "        min_value: Minimum physical value (0 for generation)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all forecasts clipped to [min_value, ∞)\n",
    "    \"\"\"\n",
    "    # Identify forecast columns (exclude unique_id, ds, etc.)\n",
    "    exclude_cols = {'unique_id', 'ds', 'cutoff', 'y', 'region', 'fuel_type'}\n",
    "    forecast_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    \n",
    "    # Count values that will be clipped\n",
    "    clip_counts = {}\n",
    "    for col in forecast_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            below_min = (df[col] < min_value).sum()\n",
    "            if below_min > 0:\n",
    "                clip_counts[col] = int(below_min)\n",
    "    \n",
    "    if clip_counts:\n",
    "        total_clipped = sum(clip_counts.values())\n",
    "        total_values = len(df) * len(forecast_cols)\n",
    "        logger.info(\n",
    "            f\"[PHYSICAL CONSTRAINT] Clipping {total_clipped} values to >= {min_value} \"\n",
    "            f\"({total_clipped/total_values:.1%} of forecast values)\"\n",
    "        )\n",
    "        for col, count in clip_counts.items():\n",
    "            logger.debug(f\"  {col}: {count} values clipped\")\n",
    "    \n",
    "    # Apply constraint\n",
    "    result = df.copy()\n",
    "    for col in forecast_cols:\n",
    "        if col in result.columns and pd.api.types.is_numeric_dtype(result[col]):\n",
    "            result[col] = result[col].clip(lower=min_value)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute forecast evaluation metrics.\n",
    "    \n",
    "    Uses RMSE and MAE (NOT MAPE because y=0 at night for solar).\n",
    "    \"\"\"\n",
    "    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true = y_true[valid_mask]\n",
    "    y_pred = y_pred[valid_mask]\n",
    "    \n",
    "    if len(y_true) == 0:\n",
    "        return {'rmse': np.nan, 'mae': np.nan, 'valid_rows': 0}\n",
    "    \n",
    "    errors = y_true - y_pred\n",
    "    \n",
    "    return {\n",
    "        'rmse': float(np.sqrt(np.mean(errors ** 2))),\n",
    "        'mae': float(np.mean(np.abs(errors))),\n",
    "        'valid_rows': int(len(y_true)),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_coverage(\n",
    "    y_true: np.ndarray,\n",
    "    y_lo: np.ndarray,\n",
    "    y_hi: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Compute prediction interval coverage.\"\"\"\n",
    "    valid = np.isfinite(y_true) & np.isfinite(y_lo) & np.isfinite(y_hi)\n",
    "    if valid.sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    in_interval = (y_true[valid] >= y_lo[valid]) & (y_true[valid] <= y_hi[valid])\n",
    "    return float(in_interval.mean())\n",
    "\n",
    "\n",
    "class RenewableForecastModel:\n",
    "    \"\"\"\n",
    "    Probabilistic forecasting model with physical constraints.\n",
    "    \n",
    "    Uses StatsForecast for efficient multi-series forecasting,\n",
    "    then enforces non-negativity on all outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon: int = 24,\n",
    "        confidence_levels: Tuple[int, int] = (80, 95),\n",
    "    ):\n",
    "        self.horizon = horizon\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.sf = None\n",
    "        self._train_df = None\n",
    "        self._exog_cols: List[str] = []\n",
    "        self.fitted = False\n",
    "    \n",
    "    def _prepare_training_df(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare training DataFrame.\n",
    "        \n",
    "        Expects preprocessed data from dataset_builder (already has time features\n",
    "        and weather aligned).\n",
    "        \"\"\"\n",
    "        required = {'unique_id', 'ds', 'y'}\n",
    "        if not required.issubset(df.columns):\n",
    "            missing = required - set(df.columns)\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty DataFrame\")\n",
    "        \n",
    "        # Check for required features\n",
    "        time_features = [c for c in TIME_FEATURES if c in df.columns]\n",
    "        weather_features = [c for c in WEATHER_VARS if c in df.columns]\n",
    "        \n",
    "        if not time_features:\n",
    "            raise ValueError(\n",
    "                \"No time features found. Data should be preprocessed by dataset_builder.\"\n",
    "            )\n",
    "        \n",
    "        self._exog_cols = time_features + weather_features\n",
    "        \n",
    "        work = df.copy()\n",
    "        work['ds'] = pd.to_datetime(work['ds'])\n",
    "        work = work.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        \n",
    "        # Validate no negatives in training data\n",
    "        neg_count = (work['y'] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            raise ValueError(\n",
    "                f\"Training data contains {neg_count} negative values. \"\n",
    "                f\"Data should be preprocessed by dataset_builder with negative_policy='clamp_to_zero'.\"\n",
    "            )\n",
    "        \n",
    "        logger.info(\n",
    "            f\"[TRAIN] Prepared: {len(work):,} rows, {work['unique_id'].nunique()} series, \"\n",
    "            f\"{len(self._exog_cols)} exog features\"\n",
    "        )\n",
    "        \n",
    "        return work\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fit models on training data.\n",
    "        \n",
    "        Args:\n",
    "            df: Preprocessed DataFrame from dataset_builder\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import MSTL, AutoARIMA, AutoETS, SeasonalNaive\n",
    "        \n",
    "        train_df = self._prepare_training_df(df)\n",
    "        \n",
    "        models = [\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "            AutoARIMA(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "        ]\n",
    "        \n",
    "        # Try to add expanded models\n",
    "        try:\n",
    "            from statsforecast.models import AutoTheta\n",
    "            models.append(AutoTheta(season_length=24))\n",
    "            logger.info(\"[FIT] Using expanded model set: +AutoTheta\")\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        self.sf = StatsForecast(models=models, freq='h', n_jobs=-1)\n",
    "        self._train_df = train_df\n",
    "        self.fitted = True\n",
    "        \n",
    "        logger.info(f\"[FIT] Fitted {len(models)} models on {len(train_df):,} rows\")\n",
    "    \n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Perform cross-validation.\n",
    "        \n",
    "        Returns:\n",
    "            (cv_results, leaderboard)\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import MSTL, AutoARIMA, AutoETS, SeasonalNaive\n",
    "        \n",
    "        train_df = self._prepare_training_df(df)\n",
    "        \n",
    "        models = [\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "            AutoARIMA(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            from statsforecast.models import AutoTheta\n",
    "            models.append(AutoTheta(season_length=24))\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        sf = StatsForecast(models=models, freq='h', n_jobs=-1)\n",
    "        \n",
    "        logger.info(\n",
    "            f\"[CV] Running: {n_windows} windows, step={step_size}h, horizon={self.horizon}h\"\n",
    "        )\n",
    "        \n",
    "        cv = sf.cross_validation(\n",
    "            df=train_df,\n",
    "            h=self.horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "        \n",
    "        # CRITICAL: Apply physical constraints to CV results\n",
    "        cv = enforce_physical_constraints(cv, min_value=0.0)\n",
    "        \n",
    "        # Build leaderboard\n",
    "        leaderboard = self._build_leaderboard(cv)\n",
    "        \n",
    "        return cv, leaderboard\n",
    "    \n",
    "    def _build_leaderboard(self, cv_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Build model comparison leaderboard from CV results.\"\"\"\n",
    "        # Find model columns (not id/ds/cutoff/y, not interval columns)\n",
    "        exclude = {'unique_id', 'ds', 'cutoff', 'y'}\n",
    "        interval_pattern = re.compile(r'-(lo|hi)-\\d+$')\n",
    "        \n",
    "        model_cols = [\n",
    "            c for c in cv_df.columns\n",
    "            if c not in exclude and not interval_pattern.search(c)\n",
    "        ]\n",
    "        \n",
    "        rows = []\n",
    "        y_true = cv_df['y'].values\n",
    "        \n",
    "        for model in model_cols:\n",
    "            y_pred = cv_df[model].values\n",
    "            metrics = compute_metrics(y_true, y_pred)\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'valid_rows': metrics['valid_rows'],\n",
    "            }\n",
    "            \n",
    "            # Add coverage for each confidence level\n",
    "            for level in self.confidence_levels:\n",
    "                lo_col = f\"{model}-lo-{level}\"\n",
    "                hi_col = f\"{model}-hi-{level}\"\n",
    "                if lo_col in cv_df.columns and hi_col in cv_df.columns:\n",
    "                    coverage = compute_coverage(\n",
    "                        y_true,\n",
    "                        cv_df[lo_col].values,\n",
    "                        cv_df[hi_col].values,\n",
    "                    )\n",
    "                    row[f'coverage_{level}'] = coverage\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        leaderboard = pd.DataFrame(rows)\n",
    "        leaderboard = leaderboard.sort_values('rmse').reset_index(drop=True)\n",
    "        \n",
    "        return leaderboard\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        future_exog: pd.DataFrame,\n",
    "        best_model: Optional[str] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate forecasts.\n",
    "        \n",
    "        Args:\n",
    "            future_exog: DataFrame with future exogenous features\n",
    "                         Must have [unique_id, ds] + exog features\n",
    "            best_model: If specified, only return this model's predictions\n",
    "            \n",
    "        Returns:\n",
    "            Forecast DataFrame with physical constraints applied\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() first\")\n",
    "        \n",
    "        # Build future X_df\n",
    "        X_df = self._build_future_X(future_exog)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        fcst = self.sf.forecast(\n",
    "            h=self.horizon,\n",
    "            df=self._train_df,\n",
    "            X_df=X_df,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "        \n",
    "        # CRITICAL: Apply physical constraints\n",
    "        fcst = enforce_physical_constraints(fcst, min_value=0.0)\n",
    "        \n",
    "        # If best_model specified, filter\n",
    "        if best_model is not None:\n",
    "            if best_model not in fcst.columns:\n",
    "                available = [c for c in fcst.columns if c not in ['unique_id', 'ds']]\n",
    "                raise ValueError(\n",
    "                    f\"Model '{best_model}' not found. Available: {available}\"\n",
    "                )\n",
    "            \n",
    "            keep_cols = ['unique_id', 'ds', best_model]\n",
    "            rename_map = {best_model: 'yhat'}\n",
    "            \n",
    "            for level in self.confidence_levels:\n",
    "                lo = f\"{best_model}-lo-{level}\"\n",
    "                hi = f\"{best_model}-hi-{level}\"\n",
    "                if lo in fcst.columns:\n",
    "                    keep_cols.append(lo)\n",
    "                    rename_map[lo] = f'yhat_lo_{level}'\n",
    "                if hi in fcst.columns:\n",
    "                    keep_cols.append(hi)\n",
    "                    rename_map[hi] = f'yhat_hi_{level}'\n",
    "            \n",
    "            fcst = fcst[keep_cols].rename(columns=rename_map)\n",
    "        \n",
    "        return fcst\n",
    "    \n",
    "    def _build_future_X(self, future_exog: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Build future exogenous feature DataFrame.\"\"\"\n",
    "        required = {'unique_id', 'ds'}\n",
    "        if not required.issubset(future_exog.columns):\n",
    "            missing = required - set(future_exog.columns)\n",
    "            raise ValueError(f\"future_exog missing: {missing}\")\n",
    "        \n",
    "        # Check exog columns\n",
    "        missing_exog = [c for c in self._exog_cols if c not in future_exog.columns]\n",
    "        if missing_exog:\n",
    "            raise ValueError(\n",
    "                f\"future_exog missing required features: {missing_exog}. \"\n",
    "                f\"Expected: {self._exog_cols}\"\n",
    "            )\n",
    "        \n",
    "        X = future_exog[['unique_id', 'ds'] + self._exog_cols].copy()\n",
    "        X = X.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "def compute_baseline_metrics(\n",
    "    cv_df: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    threshold_k: float = 2.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for drift detection.\n",
    "    \n",
    "    Args:\n",
    "        cv_df: Cross-validation results\n",
    "        model_name: Model to compute baseline for\n",
    "        threshold_k: k for threshold = mean + k*std\n",
    "        \n",
    "    Returns:\n",
    "        Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    if model_name not in cv_df.columns:\n",
    "        raise ValueError(f\"Model '{model_name}' not in CV results\")\n",
    "    \n",
    "    # Compute per-window metrics\n",
    "    def window_rmse(g):\n",
    "        metrics = compute_metrics(g['y'].values, g[model_name].values)\n",
    "        return metrics['rmse']\n",
    "    \n",
    "    per_window = cv_df.groupby(['unique_id', 'cutoff']).apply(window_rmse)\n",
    "    \n",
    "    rmse_mean = float(per_window.mean())\n",
    "    rmse_std = float(per_window.std())\n",
    "    \n",
    "    baseline = {\n",
    "        'model': model_name,\n",
    "        'rmse_mean': rmse_mean,\n",
    "        'rmse_std': rmse_std,\n",
    "        'drift_threshold_rmse': rmse_mean + threshold_k * rmse_std,\n",
    "        'n_windows': int(per_window.notna().sum()),\n",
    "    }\n",
    "    \n",
    "    return baseline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Test modeling with physical constraints.\"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    data_path = Path(\"data/renewable/modeling_dataset.parquet\")\n",
    "    if not data_path.exists():\n",
    "        print(\"Preprocessed data not found. Run dataset_builder first.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    # Run CV\n",
    "    model = RenewableForecastModel(horizon=24, confidence_levels=(80, 95))\n",
    "    cv, leaderboard = model.cross_validate(df, n_windows=3, step_size=168)\n",
    "    \n",
    "    print(\"\\nLeaderboard:\")\n",
    "    print(leaderboard.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nCV forecast stats:\")\n",
    "    print(f\"  Min forecast: {cv['MSTL_ARIMA'].min():.2f}\")\n",
    "    print(f\"  Max forecast: {cv['MSTL_ARIMA'].max():.2f}\")\n",
    "    print(f\"  Any negative: {(cv['MSTL_ARIMA'] < 0).any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Module: Pipeline Tasks\n",
    "\n",
    "**File:** `src/renewable/tasks.py`\n",
    "\n",
    "This module orchestrates the complete pipeline:\n",
    "\n",
    "1. **Fetch generation data** from EIA\n",
    "2. **Fetch weather data** from Open-Meteo\n",
    "3. **Train models** with cross-validation\n",
    "4. **Generate forecasts** with prediction intervals\n",
    "5. **Compute drift metrics** vs baseline\n",
    "\n",
    "## Key Feature: Adaptive CV\n",
    "\n",
    "Cross-validation requires sufficient data:\n",
    "```\n",
    "Minimum rows = horizon + (n_windows × step_size)\n",
    "```\n",
    "\n",
    "For short series, we **adapt** the CV settings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/renewable/tasks.py\n",
    "# file: src/renewable/tasks.py\n",
    "\"\"\"Renewable energy forecasting pipeline tasks.\n",
    "\n",
    "Idempotent tasks for:\n",
    "- Fetching EIA renewable generation data\n",
    "- Fetching weather data from Open-Meteo\n",
    "- Training probabilistic models\n",
    "- Generating forecasts with intervals\n",
    "- Computing drift metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional imports for interpretability (LightGBM + skforecast)\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from skforecast.recursive import ForecasterRecursive\n",
    "    INTERPRETABILITY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    INTERPRETABILITY_AVAILABLE = False\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.warning(\"lightgbm and/or skforecast not installed - interpretability features unavailable\")\n",
    "\n",
    "from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "from src.renewable.modeling import (\n",
    "    RenewableForecastModel,\n",
    "    _log_series_summary,\n",
    "    _add_time_features,\n",
    "    compute_baseline_metrics,\n",
    "    WEATHER_VARS,\n",
    ")\n",
    "from src.renewable.model_interpretability import (\n",
    "    InterpretabilityReport,\n",
    "    generate_full_interpretability_report,\n",
    ")\n",
    "from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "from src.renewable.dataset_builder import build_modeling_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RenewablePipelineConfig:\n",
    "    \"\"\"Configuration for renewable forecasting pipeline.\"\"\"\n",
    "\n",
    "    # Data parameters\n",
    "    regions: list[str] = field(default_factory=lambda: [\"CALI\", \"ERCO\", \"MISO\", \"PJM\", \"SWPP\"])\n",
    "    fuel_types: list[str] = field(default_factory=lambda: [\"WND\", \"SUN\"])\n",
    "    start_date: str = \"\"  # Set dynamically\n",
    "    end_date: str = \"\"  # Set dynamically\n",
    "    lookback_days: int = 30\n",
    "\n",
    "    # Forecast parameters\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "    horizon_preset: Optional[str] = None  # \"24h\" | \"48h\" | \"72h\"\n",
    "\n",
    "    # CV parameters\n",
    "    cv_windows: int = 5\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "    # Model parameters\n",
    "    enable_interpretability: bool = True  # LightGBM SHAP analysis (on by default)\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    negative_policy: str = \"clamp\"  # \"clamp\" | \"fail_loud\" | \"hybrid\"\n",
    "    hourly_grid_policy: str = \"drop_incomplete_series\"  # \"drop_incomplete_series\" | \"fail_loud\"\n",
    "\n",
    "    # Output paths\n",
    "    data_dir: str = \"data/renewable\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    # Horizon preset definitions (class-level constant)\n",
    "    _PRESETS = {\n",
    "        \"24h\": {\"horizon\": 24, \"cv_windows\": 2, \"lookback_days\": 15},\n",
    "        \"48h\": {\"horizon\": 48, \"cv_windows\": 3, \"lookback_days\": 21},\n",
    "        \"72h\": {\"horizon\": 72, \"cv_windows\": 3, \"lookback_days\": 28},\n",
    "    }\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Apply horizon preset if specified\n",
    "        if self.horizon_preset and self.horizon_preset in self._PRESETS:\n",
    "            preset = self._PRESETS[self.horizon_preset]\n",
    "            # Use object.__setattr__ since this is a dataclass\n",
    "            object.__setattr__(self, \"horizon\", preset[\"horizon\"])\n",
    "            object.__setattr__(self, \"cv_windows\", preset[\"cv_windows\"])\n",
    "            object.__setattr__(self, \"lookback_days\", preset[\"lookback_days\"])\n",
    "            logger.info(f\"[config] Applied preset '{self.horizon_preset}': horizon={preset['horizon']}h\")\n",
    "\n",
    "        # Set default dates if not provided\n",
    "        if not self.end_date:\n",
    "            self.end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        if not self.start_date:\n",
    "            end = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "            start = end - timedelta(days=self.lookback_days)\n",
    "            self.start_date = start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Validate configuration\n",
    "        warnings = self._validate()\n",
    "        for warning in warnings:\n",
    "            logger.warning(f\"[config] {warning}\")\n",
    "\n",
    "    def _validate(self) -> list[str]:\n",
    "        \"\"\"Validate configuration and return warnings.\"\"\"\n",
    "        warnings = []\n",
    "\n",
    "        # Check minimum data requirement\n",
    "        available_hours = self.lookback_days * 24\n",
    "        required_hours = self.horizon + (self.cv_windows * self.cv_step_size)\n",
    "        if available_hours < required_hours:\n",
    "            warnings.append(\n",
    "                f\"Insufficient data: need {required_hours}h, have {available_hours}h. \"\n",
    "                f\"Increase lookback_days to {(required_hours // 24) + 1} or reduce cv_windows.\"\n",
    "            )\n",
    "\n",
    "        # Warn about accuracy degradation\n",
    "        if self.horizon > 72:\n",
    "            warnings.append(\n",
    "                f\"Horizon {self.horizon}h exceeds recommended max (72h). \"\n",
    "                f\"Weather forecast accuracy degrades significantly beyond 3 days.\"\n",
    "            )\n",
    "\n",
    "        return warnings\n",
    "\n",
    "    def generation_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"generation.parquet\"\n",
    "\n",
    "    def weather_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"weather.parquet\"\n",
    "\n",
    "    def forecasts_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"forecasts.parquet\"\n",
    "\n",
    "    def baseline_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"baseline.json\"\n",
    "\n",
    "    def interpretability_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"interpretability\"\n",
    "\n",
    "    def preprocessing_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"preprocessing\"\n",
    "\n",
    "\n",
    "def fetch_renewable_data(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 1: Fetch EIA generation data for all regions and fuel types.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [unique_id, ds, y]\n",
    "    \"\"\"\n",
    "    output_path = config.generation_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_generation_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        _log_series_summary(df, value_col=\"y\", label=f\"generation_data_{source}\")\n",
    "\n",
    "        expected_series = {\n",
    "            f\"{region}_{fuel}\" for region in config.regions for fuel in config.fuel_types\n",
    "        }\n",
    "        present_series = set(df[\"unique_id\"]) if \"unique_id\" in df.columns else set()\n",
    "        missing_series = sorted(expected_series - present_series)\n",
    "        if missing_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Missing expected series (%s): %s\",\n",
    "                source,\n",
    "                missing_series,\n",
    "            )\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_generation] No generation data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"unique_id\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"unique_id\")\n",
    "        )\n",
    "        max_series_log = 25\n",
    "        if len(coverage) > max_series_log:\n",
    "            logger.info(\n",
    "                \"[fetch_generation] Coverage (%s, first %s series):\\n%s\",\n",
    "                source,\n",
    "                max_series_log,\n",
    "                coverage.head(max_series_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_generation] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_generation] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached coverage to surface missing series without refetching.\n",
    "        _log_generation_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_generation] Fetching {config.fuel_types} for {config.regions}\")\n",
    "\n",
    "    # Use longer timeout (90s) to handle slow EIA API responses\n",
    "    fetcher = EIARenewableFetcher(timeout=90)\n",
    "    all_dfs = []\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        df = fetcher.fetch_all_regions(\n",
    "            fuel_type=fuel_type,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            regions=config.regions,\n",
    "            diagnostics=fetch_diagnostics,\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined = combined.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh coverage to highlight gaps or unexpected negatives.\n",
    "    _log_generation_summary(combined, source=\"fresh\")\n",
    "\n",
    "    if fetch_diagnostics:\n",
    "        empty_series = [\n",
    "            entry\n",
    "            for entry in fetch_diagnostics\n",
    "            if entry.get(\"empty\")\n",
    "        ]\n",
    "        for entry in empty_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Empty series detail: region=%s fuel=%s total=%s pages=%s\",\n",
    "                entry.get(\"region\"),\n",
    "                entry.get(\"fuel_type\"),\n",
    "                entry.get(\"total_records\"),\n",
    "                entry.get(\"pages\"),\n",
    "            )\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_generation] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def fetch_renewable_weather(\n",
    "    config: RenewablePipelineConfig,\n",
    "    include_forecast: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 2: Fetch weather data for all regions.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        include_forecast: Include forecast weather for predictions\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [ds, region, weather_vars...]\n",
    "    \"\"\"\n",
    "    output_path = config.weather_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_weather_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_weather] No weather data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"region\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"region\")\n",
    "        )\n",
    "        max_region_log = 25\n",
    "        if len(coverage) > max_region_log:\n",
    "            logger.info(\n",
    "                \"[fetch_weather] Coverage (%s, first %s regions):\\n%s\",\n",
    "                source,\n",
    "                max_region_log,\n",
    "                coverage.head(max_region_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_weather] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "        missing_cols = [\n",
    "            col for col in OpenMeteoRenewable.WEATHER_VARS if col not in df.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing expected weather columns (%s): %s\",\n",
    "                source,\n",
    "                missing_cols,\n",
    "            )\n",
    "\n",
    "        missing_values = {\n",
    "            col: int(df[col].isna().sum())\n",
    "            for col in OpenMeteoRenewable.WEATHER_VARS\n",
    "            if col in df.columns and df[col].isna().any()\n",
    "        }\n",
    "        if missing_values:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing weather values (%s): %s\",\n",
    "                source,\n",
    "                missing_values,\n",
    "            )\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_weather] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached weather coverage to surface missing regions/columns.\n",
    "        _log_weather_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_weather] Fetching weather for {config.regions}\")\n",
    "\n",
    "    weather = OpenMeteoRenewable()\n",
    "\n",
    "    # Historical weather\n",
    "    hist_df = weather.fetch_all_regions_historical(\n",
    "        regions=config.regions,\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "    )\n",
    "\n",
    "    # Validate historical weather result\n",
    "    if hist_df.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[fetch_weather] Historical weather returned empty DataFrame. \"\n",
    "            \"fetch_all_regions_historical should raise an error on failure, \"\n",
    "            \"but received empty result. Check fetch logic.\"\n",
    "        )\n",
    "\n",
    "    if not {\"ds\", \"region\"}.issubset(hist_df.columns):\n",
    "        missing_cols = {\"ds\", \"region\"} - set(hist_df.columns)\n",
    "        raise ValueError(\n",
    "            f\"[fetch_weather] Weather DataFrame missing required columns: {missing_cols}\"\n",
    "        )\n",
    "\n",
    "    hist_regions = hist_df['region'].nunique()\n",
    "    hist_rows = len(hist_df)\n",
    "    logger.info(\n",
    "        f\"[fetch_weather] Historical: {hist_regions} regions, {hist_rows} rows\"\n",
    "    )\n",
    "\n",
    "    # Forecast weather (for prediction, prevents leakage)\n",
    "    if include_forecast:\n",
    "        fcst_df = weather.fetch_all_regions_forecast(\n",
    "            regions=config.regions,\n",
    "            horizon_hours=config.horizon + 24,  # Buffer\n",
    "        )\n",
    "\n",
    "        # Validate forecast weather result\n",
    "        if fcst_df.empty:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Forecast weather returned empty DataFrame. \"\n",
    "                \"Using historical data only for model training and predictions.\"\n",
    "            )\n",
    "            combined = hist_df\n",
    "        else:\n",
    "            fcst_rows = len(fcst_df)\n",
    "            logger.info(f\"[fetch_weather] Forecast: {fcst_rows} rows\")\n",
    "\n",
    "            # Combine, preferring forecast for overlapping times\n",
    "            combined = pd.concat([hist_df, fcst_df], ignore_index=True)\n",
    "            combined = combined.drop_duplicates(subset=[\"ds\", \"region\"], keep=\"last\")\n",
    "    else:\n",
    "        combined = hist_df\n",
    "\n",
    "    combined = combined.sort_values([\"region\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh weather coverage and missing values before saving.\n",
    "    _log_weather_summary(combined, source=\"fresh\")\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_weather] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def train_renewable_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    modeling_df: Optional[pd.DataFrame] = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Task 3: Train models and compute baseline metrics via cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        modeling_df: Preprocessed modeling dataset from build_modeling_dataset()\n",
    "                     (loads and builds from scratch if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cv_results, leaderboard, baseline_metrics)\n",
    "    \"\"\"\n",
    "    # Load and preprocess data if not provided\n",
    "    if modeling_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "        logger.info(\"[train_models] Building modeling dataset...\")\n",
    "        modeling_df, _ = build_modeling_dataset(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            negative_policy='clamp_to_zero',\n",
    "            output_dir=config.preprocessing_dir()\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[train_models] Training on {len(modeling_df)} rows\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Compute adaptive CV settings based on shortest series\n",
    "    min_series_len = modeling_df.groupby(\"unique_id\").size().min()\n",
    "\n",
    "    # CV needs: horizon + (n_windows * step_size) rows minimum\n",
    "    # Solve for n_windows: n_windows = (min_series_len - horizon) / step_size\n",
    "    available_for_cv = min_series_len - config.horizon\n",
    "\n",
    "    # Adjust step_size and n_windows to fit data\n",
    "    step_size = min(config.cv_step_size, max(24, available_for_cv // 3))\n",
    "    n_windows = min(config.cv_windows, max(2, available_for_cv // step_size))\n",
    "\n",
    "    logger.info(\n",
    "        f\"[train_models] Adaptive CV: {n_windows} windows, \"\n",
    "        f\"step={step_size}h (min_series={min_series_len} rows)\"\n",
    "    )\n",
    "\n",
    "    # Cross-validation (modeling_df already has weather merged and time features added)\n",
    "    cv_results, leaderboard = model.cross_validate(\n",
    "        df=modeling_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    baseline = compute_baseline_metrics(cv_results, model_name=best_model)\n",
    "\n",
    "    logger.info(f\"[train_models] Best model: {best_model}, RMSE: {baseline['rmse_mean']:.1f}\")\n",
    "\n",
    "    return cv_results, leaderboard, baseline\n",
    "\n",
    "\n",
    "def train_interpretability_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> dict[str, InterpretabilityReport]:\n",
    "    \"\"\"Train LightGBM models and generate interpretability reports per series.\n",
    "\n",
    "    This trains a separate LightGBM model for each series (region × fuel type)\n",
    "    and generates SHAP, partial dependence, and feature importance artifacts.\n",
    "\n",
    "    Note: LightGBM is used for interpretability only. The primary forecasts\n",
    "    come from statistical models (MSTL/ARIMA) which provide better uncertainty\n",
    "    quantification.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping series_id -> InterpretabilityReport\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Training LightGBM for {generation_df['unique_id'].nunique()} series\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    reports: dict[str, InterpretabilityReport] = {}\n",
    "    output_dir = config.interpretability_dir()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for uid in sorted(generation_df[\"unique_id\"].unique()):\n",
    "        logger.info(f\"[train_interpretability] Processing {uid}...\")\n",
    "\n",
    "        # Extract series data\n",
    "        series_data = generation_df[generation_df[\"unique_id\"] == uid].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Prepare target series with proper frequency\n",
    "        y = series_data.set_index(\"ds\")[\"y\"]\n",
    "        y.index = pd.DatetimeIndex(y.index, freq=\"h\")  # Set hourly frequency\n",
    "\n",
    "        # Prepare exogenous features\n",
    "        region = uid.split(\"_\")[0]\n",
    "        series_weather = weather_df[weather_df[\"region\"] == region].copy()\n",
    "\n",
    "        if series_weather.empty:\n",
    "            logger.warning(f\"[train_interpretability] No weather data for region {region}, skipping {uid}\")\n",
    "            continue\n",
    "\n",
    "        # Merge weather to series timestamps\n",
    "        series_data = series_data.merge(\n",
    "            series_weather[[\"ds\"] + [c for c in WEATHER_VARS if c in series_weather.columns]],\n",
    "            on=\"ds\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Add time features\n",
    "        series_data = _add_time_features(series_data)\n",
    "\n",
    "        # Build exog DataFrame aligned with y\n",
    "        exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "        exog_cols += [c for c in WEATHER_VARS if c in series_data.columns]\n",
    "        exog = series_data.set_index(\"ds\")[exog_cols]\n",
    "\n",
    "        # Check for missing weather\n",
    "        missing_weather = exog.isna().any(axis=1).sum()\n",
    "        if missing_weather > 0:\n",
    "            logger.warning(f\"[train_interpretability] {uid}: {missing_weather} rows with missing weather, filling with ffill/bfill\")\n",
    "            exog = exog.ffill().bfill()\n",
    "\n",
    "        # Fit LightGBM forecaster\n",
    "        try:\n",
    "            if not INTERPRETABILITY_AVAILABLE:\n",
    "                logger.warning(f\"[train_interpretability] {uid}: lightgbm/skforecast not available, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Create skforecast ForecasterRecursive with LightGBM estimator\n",
    "            forecaster = ForecasterRecursive(\n",
    "                estimator=LGBMRegressor(\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                ),\n",
    "                lags=168,  # 7 days of lags\n",
    "            )\n",
    "            forecaster.fit(y=y, exog=exog)\n",
    "\n",
    "            # Create training matrices for SHAP analysis\n",
    "            X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "            # Generate interpretability report\n",
    "            series_output_dir = output_dir / uid\n",
    "            report = generate_full_interpretability_report(\n",
    "                forecaster=forecaster,\n",
    "                X_train=X_train,\n",
    "                series_id=uid,\n",
    "                output_dir=series_output_dir,\n",
    "                top_n_features=5,\n",
    "                shap_sample_frac=0.5,\n",
    "                shap_max_samples=1000,\n",
    "            )\n",
    "            reports[uid] = report\n",
    "\n",
    "            logger.info(\n",
    "                f\"[train_interpretability] {uid}: top_features={report.top_features[:3]}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[train_interpretability] {uid}: Failed to train - {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Generated {len(reports)} interpretability reports\")\n",
    "    return reports\n",
    "\n",
    "\n",
    "def generate_renewable_forecasts(\n",
    "    config: RenewablePipelineConfig,\n",
    "    modeling_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    "    best_model: str = \"MSTL_ARIMA\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 4: Generate forecasts with prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        modeling_df: Preprocessed modeling dataset (if None, loads and builds)\n",
    "        weather_df: Raw weather data with forecast (if None, loads from file)\n",
    "        best_model: Model to use for forecasting\n",
    "\n",
    "    Returns:\n",
    "        Forecast DataFrame with physical constraints applied\n",
    "    \"\"\"\n",
    "    output_path = config.forecasts_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load and preprocess data if not provided\n",
    "    if modeling_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "        if weather_df is None:\n",
    "            weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "        logger.info(\"[generate_forecasts] Building modeling dataset...\")\n",
    "        modeling_df, _ = build_modeling_dataset(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            negative_policy='clamp_to_zero',\n",
    "            output_dir=config.preprocessing_dir()\n",
    "        )\n",
    "\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generating {config.horizon}h forecasts \"\n",
    "        f\"using model={best_model}\"\n",
    "    )\n",
    "\n",
    "    # Ensure datetime types\n",
    "    modeling_df = modeling_df.copy()\n",
    "    modeling_df[\"ds\"] = pd.to_datetime(modeling_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Fit on preprocessed modeling data\n",
    "    model.fit(modeling_df)\n",
    "\n",
    "    # Prepare future exogenous features for forecasting\n",
    "    # We need weather + time features for the forecast horizon\n",
    "    per_series_max = modeling_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Per-series max timestamps:\\n\"\n",
    "        f\"{per_series_max.to_dict()}\"\n",
    "    )\n",
    "\n",
    "    min_of_max = per_series_max.min()\n",
    "    global_max = modeling_df[\"ds\"].max()\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Min of series maxes: {min_of_max}, \"\n",
    "        f\"Global max: {global_max}, \"\n",
    "        f\"Delta: {(global_max - min_of_max).total_seconds() / 3600:.1f}h\"\n",
    "    )\n",
    "\n",
    "    # Get future weather beyond the last training timestamp\n",
    "    future_weather = weather_df[weather_df[\"ds\"] > min_of_max].copy()\n",
    "\n",
    "    if future_weather.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[generate_forecasts] No future weather found after last \"\n",
    "            f\"training timestamp. min_of_max={min_of_max}\"\n",
    "        )\n",
    "\n",
    "    # Build future_exog by preparing timestamps and merging weather\n",
    "    unique_ids = modeling_df[\"unique_id\"].unique()\n",
    "    future_timestamps = pd.date_range(\n",
    "        start=min_of_max + pd.Timedelta(hours=1),\n",
    "        periods=config.horizon,\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    # Create future_exog with all series x timestamps combinations\n",
    "    future_exog = pd.DataFrame([\n",
    "        {\"unique_id\": uid, \"ds\": ts}\n",
    "        for uid in unique_ids\n",
    "        for ts in future_timestamps\n",
    "    ])\n",
    "\n",
    "    # Add region for weather merge\n",
    "    future_exog[\"region\"] = future_exog[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "    # Merge weather\n",
    "    available_weather_vars = [\n",
    "        c for c in WEATHER_VARS if c in future_weather.columns\n",
    "    ]\n",
    "    future_exog = future_exog.merge(\n",
    "        future_weather[[\"ds\", \"region\"] + available_weather_vars],\n",
    "        on=[\"ds\", \"region\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Check for missing weather\n",
    "    missing_weather = future_exog[available_weather_vars].isna().any(axis=1)\n",
    "    if missing_weather.any():\n",
    "        missing_count = missing_weather.sum()\n",
    "        logger.warning(\n",
    "            f\"[generate_forecasts] {missing_count} future rows missing \"\n",
    "            f\"weather, dropping them\"\n",
    "        )\n",
    "        future_exog = future_exog[~missing_weather].reset_index(drop=True)\n",
    "\n",
    "    # Add time features (same as dataset_builder)\n",
    "    future_exog[\"hour\"] = future_exog[\"ds\"].dt.hour\n",
    "    future_exog[\"dow\"] = future_exog[\"ds\"].dt.dayofweek\n",
    "    future_exog[\"hour_sin\"] = np.sin(2 * np.pi * future_exog[\"hour\"] / 24)\n",
    "    future_exog[\"hour_cos\"] = np.cos(2 * np.pi * future_exog[\"hour\"] / 24)\n",
    "    future_exog[\"dow_sin\"] = np.sin(2 * np.pi * future_exog[\"dow\"] / 7)\n",
    "    future_exog[\"dow_cos\"] = np.cos(2 * np.pi * future_exog[\"dow\"] / 7)\n",
    "    future_exog = future_exog.drop(columns=[\"hour\", \"dow\", \"region\"])\n",
    "\n",
    "    # Generate forecasts\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generating predictions using \"\n",
    "        f\"model: {best_model}\"\n",
    "    )\n",
    "    forecasts = model.predict(future_exog=future_exog, best_model=best_model)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generated {len(forecasts)} forecast rows \"\n",
    "        f\"for {forecasts['unique_id'].nunique()} series\"\n",
    "    )\n",
    "\n",
    "    forecasts.to_parquet(output_path, index=False)\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Saved: {output_path} ({len(forecasts)} rows)\"\n",
    "    )\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "def compute_renewable_drift(\n",
    "    predictions: pd.DataFrame,\n",
    "    actuals: pd.DataFrame,\n",
    "    baseline_metrics: dict,\n",
    ") -> dict:\n",
    "    \"\"\"Task 5: Detect drift by comparing current metrics to baseline.\n",
    "\n",
    "    Drift is flagged when current RMSE > baseline_mean + 2*baseline_std\n",
    "\n",
    "    Args:\n",
    "        predictions: Forecast DataFrame with [unique_id, ds, yhat]\n",
    "        actuals: Actual values DataFrame with [unique_id, ds, y]\n",
    "        baseline_metrics: Baseline from cross-validation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with drift status and details\n",
    "    \"\"\"\n",
    "    from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "    # Merge predictions with actuals\n",
    "    merged = predictions.merge(\n",
    "        actuals[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        return {\n",
    "            \"status\": \"no_data\",\n",
    "            \"message\": \"No overlapping data between predictions and actuals\",\n",
    "        }\n",
    "\n",
    "    # Compute current metrics\n",
    "    y_true = merged[\"y\"].values\n",
    "    y_pred = merged[\"yhat\"].values\n",
    "\n",
    "    current_rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "    current_mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "\n",
    "    # Check against threshold\n",
    "    threshold = baseline_metrics.get(\"drift_threshold_rmse\", float(\"inf\"))\n",
    "    is_drifting = current_rmse > threshold\n",
    "\n",
    "    result = {\n",
    "        \"status\": \"drift_detected\" if is_drifting else \"stable\",\n",
    "        \"current_rmse\": float(current_rmse),\n",
    "        \"current_mae\": float(current_mae),\n",
    "        \"baseline_rmse\": float(baseline_metrics.get(\"rmse_mean\", 0)),\n",
    "        \"drift_threshold\": float(threshold),\n",
    "        \"threshold_exceeded_by\": float(max(0, current_rmse - threshold)),\n",
    "        \"n_predictions\": len(merged),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    if is_drifting:\n",
    "        logger.warning(\n",
    "            f\"[drift] DRIFT DETECTED: RMSE={current_rmse:.1f} > threshold={threshold:.1f}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"[drift] Stable: RMSE={current_rmse:.1f} <= threshold={threshold:.1f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Run the complete renewable forecasting pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetch generation data\n",
    "    2. Fetch weather data\n",
    "    3. Train models (CV)\n",
    "    4. Generate forecasts\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pipeline results\n",
    "    \"\"\"\n",
    "    logger.info(f\"[pipeline] Starting: {config.start_date} to {config.end_date}\")\n",
    "    logger.info(f\"[pipeline] Regions: {config.regions}\")\n",
    "    logger.info(f\"[pipeline] Fuel types: {config.fuel_types}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Fetch generation\n",
    "    generation_df = fetch_renewable_data(config, fetch_diagnostics=fetch_diagnostics)\n",
    "    results[\"generation_rows\"] = len(generation_df)\n",
    "    results[\"series_count\"] = generation_df[\"unique_id\"].nunique()\n",
    "\n",
    "    from src.renewable.validation import validate_generation_df\n",
    "\n",
    "    expected_series = [f\"{r}_{f}\" for r in config.regions for f in config.fuel_types]\n",
    "    rep = validate_generation_df(\n",
    "        generation_df,\n",
    "        expected_series=expected_series,\n",
    "        max_missing_ratio=0.02,\n",
    "        max_lag_hours=48,  # choose a value consistent with EIA publishing lag\n",
    "    )\n",
    "    if not rep.ok:\n",
    "        raise RuntimeError(f\"[pipeline][generation_validation] {rep.message} details={rep.details}\")\n",
    "\n",
    "    # Step 2: Fetch weather\n",
    "    weather_df = fetch_renewable_weather(config)\n",
    "    results[\"weather_rows\"] = len(weather_df)\n",
    "\n",
    "    # Step 2.5: Build modeling-ready dataset with preprocessing\n",
    "    logger.info(\"[pipeline] Building modeling dataset (dataset_builder.py)\")\n",
    "\n",
    "    # Map config policy to dataset_builder policy\n",
    "    policy_map = {\n",
    "        \"clamp\": \"clamp_to_zero\",\n",
    "        \"fail_loud\": \"investigate\",\n",
    "        \"hybrid\": \"clamp_to_zero\",\n",
    "    }\n",
    "    negative_policy = policy_map.get(\n",
    "        config.negative_policy, \"clamp_to_zero\"\n",
    "    )\n",
    "\n",
    "    modeling_df, prep_report = build_modeling_dataset(\n",
    "        generation_df,\n",
    "        weather_df,\n",
    "        negative_policy=negative_policy,\n",
    "        max_missing_ratio=0.02,\n",
    "        output_dir=config.preprocessing_dir()\n",
    "    )\n",
    "\n",
    "    # Extract time and weather features from output_features\n",
    "    time_features = [\n",
    "        f for f in prep_report.output_features\n",
    "        if f in ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "    ]\n",
    "    weather_features = [\n",
    "        f for f in prep_report.output_features\n",
    "        if f in prep_report.weather_vars_used\n",
    "    ]\n",
    "\n",
    "    results[\"preprocessing\"] = {\n",
    "        \"rows_input\": prep_report.input_rows,\n",
    "        \"rows_output\": prep_report.output_rows,\n",
    "        \"series_dropped\": len(prep_report.series_dropped_incomplete),\n",
    "        \"negative_action\": prep_report.negative_report.action_taken,\n",
    "        \"time_features\": time_features,\n",
    "        \"weather_features\": weather_features,\n",
    "    }\n",
    "    logger.info(\n",
    "        f\"[pipeline] Preprocessing: {prep_report.input_rows:,} → \"\n",
    "        f\"{prep_report.output_rows:,} rows\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Train and validate (on preprocessed data)\n",
    "    cv_results, leaderboard, baseline = train_renewable_models(\n",
    "        config, modeling_df\n",
    "    )\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    results[\"best_model\"] = best_model\n",
    "    results[\"best_rmse\"] = float(leaderboard.iloc[0][\"rmse\"])\n",
    "    results[\"baseline\"] = baseline\n",
    "    # Save full leaderboard for dashboard display\n",
    "    results[\"leaderboard\"] = leaderboard.to_dict(orient=\"records\")\n",
    "\n",
    "    # Step 4: Generate forecasts (use the best model from CV)\n",
    "    # Pass weather_df for future weather (forecast horizon)\n",
    "    forecasts = generate_renewable_forecasts(\n",
    "        config, modeling_df, weather_df, best_model=best_model\n",
    "    )\n",
    "    results[\"forecast_rows\"] = len(forecasts)\n",
    "\n",
    "    # Step 5: Train LightGBM models and generate interpretability reports (optional)\n",
    "    # (LightGBM is for interpretability only - MSTL/ARIMA provide primary forecasts)\n",
    "    if config.enable_interpretability:\n",
    "        logger.info(\"[pipeline] Training interpretability models (LightGBM + SHAP)\")\n",
    "        try:\n",
    "            interpretability_reports = train_interpretability_models(\n",
    "                config, generation_df, weather_df\n",
    "            )\n",
    "            results[\"interpretability\"] = {\n",
    "                \"series_count\": len(interpretability_reports),\n",
    "                \"series\": list(interpretability_reports.keys()),\n",
    "                \"output_dir\": str(config.interpretability_dir()),\n",
    "            }\n",
    "\n",
    "            # Add top features summary per series\n",
    "            for uid, report in interpretability_reports.items():\n",
    "                results[\"interpretability\"][f\"{uid}_top_features\"] = report.top_features[:3]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[pipeline] Interpretability training failed (non-fatal): {e}\")\n",
    "            results[\"interpretability\"] = {\"error\": str(e)}\n",
    "    else:\n",
    "        logger.info(\"[pipeline] Interpretability disabled (enable_interpretability=False)\")\n",
    "        results[\"interpretability\"] = {\"enabled\": False}\n",
    "\n",
    "    if fetch_diagnostics is not None:\n",
    "        results[\"fetch_diagnostics\"] = fetch_diagnostics\n",
    "\n",
    "    logger.info(f\"[pipeline] Complete. Best model: {results['best_model']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entry point for renewable pipeline.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Renewable Energy Forecasting Pipeline\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Preset Examples:\n",
    "  # Fast development (24h forecast, 2 CV windows, 15 days lookback)\n",
    "  python -m src.renewable.tasks --preset 24h\n",
    "\n",
    "  # Standard forecasting (48h forecast, 3 CV windows, 21 days lookback)\n",
    "  python -m src.renewable.tasks --preset 48h\n",
    "\n",
    "  # Extended planning (72h forecast, 3 CV windows, 28 days lookback)\n",
    "  python -m src.renewable.tasks --preset 72h\n",
    "\n",
    "Custom Examples:\n",
    "  # 24h preset but only CALI region, skip interpretability\n",
    "  python -m src.renewable.tasks --preset 24h --regions CALI --no-interpretability\n",
    "\n",
    "  # Custom: 36h forecast with 4 CV windows\n",
    "  python -m src.renewable.tasks --horizon 36 --cv-windows 4 --lookback-days 30\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Preset system (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--preset\",\n",
    "        type=str,\n",
    "        choices=[\"24h\", \"48h\", \"72h\"],\n",
    "        help=\"Quick preset: 24h (fast dev), 48h (standard), 72h (extended planning)\",\n",
    "    )\n",
    "\n",
    "    # Flags (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--no-interpretability\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disable LightGBM interpretability analysis (speeds up pipeline)\",\n",
    "    )\n",
    "\n",
    "    # Data parameters (existing)\n",
    "    parser.add_argument(\n",
    "        \"--regions\",\n",
    "        type=str,\n",
    "        help=\"Override regions (comma-separated, e.g., CALI,ERCO,MISO)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuel\",\n",
    "        type=str,\n",
    "        help=\"Override fuel types (comma-separated, e.g., WND,SUN)\",\n",
    "    )\n",
    "\n",
    "    # Forecast parameters (existing + new)\n",
    "    parser.add_argument(\n",
    "        \"--horizon\",\n",
    "        type=int,\n",
    "        help=\"Override forecast horizon in hours\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lookback-days\",\n",
    "        type=int,\n",
    "        help=\"Override lookback days\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cv-windows\",\n",
    "        type=int,\n",
    "        help=\"Override CV windows count\",\n",
    "    )\n",
    "\n",
    "    # Output parameters\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing data files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"data/renewable\",\n",
    "        help=\"Output directory (default: data/renewable)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Build config with preset support\n",
    "    if args.preset:\n",
    "        # Apply preset defaults\n",
    "        logger.info(f\"[CLI] Applying preset: {args.preset}\")\n",
    "        config = RenewablePipelineConfig(\n",
    "            horizon_preset=args.preset,  # This triggers __post_init__ to apply preset\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "        # Allow CLI overrides of preset values\n",
    "        if args.horizon is not None:\n",
    "            object.__setattr__(config, \"horizon\", args.horizon)\n",
    "            logger.info(f\"[CLI] Override: horizon={args.horizon}h\")\n",
    "        if args.lookback_days is not None:\n",
    "            object.__setattr__(config, \"lookback_days\", args.lookback_days)\n",
    "            logger.info(f\"[CLI] Override: lookback_days={args.lookback_days}\")\n",
    "        if args.cv_windows is not None:\n",
    "            object.__setattr__(config, \"cv_windows\", args.cv_windows)\n",
    "            logger.info(f\"[CLI] Override: cv_windows={args.cv_windows}\")\n",
    "\n",
    "    else:\n",
    "        # No preset: use explicit values or defaults\n",
    "        config = RenewablePipelineConfig(\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            lookback_days=args.lookback_days if args.lookback_days else 30,\n",
    "            horizon=args.horizon if args.horizon else 24,\n",
    "            cv_windows=args.cv_windows if args.cv_windows else 5,\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "    # Run pipeline\n",
    "    results = run_full_pipeline(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Series count: {results['series_count']}\")\n",
    "    print(f\"  Generation rows: {results['generation_rows']}\")\n",
    "    print(f\"  Weather rows: {results['weather_rows']}\")\n",
    "    print(f\"  Forecast rows: {results['forecast_rows']}\")\n",
    "    print(f\"  Best model: {results['best_model']}\")\n",
    "    print(f\"  Best RMSE: {results['best_rmse']:.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite persistence layer\n",
    "\n",
    "Extends the monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/db.py\n",
    "# file: src/renewable/db.py\n",
    "\"\"\"Database schema and operations for renewable forecasting.\n",
    "\n",
    "Extends the Chapter 4 monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    \"\"\"Connect to SQLite database with optimized settings.\"\"\"\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def init_renewable_db(db_path: str) -> None:\n",
    "    \"\"\"Initialize renewable forecasting database schema.\n",
    "\n",
    "    Creates tables:\n",
    "    - renewable_forecasts: Forecasts with dual intervals\n",
    "    - renewable_scores: Evaluation metrics with coverage\n",
    "    - weather_features: Weather data by region\n",
    "    - drift_alerts: Drift detection history\n",
    "    - baseline_metrics: Backtest baselines for drift thresholds\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Forecasts with dual prediction intervals\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_forecasts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        yhat_lo_80 REAL,\n",
    "        yhat_hi_80 REAL,\n",
    "        yhat_lo_95 REAL,\n",
    "        yhat_hi_95 REAL,\n",
    "        UNIQUE (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Index for efficient queries\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_region_ds\n",
    "    ON renewable_forecasts (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_fuel_ds\n",
    "    ON renewable_forecasts (fuel_type, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Evaluation scores with dual coverage\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_scores (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        scored_at TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mae REAL,\n",
    "        coverage_80 REAL,\n",
    "        coverage_95 REAL,\n",
    "        valid_rows INTEGER,\n",
    "        UNIQUE (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Weather features by region\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_features (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        region TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        temperature_2m REAL,\n",
    "        wind_speed_10m REAL,\n",
    "        wind_speed_100m REAL,\n",
    "        wind_direction_10m REAL,\n",
    "        direct_radiation REAL,\n",
    "        diffuse_radiation REAL,\n",
    "        cloud_cover REAL,\n",
    "        is_forecast INTEGER DEFAULT 0,\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE (region, ds, is_forecast)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_weather_region_ds\n",
    "    ON weather_features (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Drift detection alerts\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS drift_alerts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        alert_at TEXT NOT NULL,\n",
    "        run_id TEXT,\n",
    "        unique_id TEXT,\n",
    "        region TEXT,\n",
    "        fuel_type TEXT,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        current_rmse REAL,\n",
    "        threshold_rmse REAL,\n",
    "        message TEXT,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_drift_alerts_time\n",
    "    ON drift_alerts (alert_at);\n",
    "    \"\"\")\n",
    "\n",
    "    # Baseline metrics for drift detection\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS baseline_metrics (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        rmse_mean REAL NOT NULL,\n",
    "        rmse_std REAL NOT NULL,\n",
    "        mae_mean REAL,\n",
    "        mae_std REAL,\n",
    "        drift_threshold_rmse REAL NOT NULL,\n",
    "        drift_threshold_mae REAL,\n",
    "        n_windows INTEGER,\n",
    "        metadata_json TEXT,\n",
    "        UNIQUE (unique_id, model)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_forecasts(\n",
    "    db_path: str,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    run_id: str,\n",
    "    model: str = \"MSTL_ARIMA\",\n",
    ") -> int:\n",
    "    \"\"\"Save forecasts to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        forecasts_df: DataFrame with [unique_id, ds, yhat, yhat_lo_80, ...]\n",
    "        run_id: Pipeline run identifier\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    created_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in forecasts_df.iterrows():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        parts = unique_id.split(\"_\")\n",
    "        region = parts[0] if len(parts) > 0 else \"\"\n",
    "        fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        rows.append((\n",
    "            run_id,\n",
    "            created_at,\n",
    "            unique_id,\n",
    "            region,\n",
    "            fuel_type,\n",
    "            str(row[\"ds\"]),\n",
    "            model,\n",
    "            row.get(\"yhat\"),\n",
    "            row.get(\"yhat_lo_80\"),\n",
    "            row.get(\"yhat_hi_80\"),\n",
    "            row.get(\"yhat_lo_95\"),\n",
    "            row.get(\"yhat_hi_95\"),\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO renewable_forecasts\n",
    "        (run_id, created_at, unique_id, region, fuel_type, ds, model,\n",
    "         yhat, yhat_lo_80, yhat_hi_80, yhat_lo_95, yhat_hi_95)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_weather(\n",
    "    db_path: str,\n",
    "    weather_df: pd.DataFrame,\n",
    "    is_forecast: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"Save weather features to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        weather_df: DataFrame with [ds, region, weather_vars...]\n",
    "        is_forecast: True if this is forecast weather data\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temperature_2m\", \"wind_speed_10m\", \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\", \"direct_radiation\", \"diffuse_radiation\", \"cloud_cover\"\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in weather_df.iterrows():\n",
    "        values = [row.get(col) for col in weather_cols]\n",
    "        rows.append((\n",
    "            row[\"region\"],\n",
    "            str(row[\"ds\"]),\n",
    "            *values,\n",
    "            1 if is_forecast else 0,\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(f\"\"\"\n",
    "        INSERT OR REPLACE INTO weather_features\n",
    "        (region, ds, {', '.join(weather_cols)}, is_forecast)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_drift_alert(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    unique_id: str,\n",
    "    current_rmse: float,\n",
    "    threshold_rmse: float,\n",
    "    severity: str = \"warning\",\n",
    "    metadata: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Save drift detection alert.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        run_id: Pipeline run identifier\n",
    "        unique_id: Series identifier\n",
    "        current_rmse: Current RMSE value\n",
    "        threshold_rmse: Drift threshold\n",
    "        severity: Alert severity (info, warning, critical)\n",
    "        metadata: Additional metadata\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    parts = unique_id.split(\"_\")\n",
    "    region = parts[0] if len(parts) > 0 else \"\"\n",
    "    fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    alert_type = \"drift_detected\" if current_rmse > threshold_rmse else \"drift_check\"\n",
    "    message = (\n",
    "        f\"RMSE {current_rmse:.1f} {'>' if current_rmse > threshold_rmse else '<='} \"\n",
    "        f\"threshold {threshold_rmse:.1f}\"\n",
    "    )\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO drift_alerts\n",
    "        (alert_at, run_id, unique_id, region, fuel_type, alert_type, severity,\n",
    "         current_rmse, threshold_rmse, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        run_id,\n",
    "        unique_id,\n",
    "        region,\n",
    "        fuel_type,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        current_rmse,\n",
    "        threshold_rmse,\n",
    "        message,\n",
    "        json.dumps(metadata) if metadata else None,\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_baseline(\n",
    "    db_path: str,\n",
    "    unique_id: str,\n",
    "    model: str,\n",
    "    baseline: dict,\n",
    ") -> None:\n",
    "    \"\"\"Save baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        unique_id: Series identifier\n",
    "        model: Model name\n",
    "        baseline: Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO baseline_metrics\n",
    "        (created_at, unique_id, model, rmse_mean, rmse_std, mae_mean, mae_std,\n",
    "         drift_threshold_rmse, drift_threshold_mae, n_windows, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        unique_id,\n",
    "        model,\n",
    "        baseline.get(\"rmse_mean\"),\n",
    "        baseline.get(\"rmse_std\"),\n",
    "        baseline.get(\"mae_mean\"),\n",
    "        baseline.get(\"mae_std\"),\n",
    "        baseline.get(\"drift_threshold_rmse\"),\n",
    "        baseline.get(\"drift_threshold_mae\"),\n",
    "        baseline.get(\"n_windows\"),\n",
    "        json.dumps(baseline),\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def get_recent_forecasts(\n",
    "    db_path: str,\n",
    "    region: Optional[str] = None,\n",
    "    fuel_type: Optional[str] = None,\n",
    "    hours: int = 48,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent forecasts from database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        region: Filter by region (optional)\n",
    "        fuel_type: Filter by fuel type (optional)\n",
    "        hours: Hours of history to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with forecasts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM renewable_forecasts\n",
    "        WHERE datetime(created_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if region:\n",
    "        query += \" AND region = ?\"\n",
    "        params.append(region)\n",
    "\n",
    "    if fuel_type:\n",
    "        query += \" AND fuel_type = ?\"\n",
    "        params.append(fuel_type)\n",
    "\n",
    "    query += \" ORDER BY ds DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_drift_alerts(\n",
    "    db_path: str,\n",
    "    hours: int = 24,\n",
    "    severity: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent drift alerts.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        hours: Hours of history\n",
    "        severity: Filter by severity (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with alerts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM drift_alerts\n",
    "        WHERE datetime(alert_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if severity:\n",
    "        query += \" AND severity = ?\"\n",
    "        params.append(severity)\n",
    "\n",
    "    query += \" ORDER BY alert_at DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test database initialization\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        db_path = f\"{tmpdir}/test_renewable.db\"\n",
    "\n",
    "        print(\"Initializing database...\")\n",
    "        init_renewable_db(db_path)\n",
    "\n",
    "        print(\"Database initialized successfully!\")\n",
    "\n",
    "        # Test connection\n",
    "        con = connect(db_path)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"Tables created: {[t[0] for t in tables]}\")\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8: Dashboard\n",
    "\n",
    "**File:** `src/renewable/dashboard.py`\n",
    "\n",
    "The Streamlit dashboard provides:\n",
    "- **Forecast visualization** with prediction intervals\n",
    "- **Drift monitoring** and alerts\n",
    "- **Coverage analysis** (nominal vs empirical)\n",
    "- **Weather features** by region\n",
    "\n",
    "## Running the Dashboard\n",
    "\n",
    "```bash\n",
    "streamlit run src/renewable/dashboard.py\n",
    "```\n",
    "\n",
    "The dashboard will:\n",
    "1. Load forecasts from `data/renewable/forecasts.parquet`\n",
    "2. Display interactive charts with Plotly\n",
    "3. Show drift alerts from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dashboard.py\n",
    "# file: src/renewable/dashboard.py\n",
    "\"\"\"Streamlit dashboard for renewable energy forecasting.\n",
    "\n",
    "Provides:\n",
    "- Forecast visualization with prediction intervals\n",
    "- Drift monitoring and alerts\n",
    "- Coverage analysis (nominal vs empirical)\n",
    "- Weather features by region\n",
    "\n",
    "Run with:\n",
    "    streamlit run src/renewable/dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from src.renewable.db import (\n",
    "    connect,\n",
    "    get_drift_alerts,\n",
    "    get_recent_forecasts,\n",
    "    init_renewable_db,\n",
    ")\n",
    "from src.renewable.regions import FUEL_TYPES, REGIONS\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Renewable Forecast Dashboard\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main dashboard application.\"\"\"\n",
    "    st.title(\"⚡ Renewable Energy Forecast Dashboard\")\n",
    "    st.markdown(\"Next-24h wind/solar generation forecasts with drift monitoring\")\n",
    "\n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "\n",
    "        db_path = st.text_input(\n",
    "            \"Database Path\",\n",
    "            value=\"data/renewable/renewable.db\",\n",
    "        )\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not Path(db_path).exists():\n",
    "            init_renewable_db(db_path)\n",
    "            st.info(\"Database initialized\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Region filter\n",
    "        all_regions = list(REGIONS.keys())\n",
    "        selected_regions = st.multiselect(\n",
    "            \"Regions\",\n",
    "            options=all_regions,\n",
    "            default=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        )\n",
    "\n",
    "        # Fuel type filter\n",
    "        fuel_type = st.selectbox(\n",
    "            \"Fuel Type\",\n",
    "            options=[\"WND\", \"SUN\", \"Both\"],\n",
    "            index=0,\n",
    "        )\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Actions\n",
    "        show_debug = st.checkbox(\"Show Debug\", value=False)\n",
    "        if st.button(\"🔄 Refresh Data\", width=\"stretch\"):\n",
    "            st.rerun()\n",
    "\n",
    "        if st.button(\"📊 Run Pipeline\", width=\"stretch\"):\n",
    "            run_pipeline_from_dashboard(db_path, selected_regions, fuel_type)\n",
    "\n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
    "        \"📈 Forecasts\",\n",
    "        \"⚠️ Drift Monitor\",\n",
    "        \"📊 Coverage\",\n",
    "        \"🌤️ Weather\",\n",
    "        \"🔍 Interpretability\",\n",
    "    ])\n",
    "\n",
    "    with tab1:\n",
    "        render_forecasts_tab(db_path, selected_regions, fuel_type, show_debug=show_debug)\n",
    "\n",
    "    with tab2:\n",
    "        render_drift_tab(db_path)\n",
    "\n",
    "    with tab3:\n",
    "        render_coverage_tab(db_path)\n",
    "\n",
    "    with tab4:\n",
    "        render_weather_tab(db_path, selected_regions)\n",
    "\n",
    "    with tab5:\n",
    "        render_interpretability_tab(selected_regions, fuel_type)\n",
    "\n",
    "\n",
    "def render_forecasts_tab(db_path: str, regions: list, fuel_type: str, *, show_debug: bool = False):\n",
    "    \"\"\"Render forecast visualization with prediction intervals.\"\"\"\n",
    "    st.subheader(\"Generation Forecasts\")\n",
    "\n",
    "    forecasts_df = pd.DataFrame()\n",
    "    data_source = \"none\"\n",
    "    derived_columns: list[str] = []\n",
    "\n",
    "    # Try to load from parquet file first (pipeline output)\n",
    "    parquet_path = Path(\"data/renewable/forecasts.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            forecasts_df = pd.read_parquet(parquet_path)\n",
    "            data_source = f\"parquet:{parquet_path}\"\n",
    "            # Add region/fuel_type columns if missing\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                parts = forecasts_df[\"unique_id\"].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "                if \"region\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"region\"] = parts[0]\n",
    "                    derived_columns.append(\"region\")\n",
    "                if \"fuel_type\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"fuel_type\"] = parts[1] if parts.shape[1] > 1 else pd.NA\n",
    "                    derived_columns.append(\"fuel_type\")\n",
    "            st.success(f\"Loaded {len(forecasts_df)} forecasts from pipeline\")\n",
    "\n",
    "            # Calculate and display data freshness\n",
    "            if not forecasts_df.empty and \"ds\" in forecasts_df.columns:\n",
    "                earliest_forecast_ts = forecasts_df[\"ds\"].min()\n",
    "                now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "\n",
    "                # Forecasts start from last_data + 1h, so last_data = earliest_forecast - 1h\n",
    "                last_data_ts = earliest_forecast_ts - pd.Timedelta(hours=1)\n",
    "\n",
    "                # Ensure both timestamps are timezone-aware for comparison\n",
    "                if not hasattr(last_data_ts, 'tz') or last_data_ts.tz is None:\n",
    "                    last_data_ts = pd.Timestamp(last_data_ts, tz=\"UTC\")\n",
    "\n",
    "                data_age_hours = (now_utc - last_data_ts).total_seconds() / 3600\n",
    "\n",
    "                # Show warning if data is > 6 hours old\n",
    "                if data_age_hours > 6:\n",
    "                    st.warning(\n",
    "                        f\"⚠️ Forecasts are based on **{data_age_hours:.1f} hour old** data \"\n",
    "                        f\"(last EIA data: {last_data_ts.strftime('%b %d %H:%M')} UTC). \"\n",
    "                        f\"Click 'Refresh Forecasts' button in sidebar to update.\"\n",
    "                    )\n",
    "                else:\n",
    "                    st.info(\n",
    "                        f\"✅ Forecasts from {last_data_ts.strftime('%b %d %H:%M')} UTC data \"\n",
    "                        f\"({data_age_hours:.1f}h old)\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load parquet: {e}\")\n",
    "\n",
    "    # Fall back to database\n",
    "    if forecasts_df.empty:\n",
    "        try:\n",
    "            forecasts_df = get_recent_forecasts(db_path, hours=72)\n",
    "            data_source = f\"db:{db_path}\"\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load from database: {e}\")\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        # Show demo data\n",
    "        st.info(\"No forecasts found. Showing demo data.\")\n",
    "        forecasts_df = generate_demo_forecasts(regions, fuel_type)\n",
    "        data_source = \"demo\"\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Forecast Data\", expanded=False):\n",
    "            st.markdown(\"**Source**\")\n",
    "            st.code(data_source)\n",
    "            st.markdown(\"**Columns**\")\n",
    "            st.code(\", \".join(forecasts_df.columns.tolist()))\n",
    "\n",
    "            st.markdown(\"**Counts (pre-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "\n",
    "            if derived_columns:\n",
    "                st.markdown(\"**Derived Columns**\")\n",
    "                st.write(derived_columns)\n",
    "\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id sample**\")\n",
    "                st.write(forecasts_df[\"unique_id\"].dropna().astype(str).head(10).tolist())\n",
    "\n",
    "            if \"fuel_type\" in forecasts_df.columns:\n",
    "                st.markdown(\"**fuel_type counts**\")\n",
    "                st.dataframe(forecasts_df[\"fuel_type\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "                unknown = sorted(\n",
    "                    {str(v) for v in forecasts_df[\"fuel_type\"].dropna().unique()}\n",
    "                    - set(FUEL_TYPES.keys())\n",
    "                )\n",
    "                if unknown:\n",
    "                    st.warning(f\"Unknown fuel_type values: {unknown}\")\n",
    "\n",
    "            if \"region\" in forecasts_df.columns:\n",
    "                st.markdown(\"**region counts**\")\n",
    "                st.dataframe(forecasts_df[\"region\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "    # Filter by selections\n",
    "    if fuel_type != \"Both\":\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"fuel_type\"] == fuel_type]\n",
    "\n",
    "    if regions:\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"region\"].isin(regions)]\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Filter Result\", expanded=False):\n",
    "            st.markdown(\"**Applied Filters**\")\n",
    "            st.write({\"fuel_type\": fuel_type, \"regions\": regions})\n",
    "            st.markdown(\"**Counts (post-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id after filter**\")\n",
    "                st.write(sorted(forecasts_df[\"unique_id\"].dropna().astype(str).unique().tolist()))\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        st.warning(\"No data matching filters\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    series_options = forecasts_df[\"unique_id\"].unique().tolist()\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=series_options,\n",
    "        index=0 if series_options else None,\n",
    "        key=\"forecast_series_select\",\n",
    "    )\n",
    "\n",
    "    if selected_series:\n",
    "        series_data = forecasts_df[forecasts_df[\"unique_id\"] == selected_series].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Convert to local timezone for display\n",
    "        region_code = series_data[\"unique_id\"].iloc[0].split(\"_\")[0]\n",
    "        region_info = REGIONS.get(region_code)\n",
    "        timezone_name = region_info.timezone if region_info else \"UTC\"\n",
    "\n",
    "        # Create forecast plot with intervals\n",
    "        fig = create_forecast_plot(series_data, selected_series, timezone_name)\n",
    "        st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "        # Show data table\n",
    "        with st.expander(\"View Data\"):\n",
    "            st.dataframe(\n",
    "                series_data[[\"ds\", \"yhat\", \"yhat_lo_80\", \"yhat_hi_80\", \"yhat_lo_95\", \"yhat_hi_95\"]],\n",
    "                width=\"stretch\",\n",
    "            )\n",
    "\n",
    "\n",
    "def create_forecast_plot(df: pd.DataFrame, title: str, timezone_name: str = \"UTC\") -> go.Figure:\n",
    "    \"\"\"Create Plotly figure with forecast and prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Forecast dataframe with ds (timestamp), yhat, and interval columns\n",
    "        title: Series name for chart title\n",
    "        timezone_name: IANA timezone name for display (e.g., \"America/Chicago\")\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert timestamps to local timezone for display\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "    # Convert UTC to local timezone\n",
    "    if timezone_name != \"UTC\":\n",
    "        df[\"ds\"] = df[\"ds\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone_name)\n",
    "\n",
    "    # Get timezone abbreviation for display (e.g., \"CST\", \"PST\")\n",
    "    if timezone_name != \"UTC\" and len(df) > 0:\n",
    "        tz_abbr = df[\"ds\"].iloc[0].strftime(\"%Z\")\n",
    "    else:\n",
    "        tz_abbr = \"UTC\"\n",
    "\n",
    "    # 95% interval (outer, lighter)\n",
    "    if \"yhat_lo_95\" in df.columns and \"yhat_hi_95\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_95\"], df[\"yhat_lo_95\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"95% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # 80% interval (inner, darker)\n",
    "    if \"yhat_lo_80\" in df.columns and \"yhat_hi_80\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_80\"], df[\"yhat_lo_80\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.4)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"80% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # Point forecast\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"ds\"],\n",
    "        y=df[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"#1f77b4\", width=2),\n",
    "    ))\n",
    "\n",
    "    # Actuals if available\n",
    "    if \"y\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Actual\",\n",
    "            marker=dict(color=\"#2ca02c\", size=6),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast: {title}\",\n",
    "        xaxis_title=f\"Time ({tz_abbr})\",\n",
    "        yaxis_title=\"Generation (MWh)\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "        height=450,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def render_drift_tab(db_path: str):\n",
    "    \"\"\"Render drift monitoring and alerts.\"\"\"\n",
    "    st.subheader(\"Drift Detection\")\n",
    "\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Try to load alerts\n",
    "    try:\n",
    "        alerts_df = get_drift_alerts(db_path, hours=48)\n",
    "    except Exception:\n",
    "        alerts_df = pd.DataFrame()\n",
    "\n",
    "    # Summary metrics\n",
    "    with col1:\n",
    "        critical = len(alerts_df[alerts_df[\"severity\"] == \"critical\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\n",
    "            \"Critical Alerts\",\n",
    "            critical,\n",
    "            delta=None,\n",
    "            delta_color=\"inverse\" if critical > 0 else \"off\",\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "        warning = len(alerts_df[alerts_df[\"severity\"] == \"warning\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Warnings\", warning)\n",
    "\n",
    "    with col3:\n",
    "        stable = len(alerts_df[alerts_df[\"alert_type\"] == \"drift_check\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Stable Checks\", stable)\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    if alerts_df.empty:\n",
    "        st.info(\"No drift alerts in the last 48 hours. System is stable.\")\n",
    "\n",
    "        # Show demo drift status\n",
    "        st.markdown(\"### Demo Drift Status\")\n",
    "        demo_drift = pd.DataFrame({\n",
    "            \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "            \"Current RMSE\": [125.3, 98.7, 156.2, 45.1, 67.8],\n",
    "            \"Threshold\": [150.0, 120.0, 180.0, 60.0, 80.0],\n",
    "            \"Status\": [\"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\"],\n",
    "        })\n",
    "        st.dataframe(demo_drift, width=\"stretch\")\n",
    "    else:\n",
    "        # Show alerts table\n",
    "        st.dataframe(\n",
    "            alerts_df[[\"alert_at\", \"unique_id\", \"severity\", \"current_rmse\", \"threshold_rmse\", \"message\"]],\n",
    "            width=\"stretch\",\n",
    "        )\n",
    "\n",
    "        # Drift timeline\n",
    "        if len(alerts_df) > 1:\n",
    "            alerts_df[\"alert_at\"] = pd.to_datetime(alerts_df[\"alert_at\"])\n",
    "            fig = px.scatter(\n",
    "                alerts_df,\n",
    "                x=\"alert_at\",\n",
    "                y=\"current_rmse\",\n",
    "                color=\"severity\",\n",
    "                size=\"current_rmse\",\n",
    "                hover_data=[\"unique_id\", \"message\"],\n",
    "                title=\"Drift Timeline\",\n",
    "            )\n",
    "            fig.add_hline(\n",
    "                y=alerts_df[\"threshold_rmse\"].mean(),\n",
    "                line_dash=\"dash\",\n",
    "                annotation_text=\"Avg Threshold\",\n",
    "            )\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_coverage_tab(db_path: str):\n",
    "    \"\"\"Render coverage analysis comparing nominal vs empirical.\"\"\"\n",
    "    st.subheader(\"Prediction Interval Coverage\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    **Coverage** measures how often actual values fall within prediction intervals.\n",
    "    - **Nominal**: The expected coverage (80% or 95%)\n",
    "    - **Empirical**: The actual observed coverage\n",
    "    - **Gap**: Difference indicates calibration quality\n",
    "    \"\"\")\n",
    "\n",
    "    # Demo coverage data\n",
    "    coverage_data = pd.DataFrame({\n",
    "        \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"SWPP_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "        \"Nominal 80%\": [80, 80, 80, 80, 80, 80],\n",
    "        \"Empirical 80%\": [78.5, 82.1, 76.3, 79.8, 81.2, 77.9],\n",
    "        \"Nominal 95%\": [95, 95, 95, 95, 95, 95],\n",
    "        \"Empirical 95%\": [93.2, 96.1, 91.5, 94.8, 95.7, 92.3],\n",
    "    })\n",
    "\n",
    "    coverage_data[\"Gap 80%\"] = coverage_data[\"Empirical 80%\"] - coverage_data[\"Nominal 80%\"]\n",
    "    coverage_data[\"Gap 95%\"] = coverage_data[\"Empirical 95%\"] - coverage_data[\"Nominal 95%\"]\n",
    "\n",
    "    # Summary\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        avg_80 = coverage_data[\"Empirical 80%\"].mean()\n",
    "        st.metric(\"Avg 80% Coverage\", f\"{avg_80:.1f}%\", f\"{avg_80 - 80:.1f}%\")\n",
    "\n",
    "    with col2:\n",
    "        avg_95 = coverage_data[\"Empirical 95%\"].mean()\n",
    "        st.metric(\"Avg 95% Coverage\", f\"{avg_95:.1f}%\", f\"{avg_95 - 95:.1f}%\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Coverage comparison chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"80% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 80%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.7)\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"95% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 95%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.4)\",\n",
    "    ))\n",
    "\n",
    "    # Nominal lines\n",
    "    fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"80% Nominal\")\n",
    "    fig.add_hline(y=95, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"95% Nominal\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coverage by Series\",\n",
    "        xaxis_title=\"Series\",\n",
    "        yaxis_title=\"Coverage (%)\",\n",
    "        barmode=\"group\",\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Detailed table\n",
    "    with st.expander(\"View Coverage Data\"):\n",
    "        st.dataframe(coverage_data, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_weather_tab(db_path: str, regions: list):\n",
    "    \"\"\"Render weather features visualization.\"\"\"\n",
    "    st.subheader(\"Weather Features\")\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "    # Prefer real pipeline output; no demo fallback.\n",
    "    parquet_path = Path(\"data/renewable/weather.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            weather_df = pd.read_parquet(parquet_path)\n",
    "            st.success(f\"Loaded {len(weather_df)} weather rows from pipeline\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather parquet: {exc}\")\n",
    "\n",
    "    if weather_df.empty and Path(db_path).exists():\n",
    "        try:\n",
    "            with connect(db_path) as con:\n",
    "                weather_df = pd.read_sql_query(\n",
    "                    \"SELECT * FROM weather_features ORDER BY ds ASC\",\n",
    "                    con,\n",
    "                )\n",
    "            if not weather_df.empty:\n",
    "                st.success(f\"Loaded {len(weather_df)} weather rows from database\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather data from database: {exc}\")\n",
    "\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data available. Run the pipeline to populate weather features.\")\n",
    "        return\n",
    "\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"coerce\")\n",
    "    if regions:\n",
    "        weather_df = weather_df[weather_df[\"region\"].isin(regions)]\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data matching selected regions.\")\n",
    "        return\n",
    "\n",
    "    # Variable selector\n",
    "    weather_vars = [\n",
    "        col for col in [\"wind_speed_10m\", \"wind_speed_100m\", \"direct_radiation\", \"cloud_cover\"]\n",
    "        if col in weather_df.columns\n",
    "    ]\n",
    "    if not weather_vars:\n",
    "        st.warning(\"Weather data missing expected variables.\")\n",
    "        return\n",
    "    selected_var = st.selectbox(\"Weather Variable\", options=weather_vars)\n",
    "\n",
    "    # Plot\n",
    "    fig = px.line(\n",
    "        weather_df,\n",
    "        x=\"ds\",\n",
    "        y=selected_var,\n",
    "        color=\"region\",\n",
    "        title=f\"{selected_var} by Region\",\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Summary stats\n",
    "    st.markdown(\"### Current Conditions\")\n",
    "\n",
    "    cols = st.columns(len(regions[:4]))\n",
    "    for i, region in enumerate(regions[:4]):\n",
    "        if i < len(cols):\n",
    "            with cols[i]:\n",
    "                region_data = weather_df[weather_df[\"region\"] == region].iloc[-1] if len(weather_df[weather_df[\"region\"] == region]) > 0 else {}\n",
    "                st.metric(\n",
    "                    region,\n",
    "                    f\"{region_data.get('wind_speed_10m', 0):.1f} m/s\",\n",
    "                    help=\"Wind speed at 10m\",\n",
    "                )\n",
    "\n",
    "\n",
    "def render_interpretability_tab(regions: list, fuel_type: str):\n",
    "    \"\"\"Render model interpretability visualizations (SHAP, feature importance, PDP).\"\"\"\n",
    "    st.subheader(\"Model Interpretability\")\n",
    "\n",
    "    # Model Leaderboard Section\n",
    "    st.markdown(\"### 🏆 Model Leaderboard (Cross-Validation)\")\n",
    "\n",
    "    # Model descriptions for education\n",
    "    MODEL_INFO = {\n",
    "        \"AutoARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Auto-tuned ARIMA with automatic p,d,q selection. Good for univariate series with trend/seasonality.\",\n",
    "            \"strengths\": \"Robust, well-understood, good prediction intervals\",\n",
    "        },\n",
    "        \"MSTL_ARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Multiple Seasonal-Trend decomposition + ARIMA. Handles daily (24h) and weekly (168h) seasonality.\",\n",
    "            \"strengths\": \"Best for multi-seasonal patterns like energy data\",\n",
    "        },\n",
    "        \"AutoETS\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Exponential smoothing with automatic error/trend/season selection.\",\n",
    "            \"strengths\": \"Simple, fast, works well for smooth series\",\n",
    "        },\n",
    "        \"AutoTheta\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Theta method with automatic decomposition. Robust to outliers.\",\n",
    "            \"strengths\": \"Competition winner (M3), handles level shifts\",\n",
    "        },\n",
    "        \"CES\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Complex Exponential Smoothing. Captures complex seasonal patterns.\",\n",
    "            \"strengths\": \"Good for complex seasonality\",\n",
    "        },\n",
    "        \"SeasonalNaive\": {\n",
    "            \"type\": \"Baseline\",\n",
    "            \"description\": \"Uses value from same hour last week. Baseline benchmark.\",\n",
    "            \"strengths\": \"Simple benchmark - if beaten, models add value\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "    if run_log_path.exists():\n",
    "        try:\n",
    "            import json\n",
    "            run_log = json.loads(run_log_path.read_text())\n",
    "            pipeline_results = run_log.get(\"pipeline_results\", {})\n",
    "            leaderboard_data = pipeline_results.get(\"leaderboard\", [])\n",
    "\n",
    "            if leaderboard_data:\n",
    "                leaderboard_df = pd.DataFrame(leaderboard_data)\n",
    "                best_model = pipeline_results.get(\"best_model\", \"\")\n",
    "                best_rmse = pipeline_results.get(\"best_rmse\", 0)\n",
    "\n",
    "                # Key metrics row\n",
    "                col1, col2, col3, col4 = st.columns(4)\n",
    "                with col1:\n",
    "                    st.metric(\"Best Model\", best_model)\n",
    "                with col2:\n",
    "                    st.metric(\"Best RMSE\", f\"{best_rmse:.3f}\")\n",
    "                with col3:\n",
    "                    st.metric(\"Models Evaluated\", len(leaderboard_data))\n",
    "                with col4:\n",
    "                    # Calculate improvement over baseline\n",
    "                    baseline_rmse = leaderboard_df[leaderboard_df[\"model\"] == \"SeasonalNaive\"][\"rmse\"].values\n",
    "                    if len(baseline_rmse) > 0 and best_rmse > 0:\n",
    "                        improvement = ((baseline_rmse[0] - best_rmse) / baseline_rmse[0]) * 100\n",
    "                        st.metric(\"vs Baseline\", f\"{improvement:+.1f}%\", help=\"Improvement over SeasonalNaive\")\n",
    "                    else:\n",
    "                        st.metric(\"vs Baseline\", \"N/A\")\n",
    "\n",
    "                # Selection rationale\n",
    "                st.markdown(\"#### Why This Model?\")\n",
    "                st.info(f\"\"\"\n",
    "                **{best_model}** was selected because it has the **lowest RMSE** on cross-validation.\n",
    "\n",
    "                - **RMSE (Root Mean Square Error)**: Penalizes large errors more heavily. Best for energy forecasting where big misses are costly.\n",
    "                - **Selection method**: Time-series CV with {run_log.get('config', {}).get('cv_windows', 2)} windows, step size {run_log.get('config', {}).get('cv_step_size', 168)}h\n",
    "                - **Horizon**: {run_log.get('config', {}).get('horizon', 24)}h ahead forecasts\n",
    "                \"\"\")\n",
    "\n",
    "                # Model description for winner\n",
    "                if best_model in MODEL_INFO:\n",
    "                    info = MODEL_INFO[best_model]\n",
    "                    st.success(f\"**{info['type']} Model**: {info['description']}\")\n",
    "\n",
    "                # Full leaderboard with visualization\n",
    "                st.markdown(\"#### All Models Ranked by RMSE\")\n",
    "\n",
    "                display_cols = [c for c in [\"model\", \"rmse\", \"mae\", \"mape\", \"coverage_80\", \"coverage_95\"]\n",
    "                               if c in leaderboard_df.columns]\n",
    "\n",
    "                # Create visualization\n",
    "                if \"rmse\" in leaderboard_df.columns:\n",
    "                    fig = px.bar(\n",
    "                        leaderboard_df.sort_values(\"rmse\"),\n",
    "                        x=\"model\",\n",
    "                        y=\"rmse\",\n",
    "                        title=\"Model Comparison (Lower RMSE = Better)\",\n",
    "                        color=\"rmse\",\n",
    "                        color_continuous_scale=\"RdYlGn_r\",\n",
    "                    )\n",
    "                    fig.add_hline(y=best_rmse, line_dash=\"dash\", line_color=\"green\",\n",
    "                                  annotation_text=f\"Best: {best_rmse:.3f}\")\n",
    "                    fig.update_layout(height=350)\n",
    "                    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                # Format numeric columns for table\n",
    "                styled_df = leaderboard_df[display_cols].copy()\n",
    "                for col in [\"rmse\", \"mae\", \"mape\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "                for col in [\"coverage_80\", \"coverage_95\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.1f}%\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "                st.dataframe(styled_df, width=\"stretch\", hide_index=True)\n",
    "\n",
    "                # Coverage analysis\n",
    "                if \"coverage_80\" in leaderboard_df.columns:\n",
    "                    st.markdown(\"#### Prediction Interval Coverage\")\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Coverage** measures if prediction intervals are well-calibrated:\n",
    "                    - **80% interval** should contain ~80% of actual values\n",
    "                    - **95% interval** should contain ~95% of actual values\n",
    "                    - **Under-coverage** (<target) = intervals too narrow, overconfident\n",
    "                    - **Over-coverage** (>target) = intervals too wide, conservative\n",
    "                    \"\"\")\n",
    "\n",
    "                    coverage_df = leaderboard_df[[\"model\", \"coverage_80\", \"coverage_95\"]].copy()\n",
    "                    coverage_df[\"coverage_80_status\"] = coverage_df[\"coverage_80\"].apply(\n",
    "                        lambda x: \"Under\" if x < 75 else (\"Over\" if x > 85 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "                    coverage_df[\"coverage_95_status\"] = coverage_df[\"coverage_95\"].apply(\n",
    "                        lambda x: \"Under\" if x < 90 else (\"Over\" if x > 99 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "\n",
    "                # Model descriptions expander\n",
    "                with st.expander(\"Model Descriptions\"):\n",
    "                    for model_name, info in MODEL_INFO.items():\n",
    "                        st.markdown(f\"**{model_name}** ({info['type']})\")\n",
    "                        st.markdown(f\"- {info['description']}\")\n",
    "                        st.markdown(f\"- *Strengths*: {info['strengths']}\")\n",
    "                        st.markdown(\"---\")\n",
    "\n",
    "                # CV configuration expander\n",
    "                config = run_log.get(\"config\", {})\n",
    "                with st.expander(\"CV Configuration\"):\n",
    "                    st.write({\n",
    "                        \"cv_windows\": config.get(\"cv_windows\"),\n",
    "                        \"cv_step_size\": config.get(\"cv_step_size\"),\n",
    "                        \"horizon\": config.get(\"horizon\"),\n",
    "                        \"regions\": config.get(\"regions\"),\n",
    "                        \"fuel_types\": config.get(\"fuel_types\"),\n",
    "                        \"run_at\": run_log.get(\"run_at_utc\", \"N/A\"),\n",
    "                    })\n",
    "            else:\n",
    "                st.info(\"Leaderboard not available. Run the pipeline with the latest code to generate.\")\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load leaderboard: {e}\")\n",
    "    else:\n",
    "        st.info(\"No run log found. Run the pipeline to generate model comparison.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    st.markdown(\"### 🔍 Per-Series Interpretability\")\n",
    "    st.markdown(\"\"\"\n",
    "    **LightGBM** models are trained alongside statistical models (MSTL/ARIMA) to provide\n",
    "    interpretability insights. The statistical models generate the primary forecasts,\n",
    "    while LightGBM helps understand feature importance and relationships.\n",
    "    \"\"\")\n",
    "\n",
    "    interp_dir = Path(\"data/renewable/interpretability\")\n",
    "\n",
    "    if not interp_dir.exists():\n",
    "        st.info(\"No interpretability data available. Run the pipeline to generate SHAP and PDP plots.\")\n",
    "        return\n",
    "\n",
    "    # Get available series\n",
    "    series_dirs = sorted([d.name for d in interp_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "    if not series_dirs:\n",
    "        st.warning(\"Interpretability directory exists but contains no series data.\")\n",
    "        return\n",
    "\n",
    "    # Filter by selected regions and fuel type\n",
    "    filtered_series = []\n",
    "    for series_id in series_dirs:\n",
    "        parts = series_id.split(\"_\")\n",
    "        if len(parts) == 2:\n",
    "            region, ft = parts\n",
    "            if regions and region not in regions:\n",
    "                continue\n",
    "            if fuel_type != \"Both\" and ft != fuel_type:\n",
    "                continue\n",
    "            filtered_series.append(series_id)\n",
    "\n",
    "    if not filtered_series:\n",
    "        st.warning(\"No interpretability data for selected filters.\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=filtered_series,\n",
    "        index=0,\n",
    "        key=\"interpretability_series_select\",\n",
    "    )\n",
    "\n",
    "    if not selected_series:\n",
    "        return\n",
    "\n",
    "    series_dir = interp_dir / selected_series\n",
    "\n",
    "    # Layout: Feature Importance + SHAP Summary side by side\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"### Feature Importance\")\n",
    "        importance_path = series_dir / \"feature_importance.csv\"\n",
    "        if importance_path.exists():\n",
    "            try:\n",
    "                importance_df = pd.read_csv(importance_path)\n",
    "                # Show top 15 features\n",
    "                top_features = importance_df.head(15)\n",
    "\n",
    "                # Create bar chart\n",
    "                fig = px.bar(\n",
    "                    top_features,\n",
    "                    x=\"importance\",\n",
    "                    y=\"feature\",\n",
    "                    orientation=\"h\",\n",
    "                    title=f\"Top Features: {selected_series}\",\n",
    "                    labels={\"importance\": \"Importance\", \"feature\": \"Feature\"},\n",
    "                )\n",
    "                fig.update_layout(yaxis=dict(autorange=\"reversed\"), height=400)\n",
    "                st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                with st.expander(\"Full Feature List\"):\n",
    "                    st.dataframe(importance_df, width=\"stretch\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error loading feature importance: {e}\")\n",
    "        else:\n",
    "            st.info(\"Feature importance not available.\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"### SHAP Summary\")\n",
    "        shap_summary_path = series_dir / \"shap_summary.png\"\n",
    "        if shap_summary_path.exists():\n",
    "            st.image(str(shap_summary_path), width=\"stretch\")\n",
    "        else:\n",
    "            # Try bar plot as fallback\n",
    "            shap_bar_path = series_dir / \"shap_bar.png\"\n",
    "            if shap_bar_path.exists():\n",
    "                st.image(str(shap_bar_path), width=\"stretch\")\n",
    "            else:\n",
    "                st.info(\"SHAP summary not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # SHAP Dependence Plots\n",
    "    st.markdown(\"### SHAP Dependence Plots\")\n",
    "    st.markdown(\"Shows how individual feature values affect predictions.\")\n",
    "\n",
    "    shap_dep_files = list(series_dir.glob(\"shap_dependence_*.png\"))\n",
    "    if shap_dep_files:\n",
    "        # Create columns for dependence plots\n",
    "        n_cols = min(3, len(shap_dep_files))\n",
    "        cols = st.columns(n_cols)\n",
    "\n",
    "        for i, dep_file in enumerate(shap_dep_files[:6]):  # Limit to 6 plots\n",
    "            feature_name = dep_file.stem.replace(\"shap_dependence_\", \"\")\n",
    "            with cols[i % n_cols]:\n",
    "                st.markdown(f\"**{feature_name}**\")\n",
    "                st.image(str(dep_file), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"SHAP dependence plots not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Partial Dependence Plot\n",
    "    st.markdown(\"### Partial Dependence Plot\")\n",
    "    st.markdown(\"Shows the average effect of features on predictions (marginal effect).\")\n",
    "\n",
    "    pdp_path = series_dir / \"partial_dependence.png\"\n",
    "    if pdp_path.exists():\n",
    "        st.image(str(pdp_path), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"Partial dependence plot not available.\")\n",
    "\n",
    "    # Waterfall plot for sample prediction\n",
    "    waterfall_path = series_dir / \"shap_waterfall_sample.png\"\n",
    "    if waterfall_path.exists():\n",
    "        st.markdown(\"### Sample Prediction Explanation\")\n",
    "        st.markdown(\"SHAP waterfall showing how features contributed to a single prediction.\")\n",
    "        st.image(str(waterfall_path), width=\"stretch\")\n",
    "\n",
    "\n",
    "def generate_demo_forecasts(regions: list, fuel_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate demo forecast data for display.\"\"\"\n",
    "    data = []\n",
    "    base_time = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    fuel_types = [fuel_type] if fuel_type != \"Both\" else [\"WND\", \"SUN\"]\n",
    "\n",
    "    for region in regions[:3]:\n",
    "        for ft in fuel_types:\n",
    "            unique_id = f\"{region}_{ft}\"\n",
    "            base_value = 500 if ft == \"WND\" else 300\n",
    "\n",
    "            for h in range(24):\n",
    "                ds = base_time + timedelta(hours=h)\n",
    "\n",
    "                # Add daily pattern\n",
    "                if ft == \"SUN\":\n",
    "                    hour_factor = max(0, np.sin((ds.hour - 6) * np.pi / 12)) if 6 < ds.hour < 18 else 0\n",
    "                    yhat = base_value * hour_factor + np.random.normal(0, 20)\n",
    "                else:\n",
    "                    yhat = base_value + np.sin(ds.hour * np.pi / 12) * 100 + np.random.normal(0, 30)\n",
    "\n",
    "                yhat = max(0, yhat)\n",
    "\n",
    "                data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"region\": region,\n",
    "                    \"fuel_type\": ft,\n",
    "                    \"ds\": ds,\n",
    "                    \"yhat\": yhat,\n",
    "                    \"yhat_lo_80\": yhat * 0.85,\n",
    "                    \"yhat_hi_80\": yhat * 1.15,\n",
    "                    \"yhat_lo_95\": yhat * 0.75,\n",
    "                    \"yhat_hi_95\": yhat * 1.25,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def run_pipeline_from_dashboard(db_path: str, regions: list, fuel_type: str):\n",
    "    \"\"\"Run the forecasting pipeline from the dashboard.\"\"\"\n",
    "    with st.spinner(\"Refreshing forecasts... (may take 2-3 minutes)\"):\n",
    "        try:\n",
    "            from src.renewable.jobs import run_hourly\n",
    "\n",
    "            # Run the hourly pipeline job\n",
    "            run_hourly.main()\n",
    "\n",
    "            st.success(\"Pipeline completed! Forecasts have been updated with latest EIA data.\")\n",
    "            st.info(\"Reloading page to show new forecasts...\")\n",
    "\n",
    "            # Wait a moment then reload\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            st.rerun()\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Pipeline failed: {e}\")\n",
    "            import traceback\n",
    "            with st.expander(\"Error details\"):\n",
    "                st.code(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow integration \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/data_freshness.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/data_freshness.py\n",
    "# src/renewable/data_freshness.py\n",
    "\"\"\"\n",
    "Lightweight EIA data freshness checking.\n",
    "\n",
    "This module provides functions to check if new data is available from the EIA API\n",
    "before running the full pipeline. It compares the current max timestamps with\n",
    "the previous run's max timestamps to determine if a full pipeline run is needed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from src.renewable.regions import get_eia_respondent\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FreshnessCheckResult:\n",
    "    \"\"\"Result of a data freshness check.\"\"\"\n",
    "\n",
    "    has_new_data: bool\n",
    "    checked_at_utc: str\n",
    "    series_status: dict[str, dict] = field(default_factory=dict)\n",
    "    summary: str = \"\"\n",
    "\n",
    "\n",
    "def load_previous_max_ds(run_log_path: Path) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load per-series max_ds from previous run_log.json.\n",
    "\n",
    "    Args:\n",
    "        run_log_path: Path to run_log.json\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping unique_id -> max_ds ISO string.\n",
    "        Empty dict if file doesn't exist or is malformed.\n",
    "    \"\"\"\n",
    "    if not run_log_path.exists():\n",
    "        logger.info(\"[freshness] No previous run_log.json found - first run\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        data = json.loads(run_log_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # Navigate to diagnostics.generation_coverage.coverage\n",
    "        coverage = (\n",
    "            data.get(\"diagnostics\", {})\n",
    "            .get(\"generation_coverage\", {})\n",
    "            .get(\"coverage\", [])\n",
    "        )\n",
    "\n",
    "        if not coverage:\n",
    "            logger.warning(\"[freshness] run_log.json has no coverage data\")\n",
    "            return {}\n",
    "\n",
    "        result = {}\n",
    "        for item in coverage:\n",
    "            uid = item.get(\"unique_id\")\n",
    "            max_ds = item.get(\"max_ds\")\n",
    "            if uid and max_ds:\n",
    "                result[uid] = max_ds\n",
    "\n",
    "        logger.info(f\"[freshness] Loaded {len(result)} series from previous run_log\")\n",
    "        return result\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        logger.warning(f\"[freshness] Failed to parse run_log.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def probe_eia_latest(\n",
    "    api_key: str,\n",
    "    region: str,\n",
    "    fuel_type: str,\n",
    "    *,\n",
    "    timeout: int = 15,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch only the single most recent record from EIA API.\n",
    "\n",
    "    This is a lightweight probe that uses:\n",
    "    - length=1 (only fetch 1 record)\n",
    "    - sort by period DESC (most recent first)\n",
    "\n",
    "    Args:\n",
    "        api_key: EIA API key\n",
    "        region: Region code (CALI, ERCO, MISO, etc.)\n",
    "        fuel_type: Fuel type (WND, SUN)\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ISO timestamp string of latest record, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"data[]\": \"value\",\n",
    "            \"facets[respondent][]\": respondent,\n",
    "            \"facets[fueltype][]\": fuel_type,\n",
    "            \"frequency\": \"hourly\",\n",
    "            \"length\": 1,\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"desc\",\n",
    "        }\n",
    "\n",
    "        base_url = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "        resp = requests.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        payload = resp.json()\n",
    "        response = payload.get(\"response\", {})\n",
    "        records = response.get(\"data\", [])\n",
    "\n",
    "        if not records:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: No records returned\")\n",
    "            return None\n",
    "\n",
    "        period = records[0].get(\"period\")\n",
    "        if not period:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: Record missing 'period'\")\n",
    "            return None\n",
    "\n",
    "        # Parse to consistent ISO format\n",
    "        ts = pd.to_datetime(period, utc=True)\n",
    "        return ts.isoformat()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: API error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compare_timestamps(prev: Optional[str], current: Optional[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if current is strictly newer than prev.\n",
    "\n",
    "    Handles None values conservatively (assume new data if unknown).\n",
    "    \"\"\"\n",
    "    if not prev or not current:\n",
    "        return True  # Unknown = assume new data (conservative)\n",
    "\n",
    "    try:\n",
    "        prev_dt = pd.to_datetime(prev, utc=True)\n",
    "        curr_dt = pd.to_datetime(current, utc=True)\n",
    "        return curr_dt > prev_dt\n",
    "    except Exception:\n",
    "        return True  # Parse error = assume new data\n",
    "\n",
    "\n",
    "def check_all_series_freshness(\n",
    "    regions: list[str],\n",
    "    fuel_types: list[str],\n",
    "    run_log_path: Path,\n",
    "    api_key: str,\n",
    ") -> FreshnessCheckResult:\n",
    "    \"\"\"\n",
    "    Check all series for new data availability.\n",
    "\n",
    "    Args:\n",
    "        regions: List of region codes (e.g., [\"CALI\", \"ERCO\", \"MISO\"])\n",
    "        fuel_types: List of fuel types (e.g., [\"WND\", \"SUN\"])\n",
    "        run_log_path: Path to previous run_log.json\n",
    "        api_key: EIA API key\n",
    "\n",
    "    Returns:\n",
    "        FreshnessCheckResult with has_new_data flag and detailed status per series.\n",
    "    \"\"\"\n",
    "    checked_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # 1. Load previous max_ds values\n",
    "    prev_max_ds = load_previous_max_ds(run_log_path)\n",
    "\n",
    "    # 2. If no previous run_log, always run full pipeline (first run)\n",
    "    if not prev_max_ds:\n",
    "        return FreshnessCheckResult(\n",
    "            has_new_data=True,\n",
    "            checked_at_utc=checked_at,\n",
    "            series_status={},\n",
    "            summary=\"No previous run_log.json found - running full pipeline (first run)\",\n",
    "        )\n",
    "\n",
    "    # 3. Probe each series\n",
    "    series_status: dict[str, dict] = {}\n",
    "    has_any_new = False\n",
    "    new_series: list[str] = []\n",
    "    error_series: list[str] = []\n",
    "\n",
    "    for region in regions:\n",
    "        for fuel_type in fuel_types:\n",
    "            series_id = f\"{region}_{fuel_type}\"\n",
    "            prev = prev_max_ds.get(series_id)\n",
    "            current = probe_eia_latest(api_key, region, fuel_type)\n",
    "\n",
    "            # Determine if this series has new data\n",
    "            if current is None:\n",
    "                # API error - be conservative, assume new data\n",
    "                is_new = True\n",
    "                error_series.append(series_id)\n",
    "                logger.warning(\n",
    "                    f\"[freshness] {series_id}: probe failed, assuming new data\"\n",
    "                )\n",
    "            else:\n",
    "                is_new = _compare_timestamps(prev, current)\n",
    "\n",
    "            series_status[series_id] = {\n",
    "                \"prev_max_ds\": prev,\n",
    "                \"current_max_ds\": current,\n",
    "                \"is_new\": is_new,\n",
    "            }\n",
    "\n",
    "            if is_new:\n",
    "                has_any_new = True\n",
    "                if current is not None:\n",
    "                    new_series.append(series_id)\n",
    "\n",
    "            # Log each series check\n",
    "            status_str = \"NEW\" if is_new else \"unchanged\"\n",
    "            logger.info(\n",
    "                f\"[freshness] {series_id}: prev={prev} current={current} ({status_str})\"\n",
    "            )\n",
    "\n",
    "    # 4. Build summary\n",
    "    if error_series:\n",
    "        summary = f\"Probe errors for {error_series}, assuming new data available\"\n",
    "    elif new_series:\n",
    "        summary = f\"New data found for: {', '.join(new_series)}\"\n",
    "    else:\n",
    "        summary = \"No new data found for any series\"\n",
    "\n",
    "    return FreshnessCheckResult(\n",
    "        has_new_data=has_any_new,\n",
    "        checked_at_utc=checked_at,\n",
    "        series_status=series_status,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"EIA_API_KEY not set\")\n",
    "        exit(1)\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "\n",
    "    result = check_all_series_freshness(\n",
    "        regions=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        fuel_types=[\"WND\", \"SUN\"],\n",
    "        run_log_path=run_log_path,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFreshness Check Result:\")\n",
    "    print(f\"  has_new_data: {result.has_new_data}\")\n",
    "    print(f\"  checked_at: {result.checked_at_utc}\")\n",
    "    print(f\"  summary: {result.summary}\")\n",
    "    print(f\"\\nPer-series status:\")\n",
    "    for series_id, status in result.series_status.items():\n",
    "        print(f\"  {series_id}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/jobs/run_hourly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/jobs/run_hourly.py\n",
    "# file: src/renewable/jobs/run_hourly.py\n",
    "\"\"\"Hourly renewable pipeline entry point with validation.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.renewable.tasks import RenewablePipelineConfig, run_full_pipeline\n",
    "from src.renewable.validation import validate_generation_df\n",
    "from src.renewable.data_freshness import check_all_series_freshness, FreshnessCheckResult\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _env_list(name: str, default_csv: str) -> list[str]:\n",
    "    raw = os.getenv(name, default_csv)\n",
    "    return [item.strip() for item in raw.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int) -> int:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return int(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _expected_series(regions: list[str], fuel_types: list[str]) -> list[str]:\n",
    "    return [f\"{region}_{fuel}\" for region in regions for fuel in fuel_types]\n",
    "\n",
    "\n",
    "def _json_default(value: object) -> str:\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.isoformat()\n",
    "    if isinstance(value, datetime):\n",
    "        return value.isoformat()\n",
    "    if hasattr(value, \"item\"):\n",
    "        try:\n",
    "            return value.item()\n",
    "        except Exception:\n",
    "            return str(value)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _summarize_generation_coverage(df: pd.DataFrame) -> dict:\n",
    "    if df.empty:\n",
    "        return {\"row_count\": 0, \"series_count\": 0, \"coverage\": []}\n",
    "\n",
    "    coverage = (\n",
    "        df.groupby(\"unique_id\")[\"ds\"]\n",
    "        .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"series_count\": int(df[\"unique_id\"].nunique()),\n",
    "        \"coverage\": coverage.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _read_previous_run_summary(data_dir: str) -> dict | None:\n",
    "    \"\"\"Read previous run_log.json for rowcount comparison.\"\"\"\n",
    "    path = Path(data_dir) / \"run_log.json\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _summarize_negative_forecasts(\n",
    "    df: pd.DataFrame,\n",
    "    sample_rows: int = 5,\n",
    ") -> dict:\n",
    "    if df.empty or \"yhat\" not in df.columns:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    neg = df[df[\"yhat\"] < 0]\n",
    "    if neg.empty:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    series_summary = (\n",
    "        neg.groupby(\"unique_id\")[\"yhat\"]\n",
    "        .agg(count=\"count\", min_value=\"min\", max_value=\"max\", mean_value=\"mean\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    sample = neg[[\"unique_id\", \"ds\", \"yhat\"]].head(sample_rows)\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"negative_rows\": int(len(neg)),\n",
    "        \"series\": series_summary.to_dict(orient=\"records\"),\n",
    "        \"sample\": sample.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_hourly_pipeline() -> dict:\n",
    "    data_dir = os.getenv(\"RENEWABLE_DATA_DIR\", \"data/renewable\")\n",
    "    regions = _env_list(\"RENEWABLE_REGIONS\", \"CALI,ERCO,MISO\")\n",
    "    fuel_types = _env_list(\"RENEWABLE_FUELS\", \"WND,SUN\")\n",
    "    lookback_days = _env_int(\"LOOKBACK_DAYS\", 30)\n",
    "\n",
    "    # Horizon configuration: support both preset and direct override\n",
    "    horizon_preset = os.getenv(\"RENEWABLE_HORIZON_PRESET\", None)  # \"24h\" | \"48h\" | \"72h\"\n",
    "    horizon_override = _env_int(\"RENEWABLE_HORIZON\", 0)  # Legacy direct override\n",
    "\n",
    "    # If direct override is set, use it; otherwise use preset (or None for default)\n",
    "    if horizon_override > 0:\n",
    "        horizon = horizon_override\n",
    "        horizon_preset = None  # Ignore preset if direct override is set\n",
    "    else:\n",
    "        horizon = 24  # Default, may be overridden by preset\n",
    "\n",
    "    cv_windows = _env_int(\"RENEWABLE_CV_WINDOWS\", 2)\n",
    "    cv_step_size = _env_int(\"RENEWABLE_CV_STEP_SIZE\", 168)\n",
    "\n",
    "    start_date = os.getenv(\"RENEWABLE_START_DATE\", \"\")\n",
    "    end_date = os.getenv(\"RENEWABLE_END_DATE\", \"\")\n",
    "\n",
    "    # Check if we should force run (e.g., manual dispatch)\n",
    "    force_run = os.getenv(\"FORCE_RUN\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # Data freshness check - skip full pipeline if no new data\n",
    "    if not force_run:\n",
    "        api_key = os.getenv(\"EIA_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            print(\"WARNING: EIA_API_KEY not set, skipping freshness check\")\n",
    "        else:\n",
    "            run_log_path = Path(data_dir) / \"run_log.json\"\n",
    "            freshness = check_all_series_freshness(\n",
    "                regions=regions,\n",
    "                fuel_types=fuel_types,\n",
    "                run_log_path=run_log_path,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "\n",
    "            if not freshness.has_new_data:\n",
    "                # No new data - return early with skip status\n",
    "                skip_log = {\n",
    "                    \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"reason\": \"no_new_data\",\n",
    "                    \"freshness_check\": {\n",
    "                        \"checked_at_utc\": freshness.checked_at_utc,\n",
    "                        \"summary\": freshness.summary,\n",
    "                        \"series_status\": freshness.series_status,\n",
    "                    },\n",
    "                    \"config\": {\n",
    "                        \"regions\": regions,\n",
    "                        \"fuel_types\": fuel_types,\n",
    "                        \"data_dir\": data_dir,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                # Write skip log (append to run_log.json)\n",
    "                Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "                skip_log_path = Path(data_dir) / \"skip_log.json\"\n",
    "                skip_log_path.write_text(\n",
    "                    json.dumps(skip_log, indent=2, default=_json_default)\n",
    "                )\n",
    "\n",
    "                print(f\"SKIPPED: {freshness.summary}\")\n",
    "                print(f\"Skip log written to: {skip_log_path}\")\n",
    "\n",
    "                # Set output for GitHub Actions\n",
    "                github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "                if github_output:\n",
    "                    with open(github_output, \"a\") as f:\n",
    "                        f.write(\"status=skipped\\n\")\n",
    "\n",
    "                return skip_log\n",
    "\n",
    "            print(f\"Freshness check: {freshness.summary}\")\n",
    "    else:\n",
    "        print(\"FORCE_RUN=true - skipping freshness check\")\n",
    "\n",
    "    cfg = RenewablePipelineConfig(\n",
    "        regions=regions,\n",
    "        fuel_types=fuel_types,\n",
    "        lookback_days=lookback_days,\n",
    "        horizon=horizon,\n",
    "        horizon_preset=horizon_preset,  # Apply preset if specified\n",
    "        data_dir=data_dir,\n",
    "        overwrite=True,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "    )\n",
    "    cfg.cv_windows = cv_windows\n",
    "    cfg.cv_step_size = cv_step_size\n",
    "\n",
    "    fetch_diagnostics: list[dict] = []\n",
    "    results = run_full_pipeline(cfg, fetch_diagnostics=fetch_diagnostics)\n",
    "\n",
    "    gen_path = cfg.generation_path()\n",
    "    gen_df = pd.read_parquet(gen_path)\n",
    "    generation_coverage = _summarize_generation_coverage(gen_df)\n",
    "\n",
    "    max_lag_hours = _env_int(\"MAX_LAG_HOURS\", 48)  # EIA publishes with 12-24h delay\n",
    "    max_missing_ratio = _env_float(\"MAX_MISSING_RATIO\", 0.02)\n",
    "    report = validate_generation_df(\n",
    "        gen_df,\n",
    "        max_lag_hours=max_lag_hours,\n",
    "        max_missing_ratio=max_missing_ratio,\n",
    "        expected_series=_expected_series(regions, fuel_types),\n",
    "    )\n",
    "\n",
    "    forecasts_df = pd.read_parquet(cfg.forecasts_path())\n",
    "    negative_forecasts = _summarize_negative_forecasts(forecasts_df)\n",
    "\n",
    "    # Quality gates\n",
    "    max_rowdrop_pct = _env_float(\"MAX_ROWDROP_PCT\", 0.30)\n",
    "    max_neg_forecast_ratio = _env_float(\"MAX_NEG_FORECAST_RATIO\", 0.10)\n",
    "\n",
    "    prev_run = _read_previous_run_summary(data_dir)\n",
    "    prev_gen_rows = 0\n",
    "    if prev_run:\n",
    "        prev_gen_rows = prev_run.get(\"pipeline_results\", {}).get(\"generation_rows\", 0)\n",
    "\n",
    "    curr_gen_rows = results.get(\"generation_rows\", 0)\n",
    "    rowdrop_ok = True\n",
    "    if prev_gen_rows > 0:\n",
    "        floor_ok = int(prev_gen_rows * (1.0 - max_rowdrop_pct))\n",
    "        rowdrop_ok = curr_gen_rows >= floor_ok\n",
    "\n",
    "    neg_forecast_ratio = 0.0\n",
    "    if negative_forecasts[\"row_count\"] > 0:\n",
    "        neg_forecast_ratio = (\n",
    "            negative_forecasts[\"negative_rows\"] / negative_forecasts[\"row_count\"]\n",
    "        )\n",
    "    neg_forecast_ok = neg_forecast_ratio <= max_neg_forecast_ratio\n",
    "\n",
    "    quality_gates = {\n",
    "        \"rowdrop\": {\n",
    "            \"ok\": rowdrop_ok,\n",
    "            \"prev_rows\": prev_gen_rows,\n",
    "            \"curr_rows\": curr_gen_rows,\n",
    "            \"max_rowdrop_pct\": max_rowdrop_pct,\n",
    "        },\n",
    "        \"neg_forecast\": {\n",
    "            \"ok\": neg_forecast_ok,\n",
    "            \"ratio\": neg_forecast_ratio,\n",
    "            \"max_ratio\": max_neg_forecast_ratio,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log = {\n",
    "        \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"config\": {\n",
    "            \"regions\": regions,\n",
    "            \"fuel_types\": fuel_types,\n",
    "            \"lookback_days\": lookback_days,\n",
    "            \"horizon\": horizon,\n",
    "            \"cv_windows\": cv_windows,\n",
    "            \"cv_step_size\": cv_step_size,\n",
    "            \"data_dir\": data_dir,\n",
    "            \"start_date\": cfg.start_date,\n",
    "            \"end_date\": cfg.end_date,\n",
    "        },\n",
    "        \"pipeline_results\": results,\n",
    "        \"validation\": {\n",
    "            \"ok\": report.ok,\n",
    "            \"message\": report.message,\n",
    "            \"details\": report.details,\n",
    "        },\n",
    "        \"diagnostics\": {\n",
    "            \"fetch\": fetch_diagnostics,\n",
    "            \"generation_coverage\": generation_coverage,\n",
    "            \"negative_forecasts\": negative_forecasts,\n",
    "        },\n",
    "        \"quality_gates\": quality_gates,\n",
    "    }\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    (Path(data_dir) / \"run_log.json\").write_text(\n",
    "        json.dumps(run_log, indent=2, default=_json_default)\n",
    "    )\n",
    "\n",
    "    # Check validation\n",
    "    if not report.ok:\n",
    "        raise SystemExit(f\"VALIDATION_FAILED: {report.message} | {report.details}\")\n",
    "\n",
    "    # Check quality gates\n",
    "    if not rowdrop_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: rowdrop | \"\n",
    "            f\"curr={curr_gen_rows} prev={prev_gen_rows} max_drop={max_rowdrop_pct:.0%}\"\n",
    "        )\n",
    "    if not neg_forecast_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: neg_forecast | \"\n",
    "            f\"ratio={neg_forecast_ratio:.1%} max={max_neg_forecast_ratio:.0%}\"\n",
    "        )\n",
    "\n",
    "    # Set output for GitHub Actions (successful run)\n",
    "    github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "    if github_output:\n",
    "        with open(github_output, \"a\") as f:\n",
    "            f.write(\"status=success\\n\")\n",
    "\n",
    "    return run_log\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    run_hourly_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dag_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dag_builder.py\n",
    "# file: src/renewable/dag_builder.py\n",
    "\"\"\"Renewable pipeline DAG builder for Airflow.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "AIRFLOW_AVAILABLE = True\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "def build_hourly_dag(\n",
    "    dag_id: str = \"renewable_hourly_pipeline\",\n",
    "    schedule: str = \"17 * * * *\",\n",
    "    start_date: Optional[datetime] = None,\n",
    "    default_args: Optional[Dict[str, Any]] = None,\n",
    ") -> \"DAG\":\n",
    "    if not AIRFLOW_AVAILABLE:\n",
    "        raise ImportError(\"Airflow is not installed. Install apache-airflow to use build_hourly_dag().\")\n",
    "\n",
    "    from src.renewable.jobs.run_hourly import run_hourly_pipeline\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = datetime.utcnow() - timedelta(days=1)\n",
    "    if default_args is None:\n",
    "        default_args = DEFAULT_ARGS.copy()\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=\"Renewable hourly pipeline\",\n",
    "        schedule_interval=schedule,\n",
    "        start_date=start_date,\n",
    "        catchup=False,\n",
    "        max_active_runs=1,\n",
    "        tags=[\"renewable\", \"eia\", \"forecasting\"],\n",
    "    ) as dag:\n",
    "        PythonOperator(\n",
    "            task_id=\"run_hourly\",\n",
    "            python_callable=run_hourly_pipeline,\n",
    "        )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def build_dag_dot() -> str:\n",
    "    return \"\"\"digraph RENEWABLE_PIPELINE {\n",
    "  rankdir=LR;\n",
    "  node [shape=box, style=\"rounded,filled\", fillcolor=\"#e8f5e9\"];\n",
    "\n",
    "  run_hourly;\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# git actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/pre-commit.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/pre-commit.yml\n",
    "# file: .github/workflows/pre-commit.yml\n",
    "name: pre-commit\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  run:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - uses: pre-commit/action@v3.0.1\n",
    "\n",
    "      - name: Install test dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install pytest pandas numpy requests python-dotenv\n",
    "\n",
    "      - name: Run smoke tests\n",
    "        env:\n",
    "          PYTHONPATH: ${{ github.workspace }}\n",
    "        run: pytest tests/ -v -k \"not slow\" --tb=short || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/renewable-hourly.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/renewable-hourly.yml\n",
    "# file: .github/workflows/renewable-hourly.yml\n",
    "name: renewable-hourly\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      force_run:\n",
    "        description: 'Force full pipeline run (skip freshness check)'\n",
    "        type: boolean\n",
    "        default: false\n",
    "  schedule:\n",
    "    - cron: \"17 * * * *\"\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "concurrency:\n",
    "  group: renewable-hourly\n",
    "  cancel-in-progress: true\n",
    "\n",
    "jobs:\n",
    "  update:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 25\n",
    "    env:\n",
    "      EIA_API_KEY: ${{ secrets.EIA_API_KEY }}\n",
    "      FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.force_run && 'true' || 'false' }}\n",
    "      RENEWABLE_REGIONS: \"CALI,ERCO,MISO\"\n",
    "      RENEWABLE_FUELS: \"WND,SUN\"\n",
    "      LOOKBACK_DAYS: \"30\"\n",
    "      RENEWABLE_HORIZON: \"24\"\n",
    "      RENEWABLE_CV_WINDOWS: \"2\"\n",
    "      RENEWABLE_CV_STEP_SIZE: \"168\"\n",
    "      MAX_LAG_HOURS: \"48\"  # EIA publishes hourly data with 12-24h delay\n",
    "      MAX_MISSING_RATIO: \"0.02\"\n",
    "      RENEWABLE_DATA_DIR: \"data/renewable\"\n",
    "      RENEWABLE_N_JOBS: \"1\"\n",
    "      OMP_NUM_THREADS: \"1\"\n",
    "      MKL_NUM_THREADS: \"1\"\n",
    "      OPENBLAS_NUM_THREADS: \"1\"\n",
    "      NUMBA_NUM_THREADS: \"1\"\n",
    "      VECLIB_MAXIMUM_THREADS: \"1\"\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - name: Check EIA API key\n",
    "        run: |\n",
    "          if [ -z \"$EIA_API_KEY\" ]; then\n",
    "            echo \"EIA_API_KEY is not set. Add it to repo secrets.\" >&2\n",
    "            exit 1\n",
    "          fi\n",
    "\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # Install from pyproject.toml for single source of truth\n",
    "          # Use -e for editable install (allows imports to work correctly)\n",
    "          pip install -e .\n",
    "\n",
    "      - name: Run hourly pipeline\n",
    "        id: pipeline\n",
    "        run: |\n",
    "          python -m src.renewable.jobs.run_hourly\n",
    "\n",
    "      - name: Quality gate check\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          python -c \"\n",
    "          import json, sys\n",
    "          from pathlib import Path\n",
    "          log_path = Path('data/renewable/run_log.json')\n",
    "          if not log_path.exists():\n",
    "              print('No run_log.json found')\n",
    "              sys.exit(1)\n",
    "          log = json.loads(log_path.read_text())\n",
    "          val = log.get('validation', {})\n",
    "          if not val.get('ok'):\n",
    "              print(f'VALIDATION FAILED: {val.get(\\\"message\\\")}')\n",
    "              print(f'Details: {val.get(\\\"details\\\")}')\n",
    "              sys.exit(1)\n",
    "          gates = log.get('quality_gates', {})\n",
    "          if not gates.get('rowdrop', {}).get('ok', True):\n",
    "              print(f'ROWDROP GATE FAILED: {gates.get(\\\"rowdrop\\\")}')\n",
    "              sys.exit(1)\n",
    "          if not gates.get('neg_forecast', {}).get('ok', True):\n",
    "              print(f'NEG_FORECAST GATE FAILED: {gates.get(\\\"neg_forecast\\\")}')\n",
    "              sys.exit(1)\n",
    "          print('QUALITY GATES PASSED')\n",
    "          \"\n",
    "\n",
    "      - name: Skip notification\n",
    "        if: steps.pipeline.outputs.status == 'skipped'\n",
    "        run: |\n",
    "          echo \"### Pipeline skipped - no new EIA data\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          if [ -f data/renewable/skip_log.json ]; then\n",
    "            python -c \"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "          data = json.loads(Path('data/renewable/skip_log.json').read_text())\n",
    "          freshness = data.get('freshness_check', {})\n",
    "          print(f'- Checked at: {freshness.get(\\\"checked_at_utc\\\")}')\n",
    "          print(f'- Summary: {freshness.get(\\\"summary\\\")}')\n",
    "          \" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Summarize run\n",
    "        if: always() && steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          if [ -f data/renewable/run_log.json ]; then\n",
    "          python - <<'PY' | tee -a \"$GITHUB_STEP_SUMMARY\"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "\n",
    "          data = json.loads(Path(\"data/renewable/run_log.json\").read_text())\n",
    "          validation = data.get(\"validation\", {})\n",
    "          details = validation.get(\"details\", {})\n",
    "          pipeline = data.get(\"pipeline_results\", {})\n",
    "          interp = pipeline.get(\"interpretability\", {})\n",
    "\n",
    "          lines = [\n",
    "              \"### Renewable hourly run\",\n",
    "              f\"- run_at_utc: {data.get('run_at_utc')}\",\n",
    "              f\"- validation_ok: {validation.get('ok')}\",\n",
    "              f\"- message: {validation.get('message')}\",\n",
    "              f\"- max_ds: {details.get('max_ds')}\",\n",
    "              f\"- lag_hours: {details.get('lag_hours')}\",\n",
    "              f\"- best_model: {pipeline.get('best_model')}\",\n",
    "              f\"- best_rmse: {pipeline.get('best_rmse', 0):.1f}\",\n",
    "              \"\",\n",
    "              \"#### Interpretability\",\n",
    "              f\"- series_count: {interp.get('series_count', 0)}\",\n",
    "              f\"- output_dir: {interp.get('output_dir', 'N/A')}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          PY\n",
    "          else\n",
    "          echo \"No run_log.json found.\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Commit updated artifacts\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          git config user.name \"github-actions[bot]\"\n",
    "          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n",
    "          git add data/renewable/generation.parquet \\\n",
    "            data/renewable/weather.parquet \\\n",
    "            data/renewable/forecasts.parquet \\\n",
    "            data/renewable/run_log.json\n",
    "          # Add interpretability artifacts if they exist\n",
    "          if [ -d data/renewable/interpretability ]; then\n",
    "            git add data/renewable/interpretability/\n",
    "          fi\n",
    "          git commit -m \"renewable: hourly data update (UTC)\" || echo \"No changes to commit\"\n",
    "          git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
