{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable Energy Forecasting Pipeline\n",
    "\n",
    "This notebook walks through building a **next-24h renewable generation forecast system** with:\n",
    "\n",
    "- **EIA data integration** - Hourly wind/solar generation for US regions\n",
    "- **Weather features** - Open-Meteo integration (wind speed, solar radiation)\n",
    "- **Probabilistic forecasting** - Dual prediction intervals (80%, 95%)\n",
    "- **Drift monitoring** - Automatic detection of model degradation\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "```\n",
    "┌─────────────┐      ┌──────────────┐      ┌─────────────┐\n",
    "│  EIA API    │──┬──▶│ Data         │──┬──▶│ StatsForecast│\n",
    "│ (WND/SUN)   │  │   │ Pipeline     │  │   │ Models       │\n",
    "└─────────────┘  │   └──────────────┘  │   └─────────────┘\n",
    "                 │                     │           │\n",
    "┌─────────────┐  │   ┌──────────────┐  │   ┌─────▼──────┐\n",
    "│ Open-Meteo  │──┘   │ Validation   │  │   │Probabilistic│\n",
    "│ Weather API │      │ & Quality    │  │   │Forecasts    │\n",
    "└─────────────┘      │ Gates        │  │   │(80%, 95% CI)│\n",
    "                     └──────────────┘  │   └────────────┘\n",
    "                                       │           │\n",
    "                                       │   ┌───────▼─────┐\n",
    "                                       └──▶│  Artifacts  │\n",
    "                                           │  Commit &   │\n",
    "                                           │  Dashboard  │\n",
    "                                           └─────────────┘\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` - where `unique_id` = `{region}_{fuel_type}`\n",
    "2. **Zero-value handling**: Solar generates 0 at night - we use RMSE/MAE, NOT MAPE\n",
    "3. **Leakage prevention**: Use **forecasted** weather for predictions, not historical\n",
    "4. **Drift detection**: Threshold = mean + 2*std from backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure we have the project root in our path, configure logging and set up our pyproject.toml (will be updated over time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to project root: c:\\docker_projects\\atsaf we are currently at c:\\docker_projects\\atsaf\n",
      "Project root: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\docker_projects\\atsaf\"\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "if os.getcwd() != str(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to project root: {project_root} we are currently at {os.getcwd()}\")\n",
    "\n",
    "# Configure logging for visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[project]\n",
    "name = \"atsaf\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Applied Time Series Analysis and Forecasting - Learning with real EIA data\"\n",
    "readme = \"README.md\"\n",
    "license = {text = \"MIT\"}\n",
    "authors = [\n",
    "    {name = \"Time Series Learner\", email = \"learner@example.com\"}\n",
    "]\n",
    "keywords = [\"time-series\", \"forecasting\", \"analysis\", \"eia\", \"electricity\"]\n",
    "classifiers = [\n",
    "    \"Development Status :: 3 - Alpha\",\n",
    "    \"Intended Audience :: Education\",\n",
    "    \"Topic :: Scientific/Engineering :: Information Analysis\",\n",
    "    \"Programming Language :: Python :: 3\",\n",
    "    \"Programming Language :: Python :: 3.11\",\n",
    "    \"Programming Language :: Python :: 3.12\",\n",
    "    \"Programming Language :: Python :: 3.13\",\n",
    "]\n",
    "requires-python = \">=3.11,<3.14\"\n",
    "\n",
    "\n",
    "\n",
    "dependencies = [\n",
    "    # Data fetching and processing\n",
    "    \"requests>=2.28.0\",           # HTTP requests for API calls\n",
    "    \"pandas>=1.5.0\",              # Data manipulation and analysis\n",
    "    \"numpy>=1.24.0\",              # Numerical computing\n",
    "    \"duckdb>=0.8.0\",              # Fast SQL queries on data\n",
    "    \"python-dotenv>=1.0.0\",       # Load environment variables from .env file\n",
    "\n",
    "    # Time series forecasting\n",
    "    \"statsforecast>=1.5.0\",       # Statistical forecasting methods\n",
    "    \"mlforecast>=0.10.0\",         # ML-based forecasting\n",
    "    \"nixtla>=0.0.1\",              # Nixtla ecosystem utilities\n",
    "    \"skforecast>=0.14.0\",         # ML-based recursive forecasting\n",
    "    \"lightgbm>=4.0.0\",            # Gradient boosting for forecasting\n",
    "\n",
    "    # Model interpretability\n",
    "    \"shap>=0.50.0\",               # SHAP values for model explanations\n",
    "\n",
    "    # Statistical analysis and modeling\n",
    "    \"statsmodels>=0.14.0\",        # Statistical modeling and tests\n",
    "    \"scipy>=1.10.0\",              # Scientific computing\n",
    "    \"scikit-learn>=1.2.0\",        # Machine learning and preprocessing\n",
    "\n",
    "    # Visualization\n",
    "    \"matplotlib>=3.7.0\",          # Static plotting\n",
    "    \"seaborn>=0.12.0\",            # Statistical data visualization\n",
    "    \"plotly>=5.0.0\",              # Interactive web-based visualization\n",
    "\n",
    "    # MLOps and workflow\n",
    "    \"mlflow>=2.0.0\",              # Experiment tracking and model registry\n",
    "    \"apache-airflow>=2.5.0\",      # Workflow orchestration\n",
    "\n",
    "    # CLI and monitoring (Chapter 3-4)\n",
    "    \"typer>=0.9.0\",               # CLI framework\n",
    "    \"rich>=13.0.0\",               # Pretty CLI output\n",
    "    \"evidently>=0.4.0\",           # Drift detection and monitoring\n",
    "    \"streamlit>=1.30.0\",          # Dashboard UI\n",
    "    \"streamlit-mermaid~=0.3.0\",   # Mermaid diagrams in Streamlit (latest 0.3.0) \n",
    "    \"pydantic-settings>=2.0.0\",   # Type-safe configuration\n",
    "    \"protobuf>=4.21,<7\",\n",
    "    \"ipykernel>=6.0.0\",            # IPython kernel for Jupyter\n",
    "\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest>=7.3.0\",              # Testing framework\n",
    "    \"pytest-cov>=4.1.0\",          # Coverage reporting\n",
    "    \"black>=23.3.0\",              # Code formatter\n",
    "    \"ruff>=0.0.270\",              # Fast Python linter\n",
    "    \"mypy>=1.0.0\",                # Static type checker\n",
    "]\n",
    "\n",
    "jupyter = [\n",
    "    \"jupyter>=1.0.0\",             # Jupyter notebook\n",
    "    \"jupyterlab>=4.0.0\",          # JupyterLab\n",
    "    \"ipywidgets>=8.0.0\",          # Interactive widgets\n",
    "]\n",
    "\n",
    "[project.urls]\n",
    "Homepage = \"https://github.com/RamiKrispin/atsaf\"\n",
    "Documentation = \"https://github.com/RamiKrispin/atsaf\"\n",
    "Repository = \"https://github.com/RamiKrispin/atsaf.git\"\n",
    "Issues = \"https://github.com/RamiKrispin/atsaf/issues\"\n",
    "\n",
    "[tool.hatch.build.targets.wheel]\n",
    "packages = [\"src\"]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 100\n",
    "target-version = ['py311', 'py312', 'py313']\n",
    "include = '\\.pyi?$'\n",
    "extend-exclude = '''\n",
    "/(\n",
    "  # directories\n",
    "  \\.eggs\n",
    "  | \\.git\n",
    "  | \\.hg\n",
    "  | \\.mypy_cache\n",
    "  | \\.tox\n",
    "  | \\.venv\n",
    "  | build\n",
    "  | dist\n",
    ")/\n",
    "'''\n",
    "\n",
    "[tool.ruff]\n",
    "line-length = 100\n",
    "select = [\n",
    "    \"E\",    # pycodestyle errors\n",
    "    \"W\",    # pycodestyle warnings\n",
    "    \"F\",    # Pyflakes\n",
    "    \"I\",    # isort\n",
    "    \"C\",    # flake8-comprehensions\n",
    "    \"B\",    # flake8-bugbear\n",
    "]\n",
    "ignore = [\n",
    "    \"E501\",  # line too long (handled by black)\n",
    "    \"W503\",  # line break before binary operator\n",
    "]\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.11\"\n",
    "warn_return_any = true\n",
    "warn_unused_configs = true\n",
    "disallow_untyped_defs = false\n",
    "disallow_incomplete_defs = false\n",
    "check_untyped_defs = false\n",
    "no_implicit_optional = true\n",
    "warn_redundant_casts = true\n",
    "warn_unused_ignores = true\n",
    "warn_no_return = true\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "testpaths = [\"tests\"]\n",
    "python_files = [\"test_*.py\"]\n",
    "python_classes = [\"Test*\"]\n",
    "python_functions = [\"test_*\"]\n",
    "addopts = \"-v --cov=. --cov-report=html --cov-report=term\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 1: Region Definitions\n",
    "\n",
    "**File:** `src/renewable/regions.py`\n",
    "\n",
    "This module maps **EIA balancing authority regions** to their geographic coordinates. Why do we need coordinates?\n",
    "\n",
    "- **Weather API lookup**: Open-Meteo requires latitude/longitude\n",
    "- **Regional analysis**: Compare forecast accuracy across regions\n",
    "- **Timezone handling**: Each region has a primary timezone\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "1. **NamedTuple for RegionInfo**: Immutable, type-safe, and memory-efficient\n",
    "2. **Centroid coordinates**: Approximate centers - good enough for hourly weather\n",
    "3. **Fuel type codes**: `WND` (wind), `SUN` (solar) - match EIA's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/regions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/regions.py\n",
    "# src/renewable/regions.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class RegionInfo(NamedTuple):\n",
    "    \"\"\"Region metadata for EIA and weather lookups.\"\"\"\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    timezone: str\n",
    "    # Some internal regions may not map cleanly to an EIA respondent.\n",
    "    # We keep them in REGIONS for weather/features, but EIA fetch requires this.\n",
    "    eia_respondent: Optional[str] = None\n",
    "\n",
    "\n",
    "REGIONS: dict[str, RegionInfo] = {\n",
    "    # Western Interconnection\n",
    "    \"CALI\": RegionInfo(\n",
    "        name=\"California ISO\",\n",
    "        lat=36.7,\n",
    "        lon=-119.4,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=\"CISO\",\n",
    "    ),\n",
    "    \"NW\": RegionInfo(\n",
    "        name=\"Northwest\",\n",
    "        lat=45.5,\n",
    "        lon=-122.0,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "    \"SW\": RegionInfo(\n",
    "        name=\"Southwest\",\n",
    "        lat=33.5,\n",
    "        lon=-112.0,\n",
    "        timezone=\"America/Phoenix\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "\n",
    "    # Texas Interconnection\n",
    "    \"ERCO\": RegionInfo(\n",
    "        name=\"ERCOT (Texas)\",\n",
    "        lat=31.0,\n",
    "        lon=-100.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"ERCO\",\n",
    "    ),\n",
    "\n",
    "    # Midwest\n",
    "    \"MISO\": RegionInfo(\n",
    "        name=\"Midcontinent ISO\",\n",
    "        lat=41.0,\n",
    "        lon=-93.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"MISO\",\n",
    "    ),\n",
    "    \"PJM\": RegionInfo(\n",
    "        name=\"PJM Interconnection\",\n",
    "        lat=39.0,\n",
    "        lon=-77.0,\n",
    "        timezone=\"America/New_York\",\n",
    "        eia_respondent=\"PJM\",\n",
    "    ),\n",
    "    \"SWPP\": RegionInfo(\n",
    "        name=\"Southwest Power Pool\",\n",
    "        lat=37.0,\n",
    "        lon=-97.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"SWPP\",\n",
    "    ),\n",
    "\n",
    "    # Internal/aggregate regions kept for non-EIA use (weather/features/etc.)\n",
    "    \"SE\": RegionInfo(name=\"Southeast\", lat=33.0, lon=-84.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"FLA\": RegionInfo(name=\"Florida\", lat=28.0, lon=-82.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"CAR\": RegionInfo(name=\"Carolinas\", lat=35.5, lon=-80.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"TEN\": RegionInfo(name=\"Tennessee Valley\", lat=35.5, lon=-86.0, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "\n",
    "    \"US48\": RegionInfo(name=\"Lower 48 States\", lat=39.8, lon=-98.5, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "}\n",
    "\n",
    "FUEL_TYPES = {\"WND\": \"Wind\", \"SUN\": \"Solar\"}\n",
    "\n",
    "\n",
    "def list_regions() -> list[str]:\n",
    "    return sorted(REGIONS.keys())\n",
    "\n",
    "\n",
    "def get_region_info(region_code: str) -> RegionInfo:\n",
    "    return REGIONS[region_code]\n",
    "\n",
    "\n",
    "def get_region_coords(region_code: str) -> tuple[float, float]:\n",
    "    r = REGIONS[region_code]\n",
    "    return (r.lat, r.lon)\n",
    "\n",
    "\n",
    "def get_eia_respondent(region_code: str) -> str:\n",
    "    \"\"\"Return the code EIA expects for facets[respondent][]. Fail loudly if missing.\"\"\"\n",
    "    info = REGIONS[region_code]\n",
    "    if not info.eia_respondent:\n",
    "        raise ValueError(\n",
    "            f\"Region '{region_code}' has no configured eia_respondent. \"\n",
    "            f\"Set REGIONS['{region_code}'].eia_respondent to a verified EIA respondent code \"\n",
    "            f\"before using it for EIA fetches.\"\n",
    "        )\n",
    "    return info.eia_respondent\n",
    "\n",
    "\n",
    "def validate_region(region_code: str) -> bool:\n",
    "    return region_code in REGIONS\n",
    "\n",
    "\n",
    "def validate_fuel_type(fuel_type: str) -> bool:\n",
    "    return fuel_type in FUEL_TYPES\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run - test region functions\n",
    "\n",
    "    print(\"=== Available Regions ===\")\n",
    "    print(f\"Total regions: {len(REGIONS)}\")\n",
    "    print(f\"Region codes: {list_regions()}\")\n",
    "\n",
    "    print(\"\\n=== Example: California ===\")\n",
    "    cali_info = get_region_info(\"CALI\")\n",
    "    print(f\"Name: {cali_info.name}\")\n",
    "    print(f\"Coordinates: ({cali_info.lat}, {cali_info.lon})\")\n",
    "    print(f\"Timezone: {cali_info.timezone}\")\n",
    "\n",
    "    print(\"\\n=== Weather API Coordinates ===\")\n",
    "    for region in [\"CALI\", \"ERCO\", \"MISO\"]:\n",
    "        lat, lon = get_region_coords(region)\n",
    "        print(f\"{region}: lat={lat}, lon={lon}\")\n",
    "\n",
    "    print(\"\\n=== Fuel Types ===\")\n",
    "    for code, name in FUEL_TYPES.items():\n",
    "        print(f\"{code}: {name}\")\n",
    "\n",
    "    print(\"\\n=== Validation ===\")\n",
    "    print(f\"validate_region('CALI'): {validate_region('CALI')}\")\n",
    "    print(f\"validate_region('INVALID'): {validate_region('INVALID')}\")\n",
    "    print(f\"validate_fuel_type('WND'): {validate_fuel_type('WND')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Region Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2: EIA Data Fetcher\n",
    "\n",
    "**File:** `src/renewable/eia_renewable.py`\n",
    "\n",
    "This module fetches **hourly wind and solar generation** from the EIA API.\n",
    "\n",
    "## Critical Concepts\n",
    "\n",
    "### StatsForecast Format\n",
    "StatsForecast expects data in a specific format:\n",
    "```\n",
    "unique_id | ds                  | y\n",
    "----------|---------------------|--------\n",
    "CALI_WND  | 2024-01-01 00:00:00 | 1234.5\n",
    "CALI_WND  | 2024-01-01 01:00:00 | 1456.7\n",
    "ERCO_WND  | 2024-01-01 00:00:00 | 2345.6\n",
    "```\n",
    "\n",
    "- `unique_id`: Identifies the time series (e.g., \"CALI_WND\" = California Wind)\n",
    "- `ds`: Datetime column (timezone-naive UTC)\n",
    "- `y`: Target value (generation in MWh)\n",
    "\n",
    "### API Rate Limiting\n",
    "- EIA API has rate limits (~5 requests/second)\n",
    "- We use controlled parallelism with delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:50:02,132 - __main__ - INFO - Loaded .env via find_dotenv: c:\\docker_projects\\atsaf\\.env\n",
      "2026-01-22 16:50:02,133 - __main__ - INFO - EIA_API_KEY loaded (masked): 8pqH...8Xk7\n",
      "2026-01-22 16:50:02,133 - __main__ - INFO - Request timeout: 60 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Single Region Fetch ===\n",
      "[PAGE] region=CALI fuel=WND returned=72 offset=0 total=72 url=https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/?data%5B%5D=value&facets%5Brespondent%5D%5B%5D=CISO&facets%5Bfueltype%5D%5B%5D=WND&frequency=hourly&start=2024-12-01T00&end=2024-12-03T23&length=5000&offset=0&sort%5B0%5D%5Bcolumn%5D=period&sort%5B0%5D%5Bdirection%5D=asc\n",
      "Single region: 72 rows\n",
      "                   ds  value region fuel_type\n",
      "0 2024-12-01 00:00:00    359   CALI       WND\n",
      "1 2024-12-01 01:00:00    277   CALI       WND\n",
      "2 2024-12-01 02:00:00    498   CALI       WND\n",
      "3 2024-12-01 03:00:00    692   CALI       WND\n",
      "4 2024-12-01 04:00:00    879   CALI       WND\n",
      "\n",
      "=== Testing Multi-Region Fetch ===\n",
      "[OK] CALI: 72 rows\n",
      "[OK] ERCO: 72 rows\n",
      "[OK] MISO: 72 rows\n",
      "[SUMMARY] WND data: 3 series, 216 total rows\n",
      "\n",
      "Multi-region: 216 rows\n",
      "Series: ['CALI_WND', 'ERCO_WND', 'MISO_WND']\n",
      "\n",
      "=== Series Summary ===\n",
      "  unique_id  count  min_value  max_value   mean_value  zero_count\n",
      "0  CALI_WND     72        158        882   433.500000           0\n",
      "1  ERCO_WND     72        747      11615  5995.819444           0\n",
      "2  MISO_WND     72       4395      20101  8800.875000           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:50:39,377 - __main__ - WARNING - [fetch_region][NEGATIVE] region=CALI fuel=SUN count=39 (54.2%) range=[-57.00, -11.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE] region=CALI fuel=SUN returned=72 offset=0 total=72 url=https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/?data%5B%5D=value&facets%5Brespondent%5D%5B%5D=CISO&facets%5Bfueltype%5D%5B%5D=SUN&frequency=hourly&start=2024-12-01T00&end=2024-12-03T23&length=5000&offset=0&sort%5B0%5D%5Bcolumn%5D=period&sort%5B0%5D%5Bdirection%5D=asc\n",
      "                   ds  value region fuel_type\n",
      "0 2024-12-01 00:00:00   7805   CALI       SUN\n",
      "1 2024-12-01 01:00:00   3369   CALI       SUN\n",
      "2 2024-12-01 02:00:00    186   CALI       SUN\n",
      "3 2024-12-01 03:00:00    -47   CALI       SUN\n",
      "4 2024-12-01 04:00:00    -41   CALI       SUN 72\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/eia_renewable.py\n",
    "# src/renewable/eia_renewable.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.parse import parse_qsl, urlencode, urlsplit, urlunsplit\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from src.renewable.regions import (REGIONS, get_eia_respondent,\n",
    "                                   validate_fuel_type, validate_region)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _sanitize_url(url: str) -> str:\n",
    "    parts = urlsplit(url)\n",
    "    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k.lower() != \"api_key\"]\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, urlencode(q), parts.fragment))\n",
    "\n",
    "\n",
    "def _load_env_once(*, debug: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load .env if present.\n",
    "    - Primary: find_dotenv(usecwd=True) (walk up from CWD)\n",
    "    - Fallback: repo_root/.env based on this file location\n",
    "    Returns the path loaded (or None).\n",
    "    \"\"\"\n",
    "    # 1) Try from current working directory upward\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "        if debug:\n",
    "            logger.info(\"Loaded .env via find_dotenv: %s\", dotenv_path)\n",
    "        return dotenv_path\n",
    "\n",
    "    # 2) Fallback: assume src-layout -> repo root is ../../ from this file\n",
    "    try:\n",
    "        repo_root = Path(__file__).resolve().parents[2]\n",
    "        fallback = repo_root / \".env\"\n",
    "        if fallback.exists():\n",
    "            load_dotenv(fallback, override=False)\n",
    "            if debug:\n",
    "                logger.info(\"Loaded .env via fallback: %s\", str(fallback))\n",
    "            return str(fallback)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"No .env found to load.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "class EIARenewableFetcher:\n",
    "    BASE_URL = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "    MAX_RECORDS_PER_REQUEST = 5000\n",
    "    RATE_LIMIT_DELAY = 0.2  # 5 requests/second max\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, *, timeout: int = 60, debug_env: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize API key and configuration.\n",
    "\n",
    "        Args:\n",
    "            api_key: EIA API key (or reads from EIA_API_KEY env var)\n",
    "            timeout: Request timeout in seconds (default: 60)\n",
    "            debug_env: Enable debug logging for environment loading\n",
    "        \"\"\"\n",
    "        loaded_env = _load_env_once(debug=debug_env)\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"EIA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"EIA API key required but not found.\\n\"\n",
    "                \"- Ensure .env contains EIA_API_KEY=...\\n\"\n",
    "                \"- Ensure your process CWD is under the repo (so find_dotenv can locate it), OR\\n\"\n",
    "                \"- Pass api_key=... explicitly.\\n\"\n",
    "                f\"Loaded .env path: {loaded_env}\"\n",
    "            )\n",
    "\n",
    "        self.timeout = timeout\n",
    "        self.session = self._create_session()  # Add retry-enabled session\n",
    "\n",
    "        # Debug without leaking the key\n",
    "        if debug_env:\n",
    "            masked = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            logger.info(\"EIA_API_KEY loaded (masked): %s\", masked)\n",
    "            logger.info(\"Request timeout: %d seconds\", self.timeout)\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create requests Session with retry logic for transient errors.\"\"\"\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # Retry on server errors and rate limits\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_eia_response(payload: dict, *, request_url: Optional[str] = None) -> tuple[list[dict], dict]:\n",
    "        if not isinstance(payload, dict):\n",
    "            raise TypeError(f\"EIA payload is not a dict. type={type(payload)} url={request_url}\")\n",
    "\n",
    "        if \"error\" in payload and payload.get(\"response\") is None:\n",
    "            raise ValueError(f\"EIA returned error payload. url={request_url} error={payload.get('error')}\")\n",
    "\n",
    "        if \"response\" not in payload:\n",
    "            raise ValueError(\n",
    "                f\"EIA payload missing 'response'. url={request_url} keys={list(payload.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        response = payload.get(\"response\") or {}\n",
    "        if not isinstance(response, dict):\n",
    "            raise TypeError(f\"EIA payload['response'] is not a dict. type={type(response)} url={request_url}\")\n",
    "\n",
    "        if \"data\" not in response:\n",
    "            raise ValueError(\n",
    "                f\"EIA response missing 'data'. url={request_url} response_keys={list(response.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        records = response.get(\"data\") or []\n",
    "        if not isinstance(records, list):\n",
    "            raise TypeError(f\"EIA response['data'] is not a list. type={type(records)} url={request_url}\")\n",
    "\n",
    "        total = response.get(\"total\", None)\n",
    "        offset = response.get(\"offset\", None)\n",
    "\n",
    "        meta_obj = response.get(\"metadata\") or {}\n",
    "        if isinstance(meta_obj, dict):\n",
    "            if total is None and \"total\" in meta_obj:\n",
    "                total = meta_obj.get(\"total\")\n",
    "            if offset is None and \"offset\" in meta_obj:\n",
    "                offset = meta_obj.get(\"offset\")\n",
    "\n",
    "        try:\n",
    "            total = int(total) if total is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            offset = int(offset) if offset is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return records, {\"total\": total, \"offset\": offset}\n",
    "\n",
    "    def fetch_region(\n",
    "        self,\n",
    "        region: str,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "        diag: Optional[dict] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region):\n",
    "            raise ValueError(f\"Invalid region: {region}\")\n",
    "        if not validate_fuel_type(fuel_type):\n",
    "            raise ValueError(f\"Invalid fuel type: {fuel_type}\")\n",
    "\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        all_records: list[dict] = []\n",
    "        offset = 0\n",
    "\n",
    "        # ✅ FIX: initialize loop diagnostics counters\n",
    "        page_count = 0\n",
    "        total_hint: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"api_key\": self.api_key,\n",
    "                \"data[]\": \"value\",\n",
    "                \"facets[respondent][]\": respondent,\n",
    "                \"facets[fueltype][]\": fuel_type,\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"start\": f\"{start_date}T00\",\n",
    "                \"end\": f\"{end_date}T23\",\n",
    "                \"length\": self.MAX_RECORDS_PER_REQUEST,\n",
    "                \"offset\": offset,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"asc\",\n",
    "            }\n",
    "\n",
    "            resp = self.session.get(self.BASE_URL, params=params, timeout=self.timeout)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "\n",
    "            records, meta = self._extract_eia_response(payload, request_url=resp.url)\n",
    "\n",
    "            page_count += 1\n",
    "            if total_hint is None:\n",
    "                total_hint = meta.get(\"total\")\n",
    "\n",
    "            returned = len(records)\n",
    "\n",
    "            if debug:\n",
    "                safe_url = _sanitize_url(resp.url)\n",
    "                print(\n",
    "                    f\"[PAGE] region={region} fuel={fuel_type} returned={returned} \"\n",
    "                    f\"offset={offset} total={meta.get('total')} url={safe_url}\"\n",
    "                )\n",
    "\n",
    "            # Empty on first page: legitimate empty series for that window\n",
    "            if returned == 0 and offset == 0:\n",
    "                if diag is not None:\n",
    "                    diag.update({\n",
    "                        \"region\": region,\n",
    "                        \"fuel_type\": fuel_type,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date,\n",
    "                        \"total_records\": total_hint,\n",
    "                        \"pages\": page_count,\n",
    "                        \"rows_parsed\": 0,\n",
    "                        \"empty\": True,\n",
    "                    })\n",
    "                return pd.DataFrame(columns=[\"ds\", \"value\", \"region\", \"fuel_type\"])\n",
    "\n",
    "            if returned == 0:\n",
    "                break\n",
    "\n",
    "            all_records.extend(records)\n",
    "\n",
    "            if returned < self.MAX_RECORDS_PER_REQUEST:\n",
    "                break\n",
    "\n",
    "            offset += self.MAX_RECORDS_PER_REQUEST\n",
    "            time.sleep(self.RATE_LIMIT_DELAY)\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        missing_cols = [c for c in [\"period\", \"value\"] if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            sample_keys = sorted(set().union(*(r.keys() for r in all_records[:5]))) if all_records else []\n",
    "            raise ValueError(\n",
    "                f\"EIA records missing expected keys {missing_cols}. \"\n",
    "                f\"columns={df.columns.tolist()} sample_record_keys={sample_keys}\"\n",
    "            )\n",
    "\n",
    "        # EIA returns timestamps in UTC format WITHOUT timezone marker (e.g., \"2026-01-21T00\")\n",
    "        # Simply parse and treat as UTC (no conversion needed)\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"period\"], utc=True, errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"region\"] = region\n",
    "        df[\"fuel_type\"] = fuel_type\n",
    "\n",
    "        df = df.dropna(subset=[\"ds\", \"value\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        # Log negative values for investigation (but don't clamp - let dataset builder handle)\n",
    "        neg_mask = df[\"value\"] < 0\n",
    "        if neg_mask.any():\n",
    "            neg_count = int(neg_mask.sum())\n",
    "            neg_min = float(df.loc[neg_mask, \"value\"].min())\n",
    "            neg_max = float(df.loc[neg_mask, \"value\"].max())\n",
    "            neg_pct = 100 * neg_count / max(len(df), 1)\n",
    "            logger.warning(\n",
    "                \"[fetch_region][NEGATIVE] region=%s fuel=%s count=%d (%.1f%%) range=[%.2f, %.2f]\",\n",
    "                region, fuel_type, neg_count, neg_pct, neg_min, neg_max,\n",
    "            )\n",
    "            # Log sample for debugging\n",
    "            neg_sample = df.loc[neg_mask, [\"ds\", \"value\"]].head(5)\n",
    "            for _, row in neg_sample.iterrows():\n",
    "                logger.debug(\"  ds=%s value=%.2f\", row[\"ds\"], row[\"value\"])\n",
    "\n",
    "            # NOTE: Keeping negative values in raw data for transparency\n",
    "            # Dataset builder will handle negatives according to configured policy\n",
    "\n",
    "        if diag is not None:\n",
    "            diag.update({\n",
    "                \"region\": region,\n",
    "                \"fuel_type\": fuel_type,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"total_records\": total_hint,\n",
    "                \"pages\": page_count,\n",
    "                \"rows_parsed\": int(len(df)),\n",
    "                \"empty\": bool(len(df) == 0),\n",
    "            })\n",
    "\n",
    "        return df[[\"ds\", \"value\", \"region\", \"fuel_type\"]]\n",
    "\n",
    "    def fetch_all_regions(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        regions: Optional[list[str]] = None,\n",
    "        max_workers: int = 3,\n",
    "        diagnostics: Optional[list[dict]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Fetch generation data for all regions for a given fuel type.\n",
    "\n",
    "        Args:\n",
    "            fuel_type: Fuel type code (WND, SUN, etc.)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            regions: List of region codes (defaults to all non-US48 regions)\n",
    "            max_workers: Number of parallel workers\n",
    "            diagnostics: Optional list to collect diagnostic info\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns [unique_id, ds, y]\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If no regions could be fetched (complete failure)\n",
    "        \"\"\"\n",
    "        if regions is None:\n",
    "            # Only include regions with configured EIA respondent (exclude US48 and None)\n",
    "            regions = [\n",
    "                code for code, info in REGIONS.items()\n",
    "                if code != \"US48\" and info.eia_respondent is not None\n",
    "            ]\n",
    "\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        failed_regions: list[tuple[str, str]] = []  # (region, error_msg)\n",
    "\n",
    "        def _run_one(region: str) -> tuple[str, pd.DataFrame, dict]:\n",
    "            d: dict = {}\n",
    "            df = self.fetch_region(region, fuel_type, start_date, end_date, diag=d)\n",
    "            return region, df, d\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(_run_one, region): region for region in regions}\n",
    "            for future in as_completed(futures):\n",
    "                region = futures[future]\n",
    "                try:\n",
    "                    _, df, d = future.result()\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append(d)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                        print(f\"[OK] {region}: {len(df)} rows\")\n",
    "                    else:\n",
    "                        print(f\"[EMPTY] {region}: 0 rows\")\n",
    "                        failed_regions.append((region, \"Empty response (0 rows)\"))\n",
    "                except Exception as e:\n",
    "                    failed_regions.append((region, str(e)))\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append({\n",
    "                            \"region\": region,\n",
    "                            \"fuel_type\": fuel_type,\n",
    "                            \"start_date\": start_date,\n",
    "                            \"end_date\": end_date,\n",
    "                            \"error\": str(e),\n",
    "                        })\n",
    "                    print(f\"[FAIL] {region}: {e}\")\n",
    "\n",
    "        # Explicit validation: require at least one successful region\n",
    "        if not all_dfs:\n",
    "            error_details = \"; \".join([f\"{r[0]}({r[1][:80]})\" for r in failed_regions])\n",
    "            raise RuntimeError(\n",
    "                f\"[EIA][FETCH] Failed to fetch {fuel_type} data for ALL regions. \"\n",
    "                f\"Failures: {error_details}. \"\n",
    "                f\"Check EIA API availability, API key validity, network connectivity, \"\n",
    "                f\"and consider increasing timeout or reducing concurrency.\"\n",
    "            )\n",
    "\n",
    "        # Warn if partial failure (some regions succeeded, some failed)\n",
    "        if failed_regions:\n",
    "            failed_count = len(failed_regions)\n",
    "            total_count = len(regions)\n",
    "            print(f\"[WARNING] Partial {fuel_type} fetch: {failed_count}/{total_count} regions failed\")\n",
    "            for region, error_msg in failed_regions:\n",
    "                # Print first 100 chars of error\n",
    "                print(f\"  - {region}: {error_msg[:100]}\")\n",
    "\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined[\"unique_id\"] = combined[\"region\"] + \"_\" + combined[\"fuel_type\"]\n",
    "        combined = combined.rename(columns={\"value\": \"y\"})\n",
    "\n",
    "        result = combined[[\"unique_id\", \"ds\", \"y\"]].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        print(f\"[SUMMARY] {fuel_type} data: {result['unique_id'].nunique()} series, {len(result)} total rows\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_series_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.groupby(\"unique_id\").agg(\n",
    "            count=(\"y\", \"count\"),\n",
    "            min_value=(\"y\", \"min\"),\n",
    "            max_value=(\"y\", \"max\"),\n",
    "            mean_value=(\"y\", \"mean\"),\n",
    "            zero_count=(\"y\", lambda x: (x == 0).sum()),\n",
    "        ).reset_index()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 3: Weather Integration\n",
    "\n",
    "**File:** `src/renewable/open_meteo.py`\n",
    "\n",
    "Weather is **critical** for renewable forecasting:\n",
    "- **Wind generation** depends on wind speed (especially at hub height ~100m)\n",
    "- **Solar generation** depends on radiation and cloud cover\n",
    "\n",
    "## Key Concept: Preventing Leakage\n",
    "\n",
    "**Data leakage** occurs when training uses information that wouldn't be available at prediction time.\n",
    "\n",
    "```\n",
    "❌ WRONG: Using historical weather to predict future generation\n",
    "   - At prediction time, we don't have future actual weather!\n",
    "   \n",
    "✅ CORRECT: Use forecasted weather for predictions\n",
    "   - Training: historical weather aligned with historical generation\n",
    "   - Prediction: weather forecast for the prediction horizon\n",
    "```\n",
    "\n",
    "## Open-Meteo API\n",
    "\n",
    "Open-Meteo is **free** and requires no API key:\n",
    "- Historical API: Past weather data\n",
    "- Forecast API: Up to 16 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Historical Weather (REAL API) ===\n",
      "[OPENMETEO][HIST] status=200 url=https://archive-api.open-meteo.com/v1/archive?latitude=36.7&longitude=-119.4&start_date=2024-12-01&end_date=2024-12-03&hourly=temperature_2m%2Cwind_speed_10m%2Cwind_speed_100m%2Cwind_direction_10m%2Cdirect_radiation%2Cdiffuse_radiation%2Ccloud_cover&timezone=UTC\n",
      "[OPENMETEO][PARSE] rows=72 dup_ds=0 na_counts(sample)={'temperature_2m': 0, 'wind_speed_10m': 0, 'wind_speed_100m': 0}\n",
      "Historical rows: 72\n",
      "                   ds  temperature_2m  wind_speed_10m  wind_speed_100m  \\\n",
      "0 2024-12-01 00:00:00            12.2             5.9              7.0   \n",
      "1 2024-12-01 01:00:00             9.2             1.2              5.3   \n",
      "2 2024-12-01 02:00:00             8.4             2.4              3.6   \n",
      "3 2024-12-01 03:00:00             8.7             3.8              5.4   \n",
      "4 2024-12-01 04:00:00            10.3             0.9              2.2   \n",
      "\n",
      "   wind_direction_10m  direct_radiation  diffuse_radiation  cloud_cover region  \n",
      "0                  72              84.0               57.0          100   CALI  \n",
      "1                  27               6.0               12.0           41   CALI  \n",
      "2                  63               0.0                0.0          100   CALI  \n",
      "3                  90               0.0                0.0          100   CALI  \n",
      "4                 101               0.0                0.0          100   CALI  \n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/open_meteo.py\n",
    "# src/renewable/open_meteo.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from src.renewable.regions import get_region_coords, validate_region\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OpenMeteoEndpoints:\n",
    "    historical_url: str = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    forecast_url: str = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "\n",
    "class OpenMeteoRenewable:\n",
    "    \"\"\"\n",
    "    Fetch weather features for renewable energy forecasting.\n",
    "\n",
    "    Strict-by-default:\n",
    "    - If Open-Meteo doesn't return a requested variable, we raise.\n",
    "    - We do NOT fabricate values or silently \"fill\" missing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_VARS = [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"direct_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, timeout: int = 60, *, strict: bool = True):\n",
    "        self.timeout = timeout\n",
    "        self.strict = strict\n",
    "        self.endpoints = OpenMeteoEndpoints()\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def fetch_historical(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.historical_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][HIST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            # Log actual response content for debugging\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][HIST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        return self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "    def fetch_forecast(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        forecast_days = min((horizon_hours // 24) + 1, 16)\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"forecast_days\": forecast_days,\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.forecast_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][FCST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][FCST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        df = self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "        # Trim to requested horizon (ds is naive UTC)\n",
    "        if len(df) > 0:\n",
    "            cutoff = datetime.utcnow() + timedelta(hours=horizon_hours)\n",
    "            df = df[df[\"ds\"] <= cutoff].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_historical(lat, lon, start_date, end_date, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "    def fetch_all_regions_historical(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region(region, start_date, end_date, debug=debug)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def _parse_response(\n",
    "        self,\n",
    "        data: dict,\n",
    "        variables: list[str],\n",
    "        *,\n",
    "        debug: bool,\n",
    "        request_url: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        hourly = data.get(\"hourly\")\n",
    "        if not isinstance(hourly, dict):\n",
    "            raise ValueError(f\"Open-Meteo response missing/invalid 'hourly'. url={request_url}\")\n",
    "\n",
    "        times = hourly.get(\"time\")\n",
    "        if not isinstance(times, list) or len(times) == 0:\n",
    "            raise ValueError(f\"Open-Meteo response has no hourly time grid. url={request_url}\")\n",
    "\n",
    "        # Build ds (naive UTC)\n",
    "        ds = pd.to_datetime(times, errors=\"coerce\", utc=True).tz_localize(None)\n",
    "        if ds.isna().any():\n",
    "            bad = int(ds.isna().sum())\n",
    "            raise ValueError(f\"Open-Meteo returned unparsable times. bad={bad} url={request_url}\")\n",
    "\n",
    "        df_data = {\"ds\": ds}\n",
    "\n",
    "        # Strict variable presence: raise if missing (no silent None padding)\n",
    "        missing_vars = [v for v in variables if v not in hourly]\n",
    "        if missing_vars and self.strict:\n",
    "            raise ValueError(f\"Open-Meteo missing requested vars={missing_vars}. url={request_url}\")\n",
    "\n",
    "        for var in variables:\n",
    "            values = hourly.get(var)\n",
    "            if values is None:\n",
    "                # If not strict, keep as all-NA but be explicit (not hidden)\n",
    "                df_data[var] = [None] * len(ds)\n",
    "                continue\n",
    "\n",
    "            if not isinstance(values, list):\n",
    "                raise ValueError(f\"Open-Meteo var '{var}' not a list. type={type(values)} url={request_url}\")\n",
    "\n",
    "            if len(values) != len(ds):\n",
    "                raise ValueError(\n",
    "                    f\"Open-Meteo length mismatch for '{var}': \"\n",
    "                    f\"len(values)={len(values)} len(time)={len(ds)} url={request_url}\"\n",
    "                )\n",
    "\n",
    "            df_data[var] = pd.to_numeric(values, errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame(df_data).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if debug:\n",
    "            dup = int(df[\"ds\"].duplicated().sum())\n",
    "            na_counts = {v: int(df[v].isna().sum()) for v in variables if v in df.columns}\n",
    "            print(f\"[OPENMETEO][PARSE] rows={len(df)} dup_ds={dup} na_counts(sample)={dict(list(na_counts.items())[:3])}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region_forecast(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_forecast(lat, lon, horizon_hours=horizon_hours, variables=variables, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fetch_all_regions_forecast(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region_forecast(\n",
    "                    region, horizon_hours=horizon_hours, variables=variables, debug=debug\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Forecast weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RENEWABLE ENERGY EDA - Comprehensive Analysis\n",
      "================================================================================\n",
      "\n",
      "[1/5] Data Summary...\n",
      "   Generation: 4,358 rows, 6 series\n",
      "\n",
      "[2/5] Negative Value Investigation (CRITICAL)...\n",
      "   [WARNING] Found 403 negative values (9.25%)\n",
      "   [WARNING] Affected series: ['CALI_SUN']\n",
      "   [ANALYSIS] Root cause: UNKNOWN (confidence: LOW)\n",
      "   [RECOMMENDATION] \n",
      "\n",
      "[3/5] Seasonality Analysis...\n",
      "   [OK] Analyzed 3 series\n",
      "\n",
      "[4/5] Zero-Inflation Analysis...\n",
      "   [OK] Solar zero ratio: 15.4% (zeros at night expected)\n",
      "\n",
      "[5/5] Weather Alignment...\n",
      "   [OK] Merge success rate: 100.0%\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "[PREPROCESSING] Negative Handling: clamp_to_zero\n",
      "   Reason: \n",
      "   Confidence: LOW\n",
      "\n",
      "[MODELING] Forecast Constraints: Clip to [0, ∞)\n",
      "   Reason: Physical constraint - renewable generation cannot be negative\n",
      "\n",
      "[EVALUATION] Use RMSE/MAE, avoid MAPE\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] EDA complete. Report saved to: reports\\renewable\\eda\\20260122_165040\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/eda.py\n",
    "# file: src/renewable/eda.py\n",
    "\"\"\"\n",
    "Enhanced Exploratory Data Analysis for Renewable Energy Forecasting\n",
    "\n",
    "This module provides decision-driven EDA with emphasis on:\n",
    "1. Understanding WHY negative values exist (not just detecting them)\n",
    "2. Providing actionable recommendations based on findings\n",
    "3. Validating physical constraints for renewable energy data\n",
    "\n",
    "Key Principle: Renewable energy generation CANNOT be negative.\n",
    "- Solar panels produce 0-X power, never negative\n",
    "- Wind turbines produce 0-X power, never negative\n",
    "- Negative values in data are ALWAYS data quality issues that need investigation\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingRecommendation:\n",
    "    \"\"\"Structured recommendation from EDA for preprocessing.\"\"\"\n",
    "\n",
    "    # Negative value handling\n",
    "    negative_policy: str  # 'clamp_to_zero' | 'investigate' | 'pass_through'\n",
    "    negative_reason: str\n",
    "    negative_confidence: str  # 'HIGH' | 'MEDIUM' | 'LOW'\n",
    "\n",
    "    # Grid enforcement\n",
    "    max_missing_ratio: float = 0.02\n",
    "\n",
    "    # Series-specific overrides (for different fuel types)\n",
    "    series_overrides: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n",
    "\n",
    "    # Metadata\n",
    "    generated_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    data_summary: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EDARecommendations:\n",
    "    \"\"\"Complete EDA output with preprocessing recommendations.\"\"\"\n",
    "\n",
    "    preprocessing: PreprocessingRecommendation\n",
    "    modeling: Dict[str, Any]\n",
    "    evaluation: Dict[str, Any]\n",
    "\n",
    "    # Full investigation results for audit trail\n",
    "    negative_investigation: Dict[str, Any]\n",
    "    seasonality: Dict[str, Any]\n",
    "    zero_inflation: Dict[str, Any]\n",
    "    weather_alignment: Dict[str, Any]\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    def save(self, output_path: Path) -> None:\n",
    "        \"\"\"Save recommendations to JSON.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(asdict(self), f, indent=2, default=str)\n",
    "            f.write('\\n')\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, input_path: Path) -> 'EDARecommendations':\n",
    "        \"\"\"Load recommendations from JSON.\"\"\"\n",
    "        with open(input_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "class NegativeValueInvestigation:\n",
    "    \"\"\"\n",
    "    Deep investigation into why negative values exist in renewable generation data.\n",
    "\n",
    "    EIA Data Context:\n",
    "    - EIA reports \"net generation\" which is gross generation minus station use\n",
    "    - Station auxiliary loads (cooling, controls, etc.) can exceed generation during:\n",
    "      * Low-wind periods for wind farms\n",
    "      * Night/cloudy periods for solar (if inverters consume standby power)\n",
    "      * Startup/shutdown events\n",
    "\n",
    "    This is VALID data but represents net metering, not physical impossibility.\n",
    "    However, for FORECASTING purposes, we typically want to predict gross generation\n",
    "    or at minimum clamp to zero since negative \"production\" isn't meaningful for\n",
    "    grid planning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns [unique_id, ds, y] where y is generation\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.df['ds'] = pd.to_datetime(self.df['ds'])\n",
    "        self.df['hour'] = self.df['ds'].dt.hour\n",
    "        self.df['dow'] = self.df['ds'].dt.dayofweek\n",
    "        self.df['date'] = self.df['ds'].dt.date\n",
    "\n",
    "    def get_negative_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get high-level summary of negative values.\"\"\"\n",
    "        neg_mask = self.df['y'] < 0\n",
    "\n",
    "        summary = {\n",
    "            'total_rows': len(self.df),\n",
    "            'negative_count': int(neg_mask.sum()),\n",
    "            'negative_ratio': float(neg_mask.sum() / len(self.df)) if len(self.df) > 0 else 0,\n",
    "            'affected_series': self.df.loc[neg_mask, 'unique_id'].unique().tolist(),\n",
    "            'min_value': float(self.df['y'].min()),\n",
    "            'max_negative': float(self.df.loc[neg_mask, 'y'].max()) if neg_mask.any() else None,\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def analyze_negative_patterns(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Investigate WHEN and WHERE negatives occur to understand root cause.\n",
    "\n",
    "        Key questions:\n",
    "        1. Are negatives concentrated in specific hours? (auxiliary load pattern)\n",
    "        2. Are negatives concentrated in specific series? (regional data issue)\n",
    "        3. What's the magnitude? (small negatives = metering noise, large = real issue)\n",
    "        \"\"\"\n",
    "        neg_df = self.df[self.df['y'] < 0].copy()\n",
    "\n",
    "        if len(neg_df) == 0:\n",
    "            return {'status': 'no_negatives_found'}\n",
    "\n",
    "        analysis = {\n",
    "            'by_series': {},\n",
    "            'by_hour': {},\n",
    "            'by_dow': {},\n",
    "            'magnitude_analysis': {},\n",
    "            'temporal_clustering': {},\n",
    "        }\n",
    "\n",
    "        # 1. Analyze by series\n",
    "        for uid in neg_df['unique_id'].unique():\n",
    "            series_neg = neg_df[neg_df['unique_id'] == uid]\n",
    "            series_total = self.df[self.df['unique_id'] == uid]\n",
    "\n",
    "            fuel_type = uid.split('_')[1] if '_' in uid else 'UNKNOWN'\n",
    "\n",
    "            analysis['by_series'][uid] = {\n",
    "                'count': int(len(series_neg)),\n",
    "                'ratio': float(len(series_neg) / len(series_total)),\n",
    "                'fuel_type': fuel_type,\n",
    "                'min_value': float(series_neg['y'].min()),\n",
    "                'max_value': float(series_neg['y'].max()),\n",
    "                'mean_value': float(series_neg['y'].mean()),\n",
    "                'std_value': float(series_neg['y'].std()),\n",
    "            }\n",
    "\n",
    "        # 2. Analyze by hour (Are negatives at night for solar? Low-wind hours for wind?)\n",
    "        hour_counts = neg_df.groupby('hour').size()\n",
    "        total_by_hour = self.df.groupby('hour').size()\n",
    "        neg_ratio_by_hour = (hour_counts / total_by_hour).fillna(0)\n",
    "\n",
    "        analysis['by_hour'] = {\n",
    "            'counts': hour_counts.to_dict(),\n",
    "            'ratios': neg_ratio_by_hour.to_dict(),\n",
    "            'peak_negative_hour': int(neg_ratio_by_hour.idxmax()) if len(neg_ratio_by_hour) > 0 else None,\n",
    "        }\n",
    "\n",
    "        # 3. Magnitude analysis - categorize severity\n",
    "        neg_values = neg_df['y'].values\n",
    "        analysis['magnitude_analysis'] = {\n",
    "            'tiny_negatives_count': int((neg_values > -10).sum()),  # Likely metering noise\n",
    "            'small_negatives_count': int(((neg_values <= -10) & (neg_values > -100)).sum()),\n",
    "            'medium_negatives_count': int(((neg_values <= -100) & (neg_values > -1000)).sum()),\n",
    "            'large_negatives_count': int((neg_values <= -1000).sum()),  # Significant issue\n",
    "            'percentiles': {\n",
    "                'p5': float(np.percentile(neg_values, 5)),\n",
    "                'p25': float(np.percentile(neg_values, 25)),\n",
    "                'p50': float(np.percentile(neg_values, 50)),\n",
    "                'p75': float(np.percentile(neg_values, 75)),\n",
    "                'p95': float(np.percentile(neg_values, 95)),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 4. Check for temporal clustering (consecutive hours of negatives)\n",
    "        for uid in neg_df['unique_id'].unique():\n",
    "            series_df = self.df[self.df['unique_id'] == uid].sort_values('ds')\n",
    "            series_df['is_negative'] = series_df['y'] < 0\n",
    "\n",
    "            # Find consecutive negative runs\n",
    "            series_df['neg_block'] = (series_df['is_negative'] != series_df['is_negative'].shift()).cumsum()\n",
    "            neg_blocks = series_df[series_df['is_negative']].groupby('neg_block').agg(\n",
    "                start=('ds', 'min'),\n",
    "                end=('ds', 'max'),\n",
    "                duration_hours=('ds', 'count'),\n",
    "                min_value=('y', 'min'),\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "            if len(neg_blocks) > 0:\n",
    "                analysis['temporal_clustering'][uid] = {\n",
    "                    'num_blocks': len(neg_blocks),\n",
    "                    'avg_block_duration_hours': float(neg_blocks['duration_hours'].mean()),\n",
    "                    'max_block_duration_hours': int(neg_blocks['duration_hours'].max()),\n",
    "                    'longest_block_start': str(neg_blocks.loc[neg_blocks['duration_hours'].idxmax(), 'start']),\n",
    "                }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def determine_root_cause(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Based on patterns, determine the likely root cause and recommend action.\n",
    "\n",
    "        Possible causes:\n",
    "        1. NET GENERATION DATA: EIA reports net = gross - auxiliary. This is valid.\n",
    "        2. METERING NOISE: Tiny negatives (-1 to -10 MWh) are measurement error.\n",
    "        3. DATA REPORTING ERROR: Large sporadic negatives are likely errors.\n",
    "        4. SYSTEMATIC ISSUE: Negatives always at same time = station use pattern.\n",
    "        \"\"\"\n",
    "        patterns = self.analyze_negative_patterns()\n",
    "\n",
    "        if patterns.get('status') == 'no_negatives_found':\n",
    "            return {\n",
    "                'root_cause': 'NONE',\n",
    "                'confidence': 'HIGH',\n",
    "                'recommendation': 'No action needed - data is clean',\n",
    "                'preprocessing_policy': 'pass_through',\n",
    "            }\n",
    "\n",
    "        magnitude = patterns.get('magnitude_analysis', {})\n",
    "        total_neg = sum([\n",
    "            magnitude.get('tiny_negatives_count', 0),\n",
    "            magnitude.get('small_negatives_count', 0),\n",
    "            magnitude.get('medium_negatives_count', 0),\n",
    "            magnitude.get('large_negatives_count', 0),\n",
    "        ])\n",
    "\n",
    "        # Determine root cause based on patterns\n",
    "        root_cause_analysis = {\n",
    "            'factors': [],\n",
    "            'root_cause': 'UNKNOWN',\n",
    "            'confidence': 'LOW',\n",
    "            'recommendation': '',\n",
    "            'preprocessing_policy': 'clamp_to_zero',\n",
    "        }\n",
    "\n",
    "        # Check if mostly tiny negatives (metering noise)\n",
    "        if magnitude.get('tiny_negatives_count', 0) / max(total_neg, 1) > 0.9:\n",
    "            root_cause_analysis['factors'].append('90%+ negatives are tiny (<10 MWh)')\n",
    "            root_cause_analysis['root_cause'] = 'METERING_NOISE'\n",
    "            root_cause_analysis['confidence'] = 'HIGH'\n",
    "            root_cause_analysis['recommendation'] = (\n",
    "                'Tiny negatives are measurement noise. Safe to clamp to 0.'\n",
    "            )\n",
    "            root_cause_analysis['preprocessing_policy'] = 'clamp_to_zero'\n",
    "\n",
    "        # Check if negatives are systematic (same hours)\n",
    "        elif patterns.get('by_hour', {}).get('peak_negative_hour') is not None:\n",
    "            hour_ratios = patterns.get('by_hour', {}).get('ratios', {})\n",
    "            max_ratio = max(hour_ratios.values()) if hour_ratios else 0\n",
    "\n",
    "            if max_ratio > 0.3:  # >30% of negatives in one hour\n",
    "                root_cause_analysis['factors'].append(f'Negatives concentrated at specific hours')\n",
    "                root_cause_analysis['root_cause'] = 'NET_GENERATION_AUXILIARY_LOAD'\n",
    "                root_cause_analysis['confidence'] = 'MEDIUM'\n",
    "                root_cause_analysis['recommendation'] = (\n",
    "                    'Negatives likely represent station auxiliary loads exceeding generation. '\n",
    "                    'This is valid net generation data. For forecasting, clamp to 0 since '\n",
    "                    'we want to predict usable power output.'\n",
    "                )\n",
    "                root_cause_analysis['preprocessing_policy'] = 'clamp_to_zero'\n",
    "\n",
    "        # Check for large sporadic negatives (data errors)\n",
    "        if magnitude.get('large_negatives_count', 0) > 0:\n",
    "            root_cause_analysis['factors'].append(f\"{magnitude.get('large_negatives_count')} large negatives (<-1000 MWh)\")\n",
    "\n",
    "            # If ONLY large negatives and they're sporadic, likely errors\n",
    "            if magnitude.get('tiny_negatives_count', 0) == 0 and magnitude.get('small_negatives_count', 0) == 0:\n",
    "                root_cause_analysis['root_cause'] = 'DATA_REPORTING_ERROR'\n",
    "                root_cause_analysis['confidence'] = 'MEDIUM'\n",
    "                root_cause_analysis['recommendation'] = (\n",
    "                    'Large sporadic negatives are likely data reporting errors. '\n",
    "                    'Recommend clamping to 0 or investigating with EIA.'\n",
    "                )\n",
    "\n",
    "        return root_cause_analysis\n",
    "\n",
    "    def generate_report(self, output_dir: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive negative value investigation report.\"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        summary = self.get_negative_summary()\n",
    "        patterns = self.analyze_negative_patterns()\n",
    "        root_cause = self.determine_root_cause()\n",
    "\n",
    "        report = {\n",
    "            'summary': summary,\n",
    "            'patterns': patterns,\n",
    "            'root_cause_analysis': root_cause,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        # Save JSON report\n",
    "        report_file = output_dir / 'negative_investigation.json'\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "            f.write('\\n')  # POSIX standard: files should end with newline\n",
    "\n",
    "        # Generate visualizations if negatives exist\n",
    "        if summary['negative_count'] > 0:\n",
    "            self._plot_negative_analysis(patterns, output_dir)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _plot_negative_analysis(self, patterns: Dict, output_dir: Path) -> None:\n",
    "        \"\"\"Create diagnostic plots for negative value analysis.\"\"\"\n",
    "        neg_df = self.df[self.df['y'] < 0]\n",
    "\n",
    "        if len(neg_df) == 0:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        # 1. Distribution of negative values\n",
    "        ax = axes[0, 0]\n",
    "        neg_values = neg_df['y'].values\n",
    "        ax.hist(neg_values, bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=np.median(neg_values), color='blue', linestyle='--',\n",
    "                   label=f'Median: {np.median(neg_values):.1f}')\n",
    "        ax.set_xlabel('Generation (MWh)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Distribution of Negative Values')\n",
    "        ax.legend()\n",
    "\n",
    "        # 2. Negative ratio by hour\n",
    "        ax = axes[0, 1]\n",
    "        hour_ratios = patterns.get('by_hour', {}).get('ratios', {})\n",
    "        if hour_ratios:\n",
    "            hours = sorted(hour_ratios.keys())\n",
    "            ratios = [hour_ratios[h] for h in hours]\n",
    "            ax.bar(hours, ratios, color='orange', alpha=0.7)\n",
    "            ax.set_xlabel('Hour of Day')\n",
    "            ax.set_ylabel('Negative Ratio')\n",
    "            ax.set_title('When Do Negatives Occur? (by Hour)')\n",
    "            ax.set_xticks(range(0, 24, 2))\n",
    "\n",
    "        # 3. Negative values by series\n",
    "        ax = axes[1, 0]\n",
    "        series_data = patterns.get('by_series', {})\n",
    "        if series_data:\n",
    "            series_names = list(series_data.keys())\n",
    "            series_counts = [series_data[s]['count'] for s in series_names]\n",
    "            colors = ['red' if 'SUN' in s else 'blue' for s in series_names]\n",
    "            ax.barh(series_names, series_counts, color=colors, alpha=0.7)\n",
    "            ax.set_xlabel('Negative Count')\n",
    "            ax.set_title('Negatives by Series (Blue=Wind, Red=Solar)')\n",
    "\n",
    "        # 4. Time series with negatives highlighted\n",
    "        ax = axes[1, 1]\n",
    "        # Plot first affected series\n",
    "        affected = patterns.get('by_series', {})\n",
    "        if affected:\n",
    "            first_series = list(affected.keys())[0]\n",
    "            series_df = self.df[self.df['unique_id'] == first_series].sort_values('ds')\n",
    "            ax.plot(series_df['ds'], series_df['y'], 'b-', alpha=0.5, label='Generation')\n",
    "            neg_mask = series_df['y'] < 0\n",
    "            ax.scatter(series_df.loc[neg_mask, 'ds'], series_df.loc[neg_mask, 'y'],\n",
    "                      c='red', s=20, label='Negative values', zorder=5)\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Generation (MWh)')\n",
    "            ax.set_title(f'Time Series: {first_series}')\n",
    "            ax.legend()\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'negative_investigation.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def run_full_eda(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    ") -> EDARecommendations:\n",
    "    \"\"\"\n",
    "    Run comprehensive EDA with emphasis on understanding data quality issues.\n",
    "\n",
    "    This function produces actionable insights, not just statistics.\n",
    "\n",
    "    Args:\n",
    "        generation_df: DataFrame with columns [unique_id, ds, y]\n",
    "        weather_df: DataFrame with columns [ds, region, weather_vars...]\n",
    "        output_dir: Directory to save all outputs\n",
    "\n",
    "    Returns:\n",
    "        EDARecommendations object with structured preprocessing recommendations\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_dir = output_dir / timestamp\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RENEWABLE ENERGY EDA - Comprehensive Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {\n",
    "        'timestamp': timestamp,\n",
    "        'output_dir': str(report_dir),\n",
    "        'data_summary': {},\n",
    "        'negative_investigation': {},\n",
    "        'seasonality': {},\n",
    "        'zero_inflation': {},\n",
    "        'weather_alignment': {},\n",
    "        'recommendations': {},\n",
    "    }\n",
    "\n",
    "    # 1. Data Summary\n",
    "    print(\"\\n[1/5] Data Summary...\")\n",
    "    results['data_summary'] = {\n",
    "        'generation_rows': len(generation_df),\n",
    "        'generation_series': generation_df['unique_id'].nunique(),\n",
    "        'series_list': generation_df['unique_id'].unique().tolist(),\n",
    "        'date_range': {\n",
    "            'start': str(generation_df['ds'].min()),\n",
    "            'end': str(generation_df['ds'].max()),\n",
    "        },\n",
    "        'weather_rows': len(weather_df),\n",
    "        'weather_regions': weather_df['region'].nunique() if 'region' in weather_df.columns else 0,\n",
    "    }\n",
    "    print(f\"   Generation: {results['data_summary']['generation_rows']:,} rows, \"\n",
    "          f\"{results['data_summary']['generation_series']} series\")\n",
    "\n",
    "    # 2. CRITICAL: Negative Value Investigation\n",
    "    print(\"\\n[2/5] Negative Value Investigation (CRITICAL)...\")\n",
    "    neg_investigator = NegativeValueInvestigation(generation_df)\n",
    "    results['negative_investigation'] = neg_investigator.generate_report(\n",
    "        report_dir / 'negative_values'\n",
    "    )\n",
    "\n",
    "    neg_summary = results['negative_investigation']['summary']\n",
    "    root_cause = results['negative_investigation']['root_cause_analysis']\n",
    "\n",
    "    if neg_summary['negative_count'] > 0:\n",
    "        print(f\"   [WARNING] Found {neg_summary['negative_count']} negative values \"\n",
    "              f\"({neg_summary['negative_ratio']:.2%})\")\n",
    "        print(f\"   [WARNING] Affected series: {neg_summary['affected_series']}\")\n",
    "        print(f\"   [ANALYSIS] Root cause: {root_cause['root_cause']} \"\n",
    "              f\"(confidence: {root_cause['confidence']})\")\n",
    "        print(f\"   [RECOMMENDATION] {root_cause['recommendation']}\")\n",
    "    else:\n",
    "        print(\"   [OK] No negative values found\")\n",
    "\n",
    "    # 3. Seasonality Analysis\n",
    "    print(\"\\n[3/5] Seasonality Analysis...\")\n",
    "    seasonality_dir = report_dir / 'seasonality'\n",
    "    seasonality_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seasonality_results = _analyze_seasonality(generation_df, seasonality_dir)\n",
    "    results['seasonality'] = seasonality_results\n",
    "    print(f\"   [OK] Analyzed {len(seasonality_results.get('series_analyzed', []))} series\")\n",
    "\n",
    "    # 4. Zero-Inflation Analysis\n",
    "    print(\"\\n[4/5] Zero-Inflation Analysis...\")\n",
    "    zero_dir = report_dir / 'zero_inflation'\n",
    "    zero_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    zero_results = _analyze_zero_inflation(generation_df, zero_dir)\n",
    "    results['zero_inflation'] = zero_results\n",
    "\n",
    "    solar_series = [uid for uid in generation_df['unique_id'].unique() if 'SUN' in uid]\n",
    "    if solar_series:\n",
    "        avg_zero = sum(\n",
    "            zero_results['series_zero_ratios'].get(uid, {}).get('zero_ratio', 0)\n",
    "            for uid in solar_series\n",
    "        ) / len(solar_series)\n",
    "        print(f\"   [OK] Solar zero ratio: {avg_zero:.1%} (zeros at night expected)\")\n",
    "\n",
    "    # 5. Weather Alignment\n",
    "    print(\"\\n[5/5] Weather Alignment...\")\n",
    "    weather_dir = report_dir / 'weather_alignment'\n",
    "    weather_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    weather_results = _analyze_weather_alignment(generation_df, weather_df, weather_dir)\n",
    "    results['weather_alignment'] = weather_results\n",
    "    print(f\"   [OK] Merge success rate: {weather_results['merge_success_ratio']:.1%}\")\n",
    "\n",
    "    # Generate Final Recommendations\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Build structured preprocessing recommendations\n",
    "    preprocessing_rec = PreprocessingRecommendation(\n",
    "        negative_policy=root_cause.get('preprocessing_policy', 'clamp_to_zero'),\n",
    "        negative_reason=root_cause['recommendation'],\n",
    "        negative_confidence=root_cause['confidence'],\n",
    "        max_missing_ratio=0.02,\n",
    "        series_overrides={},\n",
    "        data_summary=results['data_summary'],\n",
    "    )\n",
    "\n",
    "    # Add series-specific patterns if negatives found\n",
    "    patterns = results['negative_investigation'].get('patterns', {})\n",
    "    if neg_summary['negative_count'] > 0:\n",
    "        for series_id in neg_summary['affected_series']:\n",
    "            series_pattern = patterns.get('by_series', {}).get(series_id, {})\n",
    "            preprocessing_rec.series_overrides[series_id] = {\n",
    "                'negative_ratio': series_pattern.get('ratio', 0),\n",
    "                'suggested_policy': root_cause.get('preprocessing_policy'),\n",
    "            }\n",
    "\n",
    "    print(f\"\\n[PREPROCESSING] Negative Handling: {preprocessing_rec.negative_policy}\")\n",
    "    print(f\"   Reason: {preprocessing_rec.negative_reason}\")\n",
    "    print(f\"   Confidence: {preprocessing_rec.negative_confidence}\")\n",
    "\n",
    "    # Modeling recommendations\n",
    "    modeling_recs = {\n",
    "        'seasonality': 'Use MSTL with season_length=[24, 168] (daily + weekly)',\n",
    "        'forecast_constraints': 'ALWAYS clip forecasts and intervals to [0, ∞)',\n",
    "        'reason': 'Physical constraint: renewable generation cannot be negative',\n",
    "    }\n",
    "    print(f\"\\n[MODELING] Forecast Constraints: Clip to [0, ∞)\")\n",
    "    print(f\"   Reason: Physical constraint - renewable generation cannot be negative\")\n",
    "\n",
    "    # Evaluation recommendations\n",
    "    avg_zero = 0.0\n",
    "    if solar_series:\n",
    "        avg_zero = sum(\n",
    "            zero_results['series_zero_ratios'].get(uid, {}).get('zero_ratio', 0)\n",
    "            for uid in solar_series\n",
    "        ) / len(solar_series)\n",
    "\n",
    "    evaluation_recs = {\n",
    "        'metrics': ['RMSE', 'MAE'],\n",
    "        'avoid': 'MAPE (undefined when y=0)',\n",
    "        'reason': f\"Solar has {avg_zero:.1%} zeros (nighttime)\" if solar_series else \"Standard metrics\",\n",
    "    }\n",
    "    print(f\"\\n[EVALUATION] Use RMSE/MAE, avoid MAPE\")\n",
    "\n",
    "    # Build final EDARecommendations object\n",
    "    recommendations = EDARecommendations(\n",
    "        preprocessing=preprocessing_rec,\n",
    "        modeling=modeling_recs,\n",
    "        evaluation=evaluation_recs,\n",
    "        negative_investigation=results['negative_investigation'],\n",
    "        seasonality=results['seasonality'],\n",
    "        zero_inflation=results['zero_inflation'],\n",
    "        weather_alignment=results['weather_alignment'],\n",
    "    )\n",
    "\n",
    "    # Save structured recommendations\n",
    "    recommendations.save(report_dir / 'recommendations.json')\n",
    "\n",
    "    # Also save full report (backward compatibility)\n",
    "    results['recommendations'] = asdict(recommendations)\n",
    "    report_file = report_dir / 'eda_report.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "        f.write('\\n')  # POSIX standard: files should end with newline\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[SUCCESS] EDA complete. Report saved to: {report_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def _analyze_seasonality(df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze seasonal patterns in generation data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "    results = {\n",
    "        'series_analyzed': [],\n",
    "        'hourly_patterns': {},\n",
    "    }\n",
    "\n",
    "    for uid in df['unique_id'].unique()[:3]:  # Analyze first 3 series\n",
    "        series_data = df[df['unique_id'] == uid].copy()\n",
    "        series_data['hour'] = series_data['ds'].dt.hour\n",
    "\n",
    "        hourly_profile = series_data.groupby('hour')['y'].agg(['mean', 'std']).reset_index()\n",
    "        results['series_analyzed'].append(uid)\n",
    "        results['hourly_patterns'][uid] = hourly_profile.to_dict(orient='records')\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, len(results['series_analyzed']),\n",
    "                            figsize=(5 * len(results['series_analyzed']), 4))\n",
    "    if len(results['series_analyzed']) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, uid in enumerate(results['series_analyzed']):\n",
    "        series_data = df[df['unique_id'] == uid].copy()\n",
    "        series_data['hour'] = series_data['ds'].dt.hour\n",
    "        hourly_mean = series_data.groupby('hour')['y'].mean()\n",
    "        hourly_std = series_data.groupby('hour')['y'].std()\n",
    "\n",
    "        axes[idx].plot(hourly_mean.index, hourly_mean.values, marker='o')\n",
    "        axes[idx].fill_between(hourly_mean.index,\n",
    "                               hourly_mean - hourly_std,\n",
    "                               hourly_mean + hourly_std, alpha=0.3)\n",
    "        axes[idx].set_xlabel('Hour of Day')\n",
    "        axes[idx].set_ylabel('Generation (MWh)')\n",
    "        axes[idx].set_title(f'{uid} - Hourly Profile')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'hourly_profiles.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _analyze_zero_inflation(df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze zero values in generation data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "\n",
    "    results = {\n",
    "        'series_zero_ratios': {},\n",
    "    }\n",
    "\n",
    "    for uid in df['unique_id'].unique():\n",
    "        series_data = df[df['unique_id'] == uid]\n",
    "        zero_count = (series_data['y'] == 0).sum()\n",
    "        total_count = len(series_data)\n",
    "\n",
    "        results['series_zero_ratios'][uid] = {\n",
    "            'zero_count': int(zero_count),\n",
    "            'total_count': int(total_count),\n",
    "            'zero_ratio': float(zero_count / total_count) if total_count > 0 else 0,\n",
    "        }\n",
    "\n",
    "    # Visualization\n",
    "    solar_series = [uid for uid in df['unique_id'].unique() if 'SUN' in uid]\n",
    "    wind_series = [uid for uid in df['unique_id'].unique() if 'WND' in uid]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Solar zeros by hour\n",
    "    if solar_series:\n",
    "        solar_df = df[df['unique_id'].isin(solar_series)]\n",
    "        # Use vectorized approach: faster and no FutureWarning\n",
    "        solar_zero_by_hour = (\n",
    "            solar_df.assign(is_zero=solar_df['y'].eq(0))\n",
    "            .groupby('hour')['is_zero']\n",
    "            .mean()\n",
    "        )\n",
    "        axes[0].bar(solar_zero_by_hour.index, solar_zero_by_hour.values,\n",
    "                   color='orange', alpha=0.7)\n",
    "        axes[0].set_xlabel('Hour of Day')\n",
    "        axes[0].set_ylabel('Zero Ratio')\n",
    "        axes[0].set_title('Solar: Zero Ratio by Hour (Night = Expected)')\n",
    "        axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Wind zeros by hour\n",
    "    if wind_series:\n",
    "        wind_df = df[df['unique_id'].isin(wind_series)]\n",
    "        # Use vectorized approach: faster and no FutureWarning\n",
    "        wind_zero_by_hour = (\n",
    "            wind_df.assign(is_zero=wind_df['y'].eq(0))\n",
    "            .groupby('hour')['is_zero']\n",
    "            .mean()\n",
    "        )\n",
    "        axes[1].bar(wind_zero_by_hour.index, wind_zero_by_hour.values,\n",
    "                   color='blue', alpha=0.7)\n",
    "        axes[1].set_xlabel('Hour of Day')\n",
    "        axes[1].set_ylabel('Zero Ratio')\n",
    "        axes[1].set_title('Wind: Zero Ratio by Hour')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'zero_inflation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _analyze_weather_alignment(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze weather-generation correlation.\"\"\"\n",
    "    generation_df = generation_df.copy()\n",
    "    weather_df = weather_df.copy()\n",
    "\n",
    "    generation_df['ds'] = pd.to_datetime(generation_df['ds'])\n",
    "    weather_df['ds'] = pd.to_datetime(weather_df['ds'])\n",
    "    generation_df['region'] = generation_df['unique_id'].str.split('_').str[0]\n",
    "\n",
    "    merged = generation_df.merge(weather_df, on=['ds', 'region'], how='left')\n",
    "\n",
    "    weather_vars = [c for c in weather_df.columns\n",
    "                   if c not in ['ds', 'region'] and c in merged.columns]\n",
    "\n",
    "    results = {\n",
    "        'merge_success_ratio': float(\n",
    "            merged[weather_vars[0]].notna().mean() if weather_vars else 0\n",
    "        ),\n",
    "        'correlation_by_fuel': {},\n",
    "    }\n",
    "\n",
    "    # Calculate correlations\n",
    "    wind_series = merged[merged['unique_id'].str.contains('WND')]\n",
    "    solar_series = merged[merged['unique_id'].str.contains('SUN')]\n",
    "\n",
    "    if len(wind_series) > 0:\n",
    "        wind_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in wind_series.columns:\n",
    "                corr = wind_series[['y', var]].corr().iloc[0, 1]\n",
    "                wind_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['WND'] = wind_corr\n",
    "\n",
    "    if len(solar_series) > 0:\n",
    "        solar_corr = {}\n",
    "        for var in weather_vars:\n",
    "            if var in solar_series.columns:\n",
    "                corr = solar_series[['y', var]].corr().iloc[0, 1]\n",
    "                solar_corr[var] = float(corr) if not pd.isna(corr) else 0.0\n",
    "        results['correlation_by_fuel']['SUN'] = solar_corr\n",
    "\n",
    "    # Save results\n",
    "    with open(output_dir / 'weather_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "        f.write('\\n')  # POSIX standard: files should end with newline\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Run EDA on renewable energy data.\"\"\"\n",
    "    import sys\n",
    "\n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "\n",
    "    if not generation_path.exists() or not weather_path.exists():\n",
    "        print(\"Data files not found. Run pipeline first.\")\n",
    "        \n",
    "\n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "    output_dir = Path(\"reports/renewable/eda\")\n",
    "\n",
    "    results = run_full_eda(generation_df, weather_df, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder based on EDA from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:50:41,213 - __main__ - INFO - ============================================================\n",
      "2026-01-22 16:50:41,213 - __main__ - INFO - DATASET BUILDER - Building Modeling Dataset\n",
      "2026-01-22 16:50:41,214 - __main__ - INFO - ============================================================\n",
      "2026-01-22 16:50:41,214 - __main__ - WARNING - [DATASET_BUILDER] No EDA recommendations - using defaults\n",
      "2026-01-22 16:50:41,214 - __main__ - WARNING -   RECOMMENDED: Run EDA first for data-driven decisions\n",
      "2026-01-22 16:50:41,218 - __main__ - INFO - Input: 4,358 rows, 6 series\n",
      "2026-01-22 16:50:41,218 - __main__ - INFO - Date range: 2025-12-23 00:00:00 to 2026-01-22 07:00:00\n",
      "2026-01-22 16:50:41,219 - __main__ - INFO - \n",
      "[1/4] Applying negative value policy (policy=investigate)...\n",
      "2026-01-22 16:50:41,221 - __main__ - INFO - ============================================================\n",
      "2026-01-22 16:50:41,221 - __main__ - INFO - DATASET BUILDER - Building Modeling Dataset\n",
      "2026-01-22 16:50:41,222 - __main__ - INFO - ============================================================\n",
      "2026-01-22 16:50:41,222 - __main__ - WARNING - [DATASET_BUILDER] No EDA recommendations - using defaults\n",
      "2026-01-22 16:50:41,222 - __main__ - WARNING -   RECOMMENDED: Run EDA first for data-driven decisions\n",
      "2026-01-22 16:50:41,225 - __main__ - INFO - Input: 4,358 rows, 6 series\n",
      "2026-01-22 16:50:41,225 - __main__ - INFO - Date range: 2025-12-23 00:00:00 to 2026-01-22 07:00:00\n",
      "2026-01-22 16:50:41,226 - __main__ - INFO - \n",
      "[1/4] Applying negative value policy (policy=clamp_to_zero)...\n",
      "2026-01-22 16:50:41,228 - __main__ - INFO - [PREPROCESSING] Clamped 403 negative values to 0 (9.25% of data)\n",
      "2026-01-22 16:50:41,229 - __main__ - INFO -   CALI_SUN: 403 negatives clamped (range: [-60.0, -5.0])\n",
      "2026-01-22 16:50:41,229 - __main__ - INFO - \n",
      "[2/4] Enforcing hourly grid (max_missing=2.0%)...\n",
      "2026-01-22 16:50:41,232 - __main__ - INFO - \n",
      "[3/4] Adding time features...\n",
      "2026-01-22 16:50:41,234 - __main__ - INFO -    Added: ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
      "2026-01-22 16:50:41,235 - __main__ - INFO - \n",
      "[4/4] Aligning weather data...\n",
      "2026-01-22 16:50:41,240 - __main__ - INFO -    Coverage: 100.0%\n",
      "2026-01-22 16:50:41,241 - __main__ - INFO -    Variables: ['temperature_2m', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'direct_radiation', 'diffuse_radiation', 'cloud_cover']\n",
      "2026-01-22 16:50:41,243 - __main__ - INFO - \n",
      "[REPORT] Saved to: data\\renewable\\preprocessing\\preprocessing_report.json\n",
      "2026-01-22 16:50:41,243 - __main__ - INFO - \n",
      "============================================================\n",
      "2026-01-22 16:50:41,244 - __main__ - INFO - DATASET BUILDER - Complete\n",
      "2026-01-22 16:50:41,244 - __main__ - INFO - Output: 4,358 rows, 6 series\n",
      "2026-01-22 16:50:41,244 - __main__ - INFO - Dropped: 0 rows\n",
      "2026-01-22 16:50:41,245 - __main__ - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST 1] Investigating negatives...\n",
      "NEGATIVE VALUES DETECTED: 403 negatives found.\n",
      "Run EDA first to investigate root cause:\n",
      "  from src.renewable.eda import run_full_eda\n",
      "  recommendations = run_full_eda(generation_df, weather_df, output_dir)\n",
      "Then use recommended policy from EDA.\n",
      "\n",
      "[TEST 2] Building with clamp_to_zero...\n",
      "\n",
      "Final dataset shape: (4358, 14)\n",
      "Columns: ['unique_id', 'ds', 'y', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'temperature_2m', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'direct_radiation', 'diffuse_radiation', 'cloud_cover']\n",
      "\n",
      "Sample:\n",
      "  unique_id                  ds     y  hour_sin  hour_cos   dow_sin  dow_cos  \\\n",
      "0  CALI_SUN 2025-12-23 00:00:00  3701  0.000000  1.000000  0.781831  0.62349   \n",
      "1  CALI_SUN 2025-12-23 01:00:00   393  0.258819  0.965926  0.781831  0.62349   \n",
      "2  CALI_SUN 2025-12-23 02:00:00    31  0.500000  0.866025  0.781831  0.62349   \n",
      "3  CALI_SUN 2025-12-23 03:00:00    37  0.707107  0.707107  0.781831  0.62349   \n",
      "4  CALI_SUN 2025-12-23 04:00:00    35  0.866025  0.500000  0.781831  0.62349   \n",
      "\n",
      "   temperature_2m  wind_speed_10m  wind_speed_100m  wind_direction_10m  \\\n",
      "0            17.5             1.9              3.5                 112   \n",
      "1            15.2             3.1              5.6                  90   \n",
      "2            14.4             3.3              3.2                  99   \n",
      "3            13.8             3.6              6.1                 127   \n",
      "4            13.2             4.5              7.9                 127   \n",
      "\n",
      "   direct_radiation  diffuse_radiation  cloud_cover  \n",
      "0              47.0               80.0          100  \n",
      "1               2.0               17.0          100  \n",
      "2               0.0                0.0          100  \n",
      "3               0.0                0.0          100  \n",
      "4               0.0                0.0          100  \n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/dataset_builder.py\n",
    "# file: src/renewable/dataset_builder.py\n",
    "\"\"\"\n",
    "Dataset Builder for Renewable Energy Forecasting\n",
    "\n",
    "This module transforms raw EIA/weather data into modeling-ready datasets with:\n",
    "1. Transparent preprocessing based on EDA findings\n",
    "2. Physical constraint enforcement (non-negativity)\n",
    "3. Comprehensive diagnostics\n",
    "\n",
    "KEY PRINCIPLE:\n",
    "Renewable energy generation CANNOT be negative. This is a physical law.\n",
    "- Solar panels: 0 to max capacity\n",
    "- Wind turbines: 0 to max capacity\n",
    "\n",
    "Any negative values in raw data are data quality issues (metering, net generation\n",
    "accounting, etc.) and should be handled transparently.\n",
    "\n",
    "Preprocessing Policies:\n",
    "- clamp_to_zero: Set negative values to 0 (recommended for most cases)\n",
    "- investigate: Fail with detailed diagnostics (for initial data exploration)\n",
    "- pass_through: No modification (only if you understand why negatives exist)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Weather variables from Open-Meteo\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "# Time features for modeling\n",
    "TIME_FEATURES = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NegativeValueReport:\n",
    "    \"\"\"Report on negative values found and handled.\"\"\"\n",
    "    total_negative_count: int\n",
    "    total_rows: int\n",
    "    negative_ratio: float\n",
    "    by_series: Dict[str, Dict[str, Any]]\n",
    "    action_taken: str\n",
    "    samples: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingReport:\n",
    "    \"\"\"Complete report of all preprocessing steps.\"\"\"\n",
    "    timestamp: str\n",
    "\n",
    "    # Input stats\n",
    "    input_rows: int\n",
    "    input_series: int\n",
    "    input_date_range: Dict[str, str]\n",
    "\n",
    "    # Negative handling\n",
    "    negative_report: NegativeValueReport\n",
    "\n",
    "    # Missing data\n",
    "    missing_hours_dropped: int\n",
    "    series_dropped_incomplete: List[str]\n",
    "\n",
    "    # Weather alignment\n",
    "    weather_coverage: float\n",
    "    weather_vars_used: List[str]\n",
    "\n",
    "    # Output stats\n",
    "    output_rows: int\n",
    "    output_series: int\n",
    "    output_features: List[str]\n",
    "\n",
    "    # Configuration used\n",
    "    config: Dict[str, Any]\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add cyclical time features (hour, day of week).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "\n",
    "    # Cyclical encoding (sin/cos transform)\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "\n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "\n",
    "def _apply_negative_policy(\n",
    "    df: pd.DataFrame,\n",
    "    policy: str,\n",
    ") -> Tuple[pd.DataFrame, NegativeValueReport]:\n",
    "    \"\"\"\n",
    "    Apply negative value policy (NO INVESTIGATION - that's EDA's job).\n",
    "\n",
    "    This function ONLY applies the policy. Investigation should be done\n",
    "    in eda.py before calling dataset builder.\n",
    "\n",
    "    Physical Reality:\n",
    "    - Renewable energy generation CANNOT be negative\n",
    "    - Negative values are ALWAYS data quality issues\n",
    "\n",
    "    Policies:\n",
    "    - clamp_to_zero: Set negatives to 0 (RECOMMENDED from EDA)\n",
    "    - investigate: Fail with message to run EDA first\n",
    "    - pass_through: No modification (if EDA recommends)\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with [unique_id, ds, y]\n",
    "        policy: Policy to apply (from EDA recommendations)\n",
    "\n",
    "    Returns:\n",
    "        (processed_df, report)\n",
    "    \"\"\"\n",
    "    neg_mask = df['y'] < 0\n",
    "    neg_count = int(neg_mask.sum())\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # DEBUG: Log what we're working with\n",
    "    logger.debug(f\"[_apply_negative_policy] Processing {total_rows} rows, found {neg_count} negatives\")\n",
    "\n",
    "    # Build report\n",
    "    by_series = {}\n",
    "    samples = []\n",
    "\n",
    "    if neg_count > 0:\n",
    "        for uid in df.loc[neg_mask, 'unique_id'].unique():\n",
    "            series_mask = (df['unique_id'] == uid) & neg_mask\n",
    "            series_neg = df.loc[series_mask]\n",
    "            series_total = len(df[df['unique_id'] == uid])\n",
    "\n",
    "            by_series[uid] = {\n",
    "                'count': int(series_mask.sum()),\n",
    "                'min_value': float(series_neg['y'].min()),\n",
    "                'max_value': float(series_neg['y'].max()),\n",
    "            }\n",
    "\n",
    "            # Just a few samples for audit\n",
    "            for _, row in series_neg.head(3).iterrows():\n",
    "                samples.append({\n",
    "                    'unique_id': row['unique_id'],\n",
    "                    'ds': str(row['ds']),\n",
    "                    'y': float(row['y']),\n",
    "                })\n",
    "\n",
    "    # Calculate negative ratio\n",
    "    negative_ratio = float(neg_count / total_rows) if total_rows > 0 else 0.0\n",
    "\n",
    "    # DEBUG: Log report creation\n",
    "    logger.debug(f\"[_apply_negative_policy] Creating report: {neg_count}/{total_rows} = {negative_ratio:.2%} negatives\")\n",
    "\n",
    "    report = NegativeValueReport(\n",
    "        total_negative_count=neg_count,\n",
    "        total_rows=total_rows,\n",
    "        negative_ratio=negative_ratio,\n",
    "        by_series=by_series,\n",
    "        action_taken=policy,\n",
    "        samples=samples,\n",
    "    )\n",
    "\n",
    "    # Apply policy\n",
    "    if neg_count == 0:\n",
    "        report.action_taken = 'none_needed'\n",
    "        return df.copy(), report\n",
    "\n",
    "    if policy == 'investigate':\n",
    "        raise ValueError(\n",
    "            f\"NEGATIVE VALUES DETECTED: {neg_count} negatives found.\\n\"\n",
    "            f\"Run EDA first to investigate root cause:\\n\"\n",
    "            f\"  from src.renewable.eda import run_full_eda\\n\"\n",
    "            f\"  recommendations = run_full_eda(generation_df, weather_df, output_dir)\\n\"\n",
    "            f\"Then use recommended policy from EDA.\"\n",
    "        )\n",
    "\n",
    "    elif policy == 'clamp_to_zero':\n",
    "        out = df.copy()\n",
    "        out['y'] = out['y'].clip(lower=0)\n",
    "        logger.info(\n",
    "            f\"[PREPROCESSING] Clamped {neg_count} negative values to 0 \"\n",
    "            f\"({100*neg_count/total_rows:.2f}% of data)\"\n",
    "        )\n",
    "\n",
    "        # Log per-series\n",
    "        for uid, info in by_series.items():\n",
    "            logger.info(\n",
    "                f\"  {uid}: {info['count']} negatives clamped \"\n",
    "                f\"(range: [{info['min_value']:.1f}, {info['max_value']:.1f}])\"\n",
    "            )\n",
    "\n",
    "        report.action_taken = 'clamped_to_zero'\n",
    "        return out, report\n",
    "\n",
    "    elif policy == 'pass_through':\n",
    "        logger.warning(f\"[PREPROCESSING] Passing through {neg_count} negative values\")\n",
    "        report.action_taken = 'passed_through'\n",
    "        return df.copy(), report\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown negative_policy: {policy}\")\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    max_missing_ratio: float = 0.02,\n",
    ") -> Tuple[pd.DataFrame, List[str], int]:\n",
    "    \"\"\"\n",
    "    Enforce complete hourly grid (no gaps).\n",
    "\n",
    "    For time series forecasting, we need continuous hourly data.\n",
    "    Series with too many gaps are dropped (no imputation - we don't fabricate data).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with [unique_id, ds, y]\n",
    "        max_missing_ratio: Maximum allowed ratio of missing hours\n",
    "\n",
    "    Returns:\n",
    "        (filtered_df, dropped_series, total_missing_hours)\n",
    "    \"\"\"\n",
    "    dropped_series = []\n",
    "    total_missing = 0\n",
    "\n",
    "    keep_rows = []\n",
    "\n",
    "    for uid, group in df.groupby('unique_id'):\n",
    "        group = group.sort_values('ds')\n",
    "        start = group['ds'].min()\n",
    "        end = group['ds'].max()\n",
    "\n",
    "        expected_hours = pd.date_range(start, end, freq='h')\n",
    "        actual_hours = len(group)\n",
    "        expected_count = len(expected_hours)\n",
    "\n",
    "        missing_count = expected_count - actual_hours\n",
    "        missing_ratio = missing_count / expected_count if expected_count > 0 else 0\n",
    "\n",
    "        total_missing += missing_count\n",
    "\n",
    "        if missing_ratio > max_missing_ratio:\n",
    "            dropped_series.append(uid)\n",
    "            logger.warning(\n",
    "                f\"[GRID] Dropping {uid}: missing {missing_count} hours \"\n",
    "                f\"({missing_ratio:.1%} > {max_missing_ratio:.1%} threshold)\"\n",
    "            )\n",
    "        else:\n",
    "            keep_rows.append(group)\n",
    "\n",
    "    if not keep_rows:\n",
    "        raise RuntimeError(\n",
    "            f\"All series dropped due to missing hours. \"\n",
    "            f\"Dropped: {dropped_series}. Consider increasing max_missing_ratio.\"\n",
    "        )\n",
    "\n",
    "    filtered = pd.concat(keep_rows, ignore_index=True)\n",
    "    return filtered, dropped_series, total_missing\n",
    "\n",
    "\n",
    "def _align_weather(\n",
    "    df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    ") -> Tuple[pd.DataFrame, float, List[str]]:\n",
    "    \"\"\"\n",
    "    Align weather data to generation timestamps.\n",
    "\n",
    "    Args:\n",
    "        df: Generation DataFrame (must have 'region' column or unique_id with region prefix)\n",
    "        weather_df: Weather DataFrame with [ds, region, weather_vars...]\n",
    "\n",
    "    Returns:\n",
    "        (merged_df, coverage_ratio, weather_vars_used)\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "\n",
    "    # Extract region from unique_id if not present\n",
    "    if 'region' not in work.columns:\n",
    "        work['region'] = work['unique_id'].str.split('_').str[0]\n",
    "\n",
    "    # Find available weather variables\n",
    "    available_vars = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "    if not available_vars:\n",
    "        raise ValueError(\n",
    "            f\"No weather variables found in weather_df. \"\n",
    "            f\"Expected: {WEATHER_VARS}, Got: {weather_df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # Merge\n",
    "    merged = work.merge(\n",
    "        weather_df[['ds', 'region'] + available_vars],\n",
    "        on=['ds', 'region'],\n",
    "        how='left',\n",
    "        validate='many_to_one',\n",
    "    )\n",
    "\n",
    "    # Check coverage\n",
    "    missing_weather = merged[available_vars].isna().any(axis=1)\n",
    "    coverage = 1 - (missing_weather.sum() / len(merged))\n",
    "\n",
    "    if missing_weather.any():\n",
    "        missing_count = int(missing_weather.sum())\n",
    "        logger.warning(\n",
    "            f\"[WEATHER] {missing_count} rows ({1-coverage:.1%}) missing weather data\"\n",
    "        )\n",
    "\n",
    "        # Drop rows with missing weather (no fabrication)\n",
    "        merged = merged[~missing_weather].reset_index(drop=True)\n",
    "        logger.warning(f\"[WEATHER] Dropped {missing_count} rows with missing weather\")\n",
    "\n",
    "    # Drop region column (not needed for modeling)\n",
    "    merged = merged.drop(columns=['region'])\n",
    "\n",
    "    return merged, coverage, available_vars\n",
    "\n",
    "\n",
    "def build_modeling_dataset(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    *,\n",
    "    negative_policy: str = 'clamp_to_zero',\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    eda_recommendations: Optional['PreprocessingRecommendation'] = None,\n",
    ") -> Tuple[pd.DataFrame, PreprocessingReport]:\n",
    "    \"\"\"\n",
    "    Build modeling-ready dataset from raw data.\n",
    "\n",
    "    RECOMMENDED: Run EDA first and pass recommendations via eda_recommendations.\n",
    "\n",
    "    Pipeline:\n",
    "    1. Validate inputs\n",
    "    2. Handle negative values (based on policy)\n",
    "    3. Enforce hourly grid (drop incomplete series)\n",
    "    4. Add time features\n",
    "    5. Align weather data\n",
    "\n",
    "    Args:\n",
    "        generation_df: Raw generation data [unique_id, ds, y]\n",
    "        weather_df: Raw weather data [ds, region, weather_vars...]\n",
    "        negative_policy: How to handle negative values (override if no EDA)\n",
    "            - 'clamp_to_zero': Set to 0 (RECOMMENDED)\n",
    "            - 'investigate': Fail with diagnostics\n",
    "            - 'pass_through': No modification\n",
    "        max_missing_ratio: Max ratio of missing hours before dropping series\n",
    "        output_dir: Optional directory for detailed reports\n",
    "        eda_recommendations: Recommendations from run_full_eda() (PREFERRED)\n",
    "\n",
    "    Returns:\n",
    "        (modeling_df, preprocessing_report)\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"DATASET BUILDER - Building Modeling Dataset\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    # Use EDA recommendations if provided\n",
    "    if eda_recommendations is not None:\n",
    "        negative_policy = eda_recommendations.negative_policy\n",
    "        max_missing_ratio = eda_recommendations.max_missing_ratio\n",
    "        logger.info(\"[DATASET_BUILDER] Using EDA recommendations\")\n",
    "        logger.info(f\"  Policy: {negative_policy} (confidence: {eda_recommendations.negative_confidence})\")\n",
    "        logger.info(f\"  Reason: {eda_recommendations.negative_reason}\")\n",
    "    else:\n",
    "        logger.warning(\"[DATASET_BUILDER] No EDA recommendations - using defaults\")\n",
    "        logger.warning(\"  RECOMMENDED: Run EDA first for data-driven decisions\")\n",
    "\n",
    "    # Validate inputs\n",
    "    required_gen = {'unique_id', 'ds', 'y'}\n",
    "    if not required_gen.issubset(generation_df.columns):\n",
    "        missing = required_gen - set(generation_df.columns)\n",
    "        raise ValueError(f\"generation_df missing columns: {missing}\")\n",
    "\n",
    "    if generation_df.empty:\n",
    "        raise ValueError(\"generation_df is empty\")\n",
    "\n",
    "    required_weather = {'ds', 'region'}\n",
    "    if not required_weather.issubset(weather_df.columns):\n",
    "        missing = required_weather - set(weather_df.columns)\n",
    "        raise ValueError(f\"weather_df missing columns: {missing}\")\n",
    "\n",
    "    # Ensure datetime\n",
    "    work = generation_df.copy()\n",
    "    work['ds'] = pd.to_datetime(work['ds'])\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df['ds'] = pd.to_datetime(weather_df['ds'])\n",
    "\n",
    "    input_rows = len(work)\n",
    "    input_series = work['unique_id'].nunique()\n",
    "    input_date_range = {\n",
    "        'start': str(work['ds'].min()),\n",
    "        'end': str(work['ds'].max()),\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Input: {input_rows:,} rows, {input_series} series\")\n",
    "    logger.info(f\"Date range: {input_date_range['start']} to {input_date_range['end']}\")\n",
    "\n",
    "    # Step 1: Apply negative value policy\n",
    "    logger.info(f\"\\n[1/4] Applying negative value policy (policy={negative_policy})...\")\n",
    "    work, neg_report = _apply_negative_policy(work, policy=negative_policy)\n",
    "\n",
    "    # Step 2: Enforce hourly grid\n",
    "    logger.info(f\"\\n[2/4] Enforcing hourly grid (max_missing={max_missing_ratio:.1%})...\")\n",
    "    work, dropped_series, missing_hours = _enforce_hourly_grid(\n",
    "        work, max_missing_ratio=max_missing_ratio\n",
    "    )\n",
    "\n",
    "    # Step 3: Add time features\n",
    "    logger.info(\"\\n[3/4] Adding time features...\")\n",
    "    work = _add_time_features(work)\n",
    "    logger.info(f\"   Added: {TIME_FEATURES}\")\n",
    "\n",
    "    # Step 4: Align weather\n",
    "    logger.info(\"\\n[4/4] Aligning weather data...\")\n",
    "    work, weather_coverage, weather_vars = _align_weather(work, weather_df)\n",
    "    logger.info(f\"   Coverage: {weather_coverage:.1%}\")\n",
    "    logger.info(f\"   Variables: {weather_vars}\")\n",
    "\n",
    "    # Sort and finalize\n",
    "    work = work.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "    output_rows = len(work)\n",
    "    output_series = work['unique_id'].nunique()\n",
    "    output_features = ['unique_id', 'ds', 'y'] + TIME_FEATURES + weather_vars\n",
    "\n",
    "    # Build report\n",
    "    report = PreprocessingReport(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        input_rows=input_rows,\n",
    "        input_series=input_series,\n",
    "        input_date_range=input_date_range,\n",
    "        negative_report=neg_report,\n",
    "        missing_hours_dropped=missing_hours,\n",
    "        series_dropped_incomplete=dropped_series,\n",
    "        weather_coverage=weather_coverage,\n",
    "        weather_vars_used=weather_vars,\n",
    "        output_rows=output_rows,\n",
    "        output_series=output_series,\n",
    "        output_features=output_features,\n",
    "        config={\n",
    "            'negative_policy': negative_policy,\n",
    "            'max_missing_ratio': max_missing_ratio,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save report if output_dir provided\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        report_dict = asdict(report)\n",
    "        report_dict['negative_report'] = asdict(report.negative_report)\n",
    "\n",
    "        report_file = output_dir / 'preprocessing_report.json'\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report_dict, f, indent=2, default=str)\n",
    "            f.write('\\n')  # POSIX standard: files should end with newline\n",
    "        logger.info(f\"\\n[REPORT] Saved to: {report_file}\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 60)\n",
    "    logger.info(\"DATASET BUILDER - Complete\")\n",
    "    logger.info(f\"Output: {output_rows:,} rows, {output_series} series\")\n",
    "    logger.info(f\"Dropped: {input_rows - output_rows:,} rows\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    return work, report\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset-Specific Builders\n",
    "# ============================================================================\n",
    "\n",
    "class RenewableDatasetBuilder:\n",
    "    \"\"\"Base class for fuel-type-specific dataset builders.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        eda_recommendations: Optional['PreprocessingRecommendation'] = None,\n",
    "    ):\n",
    "        self.fuel_type = fuel_type\n",
    "        self.eda_recommendations = eda_recommendations\n",
    "        self.config = self._get_default_config()\n",
    "\n",
    "        if eda_recommendations:\n",
    "            self.config['negative_policy'] = eda_recommendations.negative_policy\n",
    "            self.config['max_missing_ratio'] = eda_recommendations.max_missing_ratio\n",
    "\n",
    "    def _get_default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get default config for this fuel type.\"\"\"\n",
    "        return {\n",
    "            'negative_policy': 'clamp_to_zero',\n",
    "            'max_missing_ratio': 0.02,\n",
    "        }\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        generation_df: pd.DataFrame,\n",
    "        weather_df: pd.DataFrame,\n",
    "        output_dir: Optional[Path] = None,\n",
    "    ) -> Tuple[pd.DataFrame, PreprocessingReport]:\n",
    "        \"\"\"Build dataset using fuel-type-specific logic.\"\"\"\n",
    "\n",
    "        # Filter to this fuel type only\n",
    "        fuel_series = [uid for uid in generation_df['unique_id'].unique() if self.fuel_type in uid]\n",
    "\n",
    "        if not fuel_series:\n",
    "            raise ValueError(f\"No series found for fuel type: {self.fuel_type}\")\n",
    "\n",
    "        filtered_df = generation_df[generation_df['unique_id'].isin(fuel_series)].copy()\n",
    "\n",
    "        logger.info(f\"[{self.fuel_type}_BUILDER] Building dataset for {len(fuel_series)} series\")\n",
    "\n",
    "        return build_modeling_dataset(\n",
    "            filtered_df,\n",
    "            weather_df,\n",
    "            negative_policy=self.config['negative_policy'],\n",
    "            max_missing_ratio=self.config['max_missing_ratio'],\n",
    "            output_dir=output_dir,\n",
    "            eda_recommendations=self.eda_recommendations,\n",
    "        )\n",
    "\n",
    "\n",
    "class SolarDatasetBuilder(RenewableDatasetBuilder):\n",
    "    \"\"\"Solar-specific dataset builder.\"\"\"\n",
    "\n",
    "    def __init__(self, eda_recommendations: Optional['PreprocessingRecommendation'] = None):\n",
    "        super().__init__('SUN', eda_recommendations)\n",
    "\n",
    "    def _get_default_config(self) -> Dict[str, Any]:\n",
    "        config = super()._get_default_config()\n",
    "        # Solar: might tolerate slightly more missing data at night\n",
    "        config['max_missing_ratio'] = 0.03\n",
    "        return config\n",
    "\n",
    "\n",
    "class WindDatasetBuilder(RenewableDatasetBuilder):\n",
    "    \"\"\"Wind-specific dataset builder.\"\"\"\n",
    "\n",
    "    def __init__(self, eda_recommendations: Optional['PreprocessingRecommendation'] = None):\n",
    "        super().__init__('WND', eda_recommendations)\n",
    "\n",
    "\n",
    "def build_dataset_by_fuel_type(\n",
    "    generation_df: pd.DataFrame,\n",
    "    weather_df: pd.DataFrame,\n",
    "    fuel_type: str,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    eda_recommendations: Optional['PreprocessingRecommendation'] = None,\n",
    ") -> Tuple[pd.DataFrame, PreprocessingReport]:\n",
    "    \"\"\"\n",
    "    Factory function to build dataset using appropriate fuel-specific builder.\n",
    "\n",
    "    Args:\n",
    "        generation_df: Raw generation data\n",
    "        weather_df: Raw weather data\n",
    "        fuel_type: 'SUN' or 'WND'\n",
    "        output_dir: Optional output directory\n",
    "        eda_recommendations: Optional EDA recommendations\n",
    "\n",
    "    Returns:\n",
    "        (modeling_df, report)\n",
    "    \"\"\"\n",
    "    builders = {\n",
    "        'SUN': SolarDatasetBuilder,\n",
    "        'WND': WindDatasetBuilder,\n",
    "    }\n",
    "\n",
    "    if fuel_type not in builders:\n",
    "        raise ValueError(f\"Unknown fuel type: {fuel_type}\")\n",
    "\n",
    "    builder = builders[fuel_type](eda_recommendations)\n",
    "    return builder.build(generation_df, weather_df, output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Test dataset builder with real data.\"\"\"\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "\n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "\n",
    "    if not generation_path.exists() or not weather_path.exists():\n",
    "        print(\"Data files not found. Run pipeline first.\")\n",
    "        \n",
    "\n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "    # First investigate negatives\n",
    "    print(\"\\n[TEST 1] Investigating negatives...\")\n",
    "    try:\n",
    "        _, _ = build_modeling_dataset(\n",
    "            generation_df, weather_df,\n",
    "            negative_policy='investigate',\n",
    "            output_dir=Path(\"data/renewable/test_investigate\")\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # Then build with clamp\n",
    "    print(\"\\n[TEST 2] Building with clamp_to_zero...\")\n",
    "    modeling_df, report = build_modeling_dataset(\n",
    "        generation_df, weather_df,\n",
    "        negative_policy='clamp_to_zero',\n",
    "        output_dir=Path(\"data/renewable/preprocessing\")\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFinal dataset shape: {modeling_df.shape}\")\n",
    "    print(f\"Columns: {modeling_df.columns.tolist()}\")\n",
    "    print(f\"\\nSample:\")\n",
    "    print(modeling_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 4: Probabilistic Modeling\n",
    "\n",
    "**File:** `src/renewable/modeling.py`\n",
    "\n",
    "This is where the forecasting happens! We use **StatsForecast** for:\n",
    "\n",
    "1. **Multi-series forecasting**: Handle multiple regions/fuel types in one model\n",
    "2. **Probabilistic predictions**: Get prediction intervals, not just point forecasts\n",
    "3. **Weather exogenous**: Include weather features as predictors\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Prediction Intervals?\n",
    "\n",
    "Point forecasts are useful, but energy traders need **uncertainty quantification**:\n",
    "- **80% interval**: \"I'm 80% confident generation will be between X and Y\"\n",
    "- **95% interval**: Wider, for risk management\n",
    "\n",
    "### Zero-Value Safety (CRITICAL)\n",
    "\n",
    "**Solar panels generate ZERO at night!** This breaks MAPE:\n",
    "\n",
    "```\n",
    "MAPE = mean(|actual - predicted| / actual)\n",
    "\n",
    "When actual = 0:\n",
    "MAPE = |0 - pred| / 0 = undefined (division by zero!)\n",
    "```\n",
    "\n",
    "**Solution**: Always use RMSE and MAE for renewable forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/validation.py\n",
    "# file: src/renewable/validation.py\n",
    "\"\"\"Validation utilities for renewable generation data.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ValidationReport:\n",
    "    ok: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "def validate_generation_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lag_hours: int = 3,\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    expected_series: Optional[Iterable[str]] = None,\n",
    ") -> ValidationReport:\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    missing_cols = required - set(df.columns)\n",
    "    if missing_cols:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Missing required columns\",\n",
    "            {\"missing_cols\": sorted(missing_cols)},\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        return ValidationReport(False, \"Generation data is empty\", {})\n",
    "\n",
    "    work = df.copy()\n",
    "\n",
    "    work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"coerce\", utc=True)\n",
    "    if work[\"ds\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable ds values found\",\n",
    "            {\"bad_ds\": int(work[\"ds\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    work[\"y\"] = pd.to_numeric(work[\"y\"], errors=\"coerce\")\n",
    "    if work[\"y\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable y values found\",\n",
    "            {\"bad_y\": int(work[\"y\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    # Check for negative values and log warning (but allow to pass)\n",
    "    # Dataset builder will handle negatives according to configured policy\n",
    "    if (work[\"y\"] < 0).any():\n",
    "        import logging\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        neg_mask = work[\"y\"] < 0\n",
    "        neg_count = int(neg_mask.sum())\n",
    "        by_series = (\n",
    "            work[neg_mask]\n",
    "            .groupby(\"unique_id\")\n",
    "            .agg(count=(\"y\", \"count\"), min_y=(\"y\", \"min\"), max_y=(\"y\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        logger.warning(\n",
    "            \"[validation][NEGATIVE] Found %d negative values (%.1f%%) across %d series\",\n",
    "            neg_count,\n",
    "            100 * neg_count / len(work),\n",
    "            len(by_series)\n",
    "        )\n",
    "\n",
    "        for _, row in by_series.iterrows():\n",
    "            logger.warning(\n",
    "                \"  Series %s: %d negative values, range=[%.1f, %.1f]\",\n",
    "                row[\"unique_id\"], row[\"count\"], row[\"min_y\"], row[\"max_y\"]\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            \"[validation][NEGATIVE] Negatives will be handled by dataset builder \"\n",
    "            \"according to configured negative_policy\"\n",
    "        )\n",
    "\n",
    "        # Continue validation instead of failing\n",
    "        # (Dataset builder will handle negatives per policy)\n",
    "\n",
    "    dup = work.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Duplicate (unique_id, ds) rows found\",\n",
    "            {\"duplicates\": int(dup)},\n",
    "        )\n",
    "\n",
    "    if expected_series:\n",
    "        expected = sorted(set(expected_series))\n",
    "        present = sorted(set(work[\"unique_id\"]))\n",
    "        missing_series = sorted(set(expected) - set(present))\n",
    "        if missing_series:\n",
    "            return ValidationReport(\n",
    "                False,\n",
    "                \"Missing expected series\",\n",
    "                {\"missing_series\": missing_series, \"present_series\": present},\n",
    "            )\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "    max_ds = work[\"ds\"].max()\n",
    "    lag_hours = (now_utc - max_ds).total_seconds() / 3600.0\n",
    "    if lag_hours > max_lag_hours:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Data not fresh enough\",\n",
    "            {\n",
    "                \"now_utc\": now_utc.isoformat(),\n",
    "                \"max_ds\": max_ds.isoformat(),\n",
    "                \"lag_hours\": lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    series_max = work.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    series_lag = (now_utc - series_max).dt.total_seconds() / 3600.0\n",
    "    stale = series_lag[series_lag > max_lag_hours].sort_values(ascending=False)\n",
    "    if not stale.empty:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Stale series found\",\n",
    "            {\n",
    "                \"stale_series\": stale.head(10).to_dict(),\n",
    "                \"max_lag_hours\": max_lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    missing_ratios = {}\n",
    "    for uid, group in work.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")\n",
    "        start = group[\"ds\"].iloc[0]\n",
    "        end = group[\"ds\"].iloc[-1]\n",
    "        expected = int(((end - start) / pd.Timedelta(hours=1)) + 1)\n",
    "        actual = len(group)\n",
    "        missing = max(expected - actual, 0)\n",
    "        missing_ratios[uid] = missing / max(expected, 1)\n",
    "\n",
    "    worst_uid = max(missing_ratios, key=missing_ratios.get)\n",
    "    worst_ratio = missing_ratios[worst_uid]\n",
    "    if worst_ratio > max_missing_ratio:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Too many missing hourly points\",\n",
    "            {\"worst_uid\": worst_uid, \"worst_missing_ratio\": worst_ratio},\n",
    "        )\n",
    "\n",
    "    return ValidationReport(\n",
    "        True,\n",
    "        \"OK\",\n",
    "        {\n",
    "            \"row_count\": len(work),\n",
    "            \"series_count\": int(work[\"unique_id\"].nunique()),\n",
    "            \"max_ds\": max_ds.isoformat(),\n",
    "            \"lag_hours\": lag_hours,\n",
    "            \"worst_missing_ratio\": worst_ratio,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:50:41,286 - src.renewable.eia_renewable - INFO - Loaded .env via find_dotenv: c:\\docker_projects\\atsaf\\.env\n",
      "2026-01-22 16:50:41,287 - src.renewable.eia_renewable - INFO - EIA_API_KEY loaded (masked): 8pqH...8Xk7\n",
      "2026-01-22 16:50:41,287 - src.renewable.eia_renewable - INFO - Request timeout: 60 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Single Region Fetch ===\n",
      "[PAGE] region=CALI fuel=WND returned=72 offset=0 total=72 url=https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/?data%5B%5D=value&facets%5Brespondent%5D%5B%5D=CISO&facets%5Bfueltype%5D%5B%5D=WND&frequency=hourly&start=2024-12-01T00&end=2024-12-03T23&length=5000&offset=0&sort%5B0%5D%5Bcolumn%5D=period&sort%5B0%5D%5Bdirection%5D=asc\n",
      "Single region: 72 rows\n",
      "                   ds  value region fuel_type\n",
      "0 2024-12-01 00:00:00    359   CALI       WND\n",
      "1 2024-12-01 01:00:00    277   CALI       WND\n",
      "2 2024-12-01 02:00:00    498   CALI       WND\n",
      "3 2024-12-01 03:00:00    692   CALI       WND\n",
      "4 2024-12-01 04:00:00    879   CALI       WND\n",
      "\n",
      "=== Testing Multi-Region Fetch ===\n",
      "[OK] CALI: 72 rows\n",
      "[OK] MISO: 72 rows\n",
      "[OK] ERCO: 72 rows\n",
      "[SUMMARY] WND data: 3 series, 216 total rows\n",
      "\n",
      "Multi-region: 216 rows\n",
      "Series: ['CALI_WND', 'ERCO_WND', 'MISO_WND']\n",
      "\n",
      "=== Series Summary ===\n",
      "  unique_id  count  min_value  max_value   mean_value  zero_count\n",
      "0  CALI_WND     72        158        882   433.500000           0\n",
      "1  ERCO_WND     72        747      11615  5995.819444           0\n",
      "2  MISO_WND     72       4395      20101  8800.875000           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:51:13,565 - src.renewable.eia_renewable - WARNING - [fetch_region][NEGATIVE] region=CALI fuel=SUN count=39 (54.2%) range=[-57.00, -11.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE] region=CALI fuel=SUN returned=72 offset=0 total=72 url=https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/?data%5B%5D=value&facets%5Brespondent%5D%5B%5D=CISO&facets%5Bfueltype%5D%5B%5D=SUN&frequency=hourly&start=2024-12-01T00&end=2024-12-03T23&length=5000&offset=0&sort%5B0%5D%5Bcolumn%5D=period&sort%5B0%5D%5Bdirection%5D=asc\n",
      "                   ds  value region fuel_type\n",
      "0 2024-12-01 00:00:00   7805   CALI       SUN\n",
      "1 2024-12-01 01:00:00   3369   CALI       SUN\n",
      "2 2024-12-01 02:00:00    186   CALI       SUN\n",
      "3 2024-12-01 03:00:00    -47   CALI       SUN\n",
      "4 2024-12-01 04:00:00    -41   CALI       SUN 72\n",
      "=== Testing Historical Weather (REAL API) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:51:14,536 - src.renewable.dataset_builder - INFO - ============================================================\n",
      "2026-01-22 16:51:14,536 - src.renewable.dataset_builder - INFO - DATASET BUILDER - Building Modeling Dataset\n",
      "2026-01-22 16:51:14,537 - src.renewable.dataset_builder - INFO - ============================================================\n",
      "2026-01-22 16:51:14,537 - src.renewable.dataset_builder - WARNING - [DATASET_BUILDER] No EDA recommendations - using defaults\n",
      "2026-01-22 16:51:14,537 - src.renewable.dataset_builder - WARNING -   RECOMMENDED: Run EDA first for data-driven decisions\n",
      "2026-01-22 16:51:14,541 - src.renewable.dataset_builder - INFO - Input: 4,358 rows, 6 series\n",
      "2026-01-22 16:51:14,541 - src.renewable.dataset_builder - INFO - Date range: 2025-12-23 00:00:00 to 2026-01-22 07:00:00\n",
      "2026-01-22 16:51:14,541 - src.renewable.dataset_builder - INFO - \n",
      "[1/4] Applying negative value policy (policy=investigate)...\n",
      "2026-01-22 16:51:14,543 - src.renewable.dataset_builder - INFO - ============================================================\n",
      "2026-01-22 16:51:14,543 - src.renewable.dataset_builder - INFO - DATASET BUILDER - Building Modeling Dataset\n",
      "2026-01-22 16:51:14,544 - src.renewable.dataset_builder - INFO - ============================================================\n",
      "2026-01-22 16:51:14,544 - src.renewable.dataset_builder - WARNING - [DATASET_BUILDER] No EDA recommendations - using defaults\n",
      "2026-01-22 16:51:14,544 - src.renewable.dataset_builder - WARNING -   RECOMMENDED: Run EDA first for data-driven decisions\n",
      "2026-01-22 16:51:14,547 - src.renewable.dataset_builder - INFO - Input: 4,358 rows, 6 series\n",
      "2026-01-22 16:51:14,548 - src.renewable.dataset_builder - INFO - Date range: 2025-12-23 00:00:00 to 2026-01-22 07:00:00\n",
      "2026-01-22 16:51:14,548 - src.renewable.dataset_builder - INFO - \n",
      "[1/4] Applying negative value policy (policy=clamp_to_zero)...\n",
      "2026-01-22 16:51:14,550 - src.renewable.dataset_builder - INFO - [PREPROCESSING] Clamped 403 negative values to 0 (9.25% of data)\n",
      "2026-01-22 16:51:14,550 - src.renewable.dataset_builder - INFO -   CALI_SUN: 403 negatives clamped (range: [-60.0, -5.0])\n",
      "2026-01-22 16:51:14,551 - src.renewable.dataset_builder - INFO - \n",
      "[2/4] Enforcing hourly grid (max_missing=2.0%)...\n",
      "2026-01-22 16:51:14,553 - src.renewable.dataset_builder - INFO - \n",
      "[3/4] Adding time features...\n",
      "2026-01-22 16:51:14,556 - src.renewable.dataset_builder - INFO -    Added: ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
      "2026-01-22 16:51:14,556 - src.renewable.dataset_builder - INFO - \n",
      "[4/4] Aligning weather data...\n",
      "2026-01-22 16:51:14,561 - src.renewable.dataset_builder - INFO -    Coverage: 100.0%\n",
      "2026-01-22 16:51:14,562 - src.renewable.dataset_builder - INFO -    Variables: ['temperature_2m', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'direct_radiation', 'diffuse_radiation', 'cloud_cover']\n",
      "2026-01-22 16:51:14,565 - src.renewable.dataset_builder - INFO - \n",
      "[REPORT] Saved to: data\\renewable\\preprocessing\\preprocessing_report.json\n",
      "2026-01-22 16:51:14,565 - src.renewable.dataset_builder - INFO - \n",
      "============================================================\n",
      "2026-01-22 16:51:14,566 - src.renewable.dataset_builder - INFO - DATASET BUILDER - Complete\n",
      "2026-01-22 16:51:14,566 - src.renewable.dataset_builder - INFO - Output: 4,358 rows, 6 series\n",
      "2026-01-22 16:51:14,566 - src.renewable.dataset_builder - INFO - Dropped: 0 rows\n",
      "2026-01-22 16:51:14,567 - src.renewable.dataset_builder - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPENMETEO][HIST] status=200 url=https://archive-api.open-meteo.com/v1/archive?latitude=36.7&longitude=-119.4&start_date=2024-12-01&end_date=2024-12-03&hourly=temperature_2m%2Cwind_speed_10m%2Cwind_speed_100m%2Cwind_direction_10m%2Cdirect_radiation%2Cdiffuse_radiation%2Ccloud_cover&timezone=UTC\n",
      "[OPENMETEO][PARSE] rows=72 dup_ds=0 na_counts(sample)={'temperature_2m': 0, 'wind_speed_10m': 0, 'wind_speed_100m': 0}\n",
      "Historical rows: 72\n",
      "                   ds  temperature_2m  wind_speed_10m  wind_speed_100m  \\\n",
      "0 2024-12-01 00:00:00            12.2             5.9              7.0   \n",
      "1 2024-12-01 01:00:00             9.2             1.2              5.3   \n",
      "2 2024-12-01 02:00:00             8.4             2.4              3.6   \n",
      "3 2024-12-01 03:00:00             8.7             3.8              5.4   \n",
      "4 2024-12-01 04:00:00            10.3             0.9              2.2   \n",
      "\n",
      "   wind_direction_10m  direct_radiation  diffuse_radiation  cloud_cover region  \n",
      "0                  72              84.0               57.0          100   CALI  \n",
      "1                  27               6.0               12.0           41   CALI  \n",
      "2                  63               0.0                0.0          100   CALI  \n",
      "3                  90               0.0                0.0          100   CALI  \n",
      "4                 101               0.0                0.0          100   CALI  \n",
      "\n",
      "[TEST 1] Investigating negatives...\n",
      "NEGATIVE VALUES DETECTED: 403 negatives found.\n",
      "Run EDA first to investigate root cause:\n",
      "  from src.renewable.eda import run_full_eda\n",
      "  recommendations = run_full_eda(generation_df, weather_df, output_dir)\n",
      "Then use recommended policy from EDA.\n",
      "\n",
      "[TEST 2] Building with clamp_to_zero...\n",
      "\n",
      "Final dataset shape: (4358, 14)\n",
      "Columns: ['unique_id', 'ds', 'y', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'temperature_2m', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'direct_radiation', 'diffuse_radiation', 'cloud_cover']\n",
      "\n",
      "Sample:\n",
      "  unique_id                  ds     y  hour_sin  hour_cos   dow_sin  dow_cos  \\\n",
      "0  CALI_SUN 2025-12-23 00:00:00  3701  0.000000  1.000000  0.781831  0.62349   \n",
      "1  CALI_SUN 2025-12-23 01:00:00   393  0.258819  0.965926  0.781831  0.62349   \n",
      "2  CALI_SUN 2025-12-23 02:00:00    31  0.500000  0.866025  0.781831  0.62349   \n",
      "3  CALI_SUN 2025-12-23 03:00:00    37  0.707107  0.707107  0.781831  0.62349   \n",
      "4  CALI_SUN 2025-12-23 04:00:00    35  0.866025  0.500000  0.781831  0.62349   \n",
      "\n",
      "   temperature_2m  wind_speed_10m  wind_speed_100m  wind_direction_10m  \\\n",
      "0            17.5             1.9              3.5                 112   \n",
      "1            15.2             3.1              5.6                  90   \n",
      "2            14.4             3.3              3.2                  99   \n",
      "3            13.8             3.6              6.1                 127   \n",
      "4            13.2             4.5              7.9                 127   \n",
      "\n",
      "   direct_radiation  diffuse_radiation  cloud_cover  \n",
      "0              47.0               80.0          100  \n",
      "1               2.0               17.0          100  \n",
      "2               0.0                0.0          100  \n",
      "3               0.0                0.0          100  \n",
      "4               0.0                0.0          100  \n",
      "\n",
      "[TEST 3] Saving modeling dataset to data\\renewable\\modeling_dataset.parquet...\n",
      "✅ Saved 4,358 rows (64.8 KB)\n",
      "\n",
      "================================================================================\n",
      "SMOKE TEST: Renewable Forecasting Models\n",
      "================================================================================\n",
      "Loading data from: data\\renewable\\modeling_dataset.parquet\n",
      "Dataset shape: (4358, 14)\n",
      "Columns: ['unique_id', 'ds', 'y', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'temperature_2m', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'direct_radiation', 'diffuse_radiation', 'cloud_cover']\n",
      "Series: ['CALI_SUN', 'CALI_WND', 'ERCO_SUN', 'ERCO_WND', 'MISO_SUN', 'MISO_WND']\n",
      "Date range: 2025-12-23 00:00:00 to 2026-01-22 07:00:00\n",
      "\n",
      "Running cross-validation (3 windows, 168h step)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\docker_projects\\atsaf\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-22 16:51:15,773 - __main__ - INFO - [TRAIN] Prepared: 4,358 rows, 6 series, 11 exog features\n",
      "2026-01-22 16:51:15,774 - __main__ - INFO - [CV] Running: 3 windows, step=168h, horizon=24h\n",
      "2026-01-22 16:59:02,090 - __main__ - INFO - [PHYSICAL CONSTRAINT] Clipping 2772 values to >= 0.0 (24.7% of forecast values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEADERBOARD (sorted by RMSE)\n",
      "================================================================================\n",
      "        model         rmse         mae  valid_rows  coverage_80  coverage_95\n",
      "   MSTL_ARIMA  4492.481161 2804.936829         432     0.615741     0.780093\n",
      "    AutoTheta  4750.601078 2568.060959         432     0.780093     0.851852\n",
      "    AutoARIMA  4804.202861 2613.864402         432     0.759259     0.858796\n",
      "SeasonalNaive  5653.479147 3193.574074         432     0.761574     0.905093\n",
      "      AutoETS  7115.548627 3065.900143         432     0.715278     0.803241\n",
      "        index 10685.852922 7173.983796         432          NaN          NaN\n",
      "\n",
      "================================================================================\n",
      "PHYSICAL CONSTRAINT VALIDATION\n",
      "================================================================================\n",
      "Min forecast (MSTL_ARIMA): 0.00 MWh\n",
      "Max forecast (MSTL_ARIMA): 25864.32 MWh\n",
      "Any negative forecasts: False\n",
      "\n",
      "✅ SUCCESS: All forecasts are non-negative (physical constraints satisfied)\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/renewable/modeling.py\n",
    "# file: src/renewable/modeling.py\n",
    "\"\"\"\n",
    "Renewable Energy Forecasting Models\n",
    "\n",
    "This module provides probabilistic forecasting with PHYSICAL CONSTRAINTS.\n",
    "\n",
    "KEY PRINCIPLE:\n",
    "Statistical models (ARIMA, ETS, etc.) can produce negative forecasts and\n",
    "prediction intervals because they assume Gaussian errors. However:\n",
    "\n",
    "  RENEWABLE ENERGY GENERATION CANNOT BE NEGATIVE.\n",
    "\n",
    "This module enforces this physical constraint by clipping ALL forecasts\n",
    "and prediction intervals to [0, ∞). This is NOT \"defensive coding\" - it's\n",
    "applying domain knowledge about physical reality.\n",
    "\n",
    "Model Architecture:\n",
    "1. StatsForecast for multi-series probabilistic forecasting\n",
    "2. Post-processing to enforce [0, ∞) constraint\n",
    "3. Calibration check for prediction intervals\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "TIME_FEATURES = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    \"\"\"Configuration for forecasting.\"\"\"\n",
    "    horizon: int = 24\n",
    "    confidence_levels: Tuple[int, int] = (80, 95)\n",
    "\n",
    "    # Physical constraints\n",
    "    enforce_non_negative: bool = True  # ALWAYS True for renewable energy\n",
    "\n",
    "    # CV settings\n",
    "    cv_windows: int = 3\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "\n",
    "def enforce_physical_constraints(\n",
    "    df: pd.DataFrame,\n",
    "    min_value: float = 0.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce physical constraints on forecasts.\n",
    "\n",
    "    For renewable energy:\n",
    "    - Generation cannot be negative\n",
    "    - All forecast columns (point and intervals) are clipped to [0, ∞)\n",
    "\n",
    "    This is applying physical reality:\n",
    "    - A solar panel cannot generate negative power\n",
    "    - A wind turbine cannot generate negative power\n",
    "\n",
    "    Args:\n",
    "        df: Forecast DataFrame with columns like yhat, yhat_lo_80, yhat_hi_80, etc.\n",
    "        min_value: Minimum physical value (0 for generation)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all forecasts clipped to [min_value, ∞)\n",
    "    \"\"\"\n",
    "    # Identify forecast columns (exclude unique_id, ds, etc.)\n",
    "    exclude_cols = {'unique_id', 'ds', 'cutoff', 'y', 'region', 'fuel_type'}\n",
    "    forecast_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "    # Count values that will be clipped\n",
    "    clip_counts = {}\n",
    "    for col in forecast_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            below_min = (df[col] < min_value).sum()\n",
    "            if below_min > 0:\n",
    "                clip_counts[col] = int(below_min)\n",
    "\n",
    "    if clip_counts:\n",
    "        total_clipped = sum(clip_counts.values())\n",
    "        total_values = len(df) * len(forecast_cols)\n",
    "        logger.info(\n",
    "            f\"[PHYSICAL CONSTRAINT] Clipping {total_clipped} values to >= {min_value} \"\n",
    "            f\"({total_clipped/total_values:.1%} of forecast values)\"\n",
    "        )\n",
    "        for col, count in clip_counts.items():\n",
    "            logger.debug(f\"  {col}: {count} values clipped\")\n",
    "\n",
    "    # Apply constraint\n",
    "    result = df.copy()\n",
    "    for col in forecast_cols:\n",
    "        if col in result.columns and pd.api.types.is_numeric_dtype(result[col]):\n",
    "            result[col] = result[col].clip(lower=min_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute forecast evaluation metrics.\n",
    "\n",
    "    Uses RMSE and MAE (NOT MAPE because y=0 at night for solar).\n",
    "    \"\"\"\n",
    "    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true = y_true[valid_mask]\n",
    "    y_pred = y_pred[valid_mask]\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        return {'rmse': np.nan, 'mae': np.nan, 'valid_rows': 0}\n",
    "\n",
    "    errors = y_true - y_pred\n",
    "\n",
    "    return {\n",
    "        'rmse': float(np.sqrt(np.mean(errors ** 2))),\n",
    "        'mae': float(np.mean(np.abs(errors))),\n",
    "        'valid_rows': int(len(y_true)),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_coverage(\n",
    "    y_true: np.ndarray,\n",
    "    y_lo: np.ndarray,\n",
    "    y_hi: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Compute prediction interval coverage.\"\"\"\n",
    "    valid = np.isfinite(y_true) & np.isfinite(y_lo) & np.isfinite(y_hi)\n",
    "    if valid.sum() == 0:\n",
    "        return np.nan\n",
    "\n",
    "    in_interval = (y_true[valid] >= y_lo[valid]) & (y_true[valid] <= y_hi[valid])\n",
    "    return float(in_interval.mean())\n",
    "\n",
    "\n",
    "class RenewableForecastModel:\n",
    "    \"\"\"\n",
    "    Probabilistic forecasting model with physical constraints.\n",
    "\n",
    "    Uses StatsForecast for efficient multi-series forecasting,\n",
    "    then enforces non-negativity on all outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon: int = 24,\n",
    "        confidence_levels: Tuple[int, int] = (80, 95),\n",
    "    ):\n",
    "        self.horizon = horizon\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.sf = None\n",
    "        self._train_df = None\n",
    "        self._exog_cols: List[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def _prepare_training_df(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare training DataFrame.\n",
    "\n",
    "        Expects preprocessed data from dataset_builder (already has time features\n",
    "        and weather aligned).\n",
    "        \"\"\"\n",
    "        required = {'unique_id', 'ds', 'y'}\n",
    "        if not required.issubset(df.columns):\n",
    "            missing = required - set(df.columns)\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty DataFrame\")\n",
    "\n",
    "        # Check for required features\n",
    "        time_features = [c for c in TIME_FEATURES if c in df.columns]\n",
    "        weather_features = [c for c in WEATHER_VARS if c in df.columns]\n",
    "\n",
    "        if not time_features:\n",
    "            raise ValueError(\n",
    "                \"No time features found. Data should be preprocessed by dataset_builder.\"\n",
    "            )\n",
    "\n",
    "        self._exog_cols = time_features + weather_features\n",
    "\n",
    "        work = df.copy()\n",
    "        work['ds'] = pd.to_datetime(work['ds'])\n",
    "        work = work.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "        # Validate no negatives in training data\n",
    "        neg_count = (work['y'] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            raise ValueError(\n",
    "                f\"Training data contains {neg_count} negative values. \"\n",
    "                f\"Data should be preprocessed by dataset_builder with negative_policy='clamp_to_zero'.\"\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f\"[TRAIN] Prepared: {len(work):,} rows, {work['unique_id'].nunique()} series, \"\n",
    "            f\"{len(self._exog_cols)} exog features\"\n",
    "        )\n",
    "\n",
    "        return work\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fit models on training data.\n",
    "\n",
    "        Args:\n",
    "            df: Preprocessed DataFrame from dataset_builder\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import (MSTL, AutoARIMA, AutoETS,\n",
    "                                          SeasonalNaive)\n",
    "\n",
    "        train_df = self._prepare_training_df(df)\n",
    "\n",
    "        models = [\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "            AutoARIMA(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "        ]\n",
    "\n",
    "        # Try to add expanded models\n",
    "        try:\n",
    "            from statsforecast.models import AutoTheta\n",
    "            models.append(AutoTheta(season_length=24))\n",
    "            logger.info(\"[FIT] Using expanded model set: +AutoTheta\")\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "        self.sf = StatsForecast(models=models, freq='h', n_jobs=-1)\n",
    "        self._train_df = train_df\n",
    "        self.fitted = True\n",
    "\n",
    "        logger.info(f\"[FIT] Fitted {len(models)} models on {len(train_df):,} rows\")\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Perform cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            (cv_results, leaderboard)\n",
    "        \"\"\"\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import (MSTL, AutoARIMA, AutoETS,\n",
    "                                          SeasonalNaive)\n",
    "\n",
    "        train_df = self._prepare_training_df(df)\n",
    "\n",
    "        models = [\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "            AutoARIMA(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            from statsforecast.models import AutoTheta\n",
    "            models.append(AutoTheta(season_length=24))\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "        sf = StatsForecast(models=models, freq='h', n_jobs=-1)\n",
    "\n",
    "        logger.info(\n",
    "            f\"[CV] Running: {n_windows} windows, step={step_size}h, horizon={self.horizon}h\"\n",
    "        )\n",
    "\n",
    "        cv = sf.cross_validation(\n",
    "            df=train_df,\n",
    "            h=self.horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        # CRITICAL: Apply physical constraints to CV results\n",
    "        cv = enforce_physical_constraints(cv, min_value=0.0)\n",
    "\n",
    "        # Build leaderboard\n",
    "        leaderboard = self._build_leaderboard(cv)\n",
    "\n",
    "        return cv, leaderboard\n",
    "\n",
    "    def _build_leaderboard(self, cv_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Build model comparison leaderboard from CV results.\"\"\"\n",
    "        # Find model columns (not id/ds/cutoff/y, not interval columns)\n",
    "        exclude = {'unique_id', 'ds', 'cutoff', 'y'}\n",
    "        interval_pattern = re.compile(r'-(lo|hi)-\\d+$')\n",
    "\n",
    "        model_cols = [\n",
    "            c for c in cv_df.columns\n",
    "            if c not in exclude and not interval_pattern.search(c)\n",
    "        ]\n",
    "\n",
    "        rows = []\n",
    "        y_true = cv_df['y'].values\n",
    "\n",
    "        for model in model_cols:\n",
    "            y_pred = cv_df[model].values\n",
    "            metrics = compute_metrics(y_true, y_pred)\n",
    "\n",
    "            row = {\n",
    "                'model': model,\n",
    "                'rmse': metrics['rmse'],\n",
    "                'mae': metrics['mae'],\n",
    "                'valid_rows': metrics['valid_rows'],\n",
    "            }\n",
    "\n",
    "            # Add coverage for each confidence level\n",
    "            for level in self.confidence_levels:\n",
    "                lo_col = f\"{model}-lo-{level}\"\n",
    "                hi_col = f\"{model}-hi-{level}\"\n",
    "                if lo_col in cv_df.columns and hi_col in cv_df.columns:\n",
    "                    coverage = compute_coverage(\n",
    "                        y_true,\n",
    "                        cv_df[lo_col].values,\n",
    "                        cv_df[hi_col].values,\n",
    "                    )\n",
    "                    row[f'coverage_{level}'] = coverage\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        leaderboard = pd.DataFrame(rows)\n",
    "        leaderboard = leaderboard.sort_values('rmse').reset_index(drop=True)\n",
    "\n",
    "        return leaderboard\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        future_exog: pd.DataFrame,\n",
    "        best_model: Optional[str] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate forecasts.\n",
    "\n",
    "        Args:\n",
    "            future_exog: DataFrame with future exogenous features\n",
    "                         Must have [unique_id, ds] + exog features\n",
    "            best_model: If specified, only return this model's predictions\n",
    "\n",
    "        Returns:\n",
    "            Forecast DataFrame with physical constraints applied\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() first\")\n",
    "\n",
    "        # Build future X_df\n",
    "        X_df = self._build_future_X(future_exog)\n",
    "\n",
    "        # Generate forecasts\n",
    "        fcst = self.sf.forecast(\n",
    "            h=self.horizon,\n",
    "            df=self._train_df,\n",
    "            X_df=X_df,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        # CRITICAL: Apply physical constraints\n",
    "        fcst = enforce_physical_constraints(fcst, min_value=0.0)\n",
    "\n",
    "        # If best_model specified, filter\n",
    "        if best_model is not None:\n",
    "            if best_model not in fcst.columns:\n",
    "                available = [c for c in fcst.columns if c not in ['unique_id', 'ds']]\n",
    "                raise ValueError(\n",
    "                    f\"Model '{best_model}' not found. Available: {available}\"\n",
    "                )\n",
    "\n",
    "            keep_cols = ['unique_id', 'ds', best_model]\n",
    "            rename_map = {best_model: 'yhat'}\n",
    "\n",
    "            for level in self.confidence_levels:\n",
    "                lo = f\"{best_model}-lo-{level}\"\n",
    "                hi = f\"{best_model}-hi-{level}\"\n",
    "                if lo in fcst.columns:\n",
    "                    keep_cols.append(lo)\n",
    "                    rename_map[lo] = f'yhat_lo_{level}'\n",
    "                if hi in fcst.columns:\n",
    "                    keep_cols.append(hi)\n",
    "                    rename_map[hi] = f'yhat_hi_{level}'\n",
    "\n",
    "            fcst = fcst[keep_cols].rename(columns=rename_map)\n",
    "\n",
    "        return fcst\n",
    "\n",
    "    def _build_future_X(self, future_exog: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Build future exogenous feature DataFrame.\"\"\"\n",
    "        required = {'unique_id', 'ds'}\n",
    "        if not required.issubset(future_exog.columns):\n",
    "            missing = required - set(future_exog.columns)\n",
    "            raise ValueError(f\"future_exog missing: {missing}\")\n",
    "\n",
    "        # Check exog columns\n",
    "        missing_exog = [c for c in self._exog_cols if c not in future_exog.columns]\n",
    "        if missing_exog:\n",
    "            raise ValueError(\n",
    "                f\"future_exog missing required features: {missing_exog}. \"\n",
    "                f\"Expected: {self._exog_cols}\"\n",
    "            )\n",
    "\n",
    "        X = future_exog[['unique_id', 'ds'] + self._exog_cols].copy()\n",
    "        X = X.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "def compute_baseline_metrics(\n",
    "    cv_df: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    threshold_k: float = 2.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        cv_df: Cross-validation results\n",
    "        model_name: Model to compute baseline for\n",
    "        threshold_k: k for threshold = mean + k*std\n",
    "\n",
    "    Returns:\n",
    "        Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    if model_name not in cv_df.columns:\n",
    "        raise ValueError(f\"Model '{model_name}' not in CV results\")\n",
    "\n",
    "    # Compute per-window metrics\n",
    "    def window_rmse(g):\n",
    "        metrics = compute_metrics(g['y'].values, g[model_name].values)\n",
    "        return metrics['rmse']\n",
    "\n",
    "    per_window = cv_df.groupby(['unique_id', 'cutoff']).apply(\n",
    "        window_rmse, include_groups=False\n",
    "    )\n",
    "\n",
    "    rmse_mean = float(per_window.mean())\n",
    "    rmse_std = float(per_window.std())\n",
    "\n",
    "    baseline = {\n",
    "        'model': model_name,\n",
    "        'rmse_mean': rmse_mean,\n",
    "        'rmse_std': rmse_std,\n",
    "        'drift_threshold_rmse': rmse_mean + threshold_k * rmse_std,\n",
    "        'n_windows': int(per_window.notna().sum()),\n",
    "    }\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Smoke test for renewable forecasting models.\n",
    "\n",
    "    This test validates the complete modeling pipeline using real data:\n",
    "    1. Loads modeling dataset created by the pipeline\n",
    "    2. Runs cross-validation with multiple models\n",
    "    3. Validates physical constraints (no negative forecasts)\n",
    "    4. Displays performance leaderboard\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    from src.renewable.dataset_builder import build_modeling_dataset\n",
    "    from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "    from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n",
    "    \n",
    "    \n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n",
    "\n",
    "\n",
    "    generation_path = Path(\"data/renewable/generation.parquet\")\n",
    "    weather_path = Path(\"data/renewable/weather.parquet\")\n",
    "\n",
    "    if not generation_path.exists() or not weather_path.exists():\n",
    "        print(\"Data files not found. Run pipeline first.\")\n",
    "        \n",
    "\n",
    "    generation_df = pd.read_parquet(generation_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "\n",
    "\n",
    "\n",
    "    # First investigate negatives\n",
    "    print(\"\\n[TEST 1] Investigating negatives...\")\n",
    "    try:\n",
    "        _, _ = build_modeling_dataset(\n",
    "            generation_df, weather_df,\n",
    "            negative_policy='investigate',\n",
    "            output_dir=Path(\"data/renewable/test_investigate\")\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # Then build with clamp\n",
    "    print(\"\\n[TEST 2] Building with clamp_to_zero...\")\n",
    "    modeling_df, report = build_modeling_dataset(\n",
    "        generation_df, weather_df,\n",
    "        negative_policy='clamp_to_zero',\n",
    "        output_dir=Path(\"data/renewable/preprocessing\")\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFinal dataset shape: {modeling_df.shape}\")\n",
    "    print(f\"Columns: {modeling_df.columns.tolist()}\")\n",
    "    print(f\"\\nSample:\")\n",
    "    print(modeling_df.head())\n",
    "\n",
    "    # CRITICAL: Save the modeling dataset for smoke test\n",
    "    # This step was missing, causing FileNotFoundError below\n",
    "    data_path = Path(\"data/renewable/modeling_dataset.parquet\")\n",
    "    print(f\"\\n[TEST 3] Saving modeling dataset to {data_path}...\")\n",
    "    modeling_df.to_parquet(data_path, index=False)\n",
    "    print(f\"✅ Saved {len(modeling_df):,} rows ({data_path.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "    # smoke test on these\n",
    "    # Load preprocessed modeling dataset from pipeline (to verify save worked)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SMOKE TEST: Renewable Forecasting Models\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "    df = pd.read_parquet(data_path)\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Series: {df['unique_id'].unique().tolist()}\")\n",
    "    print(f\"Date range: {df['ds'].min()} to {df['ds'].max()}\")\n",
    "    print()\n",
    "\n",
    "    # Run cross-validation\n",
    "    print(\"Running cross-validation (3 windows, 168h step)...\")\n",
    "    model = RenewableForecastModel(horizon=24, confidence_levels=(80, 95))\n",
    "    cv, leaderboard = model.cross_validate(df, n_windows=3, step_size=168)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LEADERBOARD (sorted by RMSE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(leaderboard.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHYSICAL CONSTRAINT VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Min forecast (MSTL_ARIMA): {cv['MSTL_ARIMA'].min():.2f} MWh\")\n",
    "    print(f\"Max forecast (MSTL_ARIMA): {cv['MSTL_ARIMA'].max():.2f} MWh\")\n",
    "    print(f\"Any negative forecasts: {(cv['MSTL_ARIMA'] < 0).any()}\")\n",
    "\n",
    "    if (cv['MSTL_ARIMA'] < 0).any():\n",
    "        print(\"\\n⚠️ WARNING: Negative forecasts detected!\")\n",
    "        print(\"This violates physical constraints (renewable generation cannot be negative)\")\n",
    "        neg_count = (cv['MSTL_ARIMA'] < 0).sum()\n",
    "        print(f\"Count: {neg_count} out of {len(cv)} ({100*neg_count/len(cv):.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\n✅ SUCCESS: All forecasts are non-negative (physical constraints satisfied)\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Module: Pipeline Tasks\n",
    "\n",
    "**File:** `src/renewable/tasks.py`\n",
    "\n",
    "This module orchestrates the complete pipeline:\n",
    "\n",
    "1. **Fetch generation data** from EIA\n",
    "2. **Fetch weather data** from Open-Meteo\n",
    "3. **Train models** with cross-validation\n",
    "4. **Generate forecasts** with prediction intervals\n",
    "5. **Compute drift metrics** vs baseline\n",
    "\n",
    "## Key Feature: Adaptive CV\n",
    "\n",
    "Cross-validation requires sufficient data:\n",
    "```\n",
    "Minimum rows = horizon + (n_windows × step_size)\n",
    "```\n",
    "\n",
    "For short series, we **adapt** the CV settings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/tasks.py\n",
    "# file: src/renewable/tasks.py\n",
    "\"\"\"Renewable energy forecasting pipeline tasks.\n",
    "\n",
    "Idempotent tasks for:\n",
    "- Fetching EIA renewable generation data\n",
    "- Fetching weather data from Open-Meteo\n",
    "- Training probabilistic models\n",
    "- Generating forecasts with intervals\n",
    "- Computing drift metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional imports for interpretability (LightGBM + skforecast)\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from skforecast.recursive import ForecasterRecursive\n",
    "    INTERPRETABILITY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    INTERPRETABILITY_AVAILABLE = False\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.warning(\"lightgbm and/or skforecast not installed - interpretability features unavailable\")\n",
    "\n",
    "from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "from src.renewable.modeling import (\n",
    "    RenewableForecastModel,\n",
    "    compute_baseline_metrics,\n",
    "    enforce_physical_constraints,\n",
    "    WEATHER_VARS,\n",
    ")\n",
    "from src.renewable.model_interpretability import (\n",
    "    InterpretabilityReport,\n",
    "    generate_full_interpretability_report,\n",
    ")\n",
    "from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "from src.renewable.dataset_builder import _add_time_features, build_modeling_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RenewablePipelineConfig:\n",
    "    \"\"\"Configuration for renewable forecasting pipeline.\"\"\"\n",
    "\n",
    "    # Data parameters\n",
    "    regions: list[str] = field(default_factory=lambda: [\"CALI\", \"ERCO\", \"MISO\", \"PJM\", \"SWPP\"])\n",
    "    fuel_types: list[str] = field(default_factory=lambda: [\"WND\", \"SUN\"])\n",
    "    start_date: str = \"\"  # Set dynamically\n",
    "    end_date: str = \"\"  # Set dynamically\n",
    "    lookback_days: int = 30\n",
    "\n",
    "    # Forecast parameters\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "    horizon_preset: Optional[str] = None  # \"24h\" | \"48h\" | \"72h\"\n",
    "\n",
    "    # CV parameters\n",
    "    cv_windows: int = 5\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "    # Model parameters\n",
    "    enable_interpretability: bool = True  # LightGBM SHAP analysis (on by default)\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    negative_policy: str = \"clamp\"  # \"clamp\" | \"fail_loud\" | \"hybrid\"\n",
    "    hourly_grid_policy: str = \"drop_incomplete_series\"  # \"drop_incomplete_series\" | \"fail_loud\"\n",
    "\n",
    "    # Output paths\n",
    "    data_dir: str = \"data/renewable\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    # Horizon preset definitions (class-level constant)\n",
    "    _PRESETS = {\n",
    "        \"24h\": {\"horizon\": 24, \"cv_windows\": 2, \"lookback_days\": 15},\n",
    "        \"48h\": {\"horizon\": 48, \"cv_windows\": 3, \"lookback_days\": 21},\n",
    "        \"72h\": {\"horizon\": 72, \"cv_windows\": 3, \"lookback_days\": 28},\n",
    "    }\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Apply horizon preset if specified\n",
    "        if self.horizon_preset and self.horizon_preset in self._PRESETS:\n",
    "            preset = self._PRESETS[self.horizon_preset]\n",
    "            # Use object.__setattr__ since this is a dataclass\n",
    "            object.__setattr__(self, \"horizon\", preset[\"horizon\"])\n",
    "            object.__setattr__(self, \"cv_windows\", preset[\"cv_windows\"])\n",
    "            object.__setattr__(self, \"lookback_days\", preset[\"lookback_days\"])\n",
    "            logger.info(f\"[config] Applied preset '{self.horizon_preset}': horizon={preset['horizon']}h\")\n",
    "\n",
    "        # Set default dates if not provided\n",
    "        if not self.end_date:\n",
    "            self.end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        if not self.start_date:\n",
    "            end = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "            start = end - timedelta(days=self.lookback_days)\n",
    "            self.start_date = start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Validate configuration\n",
    "        warnings = self._validate()\n",
    "        for warning in warnings:\n",
    "            logger.warning(f\"[config] {warning}\")\n",
    "\n",
    "    def _validate(self) -> list[str]:\n",
    "        \"\"\"Validate configuration and return warnings.\"\"\"\n",
    "        warnings = []\n",
    "\n",
    "        # Check minimum data requirement\n",
    "        available_hours = self.lookback_days * 24\n",
    "        required_hours = self.horizon + (self.cv_windows * self.cv_step_size)\n",
    "        if available_hours < required_hours:\n",
    "            warnings.append(\n",
    "                f\"Insufficient data: need {required_hours}h, have {available_hours}h. \"\n",
    "                f\"Increase lookback_days to {(required_hours // 24) + 1} or reduce cv_windows.\"\n",
    "            )\n",
    "\n",
    "        # Warn about accuracy degradation\n",
    "        if self.horizon > 72:\n",
    "            warnings.append(\n",
    "                f\"Horizon {self.horizon}h exceeds recommended max (72h). \"\n",
    "                f\"Weather forecast accuracy degrades significantly beyond 3 days.\"\n",
    "            )\n",
    "\n",
    "        return warnings\n",
    "\n",
    "    def generation_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"generation.parquet\"\n",
    "\n",
    "    def weather_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"weather.parquet\"\n",
    "\n",
    "    def forecasts_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"forecasts.parquet\"\n",
    "\n",
    "    def baseline_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"baseline.json\"\n",
    "\n",
    "    def interpretability_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"interpretability\"\n",
    "\n",
    "    def preprocessing_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"preprocessing\"\n",
    "\n",
    "\n",
    "def fetch_renewable_data(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 1: Fetch EIA generation data for all regions and fuel types.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [unique_id, ds, y]\n",
    "    \"\"\"\n",
    "    output_path = config.generation_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_generation_summary(df: pd.DataFrame, source: str) -> None:\n",
    "\n",
    "\n",
    "        expected_series = {\n",
    "            f\"{region}_{fuel}\" for region in config.regions for fuel in config.fuel_types\n",
    "        }\n",
    "        present_series = set(df[\"unique_id\"]) if \"unique_id\" in df.columns else set()\n",
    "        missing_series = sorted(expected_series - present_series)\n",
    "        if missing_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Missing expected series (%s): %s\",\n",
    "                source,\n",
    "                missing_series,\n",
    "            )\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_generation] No generation data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"unique_id\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"unique_id\")\n",
    "        )\n",
    "        max_series_log = 25\n",
    "        if len(coverage) > max_series_log:\n",
    "            logger.info(\n",
    "                \"[fetch_generation] Coverage (%s, first %s series):\\n%s\",\n",
    "                source,\n",
    "                max_series_log,\n",
    "                coverage.head(max_series_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_generation] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_generation] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached coverage to surface missing series without refetching.\n",
    "        _log_generation_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_generation] Fetching {config.fuel_types} for {config.regions}\")\n",
    "\n",
    "    # Use longer timeout (90s) to handle slow EIA API responses\n",
    "    fetcher = EIARenewableFetcher(timeout=90)\n",
    "    all_dfs = []\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        df = fetcher.fetch_all_regions(\n",
    "            fuel_type=fuel_type,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            regions=config.regions,\n",
    "            diagnostics=fetch_diagnostics,\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined = combined.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh coverage to highlight gaps or unexpected negatives.\n",
    "    _log_generation_summary(combined, source=\"fresh\")\n",
    "\n",
    "    if fetch_diagnostics:\n",
    "        empty_series = [\n",
    "            entry\n",
    "            for entry in fetch_diagnostics\n",
    "            if entry.get(\"empty\")\n",
    "        ]\n",
    "        for entry in empty_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Empty series detail: region=%s fuel=%s total=%s pages=%s\",\n",
    "                entry.get(\"region\"),\n",
    "                entry.get(\"fuel_type\"),\n",
    "                entry.get(\"total_records\"),\n",
    "                entry.get(\"pages\"),\n",
    "            )\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_generation] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def fetch_renewable_weather(\n",
    "    config: RenewablePipelineConfig,\n",
    "    include_forecast: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 2: Fetch weather data for all regions.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        include_forecast: Include forecast weather for predictions\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [ds, region, weather_vars...]\n",
    "    \"\"\"\n",
    "    output_path = config.weather_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_weather_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_weather] No weather data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"region\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"region\")\n",
    "        )\n",
    "        max_region_log = 25\n",
    "        if len(coverage) > max_region_log:\n",
    "            logger.info(\n",
    "                \"[fetch_weather] Coverage (%s, first %s regions):\\n%s\",\n",
    "                source,\n",
    "                max_region_log,\n",
    "                coverage.head(max_region_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_weather] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "        missing_cols = [\n",
    "            col for col in OpenMeteoRenewable.WEATHER_VARS if col not in df.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing expected weather columns (%s): %s\",\n",
    "                source,\n",
    "                missing_cols,\n",
    "            )\n",
    "\n",
    "        missing_values = {\n",
    "            col: int(df[col].isna().sum())\n",
    "            for col in OpenMeteoRenewable.WEATHER_VARS\n",
    "            if col in df.columns and df[col].isna().any()\n",
    "        }\n",
    "        if missing_values:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing weather values (%s): %s\",\n",
    "                source,\n",
    "                missing_values,\n",
    "            )\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_weather] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached weather coverage to surface missing regions/columns.\n",
    "        _log_weather_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_weather] Fetching weather for {config.regions}\")\n",
    "\n",
    "    weather = OpenMeteoRenewable()\n",
    "\n",
    "    # Historical weather\n",
    "    hist_df = weather.fetch_all_regions_historical(\n",
    "        regions=config.regions,\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "    )\n",
    "\n",
    "    # Validate historical weather result\n",
    "    if hist_df.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[fetch_weather] Historical weather returned empty DataFrame. \"\n",
    "            \"fetch_all_regions_historical should raise an error on failure, \"\n",
    "            \"but received empty result. Check fetch logic.\"\n",
    "        )\n",
    "\n",
    "    if not {\"ds\", \"region\"}.issubset(hist_df.columns):\n",
    "        missing_cols = {\"ds\", \"region\"} - set(hist_df.columns)\n",
    "        raise ValueError(\n",
    "            f\"[fetch_weather] Weather DataFrame missing required columns: {missing_cols}\"\n",
    "        )\n",
    "\n",
    "    hist_regions = hist_df['region'].nunique()\n",
    "    hist_rows = len(hist_df)\n",
    "    logger.info(\n",
    "        f\"[fetch_weather] Historical: {hist_regions} regions, {hist_rows} rows\"\n",
    "    )\n",
    "\n",
    "    # Forecast weather (for prediction, prevents leakage)\n",
    "    if include_forecast:\n",
    "        fcst_df = weather.fetch_all_regions_forecast(\n",
    "            regions=config.regions,\n",
    "            horizon_hours=config.horizon + 24,  # Buffer\n",
    "        )\n",
    "\n",
    "        # Validate forecast weather result\n",
    "        if fcst_df.empty:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Forecast weather returned empty DataFrame. \"\n",
    "                \"Using historical data only for model training and predictions.\"\n",
    "            )\n",
    "            combined = hist_df\n",
    "        else:\n",
    "            fcst_rows = len(fcst_df)\n",
    "            logger.info(f\"[fetch_weather] Forecast: {fcst_rows} rows\")\n",
    "\n",
    "            # Combine, preferring forecast for overlapping times\n",
    "            combined = pd.concat([hist_df, fcst_df], ignore_index=True)\n",
    "            combined = combined.drop_duplicates(subset=[\"ds\", \"region\"], keep=\"last\")\n",
    "    else:\n",
    "        combined = hist_df\n",
    "\n",
    "    combined = combined.sort_values([\"region\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh weather coverage and missing values before saving.\n",
    "    _log_weather_summary(combined, source=\"fresh\")\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_weather] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def train_renewable_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    modeling_df: Optional[pd.DataFrame] = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Task 3: Train models and compute baseline metrics via cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        modeling_df: Preprocessed modeling dataset from build_modeling_dataset()\n",
    "                     (loads and builds from scratch if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cv_results, leaderboard, baseline_metrics)\n",
    "    \"\"\"\n",
    "    # Load and preprocess data if not provided\n",
    "    if modeling_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "        logger.info(\"[train_models] Building modeling dataset...\")\n",
    "        modeling_df, _ = build_modeling_dataset(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            negative_policy='clamp_to_zero',\n",
    "            output_dir=config.preprocessing_dir()\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[train_models] Training on {len(modeling_df)} rows\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Compute adaptive CV settings based on shortest series\n",
    "    min_series_len = modeling_df.groupby(\"unique_id\").size().min()\n",
    "\n",
    "    # CV needs: horizon + (n_windows * step_size) rows minimum\n",
    "    # Solve for n_windows: n_windows = (min_series_len - horizon) / step_size\n",
    "    available_for_cv = min_series_len - config.horizon\n",
    "\n",
    "    # Adjust step_size and n_windows to fit data\n",
    "    step_size = min(config.cv_step_size, max(24, available_for_cv // 3))\n",
    "    n_windows = min(config.cv_windows, max(2, available_for_cv // step_size))\n",
    "\n",
    "    logger.info(\n",
    "        f\"[train_models] Adaptive CV: {n_windows} windows, \"\n",
    "        f\"step={step_size}h (min_series={min_series_len} rows)\"\n",
    "    )\n",
    "\n",
    "    # Cross-validation (modeling_df already has weather merged and time features added)\n",
    "    cv_results, leaderboard = model.cross_validate(\n",
    "        df=modeling_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    baseline = compute_baseline_metrics(cv_results, model_name=best_model)\n",
    "\n",
    "    logger.info(f\"[train_models] Best model: {best_model}, RMSE: {baseline['rmse_mean']:.1f}\")\n",
    "\n",
    "    return cv_results, leaderboard, baseline\n",
    "\n",
    "\n",
    "def train_interpretability_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> dict[str, InterpretabilityReport]:\n",
    "    \"\"\"Train LightGBM models and generate interpretability reports per series.\n",
    "\n",
    "    This trains a separate LightGBM model for each series (region × fuel type)\n",
    "    and generates SHAP, partial dependence, and feature importance artifacts.\n",
    "\n",
    "    Note: LightGBM is used for interpretability only. The primary forecasts\n",
    "    come from statistical models (MSTL/ARIMA) which provide better uncertainty\n",
    "    quantification.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping series_id -> InterpretabilityReport\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Training LightGBM for {generation_df['unique_id'].nunique()} series\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    reports: dict[str, InterpretabilityReport] = {}\n",
    "    output_dir = config.interpretability_dir()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for uid in sorted(generation_df[\"unique_id\"].unique()):\n",
    "        logger.info(f\"[train_interpretability] Processing {uid}...\")\n",
    "\n",
    "        # Extract series data\n",
    "        series_data = generation_df[generation_df[\"unique_id\"] == uid].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Prepare target series with proper frequency\n",
    "        y = series_data.set_index(\"ds\")[\"y\"]\n",
    "        y.index = pd.DatetimeIndex(y.index, freq=\"h\")  # Set hourly frequency\n",
    "\n",
    "        # Prepare exogenous features\n",
    "        region = uid.split(\"_\")[0]\n",
    "        series_weather = weather_df[weather_df[\"region\"] == region].copy()\n",
    "\n",
    "        if series_weather.empty:\n",
    "            logger.warning(f\"[train_interpretability] No weather data for region {region}, skipping {uid}\")\n",
    "            continue\n",
    "\n",
    "        # Merge weather to series timestamps\n",
    "        series_data = series_data.merge(\n",
    "            series_weather[[\"ds\"] + [c for c in WEATHER_VARS if c in series_weather.columns]],\n",
    "            on=\"ds\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Add time features\n",
    "        series_data = _add_time_features(series_data)\n",
    "\n",
    "        # Build exog DataFrame aligned with y\n",
    "        exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "        exog_cols += [c for c in WEATHER_VARS if c in series_data.columns]\n",
    "        exog = series_data.set_index(\"ds\")[exog_cols]\n",
    "\n",
    "        # Check for missing weather\n",
    "        missing_weather = exog.isna().any(axis=1).sum()\n",
    "        if missing_weather > 0:\n",
    "            logger.warning(f\"[train_interpretability] {uid}: {missing_weather} rows with missing weather, filling with ffill/bfill\")\n",
    "            exog = exog.ffill().bfill()\n",
    "\n",
    "        # Fit LightGBM forecaster\n",
    "        try:\n",
    "            if not INTERPRETABILITY_AVAILABLE:\n",
    "                logger.warning(f\"[train_interpretability] {uid}: lightgbm/skforecast not available, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Create skforecast ForecasterRecursive with LightGBM estimator\n",
    "            forecaster = ForecasterRecursive(\n",
    "                estimator=LGBMRegressor(\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                ),\n",
    "                lags=168,  # 7 days of lags\n",
    "            )\n",
    "            forecaster.fit(y=y, exog=exog)\n",
    "\n",
    "            # Create training matrices for SHAP analysis\n",
    "            X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "            # Generate interpretability report\n",
    "            series_output_dir = output_dir / uid\n",
    "            report = generate_full_interpretability_report(\n",
    "                forecaster=forecaster,\n",
    "                X_train=X_train,\n",
    "                series_id=uid,\n",
    "                output_dir=series_output_dir,\n",
    "                top_n_features=5,\n",
    "                shap_sample_frac=0.5,\n",
    "                shap_max_samples=1000,\n",
    "            )\n",
    "            reports[uid] = report\n",
    "\n",
    "            logger.info(\n",
    "                f\"[train_interpretability] {uid}: top_features={report.top_features[:3]}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[train_interpretability] {uid}: Failed to train - {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Generated {len(reports)} interpretability reports\")\n",
    "    return reports\n",
    "\n",
    "\n",
    "def generate_renewable_forecasts(\n",
    "    config: RenewablePipelineConfig,\n",
    "    modeling_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    "    best_model: str = \"MSTL_ARIMA\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 4: Generate forecasts with prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        modeling_df: Preprocessed modeling dataset (if None, loads and builds)\n",
    "        weather_df: Raw weather data with forecast (if None, loads from file)\n",
    "        best_model: Model to use for forecasting\n",
    "\n",
    "    Returns:\n",
    "        Forecast DataFrame with physical constraints applied\n",
    "    \"\"\"\n",
    "    output_path = config.forecasts_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load and preprocess data if not provided\n",
    "    if modeling_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "        if weather_df is None:\n",
    "            weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "        logger.info(\"[generate_forecasts] Building modeling dataset...\")\n",
    "        modeling_df, _ = build_modeling_dataset(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            negative_policy='clamp_to_zero',\n",
    "            output_dir=config.preprocessing_dir()\n",
    "        )\n",
    "\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generating {config.horizon}h forecasts \"\n",
    "        f\"using model={best_model}\"\n",
    "    )\n",
    "\n",
    "    # Ensure datetime types\n",
    "    modeling_df = modeling_df.copy()\n",
    "    modeling_df[\"ds\"] = pd.to_datetime(modeling_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Fit on preprocessed modeling data\n",
    "    model.fit(modeling_df)\n",
    "\n",
    "    # Prepare future exogenous features for forecasting\n",
    "    # We need weather + time features for the forecast horizon\n",
    "    per_series_max = modeling_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Per-series max timestamps:\\n\"\n",
    "        f\"{per_series_max.to_dict()}\"\n",
    "    )\n",
    "\n",
    "    min_of_max = per_series_max.min()\n",
    "    global_max = modeling_df[\"ds\"].max()\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Min of series maxes: {min_of_max}, \"\n",
    "        f\"Global max: {global_max}, \"\n",
    "        f\"Delta: {(global_max - min_of_max).total_seconds() / 3600:.1f}h\"\n",
    "    )\n",
    "\n",
    "    # Get future weather beyond the last training timestamp\n",
    "    future_weather = weather_df[weather_df[\"ds\"] > min_of_max].copy()\n",
    "\n",
    "    if future_weather.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[generate_forecasts] No future weather found after last \"\n",
    "            f\"training timestamp. min_of_max={min_of_max}\"\n",
    "        )\n",
    "\n",
    "    # Build future_exog by preparing timestamps and merging weather\n",
    "    unique_ids = modeling_df[\"unique_id\"].unique()\n",
    "    future_timestamps = pd.date_range(\n",
    "        start=min_of_max + pd.Timedelta(hours=1),\n",
    "        periods=config.horizon,\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    # Create future_exog with all series x timestamps combinations\n",
    "    future_exog = pd.DataFrame([\n",
    "        {\"unique_id\": uid, \"ds\": ts}\n",
    "        for uid in unique_ids\n",
    "        for ts in future_timestamps\n",
    "    ])\n",
    "\n",
    "    # Add region for weather merge\n",
    "    future_exog[\"region\"] = future_exog[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "    # Merge weather\n",
    "    available_weather_vars = [\n",
    "        c for c in WEATHER_VARS if c in future_weather.columns\n",
    "    ]\n",
    "    future_exog = future_exog.merge(\n",
    "        future_weather[[\"ds\", \"region\"] + available_weather_vars],\n",
    "        on=[\"ds\", \"region\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Check for missing weather\n",
    "    missing_weather = future_exog[available_weather_vars].isna().any(axis=1)\n",
    "    if missing_weather.any():\n",
    "        missing_count = missing_weather.sum()\n",
    "        logger.warning(\n",
    "            f\"[generate_forecasts] {missing_count} future rows missing \"\n",
    "            f\"weather, dropping them\"\n",
    "        )\n",
    "        future_exog = future_exog[~missing_weather].reset_index(drop=True)\n",
    "\n",
    "    # Add time features (same as dataset_builder)\n",
    "    future_exog[\"hour\"] = future_exog[\"ds\"].dt.hour\n",
    "    future_exog[\"dow\"] = future_exog[\"ds\"].dt.dayofweek\n",
    "    future_exog[\"hour_sin\"] = np.sin(2 * np.pi * future_exog[\"hour\"] / 24)\n",
    "    future_exog[\"hour_cos\"] = np.cos(2 * np.pi * future_exog[\"hour\"] / 24)\n",
    "    future_exog[\"dow_sin\"] = np.sin(2 * np.pi * future_exog[\"dow\"] / 7)\n",
    "    future_exog[\"dow_cos\"] = np.cos(2 * np.pi * future_exog[\"dow\"] / 7)\n",
    "    future_exog = future_exog.drop(columns=[\"hour\", \"dow\", \"region\"])\n",
    "\n",
    "    # Generate forecasts\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generating predictions using \"\n",
    "        f\"model: {best_model}\"\n",
    "    )\n",
    "    forecasts = model.predict(future_exog=future_exog, best_model=best_model)\n",
    "\n",
    "    # CRITICAL: Apply physical constraints (renewable generation cannot be negative)\n",
    "    # This matches the constraint enforcement during cross-validation (modeling.py:301)\n",
    "    forecasts = enforce_physical_constraints(forecasts, min_value=0.0)\n",
    "    logger.info(\"[generate_forecasts] Applied physical constraints (clipped to [0, ∞))\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generated {len(forecasts)} forecast rows \"\n",
    "        f\"for {forecasts['unique_id'].nunique()} series\"\n",
    "    )\n",
    "\n",
    "    forecasts.to_parquet(output_path, index=False)\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Saved: {output_path} ({len(forecasts)} rows)\"\n",
    "    )\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "def compute_renewable_drift(\n",
    "    predictions: pd.DataFrame,\n",
    "    actuals: pd.DataFrame,\n",
    "    baseline_metrics: dict,\n",
    ") -> dict:\n",
    "    \"\"\"Task 5: Detect drift by comparing current metrics to baseline.\n",
    "\n",
    "    Drift is flagged when current RMSE > baseline_mean + 2*baseline_std\n",
    "\n",
    "    Args:\n",
    "        predictions: Forecast DataFrame with [unique_id, ds, yhat]\n",
    "        actuals: Actual values DataFrame with [unique_id, ds, y]\n",
    "        baseline_metrics: Baseline from cross-validation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with drift status and details\n",
    "    \"\"\"\n",
    "    from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "    # Merge predictions with actuals\n",
    "    merged = predictions.merge(\n",
    "        actuals[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        return {\n",
    "            \"status\": \"no_data\",\n",
    "            \"message\": \"No overlapping data between predictions and actuals\",\n",
    "        }\n",
    "\n",
    "    # Compute current metrics\n",
    "    y_true = merged[\"y\"].values\n",
    "    y_pred = merged[\"yhat\"].values\n",
    "\n",
    "    current_rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "    current_mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "\n",
    "    # Check against threshold\n",
    "    threshold = baseline_metrics.get(\"drift_threshold_rmse\", float(\"inf\"))\n",
    "    is_drifting = current_rmse > threshold\n",
    "\n",
    "    result = {\n",
    "        \"status\": \"drift_detected\" if is_drifting else \"stable\",\n",
    "        \"current_rmse\": float(current_rmse),\n",
    "        \"current_mae\": float(current_mae),\n",
    "        \"baseline_rmse\": float(baseline_metrics.get(\"rmse_mean\", 0)),\n",
    "        \"drift_threshold\": float(threshold),\n",
    "        \"threshold_exceeded_by\": float(max(0, current_rmse - threshold)),\n",
    "        \"n_predictions\": len(merged),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    if is_drifting:\n",
    "        logger.warning(\n",
    "            f\"[drift] DRIFT DETECTED: RMSE={current_rmse:.1f} > threshold={threshold:.1f}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"[drift] Stable: RMSE={current_rmse:.1f} <= threshold={threshold:.1f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    "    skip_eda: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"Run the complete renewable forecasting pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetch generation data\n",
    "    2. Fetch weather data\n",
    "    3. Run EDA and get recommendations (NEW)\n",
    "    4. Build datasets per fuel type using recommendations\n",
    "    5. Train models (CV)\n",
    "    6. Generate forecasts\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pipeline results\n",
    "    \"\"\"\n",
    "    logger.info(f\"[pipeline] Starting: {config.start_date} to {config.end_date}\")\n",
    "    logger.info(f\"[pipeline] Regions: {config.regions}\")\n",
    "    logger.info(f\"[pipeline] Fuel types: {config.fuel_types}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Fetch generation\n",
    "    generation_df = fetch_renewable_data(config, fetch_diagnostics=fetch_diagnostics)\n",
    "    results[\"generation_rows\"] = len(generation_df)\n",
    "    results[\"series_count\"] = generation_df[\"unique_id\"].nunique()\n",
    "\n",
    "    from src.renewable.validation import validate_generation_df\n",
    "\n",
    "    expected_series = [f\"{r}_{f}\" for r in config.regions for f in config.fuel_types]\n",
    "    rep = validate_generation_df(\n",
    "        generation_df,\n",
    "        expected_series=expected_series,\n",
    "        max_missing_ratio=0.02,\n",
    "        max_lag_hours=48,  # choose a value consistent with EIA publishing lag\n",
    "    )\n",
    "    if not rep.ok:\n",
    "        raise RuntimeError(f\"[pipeline][generation_validation] {rep.message} details={rep.details}\")\n",
    "\n",
    "    # Step 2: Fetch weather\n",
    "    weather_df = fetch_renewable_weather(config)\n",
    "    results[\"weather_rows\"] = len(weather_df)\n",
    "\n",
    "    # Step 3: Run EDA (NEW)\n",
    "    eda_recommendations = None\n",
    "\n",
    "    if not skip_eda:\n",
    "        logger.info(\"[pipeline] Running EDA to generate preprocessing recommendations\")\n",
    "        from src.renewable.eda import run_full_eda\n",
    "\n",
    "        eda_output_dir = Path(config.data_dir) / \"eda\"\n",
    "        eda_recommendations = run_full_eda(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            eda_output_dir,\n",
    "        )\n",
    "\n",
    "        results[\"eda\"] = {\n",
    "            \"output_dir\": str(eda_output_dir),\n",
    "            \"negative_policy\": eda_recommendations.preprocessing.negative_policy,\n",
    "            \"confidence\": eda_recommendations.preprocessing.negative_confidence,\n",
    "        }\n",
    "        logger.info(f\"[pipeline] EDA complete. Recommended policy: {eda_recommendations.preprocessing.negative_policy}\")\n",
    "    else:\n",
    "        logger.warning(\"[pipeline] Skipping EDA - using default preprocessing policies\")\n",
    "\n",
    "    # Step 4: Build datasets per fuel type (MODIFIED)\n",
    "    logger.info(\"[pipeline] Building modeling datasets (fuel-type specific)\")\n",
    "\n",
    "    from src.renewable.dataset_builder import build_dataset_by_fuel_type\n",
    "\n",
    "    fuel_datasets = {}\n",
    "    last_prep_report = None\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        logger.info(f\"[pipeline] Building {fuel_type} dataset...\")\n",
    "\n",
    "        fuel_output_dir = config.preprocessing_dir() / fuel_type.lower()\n",
    "\n",
    "        modeling_df_fuel, prep_report = build_dataset_by_fuel_type(\n",
    "            generation_df,\n",
    "            weather_df,\n",
    "            fuel_type=fuel_type,\n",
    "            output_dir=fuel_output_dir,\n",
    "            eda_recommendations=eda_recommendations.preprocessing if eda_recommendations else None,\n",
    "        )\n",
    "\n",
    "        fuel_datasets[fuel_type] = modeling_df_fuel\n",
    "        last_prep_report = prep_report  # Keep last report for backward compatibility\n",
    "\n",
    "        logger.info(f\"[pipeline] {fuel_type}: {prep_report.input_rows:,} → {prep_report.output_rows:,} rows\")\n",
    "\n",
    "    # Combine all fuel datasets\n",
    "    modeling_df = pd.concat(fuel_datasets.values(), ignore_index=True)\n",
    "\n",
    "    # DEBUG: Log dataset combination\n",
    "    logger.info(f\"[pipeline] Combined {len(fuel_datasets)} fuel-type datasets into {len(modeling_df):,} rows\")\n",
    "\n",
    "    # Save combined modeling dataset for analysis and testing\n",
    "    modeling_dataset_path = Path(config.data_dir) / \"modeling_dataset.parquet\"\n",
    "    modeling_df.to_parquet(modeling_dataset_path, index=False)\n",
    "    logger.info(f\"[pipeline] Saved modeling dataset: {modeling_dataset_path} ({len(modeling_df):,} rows)\")\n",
    "\n",
    "    # Initialize preprocessing results\n",
    "    results[\"preprocessing\"] = {\n",
    "        \"rows_input\": len(generation_df),\n",
    "        \"rows_output\": len(modeling_df),\n",
    "    }\n",
    "\n",
    "    # Use last report for backward compatibility\n",
    "    prep_report = last_prep_report\n",
    "\n",
    "    # Extract time and weather features from output_features (from last fuel type)\n",
    "    if prep_report:\n",
    "        logger.debug(\"[pipeline] Extracting features from last fuel type's preprocessing report\")\n",
    "\n",
    "        time_features = [\n",
    "            f for f in prep_report.output_features\n",
    "            if f in ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "        ]\n",
    "        weather_features = [\n",
    "            f for f in prep_report.output_features\n",
    "            if f in prep_report.weather_vars_used\n",
    "        ]\n",
    "\n",
    "        logger.debug(f\"[pipeline] Found {len(time_features)} time features, {len(weather_features)} weather features\")\n",
    "\n",
    "        results[\"preprocessing\"].update({\n",
    "            \"series_dropped\": len(prep_report.series_dropped_incomplete),\n",
    "            \"negative_action\": prep_report.negative_report.action_taken,\n",
    "            \"time_features\": time_features,\n",
    "            \"weather_features\": weather_features,\n",
    "        })\n",
    "        logger.info(\n",
    "            f\"[pipeline] Preprocessing: {len(generation_df):,} → \"\n",
    "            f\"{len(modeling_df):,} rows\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"[pipeline] No preprocessing report available - skipping feature extraction\")\n",
    "\n",
    "    # Step 5: Train and validate (on preprocessed data)\n",
    "    cv_results, leaderboard, baseline = train_renewable_models(\n",
    "        config, modeling_df\n",
    "    )\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    results[\"best_model\"] = best_model\n",
    "    results[\"best_rmse\"] = float(leaderboard.iloc[0][\"rmse\"])\n",
    "    results[\"baseline\"] = baseline\n",
    "    # Save full leaderboard for dashboard display\n",
    "    results[\"leaderboard\"] = leaderboard.to_dict(orient=\"records\")\n",
    "\n",
    "    # Step 6: Generate forecasts (use the best model from CV)\n",
    "    # Pass weather_df for future weather (forecast horizon)\n",
    "    forecasts = generate_renewable_forecasts(\n",
    "        config, modeling_df, weather_df, best_model=best_model\n",
    "    )\n",
    "    results[\"forecast_rows\"] = len(forecasts)\n",
    "\n",
    "    # Step 5: Train LightGBM models and generate interpretability reports (optional)\n",
    "    # (LightGBM is for interpretability only - MSTL/ARIMA provide primary forecasts)\n",
    "    if config.enable_interpretability:\n",
    "        logger.info(\"[pipeline] Training interpretability models (LightGBM + SHAP)\")\n",
    "        try:\n",
    "            interpretability_reports = train_interpretability_models(\n",
    "                config, generation_df, weather_df\n",
    "            )\n",
    "            results[\"interpretability\"] = {\n",
    "                \"series_count\": len(interpretability_reports),\n",
    "                \"series\": list(interpretability_reports.keys()),\n",
    "                \"output_dir\": str(config.interpretability_dir()),\n",
    "            }\n",
    "\n",
    "            # Add top features summary per series\n",
    "            for uid, report in interpretability_reports.items():\n",
    "                results[\"interpretability\"][f\"{uid}_top_features\"] = report.top_features[:3]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[pipeline] Interpretability training failed (non-fatal): {e}\")\n",
    "            results[\"interpretability\"] = {\"error\": str(e)}\n",
    "    else:\n",
    "        logger.info(\"[pipeline] Interpretability disabled (enable_interpretability=False)\")\n",
    "        results[\"interpretability\"] = {\"enabled\": False}\n",
    "\n",
    "    if fetch_diagnostics is not None:\n",
    "        results[\"fetch_diagnostics\"] = fetch_diagnostics\n",
    "\n",
    "    logger.info(f\"[pipeline] Complete. Best model: {results['best_model']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entry point for renewable pipeline.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Renewable Energy Forecasting Pipeline\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Preset Examples:\n",
    "  # Fast development (24h forecast, 2 CV windows, 15 days lookback)\n",
    "  python -m src.renewable.tasks --preset 24h\n",
    "\n",
    "  # Standard forecasting (48h forecast, 3 CV windows, 21 days lookback)\n",
    "  python -m src.renewable.tasks --preset 48h\n",
    "\n",
    "  # Extended planning (72h forecast, 3 CV windows, 28 days lookback)\n",
    "  python -m src.renewable.tasks --preset 72h\n",
    "\n",
    "Custom Examples:\n",
    "  # 24h preset but only CALI region, skip interpretability\n",
    "  python -m src.renewable.tasks --preset 24h --regions CALI --no-interpretability\n",
    "\n",
    "  # Custom: 36h forecast with 4 CV windows\n",
    "  python -m src.renewable.tasks --horizon 36 --cv-windows 4 --lookback-days 30\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Preset system (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--preset\",\n",
    "        type=str,\n",
    "        choices=[\"24h\", \"48h\", \"72h\"],\n",
    "        help=\"Quick preset: 24h (fast dev), 48h (standard), 72h (extended planning)\",\n",
    "    )\n",
    "\n",
    "    # Flags (NEW)\n",
    "    parser.add_argument(\n",
    "        \"--no-interpretability\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Disable LightGBM interpretability analysis (speeds up pipeline)\",\n",
    "    )\n",
    "\n",
    "    # Data parameters (existing)\n",
    "    parser.add_argument(\n",
    "        \"--regions\",\n",
    "        type=str,\n",
    "        help=\"Override regions (comma-separated, e.g., CALI,ERCO,MISO)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuel\",\n",
    "        type=str,\n",
    "        help=\"Override fuel types (comma-separated, e.g., WND,SUN)\",\n",
    "    )\n",
    "\n",
    "    # Forecast parameters (existing + new)\n",
    "    parser.add_argument(\n",
    "        \"--horizon\",\n",
    "        type=int,\n",
    "        help=\"Override forecast horizon in hours\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lookback-days\",\n",
    "        type=int,\n",
    "        help=\"Override lookback days\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cv-windows\",\n",
    "        type=int,\n",
    "        help=\"Override CV windows count\",\n",
    "    )\n",
    "\n",
    "    # Output parameters\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing data files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"data/renewable\",\n",
    "        help=\"Output directory (default: data/renewable)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Build config with preset support\n",
    "    if args.preset:\n",
    "        # Apply preset defaults\n",
    "        logger.info(f\"[CLI] Applying preset: {args.preset}\")\n",
    "        config = RenewablePipelineConfig(\n",
    "            horizon_preset=args.preset,  # This triggers __post_init__ to apply preset\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "        # Allow CLI overrides of preset values\n",
    "        if args.horizon is not None:\n",
    "            object.__setattr__(config, \"horizon\", args.horizon)\n",
    "            logger.info(f\"[CLI] Override: horizon={args.horizon}h\")\n",
    "        if args.lookback_days is not None:\n",
    "            object.__setattr__(config, \"lookback_days\", args.lookback_days)\n",
    "            logger.info(f\"[CLI] Override: lookback_days={args.lookback_days}\")\n",
    "        if args.cv_windows is not None:\n",
    "            object.__setattr__(config, \"cv_windows\", args.cv_windows)\n",
    "            logger.info(f\"[CLI] Override: cv_windows={args.cv_windows}\")\n",
    "\n",
    "    else:\n",
    "        # No preset: use explicit values or defaults\n",
    "        config = RenewablePipelineConfig(\n",
    "            regions=args.regions.split(\",\") if args.regions else [\"CALI\", \"ERCO\", \"MISO\"],\n",
    "            fuel_types=args.fuel.split(\",\") if args.fuel else [\"WND\", \"SUN\"],\n",
    "            lookback_days=args.lookback_days if args.lookback_days else 30,\n",
    "            horizon=args.horizon if args.horizon else 24,\n",
    "            cv_windows=args.cv_windows if args.cv_windows else 5,\n",
    "            enable_interpretability=not args.no_interpretability,\n",
    "            overwrite=args.overwrite,\n",
    "            data_dir=args.data_dir,\n",
    "        )\n",
    "\n",
    "    # Run pipeline\n",
    "    results = run_full_pipeline(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Series count: {results['series_count']}\")\n",
    "    print(f\"  Generation rows: {results['generation_rows']}\")\n",
    "    print(f\"  Weather rows: {results['weather_rows']}\")\n",
    "    print(f\"  Forecast rows: {results['forecast_rows']}\")\n",
    "    print(f\"  Best model: {results['best_model']}\")\n",
    "    print(f\"  Best RMSE: {results['best_rmse']:.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/data_freshness.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/data_freshness.py\n",
    "# src/renewable/data_freshness.py\n",
    "\"\"\"\n",
    "Lightweight EIA data freshness checking.\n",
    "\n",
    "This module provides functions to check if new data is available from the EIA API\n",
    "before running the full pipeline. It compares the current max timestamps with\n",
    "the previous run's max timestamps to determine if a full pipeline run is needed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from src.renewable.regions import get_eia_respondent\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FreshnessCheckResult:\n",
    "    \"\"\"Result of a data freshness check.\"\"\"\n",
    "\n",
    "    has_new_data: bool\n",
    "    checked_at_utc: str\n",
    "    series_status: dict[str, dict] = field(default_factory=dict)\n",
    "    summary: str = \"\"\n",
    "\n",
    "\n",
    "def load_previous_max_ds(run_log_path: Path) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load per-series max_ds from previous run_log.json.\n",
    "\n",
    "    Args:\n",
    "        run_log_path: Path to run_log.json\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping unique_id -> max_ds ISO string.\n",
    "        Empty dict if file doesn't exist or is malformed.\n",
    "    \"\"\"\n",
    "    if not run_log_path.exists():\n",
    "        logger.info(\"[freshness] No previous run_log.json found - first run\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        data = json.loads(run_log_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # Navigate to diagnostics.generation_coverage.coverage\n",
    "        coverage = (\n",
    "            data.get(\"diagnostics\", {})\n",
    "            .get(\"generation_coverage\", {})\n",
    "            .get(\"coverage\", [])\n",
    "        )\n",
    "\n",
    "        if not coverage:\n",
    "            logger.warning(\"[freshness] run_log.json has no coverage data\")\n",
    "            return {}\n",
    "\n",
    "        result = {}\n",
    "        for item in coverage:\n",
    "            uid = item.get(\"unique_id\")\n",
    "            max_ds = item.get(\"max_ds\")\n",
    "            if uid and max_ds:\n",
    "                result[uid] = max_ds\n",
    "\n",
    "        logger.info(f\"[freshness] Loaded {len(result)} series from previous run_log\")\n",
    "        return result\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        logger.warning(f\"[freshness] Failed to parse run_log.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def probe_eia_latest(\n",
    "    api_key: str,\n",
    "    region: str,\n",
    "    fuel_type: str,\n",
    "    *,\n",
    "    timeout: int = 15,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch only the single most recent record from EIA API.\n",
    "\n",
    "    This is a lightweight probe that uses:\n",
    "    - length=1 (only fetch 1 record)\n",
    "    - sort by period DESC (most recent first)\n",
    "\n",
    "    Args:\n",
    "        api_key: EIA API key\n",
    "        region: Region code (CALI, ERCO, MISO, etc.)\n",
    "        fuel_type: Fuel type (WND, SUN)\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ISO timestamp string of latest record, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"data[]\": \"value\",\n",
    "            \"facets[respondent][]\": respondent,\n",
    "            \"facets[fueltype][]\": fuel_type,\n",
    "            \"frequency\": \"hourly\",\n",
    "            \"length\": 1,\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"desc\",\n",
    "        }\n",
    "\n",
    "        base_url = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "        resp = requests.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        payload = resp.json()\n",
    "        response = payload.get(\"response\", {})\n",
    "        records = response.get(\"data\", [])\n",
    "\n",
    "        if not records:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: No records returned\")\n",
    "            return None\n",
    "\n",
    "        period = records[0].get(\"period\")\n",
    "        if not period:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: Record missing 'period'\")\n",
    "            return None\n",
    "\n",
    "        # Parse to consistent ISO format\n",
    "        ts = pd.to_datetime(period, utc=True)\n",
    "        return ts.isoformat()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: API error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compare_timestamps(prev: Optional[str], current: Optional[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if current is strictly newer than prev.\n",
    "\n",
    "    Handles None values conservatively (assume new data if unknown).\n",
    "    \"\"\"\n",
    "    if not prev or not current:\n",
    "        return True  # Unknown = assume new data (conservative)\n",
    "\n",
    "    try:\n",
    "        prev_dt = pd.to_datetime(prev, utc=True)\n",
    "        curr_dt = pd.to_datetime(current, utc=True)\n",
    "        return curr_dt > prev_dt\n",
    "    except Exception:\n",
    "        return True  # Parse error = assume new data\n",
    "\n",
    "\n",
    "def check_all_series_freshness(\n",
    "    regions: list[str],\n",
    "    fuel_types: list[str],\n",
    "    run_log_path: Path,\n",
    "    api_key: str,\n",
    ") -> FreshnessCheckResult:\n",
    "    \"\"\"\n",
    "    Check all series for new data availability.\n",
    "\n",
    "    Args:\n",
    "        regions: List of region codes (e.g., [\"CALI\", \"ERCO\", \"MISO\"])\n",
    "        fuel_types: List of fuel types (e.g., [\"WND\", \"SUN\"])\n",
    "        run_log_path: Path to previous run_log.json\n",
    "        api_key: EIA API key\n",
    "\n",
    "    Returns:\n",
    "        FreshnessCheckResult with has_new_data flag and detailed status per series.\n",
    "    \"\"\"\n",
    "    checked_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # 1. Load previous max_ds values\n",
    "    prev_max_ds = load_previous_max_ds(run_log_path)\n",
    "\n",
    "    # 2. If no previous run_log, always run full pipeline (first run)\n",
    "    if not prev_max_ds:\n",
    "        return FreshnessCheckResult(\n",
    "            has_new_data=True,\n",
    "            checked_at_utc=checked_at,\n",
    "            series_status={},\n",
    "            summary=\"No previous run_log.json found - running full pipeline (first run)\",\n",
    "        )\n",
    "\n",
    "    # 3. Probe each series\n",
    "    series_status: dict[str, dict] = {}\n",
    "    has_any_new = False\n",
    "    new_series: list[str] = []\n",
    "    error_series: list[str] = []\n",
    "\n",
    "    for region in regions:\n",
    "        for fuel_type in fuel_types:\n",
    "            series_id = f\"{region}_{fuel_type}\"\n",
    "            prev = prev_max_ds.get(series_id)\n",
    "            current = probe_eia_latest(api_key, region, fuel_type)\n",
    "\n",
    "            # Determine if this series has new data\n",
    "            if current is None:\n",
    "                # API error - be conservative, assume new data\n",
    "                is_new = True\n",
    "                error_series.append(series_id)\n",
    "                logger.warning(\n",
    "                    f\"[freshness] {series_id}: probe failed, assuming new data\"\n",
    "                )\n",
    "            else:\n",
    "                is_new = _compare_timestamps(prev, current)\n",
    "\n",
    "            series_status[series_id] = {\n",
    "                \"prev_max_ds\": prev,\n",
    "                \"current_max_ds\": current,\n",
    "                \"is_new\": is_new,\n",
    "            }\n",
    "\n",
    "            if is_new:\n",
    "                has_any_new = True\n",
    "                if current is not None:\n",
    "                    new_series.append(series_id)\n",
    "\n",
    "            # Log each series check\n",
    "            status_str = \"NEW\" if is_new else \"unchanged\"\n",
    "            logger.info(\n",
    "                f\"[freshness] {series_id}: prev={prev} current={current} ({status_str})\"\n",
    "            )\n",
    "\n",
    "    # 4. Build summary\n",
    "    if error_series:\n",
    "        summary = f\"Probe errors for {error_series}, assuming new data available\"\n",
    "    elif new_series:\n",
    "        summary = f\"New data found for: {', '.join(new_series)}\"\n",
    "    else:\n",
    "        summary = \"No new data found for any series\"\n",
    "\n",
    "    return FreshnessCheckResult(\n",
    "        has_new_data=has_any_new,\n",
    "        checked_at_utc=checked_at,\n",
    "        series_status=series_status,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"EIA_API_KEY not set\")\n",
    "        exit(1)\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "\n",
    "    result = check_all_series_freshness(\n",
    "        regions=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        fuel_types=[\"WND\", \"SUN\"],\n",
    "        run_log_path=run_log_path,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFreshness Check Result:\")\n",
    "    print(f\"  has_new_data: {result.has_new_data}\")\n",
    "    print(f\"  checked_at: {result.checked_at_utc}\")\n",
    "    print(f\"  summary: {result.summary}\")\n",
    "    print(f\"\\nPer-series status:\")\n",
    "    for series_id, status in result.series_status.items():\n",
    "        print(f\"  {series_id}: {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite persistence layer\n",
    "\n",
    "Extends the monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/db.py\n",
    "# file: src/renewable/db.py\n",
    "\"\"\"Database schema and operations for renewable forecasting.\n",
    "\n",
    "Extends the Chapter 4 monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    \"\"\"Connect to SQLite database with optimized settings.\"\"\"\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def init_renewable_db(db_path: str) -> None:\n",
    "    \"\"\"Initialize renewable forecasting database schema.\n",
    "\n",
    "    Creates tables:\n",
    "    - renewable_forecasts: Forecasts with dual intervals\n",
    "    - renewable_scores: Evaluation metrics with coverage\n",
    "    - weather_features: Weather data by region\n",
    "    - drift_alerts: Drift detection history\n",
    "    - baseline_metrics: Backtest baselines for drift thresholds\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Forecasts with dual prediction intervals\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_forecasts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        yhat_lo_80 REAL,\n",
    "        yhat_hi_80 REAL,\n",
    "        yhat_lo_95 REAL,\n",
    "        yhat_hi_95 REAL,\n",
    "        UNIQUE (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Index for efficient queries\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_region_ds\n",
    "    ON renewable_forecasts (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_fuel_ds\n",
    "    ON renewable_forecasts (fuel_type, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Evaluation scores with dual coverage\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_scores (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        scored_at TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mae REAL,\n",
    "        coverage_80 REAL,\n",
    "        coverage_95 REAL,\n",
    "        valid_rows INTEGER,\n",
    "        UNIQUE (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Weather features by region\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_features (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        region TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        temperature_2m REAL,\n",
    "        wind_speed_10m REAL,\n",
    "        wind_speed_100m REAL,\n",
    "        wind_direction_10m REAL,\n",
    "        direct_radiation REAL,\n",
    "        diffuse_radiation REAL,\n",
    "        cloud_cover REAL,\n",
    "        is_forecast INTEGER DEFAULT 0,\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE (region, ds, is_forecast)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_weather_region_ds\n",
    "    ON weather_features (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Drift detection alerts\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS drift_alerts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        alert_at TEXT NOT NULL,\n",
    "        run_id TEXT,\n",
    "        unique_id TEXT,\n",
    "        region TEXT,\n",
    "        fuel_type TEXT,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        current_rmse REAL,\n",
    "        threshold_rmse REAL,\n",
    "        message TEXT,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_drift_alerts_time\n",
    "    ON drift_alerts (alert_at);\n",
    "    \"\"\")\n",
    "\n",
    "    # Baseline metrics for drift detection\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS baseline_metrics (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        rmse_mean REAL NOT NULL,\n",
    "        rmse_std REAL NOT NULL,\n",
    "        mae_mean REAL,\n",
    "        mae_std REAL,\n",
    "        drift_threshold_rmse REAL NOT NULL,\n",
    "        drift_threshold_mae REAL,\n",
    "        n_windows INTEGER,\n",
    "        metadata_json TEXT,\n",
    "        UNIQUE (unique_id, model)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_forecasts(\n",
    "    db_path: str,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    run_id: str,\n",
    "    model: str = \"MSTL_ARIMA\",\n",
    ") -> int:\n",
    "    \"\"\"Save forecasts to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        forecasts_df: DataFrame with [unique_id, ds, yhat, yhat_lo_80, ...]\n",
    "        run_id: Pipeline run identifier\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    created_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in forecasts_df.iterrows():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        parts = unique_id.split(\"_\")\n",
    "        region = parts[0] if len(parts) > 0 else \"\"\n",
    "        fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        rows.append((\n",
    "            run_id,\n",
    "            created_at,\n",
    "            unique_id,\n",
    "            region,\n",
    "            fuel_type,\n",
    "            str(row[\"ds\"]),\n",
    "            model,\n",
    "            row.get(\"yhat\"),\n",
    "            row.get(\"yhat_lo_80\"),\n",
    "            row.get(\"yhat_hi_80\"),\n",
    "            row.get(\"yhat_lo_95\"),\n",
    "            row.get(\"yhat_hi_95\"),\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO renewable_forecasts\n",
    "        (run_id, created_at, unique_id, region, fuel_type, ds, model,\n",
    "         yhat, yhat_lo_80, yhat_hi_80, yhat_lo_95, yhat_hi_95)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_weather(\n",
    "    db_path: str,\n",
    "    weather_df: pd.DataFrame,\n",
    "    is_forecast: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"Save weather features to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        weather_df: DataFrame with [ds, region, weather_vars...]\n",
    "        is_forecast: True if this is forecast weather data\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temperature_2m\", \"wind_speed_10m\", \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\", \"direct_radiation\", \"diffuse_radiation\", \"cloud_cover\"\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in weather_df.iterrows():\n",
    "        values = [row.get(col) for col in weather_cols]\n",
    "        rows.append((\n",
    "            row[\"region\"],\n",
    "            str(row[\"ds\"]),\n",
    "            *values,\n",
    "            1 if is_forecast else 0,\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(f\"\"\"\n",
    "        INSERT OR REPLACE INTO weather_features\n",
    "        (region, ds, {', '.join(weather_cols)}, is_forecast)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_drift_alert(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    unique_id: str,\n",
    "    current_rmse: float,\n",
    "    threshold_rmse: float,\n",
    "    severity: str = \"warning\",\n",
    "    metadata: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Save drift detection alert.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        run_id: Pipeline run identifier\n",
    "        unique_id: Series identifier\n",
    "        current_rmse: Current RMSE value\n",
    "        threshold_rmse: Drift threshold\n",
    "        severity: Alert severity (info, warning, critical)\n",
    "        metadata: Additional metadata\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    parts = unique_id.split(\"_\")\n",
    "    region = parts[0] if len(parts) > 0 else \"\"\n",
    "    fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    alert_type = \"drift_detected\" if current_rmse > threshold_rmse else \"drift_check\"\n",
    "    message = (\n",
    "        f\"RMSE {current_rmse:.1f} {'>' if current_rmse > threshold_rmse else '<='} \"\n",
    "        f\"threshold {threshold_rmse:.1f}\"\n",
    "    )\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO drift_alerts\n",
    "        (alert_at, run_id, unique_id, region, fuel_type, alert_type, severity,\n",
    "         current_rmse, threshold_rmse, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        run_id,\n",
    "        unique_id,\n",
    "        region,\n",
    "        fuel_type,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        current_rmse,\n",
    "        threshold_rmse,\n",
    "        message,\n",
    "        json.dumps(metadata) if metadata else None,\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_baseline(\n",
    "    db_path: str,\n",
    "    unique_id: str,\n",
    "    model: str,\n",
    "    baseline: dict,\n",
    ") -> None:\n",
    "    \"\"\"Save baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        unique_id: Series identifier\n",
    "        model: Model name\n",
    "        baseline: Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO baseline_metrics\n",
    "        (created_at, unique_id, model, rmse_mean, rmse_std, mae_mean, mae_std,\n",
    "         drift_threshold_rmse, drift_threshold_mae, n_windows, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        unique_id,\n",
    "        model,\n",
    "        baseline.get(\"rmse_mean\"),\n",
    "        baseline.get(\"rmse_std\"),\n",
    "        baseline.get(\"mae_mean\"),\n",
    "        baseline.get(\"mae_std\"),\n",
    "        baseline.get(\"drift_threshold_rmse\"),\n",
    "        baseline.get(\"drift_threshold_mae\"),\n",
    "        baseline.get(\"n_windows\"),\n",
    "        json.dumps(baseline),\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def get_recent_forecasts(\n",
    "    db_path: str,\n",
    "    region: Optional[str] = None,\n",
    "    fuel_type: Optional[str] = None,\n",
    "    hours: int = 48,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent forecasts from database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        region: Filter by region (optional)\n",
    "        fuel_type: Filter by fuel type (optional)\n",
    "        hours: Hours of history to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with forecasts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM renewable_forecasts\n",
    "        WHERE datetime(created_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if region:\n",
    "        query += \" AND region = ?\"\n",
    "        params.append(region)\n",
    "\n",
    "    if fuel_type:\n",
    "        query += \" AND fuel_type = ?\"\n",
    "        params.append(fuel_type)\n",
    "\n",
    "    query += \" ORDER BY ds DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_drift_alerts(\n",
    "    db_path: str,\n",
    "    hours: int = 24,\n",
    "    severity: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent drift alerts.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        hours: Hours of history\n",
    "        severity: Filter by severity (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with alerts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM drift_alerts\n",
    "        WHERE datetime(alert_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if severity:\n",
    "        query += \" AND severity = ?\"\n",
    "        params.append(severity)\n",
    "\n",
    "    query += \" ORDER BY alert_at DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test database initialization\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        db_path = f\"{tmpdir}/test_renewable.db\"\n",
    "\n",
    "        print(\"Initializing database...\")\n",
    "        init_renewable_db(db_path)\n",
    "\n",
    "        print(\"Database initialized successfully!\")\n",
    "\n",
    "        # Test connection\n",
    "        con = connect(db_path)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"Tables created: {[t[0] for t in tables]}\")\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8: Dashboard\n",
    "\n",
    "**File:** `src/renewable/dashboard.py`\n",
    "\n",
    "The Streamlit dashboard provides:\n",
    "- **Forecast visualization** with prediction intervals\n",
    "- **Drift monitoring** and alerts\n",
    "- **Coverage analysis** (nominal vs empirical)\n",
    "- **Weather features** by region\n",
    "\n",
    "## Running the Dashboard\n",
    "\n",
    "```bash\n",
    "streamlit run src/renewable/dashboard.py\n",
    "```\n",
    "\n",
    "The dashboard will:\n",
    "1. Load forecasts from `data/renewable/forecasts.parquet`\n",
    "2. Display interactive charts with Plotly\n",
    "3. Show drift alerts from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dashboard.py\n",
    "# file: src/renewable/dashboard.py\n",
    "\"\"\"Streamlit dashboard for renewable energy forecasting.\n",
    "\n",
    "Provides:\n",
    "- Forecast visualization with prediction intervals\n",
    "- Drift monitoring and alerts\n",
    "- Coverage analysis (nominal vs empirical)\n",
    "- Weather features by region\n",
    "\n",
    "Run with:\n",
    "    streamlit run src/renewable/dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "import streamlit_mermaid as stmd\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from src.renewable.db import (connect, get_drift_alerts, get_recent_forecasts,\n",
    "                              init_renewable_db)\n",
    "from src.renewable.regions import FUEL_TYPES, REGIONS\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Renewable Forecast Dashboard\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main dashboard application.\"\"\"\n",
    "    st.title(\"⚡ Renewable Energy Forecast Dashboard\")\n",
    "    st.markdown(\"Next-24h wind/solar generation forecasts with drift monitoring\")\n",
    "\n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "\n",
    "        db_path = st.text_input(\n",
    "            \"Database Path\",\n",
    "            value=\"data/renewable/renewable.db\",\n",
    "        )\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not Path(db_path).exists():\n",
    "            init_renewable_db(db_path)\n",
    "            st.info(\"Database initialized\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Region filter\n",
    "        all_regions = list(REGIONS.keys())\n",
    "        selected_regions = st.multiselect(\n",
    "            \"Regions\",\n",
    "            options=all_regions,\n",
    "            default=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        )\n",
    "\n",
    "        # Fuel type filter\n",
    "        fuel_type = st.selectbox(\n",
    "            \"Fuel Type\",\n",
    "            options=[\"WND\", \"SUN\", \"Both\"],\n",
    "            index=0,\n",
    "        )\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Actions\n",
    "        show_debug = st.checkbox(\"Show Debug\", value=False)\n",
    "        if st.button(\"🔄 Refresh Data\", width=\"stretch\"):\n",
    "            st.rerun()\n",
    "\n",
    "        if st.button(\"📊 Run Pipeline\", width=\"stretch\"):\n",
    "            run_pipeline_from_dashboard(db_path, selected_regions, fuel_type)\n",
    "\n",
    "    # Main content tabs  (Data & Insights first)\n",
    "    tab_insights, tab_forecasts, tab_drift, tab_coverage, tab_weather, tab_interp = st.tabs([\n",
    "        \"📚 Data & Insights\",\n",
    "        \"📈 Forecasts\",\n",
    "        \"⚠️ Drift Monitor\",\n",
    "        \"📊 Coverage\",\n",
    "        \"🌤️ Weather\",\n",
    "        \"🔍 Interpretability\",\n",
    "    ])\n",
    "\n",
    "    with tab_insights:\n",
    "        render_insights_tab(db_path)\n",
    "\n",
    "    with tab_forecasts:\n",
    "        render_forecasts_tab(db_path, selected_regions, fuel_type, show_debug=show_debug)\n",
    "\n",
    "    with tab_drift:\n",
    "        render_drift_tab(db_path)\n",
    "\n",
    "    with tab_coverage:\n",
    "        render_coverage_tab(db_path)\n",
    "\n",
    "    with tab_weather:\n",
    "        render_weather_tab(db_path, selected_regions)\n",
    "\n",
    "    with tab_interp:\n",
    "        render_interpretability_tab(selected_regions, fuel_type)\n",
    "\n",
    "\n",
    "\n",
    "def render_forecasts_tab(db_path: str, regions: list, fuel_type: str, *, show_debug: bool = False):\n",
    "    \"\"\"Render forecast visualization with prediction intervals.\"\"\"\n",
    "    st.subheader(\"Generation Forecasts\")\n",
    "\n",
    "    forecasts_df = pd.DataFrame()\n",
    "    data_source = \"none\"\n",
    "    derived_columns: list[str] = []\n",
    "\n",
    "    # Try to load from parquet file first (pipeline output)\n",
    "    parquet_path = Path(\"data/renewable/forecasts.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            forecasts_df = pd.read_parquet(parquet_path)\n",
    "            data_source = f\"parquet:{parquet_path}\"\n",
    "            # Add region/fuel_type columns if missing\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                parts = forecasts_df[\"unique_id\"].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "                if \"region\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"region\"] = parts[0]\n",
    "                    derived_columns.append(\"region\")\n",
    "                if \"fuel_type\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"fuel_type\"] = parts[1] if parts.shape[1] > 1 else pd.NA\n",
    "                    derived_columns.append(\"fuel_type\")\n",
    "            st.success(f\"Loaded {len(forecasts_df)} forecasts from pipeline\")\n",
    "\n",
    "            # Calculate and display data freshness\n",
    "            if not forecasts_df.empty and \"ds\" in forecasts_df.columns:\n",
    "                earliest_forecast_ts = forecasts_df[\"ds\"].min()\n",
    "                now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "\n",
    "                # Forecasts start from last_data + 1h, so last_data = earliest_forecast - 1h\n",
    "                last_data_ts = earliest_forecast_ts - pd.Timedelta(hours=1)\n",
    "\n",
    "                # Ensure both timestamps are timezone-aware for comparison\n",
    "                if not hasattr(last_data_ts, 'tz') or last_data_ts.tz is None:\n",
    "                    last_data_ts = pd.Timestamp(last_data_ts, tz=\"UTC\")\n",
    "\n",
    "                data_age_hours = (now_utc - last_data_ts).total_seconds() / 3600\n",
    "\n",
    "                # Show warning if data is > 6 hours old\n",
    "                if data_age_hours > 6:\n",
    "                    st.warning(\n",
    "                        f\"⚠️ Forecasts are based on **{data_age_hours:.1f} hour old** data \"\n",
    "                        f\"(last EIA data: {last_data_ts.strftime('%b %d %H:%M')} UTC). \"\n",
    "                        f\"Click 'Refresh Forecasts' button in sidebar to update.\"\n",
    "                    )\n",
    "                else:\n",
    "                    st.info(\n",
    "                        f\"✅ Forecasts from {last_data_ts.strftime('%b %d %H:%M')} UTC data \"\n",
    "                        f\"({data_age_hours:.1f}h old)\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load parquet: {e}\")\n",
    "\n",
    "    # Fall back to database\n",
    "    if forecasts_df.empty:\n",
    "        try:\n",
    "            forecasts_df = get_recent_forecasts(db_path, hours=72)\n",
    "            data_source = f\"db:{db_path}\"\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load from database: {e}\")\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        # Show demo data\n",
    "        st.info(\"No forecasts found. Showing demo data.\")\n",
    "        forecasts_df = generate_demo_forecasts(regions, fuel_type)\n",
    "        data_source = \"demo\"\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Forecast Data\", expanded=False):\n",
    "            st.markdown(\"**Source**\")\n",
    "            st.code(data_source)\n",
    "            st.markdown(\"**Columns**\")\n",
    "            st.code(\", \".join(forecasts_df.columns.tolist()))\n",
    "\n",
    "            st.markdown(\"**Counts (pre-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "\n",
    "            if derived_columns:\n",
    "                st.markdown(\"**Derived Columns**\")\n",
    "                st.write(derived_columns)\n",
    "\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id sample**\")\n",
    "                st.write(forecasts_df[\"unique_id\"].dropna().astype(str).head(10).tolist())\n",
    "\n",
    "            if \"fuel_type\" in forecasts_df.columns:\n",
    "                st.markdown(\"**fuel_type counts**\")\n",
    "                st.dataframe(forecasts_df[\"fuel_type\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "                unknown = sorted(\n",
    "                    {str(v) for v in forecasts_df[\"fuel_type\"].dropna().unique()}\n",
    "                    - set(FUEL_TYPES.keys())\n",
    "                )\n",
    "                if unknown:\n",
    "                    st.warning(f\"Unknown fuel_type values: {unknown}\")\n",
    "\n",
    "            if \"region\" in forecasts_df.columns:\n",
    "                st.markdown(\"**region counts**\")\n",
    "                st.dataframe(forecasts_df[\"region\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "    # Filter by selections\n",
    "    if fuel_type != \"Both\":\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"fuel_type\"] == fuel_type]\n",
    "\n",
    "    if regions:\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"region\"].isin(regions)]\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Filter Result\", expanded=False):\n",
    "            st.markdown(\"**Applied Filters**\")\n",
    "            st.write({\"fuel_type\": fuel_type, \"regions\": regions})\n",
    "            st.markdown(\"**Counts (post-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id after filter**\")\n",
    "                st.write(sorted(forecasts_df[\"unique_id\"].dropna().astype(str).unique().tolist()))\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        st.warning(\"No data matching filters\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    series_options = forecasts_df[\"unique_id\"].unique().tolist()\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=series_options,\n",
    "        index=0 if series_options else None,\n",
    "        key=\"forecast_series_select\",\n",
    "    )\n",
    "\n",
    "    if selected_series:\n",
    "        series_data = forecasts_df[forecasts_df[\"unique_id\"] == selected_series].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Convert to local timezone for display\n",
    "        region_code = series_data[\"unique_id\"].iloc[0].split(\"_\")[0]\n",
    "        region_info = REGIONS.get(region_code)\n",
    "        timezone_name = region_info.timezone if region_info else \"UTC\"\n",
    "\n",
    "        # Create forecast plot with intervals\n",
    "        fig = create_forecast_plot(series_data, selected_series, timezone_name)\n",
    "        st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "        # Show data table\n",
    "        with st.expander(\"View Data\"):\n",
    "            st.dataframe(\n",
    "                series_data[[\"ds\", \"yhat\", \"yhat_lo_80\", \"yhat_hi_80\", \"yhat_lo_95\", \"yhat_hi_95\"]],\n",
    "                width=\"stretch\",\n",
    "            )\n",
    "\n",
    "\n",
    "def create_forecast_plot(df: pd.DataFrame, title: str, timezone_name: str = \"UTC\") -> go.Figure:\n",
    "    \"\"\"Create Plotly figure with forecast and prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Forecast dataframe with ds (timestamp), yhat, and interval columns\n",
    "        title: Series name for chart title\n",
    "        timezone_name: IANA timezone name for display (e.g., \"America/Chicago\")\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert timestamps to local timezone for display\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "    # Convert UTC to local timezone\n",
    "    if timezone_name != \"UTC\":\n",
    "        df[\"ds\"] = df[\"ds\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone_name)\n",
    "\n",
    "    # Get timezone abbreviation for display (e.g., \"CST\", \"PST\")\n",
    "    if timezone_name != \"UTC\" and len(df) > 0:\n",
    "        tz_abbr = df[\"ds\"].iloc[0].strftime(\"%Z\")\n",
    "    else:\n",
    "        tz_abbr = \"UTC\"\n",
    "\n",
    "    # 95% interval (outer, lighter)\n",
    "    if \"yhat_lo_95\" in df.columns and \"yhat_hi_95\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_95\"], df[\"yhat_lo_95\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"95% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # 80% interval (inner, darker)\n",
    "    if \"yhat_lo_80\" in df.columns and \"yhat_hi_80\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_80\"], df[\"yhat_lo_80\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.4)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"80% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # Point forecast\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"ds\"],\n",
    "        y=df[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"#1f77b4\", width=2),\n",
    "    ))\n",
    "\n",
    "    # Actuals if available\n",
    "    if \"y\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Actual\",\n",
    "            marker=dict(color=\"#2ca02c\", size=6),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast: {title}\",\n",
    "        xaxis_title=f\"Time ({tz_abbr})\",\n",
    "        yaxis_title=\"Generation (MWh)\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "        height=450,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def render_drift_tab(db_path: str):\n",
    "    \"\"\"Render drift monitoring and alerts.\"\"\"\n",
    "    st.subheader(\"Drift Detection\")\n",
    "\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Try to load alerts\n",
    "    try:\n",
    "        alerts_df = get_drift_alerts(db_path, hours=48)\n",
    "    except Exception:\n",
    "        alerts_df = pd.DataFrame()\n",
    "\n",
    "    # Summary metrics\n",
    "    with col1:\n",
    "        critical = len(alerts_df[alerts_df[\"severity\"] == \"critical\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\n",
    "            \"Critical Alerts\",\n",
    "            critical,\n",
    "            delta=None,\n",
    "            delta_color=\"inverse\" if critical > 0 else \"off\",\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "        warning = len(alerts_df[alerts_df[\"severity\"] == \"warning\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Warnings\", warning)\n",
    "\n",
    "    with col3:\n",
    "        stable = len(alerts_df[alerts_df[\"alert_type\"] == \"drift_check\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Stable Checks\", stable)\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    if alerts_df.empty:\n",
    "        st.info(\"No drift alerts in the last 48 hours. System is stable.\")\n",
    "\n",
    "        # Show demo drift status\n",
    "        st.markdown(\"### Demo Drift Status\")\n",
    "        demo_drift = pd.DataFrame({\n",
    "            \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "            \"Current RMSE\": [125.3, 98.7, 156.2, 45.1, 67.8],\n",
    "            \"Threshold\": [150.0, 120.0, 180.0, 60.0, 80.0],\n",
    "            \"Status\": [\"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\"],\n",
    "        })\n",
    "        st.dataframe(demo_drift, width=\"stretch\")\n",
    "    else:\n",
    "        # Show alerts table\n",
    "        st.dataframe(\n",
    "            alerts_df[[\"alert_at\", \"unique_id\", \"severity\", \"current_rmse\", \"threshold_rmse\", \"message\"]],\n",
    "            width=\"stretch\",\n",
    "        )\n",
    "\n",
    "        # Drift timeline\n",
    "        if len(alerts_df) > 1:\n",
    "            alerts_df[\"alert_at\"] = pd.to_datetime(alerts_df[\"alert_at\"])\n",
    "            fig = px.scatter(\n",
    "                alerts_df,\n",
    "                x=\"alert_at\",\n",
    "                y=\"current_rmse\",\n",
    "                color=\"severity\",\n",
    "                size=\"current_rmse\",\n",
    "                hover_data=[\"unique_id\", \"message\"],\n",
    "                title=\"Drift Timeline\",\n",
    "            )\n",
    "            fig.add_hline(\n",
    "                y=alerts_df[\"threshold_rmse\"].mean(),\n",
    "                line_dash=\"dash\",\n",
    "                annotation_text=\"Avg Threshold\",\n",
    "            )\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_coverage_tab(db_path: str):\n",
    "    \"\"\"Render coverage analysis comparing nominal vs empirical.\"\"\"\n",
    "    st.subheader(\"Prediction Interval Coverage\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    **Coverage** measures how often actual values fall within prediction intervals.\n",
    "    - **Nominal**: The expected coverage (80% or 95%)\n",
    "    - **Empirical**: The actual observed coverage\n",
    "    - **Gap**: Difference indicates calibration quality\n",
    "    \"\"\")\n",
    "\n",
    "    # Demo coverage data\n",
    "    coverage_data = pd.DataFrame({\n",
    "        \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"SWPP_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "        \"Nominal 80%\": [80, 80, 80, 80, 80, 80],\n",
    "        \"Empirical 80%\": [78.5, 82.1, 76.3, 79.8, 81.2, 77.9],\n",
    "        \"Nominal 95%\": [95, 95, 95, 95, 95, 95],\n",
    "        \"Empirical 95%\": [93.2, 96.1, 91.5, 94.8, 95.7, 92.3],\n",
    "    })\n",
    "\n",
    "    coverage_data[\"Gap 80%\"] = coverage_data[\"Empirical 80%\"] - coverage_data[\"Nominal 80%\"]\n",
    "    coverage_data[\"Gap 95%\"] = coverage_data[\"Empirical 95%\"] - coverage_data[\"Nominal 95%\"]\n",
    "\n",
    "    # Summary\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        avg_80 = coverage_data[\"Empirical 80%\"].mean()\n",
    "        st.metric(\"Avg 80% Coverage\", f\"{avg_80:.1f}%\", f\"{avg_80 - 80:.1f}%\")\n",
    "\n",
    "    with col2:\n",
    "        avg_95 = coverage_data[\"Empirical 95%\"].mean()\n",
    "        st.metric(\"Avg 95% Coverage\", f\"{avg_95:.1f}%\", f\"{avg_95 - 95:.1f}%\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Coverage comparison chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"80% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 80%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.7)\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"95% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 95%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.4)\",\n",
    "    ))\n",
    "\n",
    "    # Nominal lines\n",
    "    fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"80% Nominal\")\n",
    "    fig.add_hline(y=95, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"95% Nominal\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coverage by Series\",\n",
    "        xaxis_title=\"Series\",\n",
    "        yaxis_title=\"Coverage (%)\",\n",
    "        barmode=\"group\",\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Detailed table\n",
    "    with st.expander(\"View Coverage Data\"):\n",
    "        st.dataframe(coverage_data, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_weather_tab(db_path: str, regions: list):\n",
    "    \"\"\"Render weather features visualization.\"\"\"\n",
    "    st.subheader(\"Weather Features\")\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "    # Prefer real pipeline output; no demo fallback.\n",
    "    parquet_path = Path(\"data/renewable/weather.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            weather_df = pd.read_parquet(parquet_path)\n",
    "            st.success(f\"Loaded {len(weather_df)} weather rows from pipeline\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather parquet: {exc}\")\n",
    "\n",
    "    if weather_df.empty and Path(db_path).exists():\n",
    "        try:\n",
    "            with connect(db_path) as con:\n",
    "                weather_df = pd.read_sql_query(\n",
    "                    \"SELECT * FROM weather_features ORDER BY ds ASC\",\n",
    "                    con,\n",
    "                )\n",
    "            if not weather_df.empty:\n",
    "                st.success(f\"Loaded {len(weather_df)} weather rows from database\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather data from database: {exc}\")\n",
    "\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data available. Run the pipeline to populate weather features.\")\n",
    "        return\n",
    "\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"coerce\")\n",
    "    if regions:\n",
    "        weather_df = weather_df[weather_df[\"region\"].isin(regions)]\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data matching selected regions.\")\n",
    "        return\n",
    "\n",
    "    # Variable selector\n",
    "    weather_vars = [\n",
    "        col for col in [\"wind_speed_10m\", \"wind_speed_100m\", \"direct_radiation\", \"cloud_cover\"]\n",
    "        if col in weather_df.columns\n",
    "    ]\n",
    "    if not weather_vars:\n",
    "        st.warning(\"Weather data missing expected variables.\")\n",
    "        return\n",
    "    selected_var = st.selectbox(\"Weather Variable\", options=weather_vars)\n",
    "\n",
    "    # Plot\n",
    "    fig = px.line(\n",
    "        weather_df,\n",
    "        x=\"ds\",\n",
    "        y=selected_var,\n",
    "        color=\"region\",\n",
    "        title=f\"{selected_var} by Region\",\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Summary stats\n",
    "    st.markdown(\"### Current Conditions\")\n",
    "\n",
    "    cols = st.columns(len(regions[:4]))\n",
    "    for i, region in enumerate(regions[:4]):\n",
    "        if i < len(cols):\n",
    "            with cols[i]:\n",
    "                region_data = weather_df[weather_df[\"region\"] == region].iloc[-1] if len(weather_df[weather_df[\"region\"] == region]) > 0 else {}\n",
    "                st.metric(\n",
    "                    region,\n",
    "                    f\"{region_data.get('wind_speed_10m', 0):.1f} m/s\",\n",
    "                    help=\"Wind speed at 10m\",\n",
    "                )\n",
    "\n",
    "\n",
    "def render_interpretability_tab(regions: list, fuel_type: str):\n",
    "    \"\"\"Render model interpretability visualizations (SHAP, feature importance, PDP).\"\"\"\n",
    "    st.subheader(\"Model Interpretability\")\n",
    "\n",
    "    # Model Leaderboard Section\n",
    "    st.markdown(\"### 🏆 Model Leaderboard (Cross-Validation)\")\n",
    "\n",
    "    # Model descriptions for education\n",
    "    MODEL_INFO = {\n",
    "        \"AutoARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Auto-tuned ARIMA with automatic p,d,q selection. Good for univariate series with trend/seasonality.\",\n",
    "            \"strengths\": \"Robust, well-understood, good prediction intervals\",\n",
    "        },\n",
    "        \"MSTL_ARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Multiple Seasonal-Trend decomposition + ARIMA. Handles daily (24h) and weekly (168h) seasonality.\",\n",
    "            \"strengths\": \"Best for multi-seasonal patterns like energy data\",\n",
    "        },\n",
    "        \"AutoETS\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Exponential smoothing with automatic error/trend/season selection.\",\n",
    "            \"strengths\": \"Simple, fast, works well for smooth series\",\n",
    "        },\n",
    "        \"AutoTheta\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Theta method with automatic decomposition. Robust to outliers.\",\n",
    "            \"strengths\": \"Competition winner (M3), handles level shifts\",\n",
    "        },\n",
    "        \"CES\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Complex Exponential Smoothing. Captures complex seasonal patterns.\",\n",
    "            \"strengths\": \"Good for complex seasonality\",\n",
    "        },\n",
    "        \"SeasonalNaive\": {\n",
    "            \"type\": \"Baseline\",\n",
    "            \"description\": \"Uses value from same hour last week. Baseline benchmark.\",\n",
    "            \"strengths\": \"Simple benchmark - if beaten, models add value\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "    if run_log_path.exists():\n",
    "        try:\n",
    "            import json\n",
    "            run_log = json.loads(run_log_path.read_text())\n",
    "            pipeline_results = run_log.get(\"pipeline_results\", {})\n",
    "            leaderboard_data = pipeline_results.get(\"leaderboard\", [])\n",
    "\n",
    "            if leaderboard_data:\n",
    "                leaderboard_df = pd.DataFrame(leaderboard_data)\n",
    "                best_model = pipeline_results.get(\"best_model\", \"\")\n",
    "                best_rmse = pipeline_results.get(\"best_rmse\", 0)\n",
    "\n",
    "                # Key metrics row\n",
    "                col1, col2, col3, col4 = st.columns(4)\n",
    "                with col1:\n",
    "                    st.metric(\"Best Model\", best_model)\n",
    "                with col2:\n",
    "                    st.metric(\"Best RMSE\", f\"{best_rmse:.3f}\")\n",
    "                with col3:\n",
    "                    st.metric(\"Models Evaluated\", len(leaderboard_data))\n",
    "                with col4:\n",
    "                    # Calculate improvement over baseline\n",
    "                    baseline_rmse = leaderboard_df[leaderboard_df[\"model\"] == \"SeasonalNaive\"][\"rmse\"].values\n",
    "                    if len(baseline_rmse) > 0 and best_rmse > 0:\n",
    "                        improvement = ((baseline_rmse[0] - best_rmse) / baseline_rmse[0]) * 100\n",
    "                        st.metric(\"vs Baseline\", f\"{improvement:+.1f}%\", help=\"Improvement over SeasonalNaive\")\n",
    "                    else:\n",
    "                        st.metric(\"vs Baseline\", \"N/A\")\n",
    "\n",
    "                # Selection rationale\n",
    "                st.markdown(\"#### Why This Model?\")\n",
    "                st.info(f\"\"\"\n",
    "                **{best_model}** was selected because it has the **lowest RMSE** on cross-validation.\n",
    "\n",
    "                - **RMSE (Root Mean Square Error)**: Penalizes large errors more heavily. Best for energy forecasting where big misses are costly.\n",
    "                - **Selection method**: Time-series CV with {run_log.get('config', {}).get('cv_windows', 2)} windows, step size {run_log.get('config', {}).get('cv_step_size', 168)}h\n",
    "                - **Horizon**: {run_log.get('config', {}).get('horizon', 24)}h ahead forecasts\n",
    "                \"\"\")\n",
    "\n",
    "                # Model description for winner\n",
    "                if best_model in MODEL_INFO:\n",
    "                    info = MODEL_INFO[best_model]\n",
    "                    st.success(f\"**{info['type']} Model**: {info['description']}\")\n",
    "\n",
    "                # Full leaderboard with visualization\n",
    "                st.markdown(\"#### All Models Ranked by RMSE\")\n",
    "\n",
    "                display_cols = [c for c in [\"model\", \"rmse\", \"mae\", \"mape\", \"coverage_80\", \"coverage_95\"]\n",
    "                               if c in leaderboard_df.columns]\n",
    "\n",
    "                # Create visualization\n",
    "                if \"rmse\" in leaderboard_df.columns:\n",
    "                    fig = px.bar(\n",
    "                        leaderboard_df.sort_values(\"rmse\"),\n",
    "                        x=\"model\",\n",
    "                        y=\"rmse\",\n",
    "                        title=\"Model Comparison (Lower RMSE = Better)\",\n",
    "                        color=\"rmse\",\n",
    "                        color_continuous_scale=\"RdYlGn_r\",\n",
    "                    )\n",
    "                    fig.add_hline(y=best_rmse, line_dash=\"dash\", line_color=\"green\",\n",
    "                                  annotation_text=f\"Best: {best_rmse:.3f}\")\n",
    "                    fig.update_layout(height=350)\n",
    "                    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                # Format numeric columns for table\n",
    "                styled_df = leaderboard_df[display_cols].copy()\n",
    "                for col in [\"rmse\", \"mae\", \"mape\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "                for col in [\"coverage_80\", \"coverage_95\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.1f}%\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "                st.dataframe(styled_df, width=\"stretch\", hide_index=True)\n",
    "\n",
    "                # Coverage analysis\n",
    "                if \"coverage_80\" in leaderboard_df.columns:\n",
    "                    st.markdown(\"#### Prediction Interval Coverage\")\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Coverage** measures if prediction intervals are well-calibrated:\n",
    "                    - **80% interval** should contain ~80% of actual values\n",
    "                    - **95% interval** should contain ~95% of actual values\n",
    "                    - **Under-coverage** (<target) = intervals too narrow, overconfident\n",
    "                    - **Over-coverage** (>target) = intervals too wide, conservative\n",
    "                    \"\"\")\n",
    "\n",
    "                    coverage_df = leaderboard_df[[\"model\", \"coverage_80\", \"coverage_95\"]].copy()\n",
    "                    coverage_df[\"coverage_80_status\"] = coverage_df[\"coverage_80\"].apply(\n",
    "                        lambda x: \"Under\" if x < 75 else (\"Over\" if x > 85 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "                    coverage_df[\"coverage_95_status\"] = coverage_df[\"coverage_95\"].apply(\n",
    "                        lambda x: \"Under\" if x < 90 else (\"Over\" if x > 99 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "\n",
    "                # Model descriptions expander\n",
    "                with st.expander(\"Model Descriptions\"):\n",
    "                    for model_name, info in MODEL_INFO.items():\n",
    "                        st.markdown(f\"**{model_name}** ({info['type']})\")\n",
    "                        st.markdown(f\"- {info['description']}\")\n",
    "                        st.markdown(f\"- *Strengths*: {info['strengths']}\")\n",
    "                        st.markdown(\"---\")\n",
    "\n",
    "                # CV configuration expander\n",
    "                config = run_log.get(\"config\", {})\n",
    "                with st.expander(\"CV Configuration\"):\n",
    "                    st.write({\n",
    "                        \"cv_windows\": config.get(\"cv_windows\"),\n",
    "                        \"cv_step_size\": config.get(\"cv_step_size\"),\n",
    "                        \"horizon\": config.get(\"horizon\"),\n",
    "                        \"regions\": config.get(\"regions\"),\n",
    "                        \"fuel_types\": config.get(\"fuel_types\"),\n",
    "                        \"run_at\": run_log.get(\"run_at_utc\", \"N/A\"),\n",
    "                    })\n",
    "            else:\n",
    "                st.info(\"Leaderboard not available. Run the pipeline with the latest code to generate.\")\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load leaderboard: {e}\")\n",
    "    else:\n",
    "        st.info(\"No run log found. Run the pipeline to generate model comparison.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    st.markdown(\"### 🔍 Per-Series Interpretability\")\n",
    "    st.markdown(\"\"\"\n",
    "    **LightGBM** models are trained alongside statistical models (MSTL/ARIMA) to provide\n",
    "    interpretability insights. The statistical models generate the primary forecasts,\n",
    "    while LightGBM helps understand feature importance and relationships.\n",
    "    \"\"\")\n",
    "\n",
    "    interp_dir = Path(\"data/renewable/interpretability\")\n",
    "\n",
    "    if not interp_dir.exists():\n",
    "        st.info(\"No interpretability data available. Run the pipeline to generate SHAP and PDP plots.\")\n",
    "        return\n",
    "\n",
    "    # Get available series\n",
    "    series_dirs = sorted([d.name for d in interp_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "    if not series_dirs:\n",
    "        st.warning(\"Interpretability directory exists but contains no series data.\")\n",
    "        return\n",
    "\n",
    "    # Filter by selected regions and fuel type\n",
    "    filtered_series = []\n",
    "    for series_id in series_dirs:\n",
    "        parts = series_id.split(\"_\")\n",
    "        if len(parts) == 2:\n",
    "            region, ft = parts\n",
    "            if regions and region not in regions:\n",
    "                continue\n",
    "            if fuel_type != \"Both\" and ft != fuel_type:\n",
    "                continue\n",
    "            filtered_series.append(series_id)\n",
    "\n",
    "    if not filtered_series:\n",
    "        st.warning(\"No interpretability data for selected filters.\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=filtered_series,\n",
    "        index=0,\n",
    "        key=\"interpretability_series_select\",\n",
    "    )\n",
    "\n",
    "    if not selected_series:\n",
    "        return\n",
    "\n",
    "    series_dir = interp_dir / selected_series\n",
    "\n",
    "    # Layout: Feature Importance + SHAP Summary side by side\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"### Feature Importance\")\n",
    "        importance_path = series_dir / \"feature_importance.csv\"\n",
    "        if importance_path.exists():\n",
    "            try:\n",
    "                importance_df = pd.read_csv(importance_path)\n",
    "                # Show top 15 features\n",
    "                top_features = importance_df.head(15)\n",
    "\n",
    "                # Create bar chart\n",
    "                fig = px.bar(\n",
    "                    top_features,\n",
    "                    x=\"importance\",\n",
    "                    y=\"feature\",\n",
    "                    orientation=\"h\",\n",
    "                    title=f\"Top Features: {selected_series}\",\n",
    "                    labels={\"importance\": \"Importance\", \"feature\": \"Feature\"},\n",
    "                )\n",
    "                fig.update_layout(yaxis=dict(autorange=\"reversed\"), height=400)\n",
    "                st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                with st.expander(\"Full Feature List\"):\n",
    "                    st.dataframe(importance_df, width=\"stretch\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error loading feature importance: {e}\")\n",
    "        else:\n",
    "            st.info(\"Feature importance not available.\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"### SHAP Summary\")\n",
    "        shap_summary_path = series_dir / \"shap_summary.png\"\n",
    "        if shap_summary_path.exists():\n",
    "            st.image(str(shap_summary_path), width=\"stretch\")\n",
    "        else:\n",
    "            # Try bar plot as fallback\n",
    "            shap_bar_path = series_dir / \"shap_bar.png\"\n",
    "            if shap_bar_path.exists():\n",
    "                st.image(str(shap_bar_path), width=\"stretch\")\n",
    "            else:\n",
    "                st.info(\"SHAP summary not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # SHAP Dependence Plots\n",
    "    st.markdown(\"### SHAP Dependence Plots\")\n",
    "    st.markdown(\"Shows how individual feature values affect predictions.\")\n",
    "\n",
    "    shap_dep_files = list(series_dir.glob(\"shap_dependence_*.png\"))\n",
    "    if shap_dep_files:\n",
    "        # Create columns for dependence plots\n",
    "        n_cols = min(3, len(shap_dep_files))\n",
    "        cols = st.columns(n_cols)\n",
    "\n",
    "        for i, dep_file in enumerate(shap_dep_files[:6]):  # Limit to 6 plots\n",
    "            feature_name = dep_file.stem.replace(\"shap_dependence_\", \"\")\n",
    "            with cols[i % n_cols]:\n",
    "                st.markdown(f\"**{feature_name}**\")\n",
    "                st.image(str(dep_file), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"SHAP dependence plots not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Partial Dependence Plot\n",
    "    st.markdown(\"### Partial Dependence Plot\")\n",
    "    st.markdown(\"Shows the average effect of features on predictions (marginal effect).\")\n",
    "\n",
    "    pdp_path = series_dir / \"partial_dependence.png\"\n",
    "    if pdp_path.exists():\n",
    "        st.image(str(pdp_path), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"Partial dependence plot not available.\")\n",
    "\n",
    "    # Waterfall plot for sample prediction\n",
    "    waterfall_path = series_dir / \"shap_waterfall_sample.png\"\n",
    "    if waterfall_path.exists():\n",
    "        st.markdown(\"### Sample Prediction Explanation\")\n",
    "        st.markdown(\"SHAP waterfall showing how features contributed to a single prediction.\")\n",
    "        st.image(str(waterfall_path), width=\"stretch\")\n",
    "\n",
    "\n",
    "def generate_demo_forecasts(regions: list, fuel_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate demo forecast data for display.\"\"\"\n",
    "    data = []\n",
    "    base_time = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    fuel_types = [fuel_type] if fuel_type != \"Both\" else [\"WND\", \"SUN\"]\n",
    "\n",
    "    for region in regions[:3]:\n",
    "        for ft in fuel_types:\n",
    "            unique_id = f\"{region}_{ft}\"\n",
    "            base_value = 500 if ft == \"WND\" else 300\n",
    "\n",
    "            for h in range(24):\n",
    "                ds = base_time + timedelta(hours=h)\n",
    "\n",
    "                # Add daily pattern\n",
    "                if ft == \"SUN\":\n",
    "                    hour_factor = max(0, np.sin((ds.hour - 6) * np.pi / 12)) if 6 < ds.hour < 18 else 0\n",
    "                    yhat = base_value * hour_factor + np.random.normal(0, 20)\n",
    "                else:\n",
    "                    yhat = base_value + np.sin(ds.hour * np.pi / 12) * 100 + np.random.normal(0, 30)\n",
    "\n",
    "                yhat = max(0, yhat)\n",
    "\n",
    "                data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"region\": region,\n",
    "                    \"fuel_type\": ft,\n",
    "                    \"ds\": ds,\n",
    "                    \"yhat\": yhat,\n",
    "                    \"yhat_lo_80\": yhat * 0.85,\n",
    "                    \"yhat_hi_80\": yhat * 1.15,\n",
    "                    \"yhat_lo_95\": yhat * 0.75,\n",
    "                    \"yhat_hi_95\": yhat * 1.25,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def render_insights_tab(db_path: str):\n",
    "    \"\"\"Render comprehensive data insights including regional context and EDA results.\"\"\"\n",
    "    st.title(\"📚 Data & Insights\")\n",
    "    st.markdown(\"**Understanding the data, regions, and methodology behind renewable energy forecasting**\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 1: Pipeline Architecture\n",
    "    # ========================================================================\n",
    "    st.header(\"⚙️ Pipeline Architecture\")\n",
    "    st.markdown(\"\"\"\n",
    "This forecasting system follows a rigorous pipeline from data ingestion through model validation:\n",
    "    \"\"\")\n",
    "\n",
    "    # Mermaid diagram from README\n",
    "    code = r\"\"\"\n",
    "    graph TB\n",
    "        A[EIA API<br/>Generation Data] -->|fetch_renewable_data| B[generation.parquet<br/>unique_id, ds, y]\n",
    "        C[Open-Meteo API<br/>Weather Data] -->|fetch_renewable_weather| D[weather.parquet<br/>ds, region, weather_vars]\n",
    "\n",
    "        B --> E[EDA Module<br/>Investigation & Recommendations]\n",
    "        D --> E\n",
    "\n",
    "        E -->|Recommendations| F[Dataset Builder<br/>Fuel-Specific Preprocessing]\n",
    "\n",
    "        F -->|Validated Dataset| G[StatsForecast CV<br/>MSTL, AutoARIMA, AutoETS]\n",
    "        F -->|Optional| H[LightGBM SHAP<br/>Interpretability]\n",
    "\n",
    "        G --> I[Best Model Selection<br/>Leaderboard]\n",
    "        I --> J[Generate Forecasts<br/>24h + Intervals]\n",
    "\n",
    "        J --> K[forecasts.parquet<br/>yhat, yhat_lo, yhat_hi]\n",
    "        J --> L[Quality Gates<br/>Drift Detection]\n",
    "\n",
    "        L -->|Pass| M[Git Commit<br/>Artifact Versioning]\n",
    "        L -->|Fail| N[Pipeline Fails<br/>Manual Review]\n",
    "\n",
    "        H --> O[SHAP Reports<br/>Feature Importance]\n",
    "\n",
    "        style E fill:#e1f5ff\n",
    "        style F fill:#fff4e1\n",
    "        style G fill:#f0e1ff\n",
    "        style L fill:#ffe1e1\n",
    "    \"\"\"\n",
    "\n",
    "    stmd.st_mermaid(code)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 2: Regional Electricity Markets\n",
    "    # ========================================================================\n",
    "    st.header(\"🌍 Regional Electricity Markets\")\n",
    "    st.markdown(\"\"\"\n",
    "The United States electricity grid is managed by multiple **Independent System Operators (ISOs)**\n",
    "and **Regional Transmission Organizations (RTOs)**. This dashboard focuses on three major regions:\n",
    "    \"\"\")\n",
    "\n",
    "    # Create tabs for each region\n",
    "    region_tab1, region_tab2, region_tab3, region_tab4 = st.tabs([\n",
    "        \"🏢 ERCOT (Texas)\",\n",
    "        \"🌾 MISO (Midwest)\",\n",
    "        \"☀️ CAISO (California)\",\n",
    "        \"📊 Comparison\"\n",
    "    ])\n",
    "\n",
    "    with region_tab1:\n",
    "        st.subheader(\"ERCOT - Electric Reliability Council of Texas\")\n",
    "\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "**Geographic Coverage:**\n",
    "- **Texas** (90% of the state)\n",
    "- Does NOT include El Paso, parts of East Texas, or the Panhandle\n",
    "\n",
    "**Key Characteristics:**\n",
    "- ⚡ **Population**: ~27 million people\n",
    "- 🏢 **Unique Feature**: Operates **independently** from the rest of the US grid\n",
    "- 🔌 **Interconnection**: Texas Interconnection (isolated from Eastern and Western grids)\n",
    "- 🌞 **Renewables**: High solar and wind capacity (~35% of generation)\n",
    "- 🌡️ **Climate**: Hot summers → high cooling demand\n",
    "- 💡 **Market**: Deregulated electricity market (competitive pricing)\n",
    "\n",
    "**Why It's Different:**\n",
    "- Not subject to federal regulation (doesn't cross state borders)\n",
    "- Cannot easily import/export power from other states\n",
    "- Famous for the 2021 winter storm crisis\n",
    "- **EIA Code**: `TEX` (Texas)\n",
    "            \"\"\")\n",
    "\n",
    "        with col2:\n",
    "            st.info(\"\"\"\n",
    "**Grid Challenge**\n",
    "\n",
    "ERCOT's isolation means\n",
    "it cannot import power\n",
    "during emergencies.\n",
    "\n",
    "**Forecasting Impact**\n",
    "\n",
    "Solar & wind forecasting\n",
    "is critical - no backup\n",
    "from neighboring grids.\n",
    "            \"\"\")\n",
    "\n",
    "    with region_tab2:\n",
    "        st.subheader(\"MISO - Midcontinent Independent System Operator\")\n",
    "\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "**Geographic Coverage:**\n",
    "- **15 states** across North-Central US:\n",
    "  - **North**: North Dakota, South Dakota, Minnesota, Wisconsin, Michigan\n",
    "  - **Central**: Iowa, Illinois, Indiana, Missouri\n",
    "  - **South**: Arkansas, Louisiana, Mississippi, Texas (parts)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- ⚡ **Population**: ~45 million people\n",
    "- 🌾 **Characteristics**: Large agricultural region, diverse fuel mix\n",
    "- 💨 **Renewables**: Massive wind capacity (especially in the Great Plains)\n",
    "- 🏭 **Industry**: Heavy manufacturing (automotive, steel)\n",
    "- 🌡️ **Climate**: Four distinct seasons, cold winters, hot summers\n",
    "- 💡 **Market**: Day-ahead and real-time energy markets\n",
    "\n",
    "**Why It's Interesting:**\n",
    "- One of the largest ISOs in North America\n",
    "- Leading in wind energy integration\n",
    "- Diverse geography (from Great Lakes to Gulf Coast)\n",
    "- **EIA Code**: `MISO` (Midcontinent ISO)\n",
    "            \"\"\")\n",
    "\n",
    "        with col2:\n",
    "            st.info(\"\"\"\n",
    "**Grid Challenge**\n",
    "\n",
    "Vast geography creates\n",
    "transmission challenges\n",
    "and regional variations.\n",
    "\n",
    "**Forecasting Impact**\n",
    "\n",
    "Wind forecasting most\n",
    "critical due to Great\n",
    "Plains wind belt.\n",
    "            \"\"\")\n",
    "\n",
    "    with region_tab3:\n",
    "        st.subheader(\"CAISO - California Independent System Operator\")\n",
    "\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "**Geographic Coverage:**\n",
    "- **California** (80% of the state)\n",
    "- Small parts of Nevada\n",
    "\n",
    "**Key Characteristics:**\n",
    "- ⚡ **Population**: ~30 million people\n",
    "- 🌞 **Renewables**: Aggressive renewable energy targets (60% by 2030, 100% by 2045)\n",
    "- 🔋 **Innovation**: Leader in battery storage, rooftop solar\n",
    "- 🌡️ **Climate**: Mediterranean (hot, dry summers; mild winters)\n",
    "- 🔥 **Challenges**: Wildfires, drought, \"duck curve\" problem\n",
    "- 💡 **Market**: Complex wholesale electricity market\n",
    "\n",
    "**Why It's Unique:**\n",
    "- **Most aggressive renewable targets** in the US\n",
    "- **Duck curve problem**: Solar floods grid during day, steep ramp-up needed at sunset\n",
    "- **Net metering**: Rooftops can sell back to grid\n",
    "- High electricity prices (climate policies, infrastructure costs)\n",
    "- **EIA Code**: `CAL` (California)\n",
    "            \"\"\")\n",
    "\n",
    "        with col2:\n",
    "            st.warning(\"\"\"\n",
    "**Grid Challenge**\n",
    "\n",
    "\"Duck Curve\" problem:\n",
    "- Midday: Too much solar\n",
    "- Evening: Sharp ramp-up\n",
    "\n",
    "**Forecasting Impact**\n",
    "\n",
    "Solar forecasting MOST\n",
    "critical (50%+ renewable\n",
    "target). Need accurate\n",
    "sunset timing predictions.\n",
    "            \"\"\")\n",
    "\n",
    "    with region_tab4:\n",
    "        st.subheader(\"Regional Comparison\")\n",
    "\n",
    "        # Comparison table\n",
    "        comparison_data = {\n",
    "            \"Characteristic\": [\n",
    "                \"States Covered\",\n",
    "                \"Population\",\n",
    "                \"Grid Connection\",\n",
    "                \"Renewable %\",\n",
    "                \"Primary Challenge\",\n",
    "                \"Peak Demand Season\",\n",
    "                \"Unique Feature\",\n",
    "                \"Solar Capacity\",\n",
    "                \"Wind Capacity\",\n",
    "            ],\n",
    "            \"ERCOT (ERCO)\": [\n",
    "                \"Texas\",\n",
    "                \"27M\",\n",
    "                \"Isolated (Texas only)\",\n",
    "                \"~35% (wind, solar)\",\n",
    "                \"Grid isolation, heat waves\",\n",
    "                \"Summer (cooling)\",\n",
    "                \"No federal oversight\",\n",
    "                \"Growing fast\",\n",
    "                \"Very high\",\n",
    "            ],\n",
    "            \"MISO\": [\n",
    "                \"15 states\",\n",
    "                \"45M\",\n",
    "                \"Eastern Interconnection\",\n",
    "                \"~25% (mostly wind)\",\n",
    "                \"Winter cold, wind variability\",\n",
    "                \"Summer/Winter\",\n",
    "                \"Largest ISO\",\n",
    "                \"Moderate\",\n",
    "                \"Very high (Great Plains)\",\n",
    "            ],\n",
    "            \"CAISO (CALI)\": [\n",
    "                \"California\",\n",
    "                \"30M\",\n",
    "                \"Western Interconnection\",\n",
    "                \"~50% (solar, wind, hydro)\",\n",
    "                \"Duck curve, wildfires\",\n",
    "                \"Summer (cooling)\",\n",
    "                \"Most aggressive renewables\",\n",
    "                \"Highest in US\",\n",
    "                \"Moderate\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        st.dataframe(\n",
    "            comparison_data,\n",
    "            width=\"stretch\",\n",
    "            hide_index=True,\n",
    "        )\n",
    "\n",
    "        st.markdown(\"\"\"\n",
    "### 🎯 Forecasting Implications by Region\n",
    "\n",
    "**Solar Forecasting Priority:**\n",
    "1. 🥇 **CAISO**: Most important (50%+ renewable target, duck curve management)\n",
    "2. 🥈 **ERCOT**: Growing importance (rapid solar buildout)\n",
    "3. 🥉 **MISO**: Less critical (wind-focused region)\n",
    "\n",
    "**Wind Forecasting Priority:**\n",
    "1. 🥇 **MISO**: Most critical (Great Plains wind belt, 15-state coverage)\n",
    "2. 🥈 **ERCOT**: Very important (West Texas wind resources)\n",
    "3. 🥉 **CAISO**: Moderate importance (some Tehachapi wind)\n",
    "\n",
    "**Data Quality Observations:**\n",
    "- **CALI_SUN**: 403 negative values found (likely net metering, auxiliary loads)\n",
    "- **Other regions**: Clean data (no negative values detected)\n",
    "        \"\"\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 3: Model Performance Dashboard (NEW)\n",
    "    # ========================================================================\n",
    "    st.header(\"🎯 Model Performance Dashboard\")\n",
    "    st.markdown(\"\"\"\n",
    "This section shows the latest model performance metrics from cross-validation and\n",
    "compares different forecasting models to select the best performer.\n",
    "    \"\"\")\n",
    "\n",
    "    # Load run_log.json for model performance\n",
    "    data_dir = Path(db_path).parent\n",
    "    run_log_path = data_dir / \"run_log.json\"\n",
    "\n",
    "    if run_log_path.exists():\n",
    "        import json\n",
    "        with open(run_log_path, 'r') as f:\n",
    "            run_log = json.load(f)\n",
    "\n",
    "        pipeline_results = run_log.get('pipeline_results', {})\n",
    "\n",
    "        # Display key metrics\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        with col1:\n",
    "            best_model = pipeline_results.get('best_model', 'N/A')\n",
    "            st.metric(\"🏆 Best Model\", best_model)\n",
    "        with col2:\n",
    "            best_rmse = pipeline_results.get('best_rmse', 0)\n",
    "            st.metric(\"📊 Best RMSE\", f\"{best_rmse:,.0f}\")\n",
    "        with col3:\n",
    "            series_count = pipeline_results.get('series_count', 0)\n",
    "            st.metric(\"📈 Series Forecasted\", series_count)\n",
    "        with col4:\n",
    "            rows_out = pipeline_results.get('preprocessing', {}).get('rows_output', 0)\n",
    "            st.metric(\"📋 Training Rows\", f\"{rows_out:,}\")\n",
    "\n",
    "        # Model Leaderboard - Interactive Table\n",
    "        st.subheader(\"📊 Model Leaderboard (Cross-Validation Results)\")\n",
    "\n",
    "        leaderboard = pipeline_results.get('leaderboard', [])\n",
    "        if leaderboard:\n",
    "            import pandas as pd\n",
    "            lb_df = pd.DataFrame(leaderboard)\n",
    "\n",
    "            # Format for display\n",
    "            if 'coverage_80' in lb_df.columns:\n",
    "                lb_df['coverage_80'] = lb_df['coverage_80'].apply(lambda x: f\"{x*100:.1f}%\")\n",
    "            if 'coverage_95' in lb_df.columns:\n",
    "                lb_df['coverage_95'] = lb_df['coverage_95'].apply(lambda x: f\"{x*100:.1f}%\")\n",
    "            if 'rmse' in lb_df.columns:\n",
    "                lb_df['rmse'] = lb_df['rmse'].apply(lambda x: f\"{x:,.0f}\")\n",
    "            if 'mae' in lb_df.columns:\n",
    "                lb_df['mae'] = lb_df['mae'].apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "            st.dataframe(\n",
    "                lb_df,\n",
    "                width=\"stretch\",\n",
    "                hide_index=True,\n",
    "            )\n",
    "\n",
    "            # Interactive Model Comparison Chart\n",
    "            st.subheader(\"📈 Model Comparison (RMSE & MAE)\")\n",
    "\n",
    "            # Reload raw data for plotting\n",
    "            lb_df_raw = pd.DataFrame(leaderboard)\n",
    "\n",
    "            import plotly.graph_objects as go\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # RMSE bars\n",
    "            fig.add_trace(go.Bar(\n",
    "                name='RMSE',\n",
    "                x=lb_df_raw['model'],\n",
    "                y=lb_df_raw['rmse'],\n",
    "                marker_color='indianred',\n",
    "                text=[f\"{v:,.0f}\" for v in lb_df_raw['rmse']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            # MAE bars\n",
    "            fig.add_trace(go.Bar(\n",
    "                name='MAE',\n",
    "                x=lb_df_raw['model'],\n",
    "                y=lb_df_raw['mae'],\n",
    "                marker_color='lightseagreen',\n",
    "                text=[f\"{v:,.0f}\" for v in lb_df_raw['mae']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=\"Model Performance: Lower is Better\",\n",
    "                xaxis_title=\"Model\",\n",
    "                yaxis_title=\"Error Metric (MWh)\",\n",
    "                barmode='group',\n",
    "                height=400,\n",
    "                hovermode='x unified',\n",
    "                legend=dict(\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=1.02,\n",
    "                    xanchor=\"right\",\n",
    "                    x=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "            # Coverage Analysis\n",
    "            st.subheader(\"🎯 Prediction Interval Coverage\")\n",
    "            st.markdown(\"\"\"\n",
    "**Coverage** measures how often actual values fall within prediction intervals:\n",
    "- **80% Interval**: Should contain ~80% of actuals\n",
    "- **95% Interval**: Should contain ~95% of actuals\n",
    "            \"\"\")\n",
    "\n",
    "            fig_coverage = go.Figure()\n",
    "\n",
    "            fig_coverage.add_trace(go.Bar(\n",
    "                name='80% Coverage',\n",
    "                x=lb_df_raw['model'],\n",
    "                y=lb_df_raw['coverage_80'] * 100,\n",
    "                marker_color='skyblue',\n",
    "                text=[f\"{v*100:.1f}%\" for v in lb_df_raw['coverage_80']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            fig_coverage.add_trace(go.Bar(\n",
    "                name='95% Coverage',\n",
    "                x=lb_df_raw['model'],\n",
    "                y=lb_df_raw['coverage_95'] * 100,\n",
    "                marker_color='navy',\n",
    "                text=[f\"{v*100:.1f}%\" for v in lb_df_raw['coverage_95']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            # Add target lines\n",
    "            fig_coverage.add_hline(y=80, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"80% Target\")\n",
    "            fig_coverage.add_hline(y=95, line_dash=\"dash\", line_color=\"red\", annotation_text=\"95% Target\")\n",
    "\n",
    "            fig_coverage.update_layout(\n",
    "                title=\"Prediction Interval Coverage by Model\",\n",
    "                xaxis_title=\"Model\",\n",
    "                yaxis_title=\"Coverage (%)\",\n",
    "                barmode='group',\n",
    "                height=400,\n",
    "                yaxis_range=[0, 100]\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_coverage, width=\"stretch\")\n",
    "\n",
    "        else:\n",
    "            st.warning(\"No leaderboard data available in run log\")\n",
    "    else:\n",
    "        st.warning(\"No run log found. Run the pipeline to generate performance metrics.\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 4: Forecast Accuracy by Region (NEW)\n",
    "    # ========================================================================\n",
    "    st.header(\"🌎 Forecast Accuracy by Region\")\n",
    "    st.markdown(\"\"\"\n",
    "Analyzing forecast performance across different regions helps identify where\n",
    "the models perform best and where improvements are needed.\n",
    "    \"\"\")\n",
    "\n",
    "    # Load forecasts and generation data\n",
    "    forecasts_path = data_dir / \"forecasts.parquet\"\n",
    "    generation_path = data_dir / \"generation.parquet\"\n",
    "\n",
    "    if forecasts_path.exists() and generation_path.exists():\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        forecasts_df = pd.read_parquet(forecasts_path)\n",
    "        generation_df = pd.read_parquet(generation_path)\n",
    "\n",
    "        # Ensure datetime\n",
    "        forecasts_df['ds'] = pd.to_datetime(forecasts_df['ds'])\n",
    "        generation_df['ds'] = pd.to_datetime(generation_df['ds'])\n",
    "\n",
    "        # Merge forecasts with actuals\n",
    "        merged = forecasts_df.merge(\n",
    "            generation_df[['unique_id', 'ds', 'y']],\n",
    "            on=['unique_id', 'ds'],\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "        if not merged.empty:\n",
    "            # Extract region from unique_id\n",
    "            merged['region'] = merged['unique_id'].str.extract(r'(CALI|ERCO|MISO)')[0]\n",
    "            merged['fuel'] = merged['unique_id'].str.extract(r'(WND|SUN)')[0]\n",
    "\n",
    "            # Calculate errors\n",
    "            merged['error'] = merged['yhat'] - merged['y']\n",
    "            merged['abs_error'] = np.abs(merged['error'])\n",
    "            merged['sq_error'] = merged['error'] ** 2\n",
    "\n",
    "            # Aggregate by region\n",
    "            regional_metrics = merged.groupby('region').agg({\n",
    "                'abs_error': 'mean',\n",
    "                'sq_error': lambda x: np.sqrt(x.mean()),\n",
    "                'error': ['mean', 'std'],\n",
    "                'unique_id': 'count'\n",
    "            }).round(2)\n",
    "\n",
    "            regional_metrics.columns = ['MAE', 'RMSE', 'Bias', 'Error_Std', 'Count']\n",
    "            regional_metrics = regional_metrics.reset_index()\n",
    "\n",
    "            # Display metrics table\n",
    "            st.subheader(\"📊 Regional Performance Metrics\")\n",
    "\n",
    "            col1, col2 = st.columns([2, 1])\n",
    "\n",
    "            with col1:\n",
    "                st.dataframe(\n",
    "                    regional_metrics.style.format({\n",
    "                        'MAE': '{:.0f}',\n",
    "                        'RMSE': '{:.0f}',\n",
    "                        'Bias': '{:.0f}',\n",
    "                        'Error_Std': '{:.0f}',\n",
    "                        'Count': '{:.0f}'\n",
    "                    }),\n",
    "                    width=\"stretch\",\n",
    "                    hide_index=True,\n",
    "                )\n",
    "\n",
    "            with col2:\n",
    "                st.info(\"\"\"\n",
    "**Metrics Explained**\n",
    "\n",
    "**MAE**: Mean Absolute Error\n",
    "Lower = more accurate\n",
    "\n",
    "**RMSE**: Root Mean Squared Error\n",
    "Penalizes large errors\n",
    "\n",
    "**Bias**: Average error\n",
    "(+ = overforecast, - = underforecast)\n",
    "                \"\"\")\n",
    "\n",
    "            # Interactive regional comparison\n",
    "            st.subheader(\"📍 Regional Accuracy Comparison\")\n",
    "\n",
    "            fig_regional = go.Figure()\n",
    "\n",
    "            fig_regional.add_trace(go.Bar(\n",
    "                name='MAE',\n",
    "                x=regional_metrics['region'],\n",
    "                y=regional_metrics['MAE'],\n",
    "                marker_color='indianred',\n",
    "                text=[f\"{v:.0f}\" for v in regional_metrics['MAE']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            fig_regional.add_trace(go.Bar(\n",
    "                name='RMSE',\n",
    "                x=regional_metrics['region'],\n",
    "                y=regional_metrics['RMSE'],\n",
    "                marker_color='lightseagreen',\n",
    "                text=[f\"{v:.0f}\" for v in regional_metrics['RMSE']],\n",
    "                textposition='outside'\n",
    "            ))\n",
    "\n",
    "            fig_regional.update_layout(\n",
    "                title=\"Forecast Accuracy by Region (Lower is Better)\",\n",
    "                xaxis_title=\"Region\",\n",
    "                yaxis_title=\"Error (MWh)\",\n",
    "                barmode='group',\n",
    "                height=400\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_regional, width=\"stretch\")\n",
    "\n",
    "            # Error distribution by region\n",
    "            st.subheader(\"📉 Error Distribution by Region\")\n",
    "\n",
    "            fig_dist = go.Figure()\n",
    "\n",
    "            for region in merged['region'].unique():\n",
    "                region_data = merged[merged['region'] == region]\n",
    "                fig_dist.add_trace(go.Box(\n",
    "                    y=region_data['error'],\n",
    "                    name=region,\n",
    "                    boxmean='sd'\n",
    "                ))\n",
    "\n",
    "            fig_dist.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"Perfect Forecast\")\n",
    "\n",
    "            fig_dist.update_layout(\n",
    "                title=\"Forecast Error Distribution by Region\",\n",
    "                yaxis_title=\"Error (MWh) [Forecast - Actual]\",\n",
    "                height=400,\n",
    "                showlegend=True\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_dist, width=\"stretch\")\n",
    "\n",
    "            # Fuel type breakdown\n",
    "            st.subheader(\"⚡ Accuracy by Fuel Type\")\n",
    "\n",
    "            fuel_metrics = merged.groupby(['region', 'fuel']).agg({\n",
    "                'abs_error': 'mean',\n",
    "                'sq_error': lambda x: np.sqrt(x.mean()),\n",
    "            }).round(2).reset_index()\n",
    "            fuel_metrics.columns = ['Region', 'Fuel', 'MAE', 'RMSE']\n",
    "\n",
    "            # Create pivot for heatmap\n",
    "            pivot_mae = fuel_metrics.pivot(index='Fuel', columns='Region', values='MAE')\n",
    "\n",
    "            fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "                z=pivot_mae.values,\n",
    "                x=pivot_mae.columns,\n",
    "                y=pivot_mae.index,\n",
    "                colorscale='RdYlGn_r',\n",
    "                text=pivot_mae.values,\n",
    "                texttemplate='%{text:.0f}',\n",
    "                textfont={\"size\": 14},\n",
    "                colorbar=dict(title=\"MAE (MWh)\")\n",
    "            ))\n",
    "\n",
    "            fig_heatmap.update_layout(\n",
    "                title=\"Mean Absolute Error by Region and Fuel Type\",\n",
    "                xaxis_title=\"Region\",\n",
    "                yaxis_title=\"Fuel Type\",\n",
    "                height=300\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_heatmap, width=\"stretch\")\n",
    "\n",
    "        else:\n",
    "            st.warning(\"No matching forecast-actual pairs found for analysis\")\n",
    "    else:\n",
    "        st.warning(\"Forecast or generation data not available for accuracy analysis\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 5: Data Quality & EDA History (NEW)\n",
    "    # ========================================================================\n",
    "    st.header(\"📊 Data Quality & EDA History\")\n",
    "    st.markdown(\"\"\"\n",
    "Track data quality metrics over time and access historical EDA runs.\n",
    "    \"\"\")\n",
    "\n",
    "    # List all EDA runs\n",
    "    eda_dir = data_dir / \"eda\"\n",
    "\n",
    "    if eda_dir.exists():\n",
    "        eda_runs = sorted([d for d in eda_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "\n",
    "        if eda_runs:\n",
    "            st.subheader(\"📅 EDA Run History\")\n",
    "\n",
    "            # Create table of EDA runs\n",
    "            eda_history = []\n",
    "            for eda_run in eda_runs:\n",
    "                recs_file = eda_run / \"recommendations.json\"\n",
    "                if recs_file.exists():\n",
    "                    with open(recs_file, 'r') as f:\n",
    "                        recs = json.load(f)\n",
    "\n",
    "                    preprocessing = recs.get('preprocessing', {})\n",
    "                    eda_history.append({\n",
    "                        'Timestamp': eda_run.name,\n",
    "                        'Policy': preprocessing.get('negative_policy', 'N/A'),\n",
    "                        'Confidence': preprocessing.get('negative_confidence', 'N/A'),\n",
    "                        'Negatives Found': preprocessing.get('data_summary', {}).get('negative_count', 0),\n",
    "                        'Affected Series': len(preprocessing.get('data_summary', {}).get('affected_series', [])),\n",
    "                        'Path': str(eda_run)\n",
    "                    })\n",
    "\n",
    "            if eda_history:\n",
    "                eda_df = pd.DataFrame(eda_history)\n",
    "\n",
    "                # Display with download link\n",
    "                col1, col2 = st.columns([3, 1])\n",
    "\n",
    "                with col1:\n",
    "                    st.dataframe(\n",
    "                        eda_df[['Timestamp', 'Policy', 'Confidence', 'Negatives Found', 'Affected Series']],\n",
    "                        width=\"stretch\",\n",
    "                        hide_index=True,\n",
    "                    )\n",
    "\n",
    "                with col2:\n",
    "                    st.info(f\"\"\"\n",
    "**Total EDA Runs**\n",
    "{len(eda_runs)}\n",
    "\n",
    "**Latest Run**\n",
    "{eda_runs[0].name}\n",
    "                    \"\"\")\n",
    "\n",
    "                # Data quality trend\n",
    "                if len(eda_history) > 1:\n",
    "                    st.subheader(\"📈 Data Quality Trend\")\n",
    "\n",
    "                    fig_quality = go.Figure()\n",
    "\n",
    "                    fig_quality.add_trace(go.Scatter(\n",
    "                        x=eda_df['Timestamp'],\n",
    "                        y=eda_df['Negatives Found'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Negative Values',\n",
    "                        line=dict(color='red', width=2),\n",
    "                        marker=dict(size=8)\n",
    "                    ))\n",
    "\n",
    "                    fig_quality.update_layout(\n",
    "                        title=\"Negative Values Over Time\",\n",
    "                        xaxis_title=\"EDA Run Timestamp\",\n",
    "                        yaxis_title=\"Count of Negative Values\",\n",
    "                        height=350,\n",
    "                        hovermode='x unified'\n",
    "                    )\n",
    "\n",
    "                    st.plotly_chart(fig_quality, width=\"stretch\")\n",
    "\n",
    "                # Allow selection of specific EDA run\n",
    "                st.subheader(\"🔍 View Specific EDA Run\")\n",
    "\n",
    "                selected_run = st.selectbox(\n",
    "                    \"Select an EDA run to view details:\",\n",
    "                    options=[r.name for r in eda_runs],\n",
    "                    index=0\n",
    "                )\n",
    "\n",
    "                if selected_run:\n",
    "                    selected_path = data_dir / \"eda\" / selected_run\n",
    "                    recs_file = selected_path / \"recommendations.json\"\n",
    "\n",
    "                    if recs_file.exists():\n",
    "                        with open(recs_file, 'r') as f:\n",
    "                            selected_recs = json.load(f)\n",
    "\n",
    "                        with st.expander(f\"📄 EDA Results for {selected_run}\", expanded=False):\n",
    "                            st.json(selected_recs)\n",
    "\n",
    "                            # Links to visualization files\n",
    "                            st.markdown(\"**Download Visualizations:**\")\n",
    "                            viz_files = list(selected_path.rglob(\"*.png\"))\n",
    "                            if viz_files:\n",
    "                                for viz_file in viz_files:\n",
    "                                    rel_path = viz_file.relative_to(selected_path)\n",
    "                                    st.markdown(f\"- `{rel_path}`\")\n",
    "                            else:\n",
    "                                st.info(\"No visualizations found for this run\")\n",
    "\n",
    "            else:\n",
    "                st.info(\"No EDA recommendations found in run history\")\n",
    "        else:\n",
    "            st.info(\"No EDA runs found\")\n",
    "    else:\n",
    "        st.warning(\"EDA directory not found\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 6: Exploratory Data Analysis (Enhanced with Plotly)\n",
    "    # ========================================================================\n",
    "    st.header(\"🔬 Exploratory Data Analysis\")\n",
    "    st.markdown(\"\"\"\n",
    "Before building forecasting models, we perform comprehensive EDA to understand:\n",
    "- Data quality issues (negatives, missing values, outliers)\n",
    "- Seasonal patterns (daily, weekly cycles)\n",
    "- Zero-inflation (expected for solar at night)\n",
    "- Weather alignment (ensure weather data matches generation timestamps)\n",
    "    \"\"\")\n",
    "\n",
    "    # Load latest EDA results\n",
    "    data_dir = Path(db_path).parent\n",
    "    eda_dir = data_dir / \"eda\"\n",
    "\n",
    "    if not eda_dir.exists():\n",
    "        st.warning(\"No EDA results found. Run the pipeline to generate analysis.\")\n",
    "        return\n",
    "\n",
    "    # Get latest EDA run\n",
    "    eda_runs = sorted([d for d in eda_dir.iterdir() if d.is_dir()], reverse=True)\n",
    "    if not eda_runs:\n",
    "        st.warning(\"No EDA results found. Run the pipeline to generate analysis.\")\n",
    "        return\n",
    "\n",
    "    latest_eda = eda_runs[0]\n",
    "    st.info(f\"📅 **EDA Results from**: {latest_eda.name}\")\n",
    "\n",
    "    # Load recommendations\n",
    "    recs_file = latest_eda / \"recommendations.json\"\n",
    "    eda_report_file = latest_eda / \"eda_report.json\"\n",
    "\n",
    "    if recs_file.exists():\n",
    "        import json\n",
    "        with open(recs_file, 'r') as f:\n",
    "            recs = json.load(f)\n",
    "\n",
    "        # Data Summary\n",
    "        st.subheader(\"📊 Data Summary\")\n",
    "        if 'preprocessing' in recs and 'data_summary' in recs['preprocessing']:\n",
    "            summary = recs['preprocessing']['data_summary']\n",
    "\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.metric(\"Total Rows\", f\"{summary.get('generation_rows', 0):,}\")\n",
    "            with col2:\n",
    "                st.metric(\"Negative Values\", summary.get('negative_count', 0))\n",
    "            with col3:\n",
    "                affected = summary.get('affected_series', [])\n",
    "                st.metric(\"Affected Series\", len(affected))\n",
    "\n",
    "        # Preprocessing Recommendation\n",
    "        st.subheader(\"💡 Preprocessing Recommendation\")\n",
    "        if 'preprocessing' in recs:\n",
    "            prep = recs['preprocessing']\n",
    "\n",
    "            col1, col2 = st.columns([3, 1])\n",
    "\n",
    "            with col1:\n",
    "                st.markdown(f\"\"\"\n",
    "**Policy**: `{prep.get('negative_policy', 'N/A')}`\n",
    "\n",
    "**Reason**: {prep.get('negative_reason', 'No reason provided')}\n",
    "\n",
    "**Confidence**: {prep.get('negative_confidence', 'N/A')}\n",
    "                \"\"\")\n",
    "\n",
    "            with col2:\n",
    "                confidence = prep.get('negative_confidence', 'LOW')\n",
    "                if confidence == 'HIGH':\n",
    "                    st.success(\"✅ High Confidence\")\n",
    "                elif confidence == 'MEDIUM':\n",
    "                    st.info(\"⚠️ Medium Confidence\")\n",
    "                else:\n",
    "                    st.warning(\"⚠️ Low Confidence\")\n",
    "\n",
    "    # Visualizations (Enhanced with Interactive Plotly)\n",
    "    st.subheader(\"📈 EDA Visualizations (Interactive)\")\n",
    "\n",
    "    viz_tabs = st.tabs([\n",
    "        \"🔴 Negative Values\",\n",
    "        \"📅 Seasonality\",\n",
    "        \"0️⃣ Zero Inflation\",\n",
    "        \"🌤️ Generation Profiles\"\n",
    "    ])\n",
    "\n",
    "    with viz_tabs[0]:\n",
    "        st.markdown(\"\"\"\n",
    "**Negative Value Investigation**\n",
    "\n",
    "Physical reality: Renewable generation CANNOT be negative. Negative values indicate:\n",
    "- Net generation accounting (gross - auxiliary load)\n",
    "- Metering errors\n",
    "- Data reporting issues\n",
    "\n",
    "For forecasting, we clamp these to zero as recommended by EDA.\n",
    "        \"\"\")\n",
    "\n",
    "        # Try to load generation data for interactive viz\n",
    "        generation_path = data_dir / \"generation.parquet\"\n",
    "        if generation_path.exists():\n",
    "            gen_df = pd.read_parquet(generation_path)\n",
    "            gen_df['ds'] = pd.to_datetime(gen_df['ds'])\n",
    "\n",
    "            # Show negative values if they exist\n",
    "            negatives = gen_df[gen_df['y'] < 0]\n",
    "\n",
    "            if not negatives.empty:\n",
    "                st.warning(f\"⚠️ Found {len(negatives)} negative values across {negatives['unique_id'].nunique()} series\")\n",
    "\n",
    "                # Interactive scatter plot of negatives\n",
    "                fig_neg = go.Figure()\n",
    "\n",
    "                for series_id in negatives['unique_id'].unique():\n",
    "                    series_data = negatives[negatives['unique_id'] == series_id]\n",
    "                    fig_neg.add_trace(go.Scatter(\n",
    "                        x=series_data['ds'],\n",
    "                        y=series_data['y'],\n",
    "                        mode='markers',\n",
    "                        name=series_id,\n",
    "                        marker=dict(size=8, opacity=0.6)\n",
    "                    ))\n",
    "\n",
    "                fig_neg.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", annotation_text=\"Zero Line\")\n",
    "\n",
    "                fig_neg.update_layout(\n",
    "                    title=\"Negative Values Timeline\",\n",
    "                    xaxis_title=\"Timestamp\",\n",
    "                    yaxis_title=\"Generation (MWh)\",\n",
    "                    height=400,\n",
    "                    hovermode='x unified'\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig_neg, width=\"stretch\")\n",
    "            else:\n",
    "                st.success(\"✅ No negative values detected in current data!\")\n",
    "\n",
    "        # Show static image as reference\n",
    "        neg_img = latest_eda / \"negative_values\" / \"negative_investigation.png\"\n",
    "        if neg_img.exists():\n",
    "            with st.expander(\"📊 Static EDA Report\", expanded=False):\n",
    "                st.image(str(neg_img), width=\"stretch\")\n",
    "\n",
    "    with viz_tabs[1]:\n",
    "        st.markdown(\"\"\"\n",
    "**Seasonality Analysis**\n",
    "\n",
    "Renewable generation exhibits strong cyclical patterns:\n",
    "- **Daily**: Solar peaks at noon, wind varies by time\n",
    "- **Weekly**: Industrial demand affects generation patterns\n",
    "- **Seasonal**: Summer vs winter differences\n",
    "\n",
    "These patterns are captured by MSTL (Multiple Seasonal-Trend decomposition using LOESS) in our models.\n",
    "        \"\"\")\n",
    "\n",
    "        # Interactive hourly profile\n",
    "        if generation_path.exists():\n",
    "            gen_df = pd.read_parquet(generation_path)\n",
    "            gen_df['ds'] = pd.to_datetime(gen_df['ds'])\n",
    "            gen_df['hour'] = gen_df['ds'].dt.hour\n",
    "            gen_df['dow'] = gen_df['ds'].dt.day_name()\n",
    "\n",
    "            # Hourly profiles by series\n",
    "            fig_hour = go.Figure()\n",
    "\n",
    "            for series_id in gen_df['unique_id'].unique():\n",
    "                series_data = gen_df[gen_df['unique_id'] == series_id]\n",
    "                hourly_avg = series_data.groupby('hour')['y'].mean()\n",
    "\n",
    "                fig_hour.add_trace(go.Scatter(\n",
    "                    x=hourly_avg.index,\n",
    "                    y=hourly_avg.values,\n",
    "                    mode='lines+markers',\n",
    "                    name=series_id,\n",
    "                    line=dict(width=2)\n",
    "                ))\n",
    "\n",
    "            fig_hour.update_layout(\n",
    "                title=\"Average Generation by Hour of Day\",\n",
    "                xaxis_title=\"Hour of Day\",\n",
    "                yaxis_title=\"Average Generation (MWh)\",\n",
    "                height=450,\n",
    "                hovermode='x unified',\n",
    "                xaxis=dict(tickmode='linear', tick0=0, dtick=2)\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_hour, width=\"stretch\")\n",
    "\n",
    "            # Day of week profile\n",
    "            st.markdown(\"**Weekly Patterns**\")\n",
    "\n",
    "            dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            dow_avg = gen_df.groupby(['unique_id', 'dow'])['y'].mean().reset_index()\n",
    "\n",
    "            fig_dow = go.Figure()\n",
    "\n",
    "            for series_id in dow_avg['unique_id'].unique():\n",
    "                series_data = dow_avg.loc[dow_avg[\"unique_id\"] == series_id].copy()\n",
    "                series_data.loc[:, \"dow\"] = pd.Categorical(\n",
    "                    series_data[\"dow\"],\n",
    "                    categories=dow_order,\n",
    "                    ordered=True,\n",
    "                )\n",
    "                series_data = series_data.sort_values('dow')\n",
    "\n",
    "                fig_dow.add_trace(go.Bar(\n",
    "                    x=series_data['dow'],\n",
    "                    y=series_data['y'],\n",
    "                    name=series_id\n",
    "                ))\n",
    "\n",
    "            fig_dow.update_layout(\n",
    "                title=\"Average Generation by Day of Week\",\n",
    "                xaxis_title=\"Day of Week\",\n",
    "                yaxis_title=\"Average Generation (MWh)\",\n",
    "                height=400,\n",
    "                barmode='group'\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_dow, width=\"stretch\")\n",
    "\n",
    "        # Show static image as reference\n",
    "        season_img = latest_eda / \"seasonality\" / \"hourly_profiles.png\"\n",
    "        if season_img.exists():\n",
    "            with st.expander(\"📊 Static EDA Report\", expanded=False):\n",
    "                st.image(str(season_img), width=\"stretch\")\n",
    "\n",
    "    with viz_tabs[2]:\n",
    "        st.markdown(\"\"\"\n",
    "**Zero-Inflation Analysis**\n",
    "\n",
    "**Expected Zeros**:\n",
    "- **Solar**: Nighttime generation is ALWAYS zero (sun not shining)\n",
    "- **Wind**: Rarely zero (wind always has some component)\n",
    "\n",
    "Zero-inflation is normal for solar and factored into model selection.\n",
    "We avoid MAPE (Mean Absolute Percentage Error) as it's undefined when y=0.\n",
    "        \"\"\")\n",
    "\n",
    "        # Interactive zero analysis\n",
    "        if generation_path.exists():\n",
    "            gen_df = pd.read_parquet(generation_path)\n",
    "            gen_df['ds'] = pd.to_datetime(gen_df['ds'])\n",
    "            gen_df['hour'] = gen_df['ds'].dt.hour\n",
    "\n",
    "            # Calculate zero ratio by hour for each series\n",
    "            fig_zero = go.Figure()\n",
    "\n",
    "            for series_id in gen_df['unique_id'].unique():\n",
    "                series_data = gen_df[gen_df['unique_id'] == series_id]\n",
    "                # Use vectorized approach: faster and no FutureWarning\n",
    "                zero_by_hour = (\n",
    "                    series_data.assign(is_zero=series_data['y'].eq(0))\n",
    "                    .groupby('hour')['is_zero']\n",
    "                    .mean() * 100\n",
    "                )\n",
    "\n",
    "                fig_zero.add_trace(go.Bar(\n",
    "                    x=zero_by_hour.index,\n",
    "                    y=zero_by_hour.values,\n",
    "                    name=series_id,\n",
    "                    opacity=0.7\n",
    "                ))\n",
    "\n",
    "            fig_zero.update_layout(\n",
    "                title=\"Zero-Inflation by Hour of Day\",\n",
    "                xaxis_title=\"Hour of Day\",\n",
    "                yaxis_title=\"Percentage of Zero Values (%)\",\n",
    "                height=450,\n",
    "                barmode='group',\n",
    "                xaxis=dict(tickmode='linear', tick0=0, dtick=2)\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_zero, width=\"stretch\")\n",
    "\n",
    "            # Summary statistics\n",
    "            col1, col2 = st.columns(2)\n",
    "\n",
    "            with col1:\n",
    "                st.markdown(\"**Solar Series (Expected Zeros)**\")\n",
    "                solar_series = [s for s in gen_df['unique_id'].unique() if 'SUN' in s]\n",
    "                for series_id in solar_series:\n",
    "                    zero_pct = (gen_df[gen_df['unique_id'] == series_id]['y'] == 0).mean() * 100\n",
    "                    st.metric(series_id, f\"{zero_pct:.1f}%\")\n",
    "\n",
    "            with col2:\n",
    "                st.markdown(\"**Wind Series (Minimal Zeros)**\")\n",
    "                wind_series = [s for s in gen_df['unique_id'].unique() if 'WND' in s]\n",
    "                for series_id in wind_series:\n",
    "                    zero_pct = (gen_df[gen_df['unique_id'] == series_id]['y'] == 0).mean() * 100\n",
    "                    st.metric(series_id, f\"{zero_pct:.1f}%\")\n",
    "\n",
    "        # Show static image as reference\n",
    "        zero_img = latest_eda / \"zero_inflation\" / \"zero_inflation.png\"\n",
    "        if zero_img.exists():\n",
    "            with st.expander(\"📊 Static EDA Report\", expanded=False):\n",
    "                st.image(str(zero_img), width=\"stretch\")\n",
    "\n",
    "    with viz_tabs[3]:\n",
    "        st.markdown(\"\"\"\n",
    "**Generation Profiles Over Time**\n",
    "\n",
    "Interactive time series view of generation data across all regions and fuel types.\n",
    "Use the legend to toggle series on/off.\n",
    "        \"\"\")\n",
    "\n",
    "        if generation_path.exists():\n",
    "            gen_df = pd.read_parquet(generation_path)\n",
    "            gen_df['ds'] = pd.to_datetime(gen_df['ds'])\n",
    "\n",
    "            # Time series plot\n",
    "            fig_ts = go.Figure()\n",
    "\n",
    "            for series_id in gen_df['unique_id'].unique():\n",
    "                series_data = gen_df[gen_df['unique_id'] == series_id].sort_values('ds')\n",
    "                fig_ts.add_trace(go.Scatter(\n",
    "                    x=series_data['ds'],\n",
    "                    y=series_data['y'],\n",
    "                    mode='lines',\n",
    "                    name=series_id,\n",
    "                    line=dict(width=1.5),\n",
    "                    opacity=0.8\n",
    "                ))\n",
    "\n",
    "            fig_ts.update_layout(\n",
    "                title=\"Generation Time Series (All Series)\",\n",
    "                xaxis_title=\"Timestamp\",\n",
    "                yaxis_title=\"Generation (MWh)\",\n",
    "                height=500,\n",
    "                hovermode='x unified',\n",
    "                legend=dict(\n",
    "                    orientation=\"v\",\n",
    "                    yanchor=\"top\",\n",
    "                    y=1,\n",
    "                    xanchor=\"left\",\n",
    "                    x=1.01\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Add range slider\n",
    "            fig_ts.update_xaxes(\n",
    "                rangeslider_visible=True,\n",
    "                rangeselector=dict(\n",
    "                    buttons=list([\n",
    "                        dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n",
    "                        dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n",
    "                        dict(count=14, label=\"2w\", step=\"day\", stepmode=\"backward\"),\n",
    "                        dict(step=\"all\", label=\"All\")\n",
    "                    ])\n",
    "                )\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig_ts, width=\"stretch\")\n",
    "\n",
    "            # Summary statistics\n",
    "            st.markdown(\"**Summary Statistics**\")\n",
    "            summary_stats = gen_df.groupby('unique_id')['y'].agg(['count', 'mean', 'std', 'min', 'max']).round(2)\n",
    "            summary_stats.columns = ['Count', 'Mean (MWh)', 'Std Dev', 'Min', 'Max']\n",
    "            st.dataframe(summary_stats, width=\"stretch\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 4: Model & Architecture Details\n",
    "    # ========================================================================\n",
    "    st.header(\"🤖 Modeling Approach\")\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.subheader(\"Why StatsForecast?\")\n",
    "        st.markdown(\"\"\"\n",
    "We use **StatsForecast** (Nixtla) instead of Prophet or MLForecast:\n",
    "\n",
    "✅ **Native multi-series support** (10x faster)\n",
    "✅ **Built-in prediction intervals** (no conformal prediction needed)\n",
    "✅ **Production-ready** (battle-tested at scale)\n",
    "✅ **Exogenous regressors** (weather variables)\n",
    "\n",
    "**Models Tested**:\n",
    "- **MSTL**: Multiple Seasonal-Trend decomposition\n",
    "- **AutoARIMA**: Automatic ARIMA model selection\n",
    "- **AutoETS**: Exponential smoothing state space\n",
    "\n",
    "Best model selected via cross-validation (RMSE).\n",
    "        \"\"\")\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Weather Features\")\n",
    "        st.markdown(\"\"\"\n",
    "**7 Key Weather Variables** (Open-Meteo API):\n",
    "\n",
    "☀️ **Solar-Related**:\n",
    "- Direct solar radiation\n",
    "- Diffuse solar radiation\n",
    "- Cloud cover\n",
    "\n",
    "💨 **Wind-Related**:\n",
    "- Wind speed at 10m\n",
    "- Wind speed at 100m\n",
    "\n",
    "🌡️ **General**:\n",
    "- Temperature at 2m\n",
    "\n",
    "These features are strongly correlated with generation and improve forecast accuracy by 15-20%.\n",
    "        \"\"\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Section 5: Data Sources\n",
    "    # ========================================================================\n",
    "    st.header(\"📡 Data Sources\")\n",
    "\n",
    "    source_col1, source_col2 = st.columns(2)\n",
    "\n",
    "    with source_col1:\n",
    "        st.subheader(\"🏭 EIA RTO Fuel-Type Data\")\n",
    "        st.markdown(\"\"\"\n",
    "**Energy Information Administration (EIA)**\n",
    "\n",
    "- **Authoritative**: Official US electricity generation data\n",
    "- **Coverage**: Hourly granularity covering 80%+ of US grid\n",
    "- **Accessibility**: Free API with key (no usage limits)\n",
    "- **Timeliness**: Real-time with 12-48h publishing lag\n",
    "- **Quality**: High (direct from RTOs/ISOs)\n",
    "\n",
    "🔗 [EIA API Documentation](https://www.eia.gov/opendata/)\n",
    "        \"\"\")\n",
    "\n",
    "    with source_col2:\n",
    "        st.subheader(\"🌤️ Open-Meteo Weather API\")\n",
    "        st.markdown(\"\"\"\n",
    "**Open-Meteo: Open-source Weather API**\n",
    "\n",
    "- **Free & Open**: No authentication, unlimited requests\n",
    "- **Leakage Prevention**: Separate historical + forecast endpoints\n",
    "- **Global Coverage**: Works for any lat/lon coordinate\n",
    "- **Variables**: 7 key features correlated with generation\n",
    "- **Reliability**: 99.9%+ uptime\n",
    "\n",
    "🔗 [Open-Meteo Documentation](https://open-meteo.com/en/docs)\n",
    "        \"\"\")\n",
    "\n",
    "    # Final note\n",
    "    st.markdown(\"---\")\n",
    "    st.info(\"\"\"\n",
    "💡 **Note**: This dashboard provides real-time insights into renewable energy forecasting.\n",
    "For technical details, see the [GitHub repository](https://github.com/yourusername/atsaf)\n",
    "and the Jupyter notebook in `/chapters/renewable_energy_forecasting.ipynb`.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def run_pipeline_from_dashboard(db_path: str, regions: list, fuel_type: str):\n",
    "    \"\"\"Run the forecasting pipeline from the dashboard.\"\"\"\n",
    "    with st.spinner(\"Refreshing forecasts... (may take 2-3 minutes)\"):\n",
    "        try:\n",
    "            import os\n",
    "\n",
    "            from src.renewable.jobs import run_hourly\n",
    "\n",
    "            # IMPORTANT: Force pipeline to run even if no new data\n",
    "            # When user explicitly clicks \"Run Pipeline\", they want it to run\n",
    "            # regardless of data freshness (which is only relevant for automated cron jobs)\n",
    "            original_force_run = os.environ.get(\"FORCE_RUN\")\n",
    "            os.environ[\"FORCE_RUN\"] = \"true\"\n",
    "\n",
    "            try:\n",
    "                # Run the hourly pipeline job\n",
    "                run_hourly.main()\n",
    "            finally:\n",
    "                # Restore original FORCE_RUN setting\n",
    "                if original_force_run is None:\n",
    "                    os.environ.pop(\"FORCE_RUN\", None)\n",
    "                else:\n",
    "                    os.environ[\"FORCE_RUN\"] = original_force_run\n",
    "\n",
    "            st.success(\"Pipeline completed! Forecasts have been updated with latest EIA data.\")\n",
    "            st.info(\"Reloading page to show new forecasts...\")\n",
    "\n",
    "            # Wait a moment then reload\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            st.rerun()\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Pipeline failed: {e}\")\n",
    "            import traceback\n",
    "            with st.expander(\"Error details\"):\n",
    "                st.code(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/jobs/run_hourly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/jobs/run_hourly.py\n",
    "# file: src/renewable/jobs/run_hourly.py\n",
    "\"\"\"Hourly renewable pipeline entry point with validation.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.renewable.tasks import RenewablePipelineConfig, run_full_pipeline\n",
    "from src.renewable.validation import validate_generation_df\n",
    "from src.renewable.data_freshness import check_all_series_freshness, FreshnessCheckResult\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress DeprecationWarning from statsforecast library (invalid escape sequences)\n",
    "# These are in third-party code we cannot fix directly\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='statsforecast')\n",
    "\n",
    "\n",
    "def _env_list(name: str, default_csv: str) -> list[str]:\n",
    "    raw = os.getenv(name, default_csv)\n",
    "    return [item.strip() for item in raw.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int) -> int:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return int(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _expected_series(regions: list[str], fuel_types: list[str]) -> list[str]:\n",
    "    return [f\"{region}_{fuel}\" for region in regions for fuel in fuel_types]\n",
    "\n",
    "\n",
    "def _json_default(value: object) -> str:\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.isoformat()\n",
    "    if isinstance(value, datetime):\n",
    "        return value.isoformat()\n",
    "    if hasattr(value, \"item\"):\n",
    "        try:\n",
    "            return value.item()\n",
    "        except Exception:\n",
    "            return str(value)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _summarize_generation_coverage(df: pd.DataFrame) -> dict:\n",
    "    if df.empty:\n",
    "        return {\"row_count\": 0, \"series_count\": 0, \"coverage\": []}\n",
    "\n",
    "    coverage = (\n",
    "        df.groupby(\"unique_id\")[\"ds\"]\n",
    "        .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"series_count\": int(df[\"unique_id\"].nunique()),\n",
    "        \"coverage\": coverage.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _read_previous_run_summary(data_dir: str) -> dict | None:\n",
    "    \"\"\"Read previous run_log.json for rowcount comparison.\"\"\"\n",
    "    path = Path(data_dir) / \"run_log.json\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _summarize_negative_forecasts(\n",
    "    df: pd.DataFrame,\n",
    "    sample_rows: int = 5,\n",
    ") -> dict:\n",
    "    if df.empty or \"yhat\" not in df.columns:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    neg = df[df[\"yhat\"] < 0]\n",
    "    if neg.empty:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    series_summary = (\n",
    "        neg.groupby(\"unique_id\")[\"yhat\"]\n",
    "        .agg(count=\"count\", min_value=\"min\", max_value=\"max\", mean_value=\"mean\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    sample = neg[[\"unique_id\", \"ds\", \"yhat\"]].head(sample_rows)\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"negative_rows\": int(len(neg)),\n",
    "        \"series\": series_summary.to_dict(orient=\"records\"),\n",
    "        \"sample\": sample.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_hourly_pipeline() -> dict:\n",
    "    data_dir = os.getenv(\"RENEWABLE_DATA_DIR\", \"data/renewable\")\n",
    "    regions = _env_list(\"RENEWABLE_REGIONS\", \"CALI,ERCO,MISO\")\n",
    "    fuel_types = _env_list(\"RENEWABLE_FUELS\", \"WND,SUN\")\n",
    "    lookback_days = _env_int(\"LOOKBACK_DAYS\", 30)\n",
    "\n",
    "    # Horizon configuration: support both preset and direct override\n",
    "    horizon_preset = os.getenv(\"RENEWABLE_HORIZON_PRESET\", None)  # \"24h\" | \"48h\" | \"72h\"\n",
    "    horizon_override = _env_int(\"RENEWABLE_HORIZON\", 0)  # Legacy direct override\n",
    "\n",
    "    # If direct override is set, use it; otherwise use preset (or None for default)\n",
    "    if horizon_override > 0:\n",
    "        horizon = horizon_override\n",
    "        horizon_preset = None  # Ignore preset if direct override is set\n",
    "    else:\n",
    "        horizon = 24  # Default, may be overridden by preset\n",
    "\n",
    "    cv_windows = _env_int(\"RENEWABLE_CV_WINDOWS\", 2)\n",
    "    cv_step_size = _env_int(\"RENEWABLE_CV_STEP_SIZE\", 168)\n",
    "\n",
    "    start_date = os.getenv(\"RENEWABLE_START_DATE\", \"\")\n",
    "    end_date = os.getenv(\"RENEWABLE_END_DATE\", \"\")\n",
    "\n",
    "    # Check if we should force run (e.g., manual dispatch from dashboard)\n",
    "    force_run = os.getenv(\"FORCE_RUN\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # DEBUG: Log force_run status\n",
    "    print(f\"[pipeline] FORCE_RUN={force_run}\")\n",
    "\n",
    "    # Data freshness check - skip full pipeline if no new data\n",
    "    if not force_run:\n",
    "        api_key = os.getenv(\"EIA_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            print(\"WARNING: EIA_API_KEY not set, skipping freshness check\")\n",
    "        else:\n",
    "            run_log_path = Path(data_dir) / \"run_log.json\"\n",
    "            freshness = check_all_series_freshness(\n",
    "                regions=regions,\n",
    "                fuel_types=fuel_types,\n",
    "                run_log_path=run_log_path,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "\n",
    "            if not freshness.has_new_data:\n",
    "                # No new data - return early with skip status\n",
    "                skip_log = {\n",
    "                    \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"reason\": \"no_new_data\",\n",
    "                    \"freshness_check\": {\n",
    "                        \"checked_at_utc\": freshness.checked_at_utc,\n",
    "                        \"summary\": freshness.summary,\n",
    "                        \"series_status\": freshness.series_status,\n",
    "                    },\n",
    "                    \"config\": {\n",
    "                        \"regions\": regions,\n",
    "                        \"fuel_types\": fuel_types,\n",
    "                        \"data_dir\": data_dir,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                # Write skip log (append to run_log.json)\n",
    "                Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "                skip_log_path = Path(data_dir) / \"skip_log.json\"\n",
    "                skip_log_path.write_text(\n",
    "                    json.dumps(skip_log, indent=2, default=_json_default)\n",
    "                )\n",
    "\n",
    "                print(f\"SKIPPED: {freshness.summary}\")\n",
    "                print(f\"Skip log written to: {skip_log_path}\")\n",
    "\n",
    "                # Set output for GitHub Actions\n",
    "                github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "                if github_output:\n",
    "                    with open(github_output, \"a\") as f:\n",
    "                        f.write(\"status=skipped\\n\")\n",
    "\n",
    "                return skip_log\n",
    "\n",
    "            print(f\"Freshness check: {freshness.summary}\")\n",
    "    else:\n",
    "        print(\"[pipeline] FORCE_RUN=true - bypassing freshness check (manual run requested)\")\n",
    "        print(\"[pipeline] Pipeline will run regardless of data freshness\")\n",
    "\n",
    "    cfg = RenewablePipelineConfig(\n",
    "        regions=regions,\n",
    "        fuel_types=fuel_types,\n",
    "        lookback_days=lookback_days,\n",
    "        horizon=horizon,\n",
    "        horizon_preset=horizon_preset,  # Apply preset if specified\n",
    "        cv_windows=cv_windows,          # Pass to constructor to validate with correct value\n",
    "        cv_step_size=cv_step_size,      # Pass to constructor\n",
    "        data_dir=data_dir,\n",
    "        overwrite=True,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "    )\n",
    "    # Post-construction assignments no longer needed\n",
    "\n",
    "    # Add option to skip EDA for fast iteration (set SKIP_EDA=true in .env)\n",
    "    skip_eda = os.getenv(\"SKIP_EDA\", \"false\").lower() == \"true\"\n",
    "\n",
    "    fetch_diagnostics: list[dict] = []\n",
    "    results = run_full_pipeline(\n",
    "        cfg,\n",
    "        fetch_diagnostics=fetch_diagnostics,\n",
    "        skip_eda=skip_eda,\n",
    "    )\n",
    "\n",
    "    gen_path = cfg.generation_path()\n",
    "    gen_df = pd.read_parquet(gen_path)\n",
    "    generation_coverage = _summarize_generation_coverage(gen_df)\n",
    "\n",
    "    max_lag_hours = _env_int(\"MAX_LAG_HOURS\", 48)  # EIA publishes with 12-24h delay\n",
    "    max_missing_ratio = _env_float(\"MAX_MISSING_RATIO\", 0.02)\n",
    "    report = validate_generation_df(\n",
    "        gen_df,\n",
    "        max_lag_hours=max_lag_hours,\n",
    "        max_missing_ratio=max_missing_ratio,\n",
    "        expected_series=_expected_series(regions, fuel_types),\n",
    "    )\n",
    "\n",
    "    forecasts_df = pd.read_parquet(cfg.forecasts_path())\n",
    "    negative_forecasts = _summarize_negative_forecasts(forecasts_df)\n",
    "\n",
    "    # Quality gates\n",
    "    max_rowdrop_pct = _env_float(\"MAX_ROWDROP_PCT\", 0.30)\n",
    "    max_neg_forecast_ratio = _env_float(\"MAX_NEG_FORECAST_RATIO\", 0.10)\n",
    "\n",
    "    prev_run = _read_previous_run_summary(data_dir)\n",
    "    prev_gen_rows = 0\n",
    "    if prev_run:\n",
    "        prev_gen_rows = prev_run.get(\"pipeline_results\", {}).get(\"generation_rows\", 0)\n",
    "\n",
    "    curr_gen_rows = results.get(\"generation_rows\", 0)\n",
    "    rowdrop_ok = True\n",
    "    if prev_gen_rows > 0:\n",
    "        floor_ok = int(prev_gen_rows * (1.0 - max_rowdrop_pct))\n",
    "        rowdrop_ok = curr_gen_rows >= floor_ok\n",
    "\n",
    "    neg_forecast_ratio = 0.0\n",
    "    if negative_forecasts[\"row_count\"] > 0:\n",
    "        neg_forecast_ratio = (\n",
    "            negative_forecasts[\"negative_rows\"] / negative_forecasts[\"row_count\"]\n",
    "        )\n",
    "    neg_forecast_ok = neg_forecast_ratio <= max_neg_forecast_ratio\n",
    "\n",
    "    quality_gates = {\n",
    "        \"rowdrop\": {\n",
    "            \"ok\": rowdrop_ok,\n",
    "            \"prev_rows\": prev_gen_rows,\n",
    "            \"curr_rows\": curr_gen_rows,\n",
    "            \"max_rowdrop_pct\": max_rowdrop_pct,\n",
    "        },\n",
    "        \"neg_forecast\": {\n",
    "            \"ok\": neg_forecast_ok,\n",
    "            \"ratio\": neg_forecast_ratio,\n",
    "            \"max_ratio\": max_neg_forecast_ratio,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log = {\n",
    "        \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"config\": {\n",
    "            \"regions\": regions,\n",
    "            \"fuel_types\": fuel_types,\n",
    "            \"lookback_days\": lookback_days,\n",
    "            \"horizon\": horizon,\n",
    "            \"cv_windows\": cv_windows,\n",
    "            \"cv_step_size\": cv_step_size,\n",
    "            \"data_dir\": data_dir,\n",
    "            \"start_date\": cfg.start_date,\n",
    "            \"end_date\": cfg.end_date,\n",
    "        },\n",
    "        \"pipeline_results\": results,\n",
    "        \"validation\": {\n",
    "            \"ok\": report.ok,\n",
    "            \"message\": report.message,\n",
    "            \"details\": report.details,\n",
    "        },\n",
    "        \"diagnostics\": {\n",
    "            \"fetch\": fetch_diagnostics,\n",
    "            \"generation_coverage\": generation_coverage,\n",
    "            \"negative_forecasts\": negative_forecasts,\n",
    "        },\n",
    "        \"quality_gates\": quality_gates,\n",
    "    }\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    (Path(data_dir) / \"run_log.json\").write_text(\n",
    "        json.dumps(run_log, indent=2, default=_json_default)\n",
    "    )\n",
    "\n",
    "    # Check validation\n",
    "    if not report.ok:\n",
    "        raise SystemExit(f\"VALIDATION_FAILED: {report.message} | {report.details}\")\n",
    "\n",
    "    # Check quality gates\n",
    "    if not rowdrop_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: rowdrop | \"\n",
    "            f\"curr={curr_gen_rows} prev={prev_gen_rows} max_drop={max_rowdrop_pct:.0%}\"\n",
    "        )\n",
    "    if not neg_forecast_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: neg_forecast | \"\n",
    "            f\"ratio={neg_forecast_ratio:.1%} max={max_neg_forecast_ratio:.0%}\"\n",
    "        )\n",
    "\n",
    "    # Set output for GitHub Actions (successful run)\n",
    "    github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "    if github_output:\n",
    "        with open(github_output, \"a\") as f:\n",
    "            f.write(\"status=success\\n\")\n",
    "\n",
    "    return run_log\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    run_hourly_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dag_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dag_builder.py\n",
    "# file: src/renewable/dag_builder.py\n",
    "\"\"\"Renewable pipeline DAG builder for Airflow.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "AIRFLOW_AVAILABLE = True\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "def build_hourly_dag(\n",
    "    dag_id: str = \"renewable_hourly_pipeline\",\n",
    "    schedule: str = \"17 * * * *\",\n",
    "    start_date: Optional[datetime] = None,\n",
    "    default_args: Optional[Dict[str, Any]] = None,\n",
    ") -> \"DAG\":\n",
    "    if not AIRFLOW_AVAILABLE:\n",
    "        raise ImportError(\"Airflow is not installed. Install apache-airflow to use build_hourly_dag().\")\n",
    "\n",
    "    from src.renewable.jobs.run_hourly import run_hourly_pipeline\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = datetime.utcnow() - timedelta(days=1)\n",
    "    if default_args is None:\n",
    "        default_args = DEFAULT_ARGS.copy()\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=\"Renewable hourly pipeline\",\n",
    "        schedule_interval=schedule,\n",
    "        start_date=start_date,\n",
    "        catchup=False,\n",
    "        max_active_runs=1,\n",
    "        tags=[\"renewable\", \"eia\", \"forecasting\"],\n",
    "    ) as dag:\n",
    "        PythonOperator(\n",
    "            task_id=\"run_hourly\",\n",
    "            python_callable=run_hourly_pipeline,\n",
    "        )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def build_dag_dot() -> str:\n",
    "    return \"\"\"digraph RENEWABLE_PIPELINE {\n",
    "  rankdir=LR;\n",
    "  node [shape=box, style=\"rounded,filled\", fillcolor=\"#e8f5e9\"];\n",
    "\n",
    "  run_hourly;\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# git actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/pre-commit.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/pre-commit.yml\n",
    "# file: .github/workflows/pre-commit.yml\n",
    "name: pre-commit\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  run:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - uses: pre-commit/action@v3.0.1\n",
    "\n",
    "      - name: Install test dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install pytest pandas numpy requests python-dotenv\n",
    "\n",
    "      - name: Run smoke tests\n",
    "        env:\n",
    "          PYTHONPATH: ${{ github.workspace }}\n",
    "        run: pytest tests/ -v -k \"not slow\" --tb=short || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/renewable-hourly.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/renewable-hourly.yml\n",
    "# file: .github/workflows/renewable-hourly.yml\n",
    "name: renewable-hourly\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      force_run:\n",
    "        description: 'Force full pipeline run (skip freshness check)'\n",
    "        type: boolean\n",
    "        default: false\n",
    "  schedule:\n",
    "    - cron: \"17 * * * *\"\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "concurrency:\n",
    "  group: renewable-hourly\n",
    "  cancel-in-progress: true\n",
    "\n",
    "jobs:\n",
    "  update:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 25\n",
    "    env:\n",
    "      EIA_API_KEY: ${{ secrets.EIA_API_KEY }}\n",
    "      FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.force_run && 'true' || 'false' }}\n",
    "      RENEWABLE_REGIONS: \"CALI,ERCO,MISO\"\n",
    "      RENEWABLE_FUELS: \"WND,SUN\"\n",
    "      LOOKBACK_DAYS: \"30\"\n",
    "      RENEWABLE_HORIZON: \"24\"\n",
    "      RENEWABLE_CV_WINDOWS: \"2\"\n",
    "      RENEWABLE_CV_STEP_SIZE: \"168\"\n",
    "      MAX_LAG_HOURS: \"48\"  # EIA publishes hourly data with 12-24h delay\n",
    "      MAX_MISSING_RATIO: \"0.02\"\n",
    "      RENEWABLE_DATA_DIR: \"data/renewable\"\n",
    "      RENEWABLE_N_JOBS: \"1\"\n",
    "      OMP_NUM_THREADS: \"1\"\n",
    "      MKL_NUM_THREADS: \"1\"\n",
    "      OPENBLAS_NUM_THREADS: \"1\"\n",
    "      NUMBA_NUM_THREADS: \"1\"\n",
    "      VECLIB_MAXIMUM_THREADS: \"1\"\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - name: Check EIA API key\n",
    "        run: |\n",
    "          if [ -z \"$EIA_API_KEY\" ]; then\n",
    "            echo \"EIA_API_KEY is not set. Add it to repo secrets.\" >&2\n",
    "            exit 1\n",
    "          fi\n",
    "\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # Install from pyproject.toml for single source of truth\n",
    "          # Use -e for editable install (allows imports to work correctly)\n",
    "          pip install -e .\n",
    "\n",
    "      - name: Run hourly pipeline\n",
    "        id: pipeline\n",
    "        run: |\n",
    "          python -m src.renewable.jobs.run_hourly\n",
    "\n",
    "      - name: Quality gate check\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          python -c \"\n",
    "          import json, sys\n",
    "          from pathlib import Path\n",
    "          log_path = Path('data/renewable/run_log.json')\n",
    "          if not log_path.exists():\n",
    "              print('No run_log.json found')\n",
    "              \n",
    "          log = json.loads(log_path.read_text())\n",
    "          val = log.get('validation', {})\n",
    "          if not val.get('ok'):\n",
    "              print(f'VALIDATION FAILED: {val.get(\\\"message\\\")}')\n",
    "              print(f'Details: {val.get(\\\"details\\\")}')\n",
    "              \n",
    "          gates = log.get('quality_gates', {})\n",
    "          if not gates.get('rowdrop', {}).get('ok', True):\n",
    "              print(f'ROWDROP GATE FAILED: {gates.get(\\\"rowdrop\\\")}')\n",
    "              \n",
    "          if not gates.get('neg_forecast', {}).get('ok', True):\n",
    "              print(f'NEG_FORECAST GATE FAILED: {gates.get(\\\"neg_forecast\\\")}')\n",
    "              \n",
    "          print('QUALITY GATES PASSED')\n",
    "          \"\n",
    "\n",
    "      - name: Skip notification\n",
    "        if: steps.pipeline.outputs.status == 'skipped'\n",
    "        run: |\n",
    "          echo \"### Pipeline skipped - no new EIA data\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          if [ -f data/renewable/skip_log.json ]; then\n",
    "            python -c \"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "          data = json.loads(Path('data/renewable/skip_log.json').read_text())\n",
    "          freshness = data.get('freshness_check', {})\n",
    "          print(f'- Checked at: {freshness.get(\\\"checked_at_utc\\\")}')\n",
    "          print(f'- Summary: {freshness.get(\\\"summary\\\")}')\n",
    "          \" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Summarize run\n",
    "        if: always() && steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          if [ -f data/renewable/run_log.json ]; then\n",
    "          python - <<'PY' | tee -a \"$GITHUB_STEP_SUMMARY\"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "\n",
    "          data = json.loads(Path(\"data/renewable/run_log.json\").read_text())\n",
    "          validation = data.get(\"validation\", {})\n",
    "          details = validation.get(\"details\", {})\n",
    "          pipeline = data.get(\"pipeline_results\", {})\n",
    "          interp = pipeline.get(\"interpretability\", {})\n",
    "\n",
    "          lines = [\n",
    "              \"### Renewable hourly run\",\n",
    "              f\"- run_at_utc: {data.get('run_at_utc')}\",\n",
    "              f\"- validation_ok: {validation.get('ok')}\",\n",
    "              f\"- message: {validation.get('message')}\",\n",
    "              f\"- max_ds: {details.get('max_ds')}\",\n",
    "              f\"- lag_hours: {details.get('lag_hours')}\",\n",
    "              f\"- best_model: {pipeline.get('best_model')}\",\n",
    "              f\"- best_rmse: {pipeline.get('best_rmse', 0):.1f}\",\n",
    "              \"\",\n",
    "              \"#### Interpretability\",\n",
    "              f\"- series_count: {interp.get('series_count', 0)}\",\n",
    "              f\"- output_dir: {interp.get('output_dir', 'N/A')}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          PY\n",
    "          else\n",
    "          echo \"No run_log.json found.\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Commit updated artifacts\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          git config user.name \"github-actions[bot]\"\n",
    "          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n",
    "          git add data/renewable/generation.parquet \\\n",
    "            data/renewable/weather.parquet \\\n",
    "            data/renewable/forecasts.parquet \\\n",
    "            data/renewable/run_log.json\n",
    "          # Add interpretability artifacts if they exist\n",
    "          if [ -d data/renewable/interpretability ]; then\n",
    "            git add data/renewable/interpretability/\n",
    "          fi\n",
    "          git commit -m \"renewable: hourly data update (UTC)\" || echo \"No changes to commit\"\n",
    "          git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
