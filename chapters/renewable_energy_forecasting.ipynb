{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos:\n",
    "- update preprocessing data so it's organized and in the data pull rather than in the preprocessing\n",
    "- update so we have sun data\n",
    "- update top of notebook markdown with actual tree\n",
    "- go over the full explanation of why we would choose this data, why we would choose this model, \n",
    "    - ensure to include decision making and thought process not just end results, \n",
    "    - archive the notebooks, \n",
    "    - update the readme, \n",
    "    - ensure this is software that is automated, \n",
    "    - finally post on linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewable Energy Forecasting Pipeline\n",
    "\n",
    "This notebook walks through building a **next-24h renewable generation forecast system** with:\n",
    "\n",
    "- **EIA data integration** - Hourly wind/solar generation for US regions\n",
    "- **Weather features** - Open-Meteo integration (wind speed, solar radiation)\n",
    "- **Probabilistic forecasting** - Dual prediction intervals (80%, 95%)\n",
    "- **Drift monitoring** - Automatic detection of model degradation\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "```\n",
    "┌─────────────┐      ┌──────────────┐      ┌─────────────┐\n",
    "│  EIA API    │──┬──▶│ Data         │──┬──▶│ StatsForecast│\n",
    "│ (WND/SUN)   │  │   │ Pipeline     │  │   │ Models       │\n",
    "└─────────────┘  │   └──────────────┘  │   └─────────────┘\n",
    "                 │                     │           │\n",
    "┌─────────────┐  │   ┌──────────────┐  │   ┌─────▼──────┐\n",
    "│ Open-Meteo  │──┘   │ Validation   │  │   │Probabilistic│\n",
    "│ Weather API │      │ & Quality    │  │   │Forecasts    │\n",
    "└─────────────┘      │ Gates        │  │   │(80%, 95% CI)│\n",
    "                     └──────────────┘  │   └────────────┘\n",
    "                                       │           │\n",
    "                                       │   ┌───────▼─────┐\n",
    "                                       └──▶│  Artifacts  │\n",
    "                                           │  Commit &   │\n",
    "                                           │  Dashboard  │\n",
    "                                           └─────────────┘\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **StatsForecast format**: `[unique_id, ds, y]` - where `unique_id` = `{region}_{fuel_type}`\n",
    "2. **Zero-value handling**: Solar generates 0 at night - we use RMSE/MAE, NOT MAPE\n",
    "3. **Leakage prevention**: Use **forecasted** weather for predictions, not historical\n",
    "4. **Drift detection**: Threshold = mean + 2*std from backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's ensure we have the project root in our path and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to project root: c:\\docker_projects\\atsaf we are currently at c:\\docker_projects\\atsaf\n",
      "Project root: c:\\docker_projects\\atsaf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\docker_projects\\atsaf\"\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "if os.getcwd() != str(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to project root: {project_root} we are currently at {os.getcwd()}\")\n",
    "\n",
    "# Configure logging for visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 1: Region Definitions\n",
    "\n",
    "**File:** `src/renewable/regions.py`\n",
    "\n",
    "This module maps **EIA balancing authority regions** to their geographic coordinates. Why do we need coordinates?\n",
    "\n",
    "- **Weather API lookup**: Open-Meteo requires latitude/longitude\n",
    "- **Regional analysis**: Compare forecast accuracy across regions\n",
    "- **Timezone handling**: Each region has a primary timezone\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "1. **NamedTuple for RegionInfo**: Immutable, type-safe, and memory-efficient\n",
    "2. **Centroid coordinates**: Approximate centers - good enough for hourly weather\n",
    "3. **Fuel type codes**: `WND` (wind), `SUN` (solar) - match EIA's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/regions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/regions.py\n",
    "# src/renewable/regions.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "class RegionInfo(NamedTuple):\n",
    "    \"\"\"Region metadata for EIA and weather lookups.\"\"\"\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    timezone: str\n",
    "    # Some internal regions may not map cleanly to an EIA respondent.\n",
    "    # We keep them in REGIONS for weather/features, but EIA fetch requires this.\n",
    "    eia_respondent: Optional[str] = None\n",
    "\n",
    "\n",
    "REGIONS: dict[str, RegionInfo] = {\n",
    "    # Western Interconnection\n",
    "    \"CALI\": RegionInfo(\n",
    "        name=\"California ISO\",\n",
    "        lat=36.7,\n",
    "        lon=-119.4,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=\"CISO\",\n",
    "    ),\n",
    "    \"NW\": RegionInfo(\n",
    "        name=\"Northwest\",\n",
    "        lat=45.5,\n",
    "        lon=-122.0,\n",
    "        timezone=\"America/Los_Angeles\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "    \"SW\": RegionInfo(\n",
    "        name=\"Southwest\",\n",
    "        lat=33.5,\n",
    "        lon=-112.0,\n",
    "        timezone=\"America/Phoenix\",\n",
    "        eia_respondent=None,  # intentionally unset until verified\n",
    "    ),\n",
    "\n",
    "    # Texas Interconnection\n",
    "    \"ERCO\": RegionInfo(\n",
    "        name=\"ERCOT (Texas)\",\n",
    "        lat=31.0,\n",
    "        lon=-100.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"ERCO\",\n",
    "    ),\n",
    "\n",
    "    # Midwest\n",
    "    \"MISO\": RegionInfo(\n",
    "        name=\"Midcontinent ISO\",\n",
    "        lat=41.0,\n",
    "        lon=-93.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"MISO\",\n",
    "    ),\n",
    "    \"PJM\": RegionInfo(\n",
    "        name=\"PJM Interconnection\",\n",
    "        lat=39.0,\n",
    "        lon=-77.0,\n",
    "        timezone=\"America/New_York\",\n",
    "        eia_respondent=\"PJM\",\n",
    "    ),\n",
    "    \"SWPP\": RegionInfo(\n",
    "        name=\"Southwest Power Pool\",\n",
    "        lat=37.0,\n",
    "        lon=-97.0,\n",
    "        timezone=\"America/Chicago\",\n",
    "        eia_respondent=\"SWPP\",\n",
    "    ),\n",
    "\n",
    "    # Internal/aggregate regions kept for non-EIA use (weather/features/etc.)\n",
    "    \"SE\": RegionInfo(name=\"Southeast\", lat=33.0, lon=-84.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"FLA\": RegionInfo(name=\"Florida\", lat=28.0, lon=-82.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"CAR\": RegionInfo(name=\"Carolinas\", lat=35.5, lon=-80.0, timezone=\"America/New_York\", eia_respondent=None),\n",
    "    \"TEN\": RegionInfo(name=\"Tennessee Valley\", lat=35.5, lon=-86.0, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "\n",
    "    \"US48\": RegionInfo(name=\"Lower 48 States\", lat=39.8, lon=-98.5, timezone=\"America/Chicago\", eia_respondent=None),\n",
    "}\n",
    "\n",
    "FUEL_TYPES = {\"WND\": \"Wind\", \"SUN\": \"Solar\"}\n",
    "\n",
    "\n",
    "def list_regions() -> list[str]:\n",
    "    return sorted(REGIONS.keys())\n",
    "\n",
    "\n",
    "def get_region_info(region_code: str) -> RegionInfo:\n",
    "    return REGIONS[region_code]\n",
    "\n",
    "\n",
    "def get_region_coords(region_code: str) -> tuple[float, float]:\n",
    "    r = REGIONS[region_code]\n",
    "    return (r.lat, r.lon)\n",
    "\n",
    "\n",
    "def get_eia_respondent(region_code: str) -> str:\n",
    "    \"\"\"Return the code EIA expects for facets[respondent][]. Fail loudly if missing.\"\"\"\n",
    "    info = REGIONS[region_code]\n",
    "    if not info.eia_respondent:\n",
    "        raise ValueError(\n",
    "            f\"Region '{region_code}' has no configured eia_respondent. \"\n",
    "            f\"Set REGIONS['{region_code}'].eia_respondent to a verified EIA respondent code \"\n",
    "            f\"before using it for EIA fetches.\"\n",
    "        )\n",
    "    return info.eia_respondent\n",
    "\n",
    "\n",
    "def validate_region(region_code: str) -> bool:\n",
    "    return region_code in REGIONS\n",
    "\n",
    "\n",
    "def validate_fuel_type(fuel_type: str) -> bool:\n",
    "    return fuel_type in FUEL_TYPES\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run - test region functions\n",
    "\n",
    "    print(\"=== Available Regions ===\")\n",
    "    print(f\"Total regions: {len(REGIONS)}\")\n",
    "    print(f\"Region codes: {list_regions()}\")\n",
    "\n",
    "    print(\"\\n=== Example: California ===\")\n",
    "    cali_info = get_region_info(\"CALI\")\n",
    "    print(f\"Name: {cali_info.name}\")\n",
    "    print(f\"Coordinates: ({cali_info.lat}, {cali_info.lon})\")\n",
    "    print(f\"Timezone: {cali_info.timezone}\")\n",
    "\n",
    "    print(\"\\n=== Weather API Coordinates ===\")\n",
    "    for region in [\"CALI\", \"ERCO\", \"MISO\"]:\n",
    "        lat, lon = get_region_coords(region)\n",
    "        print(f\"{region}: lat={lat}, lon={lon}\")\n",
    "\n",
    "    print(\"\\n=== Fuel Types ===\")\n",
    "    for code, name in FUEL_TYPES.items():\n",
    "        print(f\"{code}: {name}\")\n",
    "\n",
    "    print(\"\\n=== Validation ===\")\n",
    "    print(f\"validate_region('CALI'): {validate_region('CALI')}\")\n",
    "    print(f\"validate_region('INVALID'): {validate_region('INVALID')}\")\n",
    "    print(f\"validate_fuel_type('WND'): {validate_fuel_type('WND')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Region Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2: EIA Data Fetcher\n",
    "\n",
    "**File:** `src/renewable/eia_renewable.py`\n",
    "\n",
    "This module fetches **hourly wind and solar generation** from the EIA API.\n",
    "\n",
    "## Critical Concepts\n",
    "\n",
    "### StatsForecast Format\n",
    "StatsForecast expects data in a specific format:\n",
    "```\n",
    "unique_id | ds                  | y\n",
    "----------|---------------------|--------\n",
    "CALI_WND  | 2024-01-01 00:00:00 | 1234.5\n",
    "CALI_WND  | 2024-01-01 01:00:00 | 1456.7\n",
    "ERCO_WND  | 2024-01-01 00:00:00 | 2345.6\n",
    "```\n",
    "\n",
    "- `unique_id`: Identifies the time series (e.g., \"CALI_WND\" = California Wind)\n",
    "- `ds`: Datetime column (timezone-naive UTC)\n",
    "- `y`: Target value (generation in MWh)\n",
    "\n",
    "### API Rate Limiting\n",
    "- EIA API has rate limits (~5 requests/second)\n",
    "- We use controlled parallelism with delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/eia_renewable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/eia_renewable.py\n",
    "# src/renewable/eia_renewable.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode\n",
    "\n",
    "from src.renewable.regions import REGIONS, get_eia_respondent, validate_fuel_type, validate_region\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _sanitize_url(url: str) -> str:\n",
    "    parts = urlsplit(url)\n",
    "    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k.lower() != \"api_key\"]\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, urlencode(q), parts.fragment))\n",
    "\n",
    "\n",
    "def _load_env_once(*, debug: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load .env if present.\n",
    "    - Primary: find_dotenv(usecwd=True) (walk up from CWD)\n",
    "    - Fallback: repo_root/.env based on this file location\n",
    "    Returns the path loaded (or None).\n",
    "    \"\"\"\n",
    "    # 1) Try from current working directory upward\n",
    "    dotenv_path = find_dotenv(usecwd=True)\n",
    "    if dotenv_path:\n",
    "        load_dotenv(dotenv_path, override=False)\n",
    "        if debug:\n",
    "            logger.info(\"Loaded .env via find_dotenv: %s\", dotenv_path)\n",
    "        return dotenv_path\n",
    "\n",
    "    # 2) Fallback: assume src-layout -> repo root is ../../ from this file\n",
    "    try:\n",
    "        repo_root = Path(__file__).resolve().parents[2]\n",
    "        fallback = repo_root / \".env\"\n",
    "        if fallback.exists():\n",
    "            load_dotenv(fallback, override=False)\n",
    "            if debug:\n",
    "                logger.info(\"Loaded .env via fallback: %s\", str(fallback))\n",
    "            return str(fallback)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"No .env found to load.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "class EIARenewableFetcher:\n",
    "    BASE_URL = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "    MAX_RECORDS_PER_REQUEST = 5000\n",
    "    RATE_LIMIT_DELAY = 0.2  # 5 requests/second max\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, *, debug_env: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize API key. Pulls from:\n",
    "        1) explicit api_key argument\n",
    "        2) environment variable EIA_API_KEY (optionally loaded from .env)\n",
    "        \"\"\"\n",
    "        loaded_env = _load_env_once(debug=debug_env)\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"EIA_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"EIA API key required but not found.\\n\"\n",
    "                \"- Ensure .env contains EIA_API_KEY=...\\n\"\n",
    "                \"- Ensure your process CWD is under the repo (so find_dotenv can locate it), OR\\n\"\n",
    "                \"- Pass api_key=... explicitly.\\n\"\n",
    "                f\"Loaded .env path: {loaded_env}\"\n",
    "            )\n",
    "\n",
    "        # Debug without leaking the key\n",
    "        if debug_env:\n",
    "            masked = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            logger.info(\"EIA_API_KEY loaded (masked): %s\", masked)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_eia_response(payload: dict, *, request_url: Optional[str] = None) -> tuple[list[dict], dict]:\n",
    "        if not isinstance(payload, dict):\n",
    "            raise TypeError(f\"EIA payload is not a dict. type={type(payload)} url={request_url}\")\n",
    "\n",
    "        if \"error\" in payload and payload.get(\"response\") is None:\n",
    "            raise ValueError(f\"EIA returned error payload. url={request_url} error={payload.get('error')}\")\n",
    "\n",
    "        if \"response\" not in payload:\n",
    "            raise ValueError(\n",
    "                f\"EIA payload missing 'response'. url={request_url} keys={list(payload.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        response = payload.get(\"response\") or {}\n",
    "        if not isinstance(response, dict):\n",
    "            raise TypeError(f\"EIA payload['response'] is not a dict. type={type(response)} url={request_url}\")\n",
    "\n",
    "        if \"data\" not in response:\n",
    "            raise ValueError(\n",
    "                f\"EIA response missing 'data'. url={request_url} response_keys={list(response.keys())[:25]}\"\n",
    "            )\n",
    "\n",
    "        records = response.get(\"data\") or []\n",
    "        if not isinstance(records, list):\n",
    "            raise TypeError(f\"EIA response['data'] is not a list. type={type(records)} url={request_url}\")\n",
    "\n",
    "        total = response.get(\"total\", None)\n",
    "        offset = response.get(\"offset\", None)\n",
    "\n",
    "        meta_obj = response.get(\"metadata\") or {}\n",
    "        if isinstance(meta_obj, dict):\n",
    "            if total is None and \"total\" in meta_obj:\n",
    "                total = meta_obj.get(\"total\")\n",
    "            if offset is None and \"offset\" in meta_obj:\n",
    "                offset = meta_obj.get(\"offset\")\n",
    "\n",
    "        try:\n",
    "            total = int(total) if total is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            offset = int(offset) if offset is not None else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return records, {\"total\": total, \"offset\": offset}\n",
    "\n",
    "    def fetch_region(\n",
    "        self,\n",
    "        region: str,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "        diag: Optional[dict] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region):\n",
    "            raise ValueError(f\"Invalid region: {region}\")\n",
    "        if not validate_fuel_type(fuel_type):\n",
    "            raise ValueError(f\"Invalid fuel type: {fuel_type}\")\n",
    "\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        all_records: list[dict] = []\n",
    "        offset = 0\n",
    "\n",
    "        # ✅ FIX: initialize loop diagnostics counters\n",
    "        page_count = 0\n",
    "        total_hint: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"api_key\": self.api_key,\n",
    "                \"data[]\": \"value\",\n",
    "                \"facets[respondent][]\": respondent,\n",
    "                \"facets[fueltype][]\": fuel_type,\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"start\": f\"{start_date}T00\",\n",
    "                \"end\": f\"{end_date}T23\",\n",
    "                \"length\": self.MAX_RECORDS_PER_REQUEST,\n",
    "                \"offset\": offset,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"asc\",\n",
    "            }\n",
    "\n",
    "            resp = requests.get(self.BASE_URL, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "\n",
    "            records, meta = self._extract_eia_response(payload, request_url=resp.url)\n",
    "\n",
    "            page_count += 1\n",
    "            if total_hint is None:\n",
    "                total_hint = meta.get(\"total\")\n",
    "\n",
    "            returned = len(records)\n",
    "\n",
    "            if debug:\n",
    "                safe_url = _sanitize_url(resp.url)\n",
    "                print(\n",
    "                    f\"[PAGE] region={region} fuel={fuel_type} returned={returned} \"\n",
    "                    f\"offset={offset} total={meta.get('total')} url={safe_url}\"\n",
    "                )\n",
    "\n",
    "            # Empty on first page: legitimate empty series for that window\n",
    "            if returned == 0 and offset == 0:\n",
    "                if diag is not None:\n",
    "                    diag.update({\n",
    "                        \"region\": region,\n",
    "                        \"fuel_type\": fuel_type,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date,\n",
    "                        \"total_records\": total_hint,\n",
    "                        \"pages\": page_count,\n",
    "                        \"rows_parsed\": 0,\n",
    "                        \"empty\": True,\n",
    "                    })\n",
    "                return pd.DataFrame(columns=[\"ds\", \"value\", \"region\", \"fuel_type\"])\n",
    "\n",
    "            if returned == 0:\n",
    "                break\n",
    "\n",
    "            all_records.extend(records)\n",
    "\n",
    "            if returned < self.MAX_RECORDS_PER_REQUEST:\n",
    "                break\n",
    "\n",
    "            offset += self.MAX_RECORDS_PER_REQUEST\n",
    "            time.sleep(self.RATE_LIMIT_DELAY)\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        missing_cols = [c for c in [\"period\", \"value\"] if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            sample_keys = sorted(set().union(*(r.keys() for r in all_records[:5]))) if all_records else []\n",
    "            raise ValueError(\n",
    "                f\"EIA records missing expected keys {missing_cols}. \"\n",
    "                f\"columns={df.columns.tolist()} sample_record_keys={sample_keys}\"\n",
    "            )\n",
    "\n",
    "        df[\"ds\"] = pd.to_datetime(df[\"period\"], utc=True, errors=\"coerce\").dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"region\"] = region\n",
    "        df[\"fuel_type\"] = fuel_type\n",
    "\n",
    "        df = df.dropna(subset=[\"ds\", \"value\"]).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        # DEBUG: Log negative values for investigation\n",
    "        neg_mask = df[\"value\"] < 0\n",
    "        if neg_mask.any():\n",
    "            neg_count = int(neg_mask.sum())\n",
    "            neg_min = float(df.loc[neg_mask, \"value\"].min())\n",
    "            neg_max = float(df.loc[neg_mask, \"value\"].max())\n",
    "            neg_sample = df.loc[neg_mask, [\"ds\", \"value\"]].head(10)\n",
    "            logger.warning(\n",
    "                \"[fetch_region][NEGATIVE] region=%s fuel=%s count=%d min=%.2f max=%.2f\",\n",
    "                region, fuel_type, neg_count, neg_min, neg_max,\n",
    "            )\n",
    "            for _, row in neg_sample.iterrows():\n",
    "                logger.warning(\"  ds=%s value=%.2f\", row[\"ds\"], row[\"value\"])\n",
    "\n",
    "            # Clamp negative values to zero (preserves hourly grid for modeling)\n",
    "            # Note: Removing rows would create gaps that cause series to be dropped\n",
    "            logger.warning(\n",
    "                \"[fetch_region][CLAMP] Clamping %d negative values to 0 for %s_%s (%.1f%%)\",\n",
    "                neg_count, region, fuel_type, 100 * neg_count / max(len(df), 1),\n",
    "            )\n",
    "            df[\"value\"] = df[\"value\"].clip(lower=0)\n",
    "\n",
    "        if diag is not None:\n",
    "            diag.update({\n",
    "                \"region\": region,\n",
    "                \"fuel_type\": fuel_type,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"total_records\": total_hint,\n",
    "                \"pages\": page_count,\n",
    "                \"rows_parsed\": int(len(df)),\n",
    "                \"empty\": bool(len(df) == 0),\n",
    "            })\n",
    "\n",
    "        return df[[\"ds\", \"value\", \"region\", \"fuel_type\"]]\n",
    "\n",
    "\n",
    "    def fetch_all_regions(\n",
    "        self,\n",
    "        fuel_type: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        regions: Optional[list[str]] = None,\n",
    "        max_workers: int = 3,\n",
    "        diagnostics: Optional[list[dict]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if regions is None:\n",
    "            regions = [r for r in REGIONS.keys() if r != \"US48\"]\n",
    "\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "\n",
    "        def _run_one(region: str) -> tuple[str, pd.DataFrame, dict]:\n",
    "            d: dict = {}\n",
    "            df = self.fetch_region(region, fuel_type, start_date, end_date, diag=d)\n",
    "            return region, df, d\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(_run_one, region): region for region in regions}\n",
    "            for future in as_completed(futures):\n",
    "                region = futures[future]\n",
    "                try:\n",
    "                    _, df, d = future.result()\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append(d)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                        print(f\"[OK] {region}: {len(df)} rows\")\n",
    "                    else:\n",
    "                        print(f\"[EMPTY] {region}: 0 rows\")\n",
    "                except Exception as e:\n",
    "                    if diagnostics is not None:\n",
    "                        diagnostics.append({\n",
    "                            \"region\": region,\n",
    "                            \"fuel_type\": fuel_type,\n",
    "                            \"start_date\": start_date,\n",
    "                            \"end_date\": end_date,\n",
    "                            \"error\": str(e),\n",
    "                        })\n",
    "                    print(f\"[FAIL] {region}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame(columns=[\"unique_id\", \"ds\", \"y\"])\n",
    "\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined[\"unique_id\"] = combined[\"region\"] + \"_\" + combined[\"fuel_type\"]\n",
    "        combined = combined.rename(columns={\"value\": \"y\"})\n",
    "        return combined[[\"unique_id\", \"ds\", \"y\"]].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    def get_series_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.groupby(\"unique_id\").agg(\n",
    "            count=(\"y\", \"count\"),\n",
    "            min_value=(\"y\", \"min\"),\n",
    "            max_value=(\"y\", \"max\"),\n",
    "            mean_value=(\"y\", \"mean\"),\n",
    "            zero_count=(\"y\", lambda x: (x == 0).sum()),\n",
    "        ).reset_index()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "\n",
    "    print(\"=== Testing Single Region Fetch ===\")\n",
    "    df_single = fetcher.fetch_region(\"CALI\", \"WND\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Single region: {len(df_single)} rows\")\n",
    "    print(df_single.head())\n",
    "\n",
    "    print(\"\\n=== Testing Multi-Region Fetch ===\")\n",
    "    df_multi = fetcher.fetch_all_regions(\"WND\", \"2024-12-01\", \"2024-12-03\", regions=[\"CALI\", \"ERCO\", \"MISO\"])\n",
    "    print(f\"\\nMulti-region: {len(df_multi)} rows\")\n",
    "    print(f\"Series: {df_multi['unique_id'].unique().tolist()}\")\n",
    "\n",
    "    print(\"\\n=== Series Summary ===\")\n",
    "    print(fetcher.get_series_summary(df_multi))\n",
    "\n",
    "    # sun checks:\n",
    "    f = EIARenewableFetcher()\n",
    "    df = f.fetch_region(\"CALI\", \"SUN\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(df.head(), len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 3: Weather Integration\n",
    "\n",
    "**File:** `src/renewable/open_meteo.py`\n",
    "\n",
    "Weather is **critical** for renewable forecasting:\n",
    "- **Wind generation** depends on wind speed (especially at hub height ~100m)\n",
    "- **Solar generation** depends on radiation and cloud cover\n",
    "\n",
    "## Key Concept: Preventing Leakage\n",
    "\n",
    "**Data leakage** occurs when training uses information that wouldn't be available at prediction time.\n",
    "\n",
    "```\n",
    "❌ WRONG: Using historical weather to predict future generation\n",
    "   - At prediction time, we don't have future actual weather!\n",
    "   \n",
    "✅ CORRECT: Use forecasted weather for predictions\n",
    "   - Training: historical weather aligned with historical generation\n",
    "   - Prediction: weather forecast for the prediction horizon\n",
    "```\n",
    "\n",
    "## Open-Meteo API\n",
    "\n",
    "Open-Meteo is **free** and requires no API key:\n",
    "- Historical API: Past weather data\n",
    "- Forecast API: Up to 16 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/open_meteo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/open_meteo.py\n",
    "# src/renewable/open_meteo.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from src.renewable.regions import get_region_coords, validate_region\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OpenMeteoEndpoints:\n",
    "    historical_url: str = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    forecast_url: str = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "\n",
    "class OpenMeteoRenewable:\n",
    "    \"\"\"\n",
    "    Fetch weather features for renewable energy forecasting.\n",
    "\n",
    "    Strict-by-default:\n",
    "    - If Open-Meteo doesn't return a requested variable, we raise.\n",
    "    - We do NOT fabricate values or silently \"fill\" missing columns.\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_VARS = [\n",
    "        \"temperature_2m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"direct_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, timeout: int = 60, *, strict: bool = True):\n",
    "        self.timeout = timeout\n",
    "        self.strict = strict\n",
    "        self.endpoints = OpenMeteoEndpoints()\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.0,  # 1s, 2s, 4s between retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=frozenset([\"GET\"]),\n",
    "            connect=3,  # Retry on connection errors\n",
    "            read=3,     # Retry on read timeouts\n",
    "        )\n",
    "        session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        return session\n",
    "\n",
    "    def fetch_historical(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.historical_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][HIST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            # Log actual response content for debugging\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][HIST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        return self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "    def fetch_forecast(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if variables is None:\n",
    "            variables = self.WEATHER_VARS\n",
    "\n",
    "        forecast_days = min((horizon_hours // 24) + 1, 16)\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"hourly\": \",\".join(variables),\n",
    "            \"timezone\": \"UTC\",\n",
    "            \"forecast_days\": forecast_days,\n",
    "        }\n",
    "\n",
    "        resp = self.session.get(self.endpoints.forecast_url, params=params, timeout=self.timeout)\n",
    "        if debug:\n",
    "            print(f\"[OPENMETEO][FCST] status={resp.status_code} url={resp.url}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.JSONDecodeError as e:\n",
    "            content_preview = resp.text[:500] if resp.text else \"(empty)\"\n",
    "            raise ValueError(\n",
    "                f\"[OPENMETEO][FCST] Invalid JSON response. \"\n",
    "                f\"status={resp.status_code} content_preview={content_preview}\"\n",
    "            ) from e\n",
    "\n",
    "        df = self._parse_response(data, variables, debug=debug, request_url=resp.url)\n",
    "\n",
    "        # Trim to requested horizon (ds is naive UTC)\n",
    "        if len(df) > 0:\n",
    "            cutoff = datetime.utcnow() + timedelta(hours=horizon_hours)\n",
    "            df = df[df[\"ds\"] <= cutoff].reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_historical(lat, lon, start_date, end_date, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "    def fetch_all_regions_historical(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region(region, start_date, end_date, debug=debug)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def _parse_response(\n",
    "        self,\n",
    "        data: dict,\n",
    "        variables: list[str],\n",
    "        *,\n",
    "        debug: bool,\n",
    "        request_url: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        hourly = data.get(\"hourly\")\n",
    "        if not isinstance(hourly, dict):\n",
    "            raise ValueError(f\"Open-Meteo response missing/invalid 'hourly'. url={request_url}\")\n",
    "\n",
    "        times = hourly.get(\"time\")\n",
    "        if not isinstance(times, list) or len(times) == 0:\n",
    "            raise ValueError(f\"Open-Meteo response has no hourly time grid. url={request_url}\")\n",
    "\n",
    "        # Build ds (naive UTC)\n",
    "        ds = pd.to_datetime(times, errors=\"coerce\", utc=True).tz_localize(None)\n",
    "        if ds.isna().any():\n",
    "            bad = int(ds.isna().sum())\n",
    "            raise ValueError(f\"Open-Meteo returned unparsable times. bad={bad} url={request_url}\")\n",
    "\n",
    "        df_data = {\"ds\": ds}\n",
    "\n",
    "        # Strict variable presence: raise if missing (no silent None padding)\n",
    "        missing_vars = [v for v in variables if v not in hourly]\n",
    "        if missing_vars and self.strict:\n",
    "            raise ValueError(f\"Open-Meteo missing requested vars={missing_vars}. url={request_url}\")\n",
    "\n",
    "        for var in variables:\n",
    "            values = hourly.get(var)\n",
    "            if values is None:\n",
    "                # If not strict, keep as all-NA but be explicit (not hidden)\n",
    "                df_data[var] = [None] * len(ds)\n",
    "                continue\n",
    "\n",
    "            if not isinstance(values, list):\n",
    "                raise ValueError(f\"Open-Meteo var '{var}' not a list. type={type(values)} url={request_url}\")\n",
    "\n",
    "            if len(values) != len(ds):\n",
    "                raise ValueError(\n",
    "                    f\"Open-Meteo length mismatch for '{var}': \"\n",
    "                    f\"len(values)={len(values)} len(time)={len(ds)} url={request_url}\"\n",
    "                )\n",
    "\n",
    "            df_data[var] = pd.to_numeric(values, errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame(df_data).sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "        if debug:\n",
    "            dup = int(df[\"ds\"].duplicated().sum())\n",
    "            na_counts = {v: int(df[v].isna().sum()) for v in variables if v in df.columns}\n",
    "            print(f\"[OPENMETEO][PARSE] rows={len(df)} dup_ds={dup} na_counts(sample)={dict(list(na_counts.items())[:3])}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fetch_for_region_forecast(\n",
    "        self,\n",
    "        region_code: str,\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not validate_region(region_code):\n",
    "            raise ValueError(f\"Invalid region_code: {region_code}\")\n",
    "\n",
    "        lat, lon = get_region_coords(region_code)\n",
    "        df = self.fetch_forecast(lat, lon, horizon_hours=horizon_hours, variables=variables, debug=debug)\n",
    "        df[\"region\"] = region_code\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fetch_all_regions_forecast(\n",
    "        self,\n",
    "        regions: list[str],\n",
    "        horizon_hours: int = 48,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        *,\n",
    "        debug: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        all_dfs: list[pd.DataFrame] = []\n",
    "        for region in regions:\n",
    "            try:\n",
    "                df = self.fetch_for_region_forecast(\n",
    "                    region, horizon_hours=horizon_hours, variables=variables, debug=debug\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "                print(f\"[OK] Forecast weather for {region}: {len(df)} rows\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: TIMEOUT after {self.timeout}s - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: CONNECTION_ERROR - {type(e).__name__}: {e}\")\n",
    "            except requests.exceptions.JSONDecodeError as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: JSON_PARSE_ERROR - {type(e).__name__}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] Forecast weather for {region}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        if not all_dfs:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return (\n",
    "            pd.concat(all_dfs, ignore_index=True)\n",
    "            .sort_values([\"region\", \"ds\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Real API smoke test (no key needed)\n",
    "    weather = OpenMeteoRenewable(strict=True)\n",
    "\n",
    "    print(\"=== Testing Historical Weather (REAL API) ===\")\n",
    "    hist_df = weather.fetch_for_region(\"CALI\", \"2024-12-01\", \"2024-12-03\", debug=True)\n",
    "    print(f\"Historical rows: {len(hist_df)}\")\n",
    "    print(hist_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 4: Probabilistic Modeling\n",
    "\n",
    "**File:** `src/renewable/modeling.py`\n",
    "\n",
    "This is where the forecasting happens! We use **StatsForecast** for:\n",
    "\n",
    "1. **Multi-series forecasting**: Handle multiple regions/fuel types in one model\n",
    "2. **Probabilistic predictions**: Get prediction intervals, not just point forecasts\n",
    "3. **Weather exogenous**: Include weather features as predictors\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Prediction Intervals?\n",
    "\n",
    "Point forecasts are useful, but energy traders need **uncertainty quantification**:\n",
    "- **80% interval**: \"I'm 80% confident generation will be between X and Y\"\n",
    "- **95% interval**: Wider, for risk management\n",
    "\n",
    "### Zero-Value Safety (CRITICAL)\n",
    "\n",
    "**Solar panels generate ZERO at night!** This breaks MAPE:\n",
    "\n",
    "```\n",
    "MAPE = mean(|actual - predicted| / actual)\n",
    "\n",
    "When actual = 0:\n",
    "MAPE = |0 - pred| / 0 = undefined (division by zero!)\n",
    "```\n",
    "\n",
    "**Solution**: Always use RMSE and MAE for renewable forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/validation.py\n",
    "# file: src/renewable/validation.py\n",
    "\"\"\"Validation utilities for renewable generation data.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ValidationReport:\n",
    "    ok: bool\n",
    "    message: str\n",
    "    details: dict\n",
    "\n",
    "\n",
    "def validate_generation_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lag_hours: int = 3,\n",
    "    max_missing_ratio: float = 0.02,\n",
    "    expected_series: Optional[Iterable[str]] = None,\n",
    ") -> ValidationReport:\n",
    "    required = {\"unique_id\", \"ds\", \"y\"}\n",
    "    missing_cols = required - set(df.columns)\n",
    "    if missing_cols:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Missing required columns\",\n",
    "            {\"missing_cols\": sorted(missing_cols)},\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        return ValidationReport(False, \"Generation data is empty\", {})\n",
    "\n",
    "    work = df.copy()\n",
    "\n",
    "    work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"coerce\", utc=True)\n",
    "    if work[\"ds\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable ds values found\",\n",
    "            {\"bad_ds\": int(work[\"ds\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    work[\"y\"] = pd.to_numeric(work[\"y\"], errors=\"coerce\")\n",
    "    if work[\"y\"].isna().any():\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Unparseable y values found\",\n",
    "            {\"bad_y\": int(work[\"y\"].isna().sum())},\n",
    "        )\n",
    "\n",
    "    if (work[\"y\"] < 0).any():\n",
    "        neg_mask = work[\"y\"] < 0\n",
    "        by_series = (\n",
    "            work[neg_mask]\n",
    "            .groupby(\"unique_id\")\n",
    "            .agg(count=(\"y\", \"count\"), min_y=(\"y\", \"min\"), max_y=(\"y\", \"max\"))\n",
    "            .reset_index()\n",
    "            .to_dict(orient=\"records\")\n",
    "        )\n",
    "        sample = (\n",
    "            work.loc[neg_mask, [\"unique_id\", \"ds\", \"y\"]]\n",
    "            .head(10)\n",
    "            .to_dict(orient=\"records\")\n",
    "        )\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Negative generation values found\",\n",
    "            {\"neg_y\": int(neg_mask.sum()), \"by_series\": by_series, \"sample\": sample},\n",
    "        )\n",
    "\n",
    "    dup = work.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Duplicate (unique_id, ds) rows found\",\n",
    "            {\"duplicates\": int(dup)},\n",
    "        )\n",
    "\n",
    "    if expected_series:\n",
    "        expected = sorted(set(expected_series))\n",
    "        present = sorted(set(work[\"unique_id\"]))\n",
    "        missing_series = sorted(set(expected) - set(present))\n",
    "        if missing_series:\n",
    "            return ValidationReport(\n",
    "                False,\n",
    "                \"Missing expected series\",\n",
    "                {\"missing_series\": missing_series, \"present_series\": present},\n",
    "            )\n",
    "\n",
    "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "    max_ds = work[\"ds\"].max()\n",
    "    lag_hours = (now_utc - max_ds).total_seconds() / 3600.0\n",
    "    if lag_hours > max_lag_hours:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Data not fresh enough\",\n",
    "            {\n",
    "                \"now_utc\": now_utc.isoformat(),\n",
    "                \"max_ds\": max_ds.isoformat(),\n",
    "                \"lag_hours\": lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    series_max = work.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    series_lag = (now_utc - series_max).dt.total_seconds() / 3600.0\n",
    "    stale = series_lag[series_lag > max_lag_hours].sort_values(ascending=False)\n",
    "    if not stale.empty:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Stale series found\",\n",
    "            {\n",
    "                \"stale_series\": stale.head(10).to_dict(),\n",
    "                \"max_lag_hours\": max_lag_hours,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    missing_ratios = {}\n",
    "    for uid, group in work.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")\n",
    "        start = group[\"ds\"].iloc[0]\n",
    "        end = group[\"ds\"].iloc[-1]\n",
    "        expected = int(((end - start) / pd.Timedelta(hours=1)) + 1)\n",
    "        actual = len(group)\n",
    "        missing = max(expected - actual, 0)\n",
    "        missing_ratios[uid] = missing / max(expected, 1)\n",
    "\n",
    "    worst_uid = max(missing_ratios, key=missing_ratios.get)\n",
    "    worst_ratio = missing_ratios[worst_uid]\n",
    "    if worst_ratio > max_missing_ratio:\n",
    "        return ValidationReport(\n",
    "            False,\n",
    "            \"Too many missing hourly points\",\n",
    "            {\"worst_uid\": worst_uid, \"worst_missing_ratio\": worst_ratio},\n",
    "        )\n",
    "\n",
    "    return ValidationReport(\n",
    "        True,\n",
    "        \"OK\",\n",
    "        {\n",
    "            \"row_count\": len(work),\n",
    "            \"series_count\": int(work[\"unique_id\"].nunique()),\n",
    "            \"max_ds\": max_ds.isoformat(),\n",
    "            \"lag_hours\": lag_hours,\n",
    "            \"worst_missing_ratio\": worst_ratio,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/modeling.py\n",
    "# file: src/renewable/modeling.py\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "\n",
    "WEATHER_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"direct_radiation\",\n",
    "    \"diffuse_radiation\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "\n",
    "def _log_series_summary(df: pd.DataFrame, *, value_col: str = \"y\", label: str = \"series\") -> None:\n",
    "    if df.empty:\n",
    "        print(f\"[{label}] EMPTY\")\n",
    "        return\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ds\"] = pd.to_datetime(tmp[\"ds\"], errors=\"coerce\")\n",
    "\n",
    "    def _mode_delta_hours(g: pd.Series) -> float:\n",
    "        d = g.sort_values().diff().dropna()\n",
    "        if d.empty:\n",
    "            return float(\"nan\")\n",
    "        return float(d.dt.total_seconds().div(3600).mode().iloc[0])\n",
    "\n",
    "    g = tmp.groupby(\"unique_id\").agg(\n",
    "        rows=(value_col, \"count\"),\n",
    "        na_y=(value_col, lambda s: int(s.isna().sum())),\n",
    "        min_ds=(\"ds\", \"min\"),\n",
    "        max_ds=(\"ds\", \"max\"),\n",
    "        min_y=(value_col, \"min\"),\n",
    "        max_y=(value_col, \"max\"),\n",
    "        mean_y=(value_col, \"mean\"),\n",
    "        zero_y=(value_col, lambda s: int((s == 0).sum())),\n",
    "        mode_delta_hours=(\"ds\", _mode_delta_hours),\n",
    "    ).reset_index().sort_values(\"unique_id\")\n",
    "\n",
    "    print(f\"[{label}] series={g['unique_id'].nunique()} rows={len(tmp)}\")\n",
    "    print(g.head(20).to_string(index=False))\n",
    "\n",
    "def _missing_hour_blocks(ds: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp, int]]:\n",
    "    \"\"\"\n",
    "    Return contiguous blocks of missing hourly timestamps.\n",
    "    Each tuple: (block_start, block_end, n_hours)\n",
    "    \"\"\"\n",
    "    ds = pd.to_datetime(ds, errors=\"raise\").sort_values()\n",
    "    start, end = ds.iloc[0], ds.iloc[-1]\n",
    "    expected = pd.date_range(start, end, freq=\"h\")\n",
    "    missing = expected.difference(ds)\n",
    "\n",
    "    if missing.empty:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    block_start = missing[0]\n",
    "    prev = missing[0]\n",
    "    for t in missing[1:]:\n",
    "        if t - prev == pd.Timedelta(hours=1):\n",
    "            prev = t\n",
    "        else:\n",
    "            n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "            blocks.append((block_start, prev, n))\n",
    "            block_start = t\n",
    "            prev = t\n",
    "    n = int((prev - block_start).total_seconds() / 3600) + 1\n",
    "    blocks.append((block_start, prev, n))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def _hourly_grid_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\n",
    "        \"unique_id\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "        \"expected_hours\",\n",
    "        \"actual_hours\",\n",
    "        \"missing_hours\",\n",
    "        \"missing_ratio\",\n",
    "        \"n_missing_blocks\",\n",
    "        \"largest_missing_block_hours\",\n",
    "        \"first_missing_block_start\",\n",
    "        \"first_missing_block_end\",\n",
    "    ]\n",
    "\n",
    "    if df.empty:\n",
    "        # Return an empty report with a stable schema (so callers can fail-loud cleanly)\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    rows = []\n",
    "    for uid, g in df.groupby(\"unique_id\"):\n",
    "        g = g.sort_values(\"ds\")\n",
    "        start, end = g[\"ds\"].iloc[0], g[\"ds\"].iloc[-1]\n",
    "        expected = pd.date_range(start, end, freq=\"h\")\n",
    "        missing = expected.difference(g[\"ds\"])\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"unique_id\": uid,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"expected_hours\": int(len(expected)),\n",
    "                \"actual_hours\": int(len(g)),\n",
    "                \"missing_hours\": int(len(missing)),\n",
    "                \"missing_ratio\": float(len(missing) / max(len(expected), 1)),\n",
    "                \"n_missing_blocks\": int(len(blocks)),\n",
    "                \"largest_missing_block_hours\": int(max([b[2] for b in blocks], default=0)),\n",
    "                \"first_missing_block_start\": blocks[0][0] if blocks else pd.NaT,\n",
    "                \"first_missing_block_end\": blocks[0][1] if blocks else pd.NaT,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rep = pd.DataFrame(rows)\n",
    "    return rep.sort_values([\"missing_ratio\", \"missing_hours\"], ascending=False)\n",
    "\n",
    "\n",
    "def _enforce_hourly_grid(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    label: str,\n",
    "    policy: str = \"raise\",  # \"raise\" | \"drop_incomplete_series\"\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Cannot enforce hourly grid: input dataframe is empty. \"\n",
    "            \"This is upstream (fetch) failure, not a grid issue.\"\n",
    "        )\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    if rep.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] No series found to report on (rep empty). \"\n",
    "            \"This indicates upstream emptiness or missing 'unique_id' groups.\"\n",
    "        )\n",
    "\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "\n",
    "    if worst[\"missing_hours\"] == 0:\n",
    "        return df\n",
    "\n",
    "    print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "\n",
    "    if policy == \"drop_incomplete_series\":\n",
    "        bad_uids = rep.loc[rep[\"missing_hours\"] > 0, \"unique_id\"].tolist()\n",
    "        kept = df.loc[~df[\"unique_id\"].isin(bad_uids)].copy()\n",
    "        print(f\"[{label}][GRID] policy=drop_incomplete_series dropped={bad_uids} kept_series={kept['unique_id'].nunique()}\")\n",
    "        if kept.empty:\n",
    "            raise RuntimeError(f\"[{label}][GRID] all series dropped due to missing hours\")\n",
    "        return kept\n",
    "\n",
    "    worst_uid = worst[\"unique_id\"]\n",
    "    g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "    blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "    raise RuntimeError(\n",
    "        f\"[{label}][GRID] Missing hours detected (no imputation). \"\n",
    "        f\"worst_unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "        f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _validate_hourly_grid_fail_loud(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_missing_ratio: float = 0.0,\n",
    "    label: str = \"generation\",\n",
    ") -> None:\n",
    "    # Keep your original basic checks:\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"[{label}] empty dataframe\")\n",
    "\n",
    "    bad = df[\"ds\"].isna().sum()\n",
    "    if bad:\n",
    "        raise RuntimeError(f\"[{label}] ds has NaT values bad={int(bad)}\")\n",
    "\n",
    "    dup = df.duplicated(subset=[\"unique_id\", \"ds\"]).sum()\n",
    "    if dup:\n",
    "        raise RuntimeError(f\"[{label}] duplicate (unique_id, ds) rows dup={int(dup)}\")\n",
    "\n",
    "    rep = _hourly_grid_report(df)\n",
    "    worst = rep.iloc[0].to_dict()\n",
    "    if worst[\"missing_ratio\"] > max_missing_ratio:\n",
    "        print(f\"[{label}][GRID] report (top):\\n{rep.head(10).to_string(index=False)}\")\n",
    "        worst_uid = worst[\"unique_id\"]\n",
    "        g = df[df[\"unique_id\"] == worst_uid].sort_values(\"ds\")\n",
    "        blocks = _missing_hour_blocks(g[\"ds\"])\n",
    "        raise RuntimeError(\n",
    "            f\"[{label}][GRID] Missing hours detected (no imputation allowed). \"\n",
    "            f\"unique_id={worst_uid} missing_hours={worst['missing_hours']} \"\n",
    "            f\"missing_ratio={worst['missing_ratio']:.3f} blocks(sample)={blocks[:3]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = out[\"ds\"].dt.hour\n",
    "    out[\"dow\"] = out[\"ds\"].dt.dayofweek\n",
    "\n",
    "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "\n",
    "    out[\"dow_sin\"] = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"] = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "\n",
    "    return out.drop(columns=[\"hour\", \"dow\"])\n",
    "\n",
    "def _infer_model_columns(cv_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Infer StatsForecast model prediction columns from a cross_validation dataframe.\n",
    "\n",
    "    We treat as \"model columns\" those that:\n",
    "      - are not core columns (unique_id, ds, cutoff, y)\n",
    "      - are not metadata columns (index, level_0, etc.)\n",
    "      - are not interval columns like '<model>-lo-80' or '<model>-hi-95'\n",
    "    \"\"\"\n",
    "    # Core columns from StatsForecast + pandas residuals from reset_index()\n",
    "    core = {\"unique_id\", \"ds\", \"cutoff\", \"y\", \"index\", \"level_0\", \"level_1\"}\n",
    "    cols = [c for c in cv_df.columns if c not in core]\n",
    "\n",
    "    model_cols: set[str] = set()\n",
    "    interval_pat = re.compile(r\"-(lo|hi)-\\d+$\")\n",
    "    for c in cols:\n",
    "        if interval_pat.search(c):\n",
    "            continue\n",
    "        model_cols.add(c)\n",
    "\n",
    "    return sorted(model_cols)\n",
    "\n",
    "\n",
    "def compute_leaderboard(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    confidence_levels: tuple[int, int] = (80, 95),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an aggregated leaderboard from StatsForecast cross_validation output.\n",
    "\n",
    "    Returns columns:\n",
    "      - model, rmse, mae, mape, valid_rows\n",
    "      - coverage_<level> if interval columns exist\n",
    "    \"\"\"\n",
    "    required = {\"y\", \"unique_id\", \"ds\", \"cutoff\"}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"[leaderboard] cv_df missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    model_cols = _infer_model_columns(cv_df)\n",
    "    if not model_cols:\n",
    "        raise RuntimeError(\n",
    "            f\"[leaderboard] Could not infer any model prediction columns. \"\n",
    "            f\"cv_df columns={cv_df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    y_true = cv_df[\"y\"].to_numpy()\n",
    "\n",
    "    for m in model_cols:\n",
    "        if m not in cv_df.columns:\n",
    "            continue\n",
    "\n",
    "        y_pred = cv_df[m].to_numpy()\n",
    "        valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        valid_rows = int(valid_mask.sum())\n",
    "\n",
    "        metrics = {\n",
    "            \"model\": m,\n",
    "            \"rmse\": float(ForecastMetrics.rmse(y_true, y_pred)),\n",
    "            \"mae\": float(ForecastMetrics.mae(y_true, y_pred)),\n",
    "            \"mape\": float(ForecastMetrics.mape(y_true, y_pred)),\n",
    "            \"valid_rows\": valid_rows,\n",
    "        }\n",
    "\n",
    "        # Coverage if interval columns exist\n",
    "        for lvl in confidence_levels:\n",
    "            lo_col = f\"{m}-lo-{lvl}\"\n",
    "            hi_col = f\"{m}-hi-{lvl}\"\n",
    "            if lo_col in cv_df.columns and hi_col in cv_df.columns:\n",
    "                cov = ForecastMetrics.coverage(\n",
    "                    y_true,\n",
    "                    cv_df[lo_col].to_numpy(),\n",
    "                    cv_df[hi_col].to_numpy(),\n",
    "                )\n",
    "                metrics[f\"coverage_{lvl}\"] = float(cov)\n",
    "\n",
    "        rows.append(metrics)\n",
    "\n",
    "    lb = pd.DataFrame(rows)\n",
    "    if lb.empty:\n",
    "        raise RuntimeError(\"[leaderboard] computed empty leaderboard (no usable model columns).\")\n",
    "\n",
    "    # Fail-loud sorting: rmse NaNs should sort last\n",
    "    lb = lb.sort_values([\"rmse\"], ascending=True, na_position=\"last\").reset_index(drop=True)\n",
    "    return lb\n",
    "\n",
    "\n",
    "def compute_baseline_metrics(\n",
    "    cv_df: pd.DataFrame,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    threshold_k: float = 2.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for drift detection from CV output.\n",
    "\n",
    "    We compute RMSE/MAE per (unique_id, cutoff) window, then aggregate:\n",
    "      rmse_mean, rmse_std, drift_threshold_rmse = mean + k*std\n",
    "\n",
    "    No imputation/filling: metrics are computed only from finite values.\n",
    "    \"\"\"\n",
    "    required = {\"unique_id\", \"cutoff\", \"y\", model_name}\n",
    "    missing = required - set(cv_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"[baseline] cv_df missing required columns for model '{model_name}': {sorted(missing)}\"\n",
    "        )\n",
    "\n",
    "    # Compute per-window metrics (unique_id, cutoff)\n",
    "    def _window_metrics(g: pd.DataFrame) -> pd.Series:\n",
    "        yt = g[\"y\"].to_numpy()\n",
    "        yp = g[model_name].to_numpy()\n",
    "        valid = np.isfinite(yt) & np.isfinite(yp)\n",
    "        if valid.sum() == 0:\n",
    "            return pd.Series({\"rmse\": np.nan, \"mae\": np.nan, \"valid_rows\": 0})\n",
    "        return pd.Series({\n",
    "            \"rmse\": ForecastMetrics.rmse(yt, yp),\n",
    "            \"mae\": ForecastMetrics.mae(yt, yp),\n",
    "            \"valid_rows\": int(valid.sum()),\n",
    "        })\n",
    "\n",
    "    per_window = (\n",
    "        cv_df.groupby([\"unique_id\", \"cutoff\"], sort=False, dropna=False)\n",
    "        .apply(_window_metrics)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Fail loud if baseline is entirely NaN\n",
    "    if per_window[\"rmse\"].notna().sum() == 0:\n",
    "        sample_cols = [\"unique_id\", \"cutoff\", \"y\", model_name]\n",
    "        raise RuntimeError(\n",
    "            \"[baseline] All per-window RMSE are NaN. \"\n",
    "            \"This usually means predictions or y are non-finite everywhere. \"\n",
    "            f\"Sample:\\n{cv_df[sample_cols].head(20).to_string(index=False)}\"\n",
    "        )\n",
    "\n",
    "    rmse_mean = float(per_window[\"rmse\"].mean(skipna=True))\n",
    "    rmse_std = float(per_window[\"rmse\"].std(skipna=True, ddof=0))\n",
    "    mae_mean = float(per_window[\"mae\"].mean(skipna=True))\n",
    "    mae_std = float(per_window[\"mae\"].std(skipna=True, ddof=0))\n",
    "\n",
    "    baseline = {\n",
    "        \"model\": model_name,\n",
    "        \"rmse_mean\": rmse_mean,\n",
    "        \"rmse_std\": rmse_std,\n",
    "        \"mae_mean\": mae_mean,\n",
    "        \"mae_std\": mae_std,\n",
    "        \"drift_threshold_rmse\": float(rmse_mean + threshold_k * rmse_std),\n",
    "        \"drift_threshold_mae\": float(mae_mean + threshold_k * mae_std),\n",
    "        \"n_series\": int(per_window[\"unique_id\"].nunique()),\n",
    "        \"n_windows\": int(per_window[\"cutoff\"].nunique()),\n",
    "        \"per_window_rows\": int(len(per_window)),\n",
    "    }\n",
    "\n",
    "    # Optional per-series baseline (useful later if you want drift per series)\n",
    "    per_series = (\n",
    "        per_window.groupby(\"unique_id\")[[\"rmse\", \"mae\"]]\n",
    "        .agg(rmse_mean=(\"rmse\", \"mean\"), rmse_std=(\"rmse\", lambda s: s.std(ddof=0)),\n",
    "             mae_mean=(\"mae\", \"mean\"), mae_std=(\"mae\", lambda s: s.std(ddof=0)))\n",
    "        .reset_index()\n",
    "    )\n",
    "    per_series[\"drift_threshold_rmse\"] = per_series[\"rmse_mean\"] + threshold_k * per_series[\"rmse_std\"]\n",
    "    per_series[\"drift_threshold_mae\"] = per_series[\"mae_mean\"] + threshold_k * per_series[\"mae_std\"]\n",
    "    baseline[\"per_series\"] = per_series.to_dict(orient=\"records\")\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "\n",
    "\n",
    "class RenewableForecastModel:\n",
    "    def __init__(self, horizon: int = 24, confidence_levels: tuple[int, int] = (80, 95)):\n",
    "        self.horizon = horizon\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.sf = None\n",
    "        self._train_df = None  # contains y + exog columns\n",
    "        self._exog_cols: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def prepare_training_df(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "        req = {\"unique_id\", \"ds\", \"y\"}\n",
    "        if not req.issubset(df.columns):\n",
    "            raise ValueError(f\"generation df missing cols={sorted(req - set(df.columns))}\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise RuntimeError(\n",
    "                \"[generation] Empty generation dataframe passed into modeling. \"\n",
    "                \"This is upstream (EIA fetch/cache) failure — inspect fetch_diagnostics and fetch_generation logs.\"\n",
    "            )\n",
    "\n",
    "        work = df.copy()\n",
    "        work[\"ds\"] = pd.to_datetime(work[\"ds\"], errors=\"raise\")\n",
    "        work = work.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "        y_null = work[\"y\"].isna()\n",
    "        if y_null.any():\n",
    "            sample = work.loc[y_null, [\"unique_id\", \"ds\", \"y\"]].head(25)\n",
    "            raise RuntimeError(\n",
    "                f\"[generation][Y] Found null y values (no imputation). rows={int(y_null.sum())}. \"\n",
    "                f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "            )\n",
    "\n",
    "        work = _enforce_hourly_grid(work, label=\"generation\", policy=\"drop_incomplete_series\")\n",
    "        work = _add_time_features(work)\n",
    "\n",
    "        if weather_df is not None and not weather_df.empty:\n",
    "            if not {\"ds\", \"region\"}.issubset(weather_df.columns):\n",
    "                raise ValueError(\"weather_df must have columns ['ds','region', ...]\")\n",
    "\n",
    "            work[\"region\"] = work[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "            wcols = [c for c in WEATHER_VARS if c in weather_df.columns]\n",
    "            if not wcols:\n",
    "                raise ValueError(\"weather_df has none of expected WEATHER_VARS\")\n",
    "\n",
    "            merged = work.merge(\n",
    "                weather_df[[\"ds\", \"region\"] + wcols],\n",
    "                on=[\"ds\", \"region\"],\n",
    "                how=\"left\",\n",
    "                validate=\"many_to_one\",\n",
    "            )\n",
    "\n",
    "            missing_any = merged[wcols].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = merged.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + wcols].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[weather][ALIGN] Missing weather after merge rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "            work = merged.drop(columns=[\"region\"])\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"] + wcols\n",
    "        else:\n",
    "            self._exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "\n",
    "        return work\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, weather_df: Optional[pd.DataFrame] = None) -> None:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, SeasonalNaive, AutoETS, MSTL\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "\n",
    "        self.sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "        self._train_df = train_df\n",
    "        self.fitted = True\n",
    "\n",
    "        print(f\"[fit] rows={len(train_df)} series={train_df['unique_id'].nunique()} exog_cols={self._exog_cols}\")\n",
    "\n",
    "    def build_future_X_df(self, future_weather: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build future X_df for forecast horizon using forecast weather.\n",
    "        Must include: unique_id, ds, and exactly the exog columns used in training.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        if future_weather is None or future_weather.empty:\n",
    "            raise RuntimeError(\"future_weather required to forecast with regressors (no fabrication).\")\n",
    "\n",
    "        if not {\"ds\", \"region\"}.issubset(future_weather.columns):\n",
    "            raise ValueError(\"future_weather must have columns ['ds','region', ...]\")\n",
    "\n",
    "        # Create the future ds grid per series\n",
    "        last_ds = self._train_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "        frames = []\n",
    "        for uid, end in last_ds.items():\n",
    "            future_ds = pd.date_range(end + pd.Timedelta(hours=1), periods=self.horizon, freq=\"h\")\n",
    "            frames.append(pd.DataFrame({\"unique_id\": uid, \"ds\": future_ds}))\n",
    "        X = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        X = _add_time_features(X)\n",
    "        X[\"region\"] = X[\"unique_id\"].str.split(\"_\").str[0]\n",
    "\n",
    "        wcols = [c for c in WEATHER_VARS if c in future_weather.columns]\n",
    "        X = X.merge(\n",
    "            future_weather[[\"ds\", \"region\"] + wcols],\n",
    "            on=[\"ds\", \"region\"],\n",
    "            how=\"left\",\n",
    "            validate=\"many_to_one\",\n",
    "        )\n",
    "\n",
    "        # Fail loud on missing future regressors\n",
    "        needed = [c for c in self._exog_cols if c not in [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]]  # weather cols\n",
    "        if needed:\n",
    "            missing_any = X[needed].isna().any(axis=1)\n",
    "            if missing_any.any():\n",
    "                sample = X.loc[missing_any, [\"unique_id\", \"ds\", \"region\"] + needed].head(10)\n",
    "                raise RuntimeError(\n",
    "                    f\"[future_weather][ALIGN] Missing future weather rows={int(missing_any.sum())}. \"\n",
    "                    f\"Sample:\\n{sample.to_string(index=False)}\"\n",
    "                )\n",
    "\n",
    "        X = X.drop(columns=[\"region\"])\n",
    "        keep = [\"unique_id\", \"ds\"] + self._exog_cols\n",
    "        return X[keep].sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    def predict(self, future_weather: pd.DataFrame, best_model: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Generate forecasts using fitted models.\n",
    "\n",
    "        Args:\n",
    "            future_weather: Future weather data for forecast period\n",
    "            best_model: Optional model name to use for predictions. If provided, only this model's\n",
    "                       predictions will be included in output (as 'yhat' column). If None, all\n",
    "                       fitted models' predictions are returned.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with forecast predictions. If best_model is specified, includes:\n",
    "            - unique_id, ds: identifiers\n",
    "            - yhat: point forecast from best_model\n",
    "            - yhat-lo-{level}, yhat-hi-{level}: prediction intervals from best_model\n",
    "\n",
    "            If best_model is None, includes predictions from all fitted models.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "\n",
    "        X_df = self.build_future_X_df(future_weather)\n",
    "\n",
    "        # IMPORTANT: If you fit models using exogenous regressors, you must supply X_df at forecast time.\n",
    "        fcst = self.sf.forecast(\n",
    "            h=self.horizon,\n",
    "            df=self._train_df,\n",
    "            X_df=X_df,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        # Apply minimal physical constraints for solar series\n",
    "        fcst = self._apply_minimal_solar_constraints(fcst, X_df)\n",
    "\n",
    "        # If best_model is specified, filter to only that model's predictions\n",
    "        if best_model is not None:\n",
    "            if best_model not in fcst.columns:\n",
    "                available_models = [c for c in fcst.columns if c not in ['unique_id', 'ds']]\n",
    "                raise ValueError(\n",
    "                    f\"[predict] best_model '{best_model}' not found in forecast output. \"\n",
    "                    f\"Available models: {available_models}\"\n",
    "                )\n",
    "\n",
    "            # Extract best model's predictions and rename to standard 'yhat' format\n",
    "            keep_cols = ['unique_id', 'ds', best_model]\n",
    "\n",
    "            # Also keep prediction interval columns for the best model\n",
    "            for level in self.confidence_levels:\n",
    "                lo_col = f\"{best_model}-lo-{level}\"\n",
    "                hi_col = f\"{best_model}-hi-{level}\"\n",
    "                if lo_col in fcst.columns:\n",
    "                    keep_cols.append(lo_col)\n",
    "                if hi_col in fcst.columns:\n",
    "                    keep_cols.append(hi_col)\n",
    "\n",
    "            fcst = fcst[keep_cols].copy()\n",
    "\n",
    "            # Rename model column to 'yhat' and interval columns to match\n",
    "            # NOTE: Using underscores (not hyphens) to match dashboard expectations\n",
    "            rename_map = {best_model: 'yhat'}\n",
    "            for level in self.confidence_levels:\n",
    "                old_lo = f\"{best_model}-lo-{level}\"\n",
    "                old_hi = f\"{best_model}-hi-{level}\"\n",
    "                if old_lo in fcst.columns:\n",
    "                    rename_map[old_lo] = f\"yhat_lo_{level}\"  # Changed hyphen to underscore\n",
    "                if old_hi in fcst.columns:\n",
    "                    rename_map[old_hi] = f\"yhat_hi_{level}\"  # Changed hyphen to underscore\n",
    "\n",
    "            fcst = fcst.rename(columns=rename_map)\n",
    "\n",
    "        return fcst\n",
    "\n",
    "    def _apply_minimal_solar_constraints(\n",
    "        self,\n",
    "        fcst: pd.DataFrame,\n",
    "        X_df: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply ONLY minimal physical constraints for impossible cases.\n",
    "\n",
    "        Philosophy: Let the model learn natural patterns. Only intervene when\n",
    "        physics is violated (e.g., generation when sun is below horizon).\n",
    "\n",
    "        Constraint: Zero generation when BOTH radiation sources are zero\n",
    "        (sun is definitely below horizon).\n",
    "        \"\"\"\n",
    "        # Merge forecast with features to get radiation data\n",
    "        fcst_with_features = fcst.merge(\n",
    "            X_df[[\"unique_id\", \"ds\", \"direct_radiation\", \"diffuse_radiation\"]],\n",
    "            on=[\"unique_id\", \"ds\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Identify solar series\n",
    "        solar_mask = fcst_with_features[\"unique_id\"].str.endswith(\"_SUN\")\n",
    "\n",
    "        # ONLY constraint: Zero generation when BOTH radiation sources are zero\n",
    "        # (sun is definitely below horizon)\n",
    "        no_sun_mask = (\n",
    "            (fcst_with_features[\"direct_radiation\"] == 0) &\n",
    "            (fcst_with_features[\"diffuse_radiation\"] == 0)\n",
    "        )\n",
    "\n",
    "        # Apply constraint to solar series\n",
    "        constrain_mask = solar_mask & no_sun_mask\n",
    "\n",
    "        # Set all forecast columns to 0 (cannot generate without any sunlight)\n",
    "        # Note: At this point, columns are model names (AutoARIMA, MSTL_ARIMA, etc.)\n",
    "        # and their intervals (model-lo-80, model-hi-80, etc.), not \"yhat\"\n",
    "        # Exclude unique_id, ds, and feature columns\n",
    "        exclude_cols = {'unique_id', 'ds', 'direct_radiation', 'diffuse_radiation'}\n",
    "        forecast_cols = [c for c in fcst_with_features.columns if c not in exclude_cols]\n",
    "\n",
    "        for col in forecast_cols:\n",
    "            fcst_with_features.loc[constrain_mask, col] = 0.0\n",
    "\n",
    "        return fcst_with_features.drop(columns=[\"direct_radiation\", \"diffuse_radiation\"], errors=\"ignore\")\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        weather_df: Optional[pd.DataFrame] = None,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "        expanded_models: bool = True,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, SeasonalNaive, AutoETS, MSTL\n",
    "\n",
    "        train_df = self.prepare_training_df(df, weather_df)\n",
    "\n",
    "        # Core models\n",
    "        models = [\n",
    "            AutoARIMA(season_length=24),\n",
    "            SeasonalNaive(season_length=24),\n",
    "            AutoETS(season_length=24),\n",
    "            MSTL(season_length=[24, 168], trend_forecaster=AutoARIMA(), alias=\"MSTL_ARIMA\"),\n",
    "        ]\n",
    "\n",
    "        # Add expanded models if requested\n",
    "        if expanded_models:\n",
    "            try:\n",
    "                from statsforecast.models import AutoTheta, AutoCES\n",
    "                models.extend([\n",
    "                    AutoTheta(season_length=24),\n",
    "                    AutoCES(season_length=24),\n",
    "                ])\n",
    "                print(\"[cv] Using expanded model set: +AutoTheta, +AutoCES\")\n",
    "            except ImportError:\n",
    "                print(\"[cv] AutoTheta/AutoCES not available, using core models only\")\n",
    "\n",
    "        sf = StatsForecast(models=models, freq=\"h\", n_jobs=-1)\n",
    "\n",
    "        print(\n",
    "            f\"[cv] windows={n_windows} step={step_size} h={self.horizon} \"\n",
    "            f\"rows={len(train_df)} series={train_df['unique_id'].nunique()}\"\n",
    "        )\n",
    "\n",
    "        cv = sf.cross_validation(\n",
    "            df=train_df,\n",
    "            h=self.horizon,\n",
    "            step_size=step_size,\n",
    "            n_windows=n_windows,\n",
    "            level=list(self.confidence_levels),\n",
    "        ).reset_index()\n",
    "\n",
    "        leaderboard = compute_leaderboard(cv, confidence_levels=self.confidence_levels)\n",
    "        return cv, leaderboard\n",
    "\n",
    "\n",
    "class RenewableLGBMForecaster:\n",
    "    \"\"\"\n",
    "    LightGBM-based forecaster with interpretability support.\n",
    "\n",
    "    This forecaster is designed for model interpretability (SHAP, feature importance)\n",
    "    rather than production forecasting. Use alongside RenewableForecastModel which\n",
    "    provides better uncertainty quantification via statistical models.\n",
    "\n",
    "    Uses skforecast's ForecasterRecursive with LightGBM as the base estimator,\n",
    "    along with rolling window features for temporal patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon: int = 24,\n",
    "        lags: int = 168,  # 7 days of lags\n",
    "        rolling_window_sizes: list[int] | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LightGBM forecaster.\n",
    "\n",
    "        Args:\n",
    "            horizon: Forecast horizon in hours\n",
    "            lags: Number of lag features (default 168 = 7 days)\n",
    "            rolling_window_sizes: Window sizes for rolling features (default [24, 168])\n",
    "        \"\"\"\n",
    "        self.horizon = horizon\n",
    "        self.lags = lags\n",
    "        self.rolling_window_sizes = rolling_window_sizes or [24, 168]\n",
    "        self.forecaster = None\n",
    "        self._exog_features: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, y: pd.Series, exog: Optional[pd.DataFrame] = None) -> None:\n",
    "        \"\"\"\n",
    "        Fit LightGBM forecaster with rolling features.\n",
    "\n",
    "        Args:\n",
    "            y: Target time series (must have DatetimeIndex)\n",
    "            exog: Optional exogenous features (must have same index as y)\n",
    "        \"\"\"\n",
    "        # Import here to make dependencies optional\n",
    "        try:\n",
    "            from lightgbm import LGBMRegressor\n",
    "            from skforecast.recursive import ForecasterRecursive\n",
    "            from skforecast.preprocessing import RollingFeatures\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\n",
    "                \"LightGBM forecaster requires: lightgbm, skforecast. \"\n",
    "                f\"Install with: pip install lightgbm skforecast. Error: {e}\"\n",
    "            )\n",
    "\n",
    "        # Create rolling features\n",
    "        # Note: skforecast requires window_sizes length to match stats length\n",
    "        # We use 'mean' for each window size to capture different temporal patterns\n",
    "        stats_list = ['mean'] * len(self.rolling_window_sizes)\n",
    "        window_features = RollingFeatures(\n",
    "            stats=stats_list,\n",
    "            window_sizes=self.rolling_window_sizes,\n",
    "        )\n",
    "\n",
    "        # Initialize forecaster\n",
    "        self.forecaster = ForecasterRecursive(\n",
    "            estimator=LGBMRegressor(\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                num_leaves=31,\n",
    "                min_child_samples=20,\n",
    "            ),\n",
    "            lags=self.lags,\n",
    "            window_features=window_features,\n",
    "        )\n",
    "\n",
    "        # Store exog feature names\n",
    "        if exog is not None:\n",
    "            self._exog_features = exog.columns.tolist()\n",
    "        else:\n",
    "            self._exog_features = []\n",
    "\n",
    "        # Fit the model\n",
    "        self.forecaster.fit(y=y, exog=exog)\n",
    "        self.fitted = True\n",
    "\n",
    "        n_features = len(self.get_feature_importances())\n",
    "        print(f\"[LGBMForecaster.fit] fitted with {n_features} features, lags={self.lags}\")\n",
    "\n",
    "    def predict(self, steps: int, exog: Optional[pd.DataFrame] = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Generate predictions.\n",
    "\n",
    "        Args:\n",
    "            steps: Number of steps to forecast\n",
    "            exog: Exogenous features for forecast period\n",
    "\n",
    "        Returns:\n",
    "            Series of predictions\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before predict()\")\n",
    "\n",
    "        return self.forecaster.predict(steps=steps, exog=exog)\n",
    "\n",
    "    def get_feature_importances(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract feature importance from fitted model.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with 'feature' and 'importance' columns\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before get_feature_importances()\")\n",
    "\n",
    "        return self.forecaster.get_feature_importances()\n",
    "\n",
    "    def create_train_X_y(\n",
    "        self,\n",
    "        y: pd.Series,\n",
    "        exog: Optional[pd.DataFrame] = None,\n",
    "    ) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Create training matrices for SHAP analysis.\n",
    "\n",
    "        Args:\n",
    "            y: Target time series\n",
    "            exog: Optional exogenous features\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (X_train, y_train)\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before create_train_X_y()\")\n",
    "\n",
    "        return self.forecaster.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "    @property\n",
    "    def regressor(self):\n",
    "        \"\"\"\n",
    "        Access internal LightGBM estimator for SHAP.\n",
    "\n",
    "        Returns:\n",
    "            The fitted LGBMRegressor instance\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.forecaster is None:\n",
    "            raise RuntimeError(\"Must call fit() before accessing regressor\")\n",
    "\n",
    "        # Use 'estimator' (new API) with fallback to 'regressor' (deprecated)\n",
    "        if hasattr(self.forecaster, \"estimator\"):\n",
    "            return self.forecaster.estimator\n",
    "        return self.forecaster.regressor\n",
    "\n",
    "    @property\n",
    "    def exog_features(self) -> list[str]:\n",
    "        \"\"\"Return list of exogenous feature names used in training.\"\"\"\n",
    "        return self._exog_features.copy()\n",
    "\n",
    "\n",
    "class RenewableMLForecast:\n",
    "    \"\"\"\n",
    "    MLForecast-based forecaster with LightGBM and conformal prediction intervals.\n",
    "\n",
    "    This forecaster uses MLForecast for multi-series forecasting with tree-based\n",
    "    models (LightGBM), which can capture nonlinear weather→generation relationships\n",
    "    better than linear statistical models.\n",
    "\n",
    "    Key features:\n",
    "    - Native support for multiple series in long format\n",
    "    - Automatic lag and date features\n",
    "    - Conformal prediction intervals for calibrated uncertainty\n",
    "    - Fast training suitable for hourly retraining\n",
    "\n",
    "    Use alongside StatsForecast models and let CV decide the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon: int = 24,\n",
    "        lags: list[int] | None = None,\n",
    "        date_features: list[str] | None = None,\n",
    "        confidence_levels: tuple[int, int] = (80, 95),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MLForecast forecaster.\n",
    "\n",
    "        Args:\n",
    "            horizon: Forecast horizon in hours\n",
    "            lags: Lag features to use (default: [1, 2, 3, 6, 12, 24, 48, 168])\n",
    "            date_features: Date features to extract (default: hour, dayofweek)\n",
    "            confidence_levels: Confidence levels for prediction intervals\n",
    "        \"\"\"\n",
    "        self.horizon = horizon\n",
    "        self.lags = lags or [1, 2, 3, 6, 12, 24, 48, 168]\n",
    "        self.date_features = date_features or ['hour', 'dayofweek']\n",
    "        self.confidence_levels = confidence_levels\n",
    "        self.mlf = None\n",
    "        self._exog_cols: list[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        static_features: Optional[list[str]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Fit MLForecast model on multi-series data.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns [unique_id, ds, y] + optional exog features\n",
    "            static_features: List of static feature column names (per-series)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from mlforecast import MLForecast\n",
    "            from mlforecast.target_transforms import Differences\n",
    "            from lightgbm import LGBMRegressor\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\n",
    "                \"MLForecast requires: mlforecast, lightgbm. \"\n",
    "                f\"Install with: pip install mlforecast lightgbm. Error: {e}\"\n",
    "            )\n",
    "\n",
    "        # Identify exogenous columns (not unique_id, ds, y)\n",
    "        core_cols = {'unique_id', 'ds', 'y'}\n",
    "        self._exog_cols = [c for c in df.columns if c not in core_cols]\n",
    "\n",
    "        # Create MLForecast instance with LightGBM\n",
    "        self.mlf = MLForecast(\n",
    "            models={\n",
    "                'LGBMRegressor': LGBMRegressor(\n",
    "                    random_state=42,\n",
    "                    verbose=-1,\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=6,\n",
    "                    num_leaves=31,\n",
    "                    min_child_samples=20,\n",
    "                ),\n",
    "            },\n",
    "            freq='h',\n",
    "            lags=self.lags,\n",
    "            date_features=self.date_features,\n",
    "            target_transforms=[Differences([24])],  # Remove daily seasonality\n",
    "        )\n",
    "\n",
    "        # Fit the model - specify empty static_features if exog columns exist\n",
    "        # (all weather features are dynamic/time-varying)\n",
    "        if static_features is None and self._exog_cols:\n",
    "            static_features = []  # All features are dynamic\n",
    "        self.mlf.fit(df, static_features=static_features)\n",
    "        self.fitted = True\n",
    "\n",
    "        print(f\"[MLForecast.fit] fitted with lags={self.lags}, exog={self._exog_cols}\")\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate point predictions.\n",
    "\n",
    "        Args:\n",
    "            h: Forecast horizon\n",
    "            X_df: Future exogenous features DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with predictions\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.mlf is None:\n",
    "            raise RuntimeError(\"Must call fit() before predict()\")\n",
    "\n",
    "        return self.mlf.predict(h=h, X_df=X_df)\n",
    "\n",
    "    def predict_with_intervals(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "        n_windows: int = 3,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate predictions with conformal prediction intervals.\n",
    "\n",
    "        Uses conformal prediction to generate calibrated intervals based on\n",
    "        cross-validation residuals.\n",
    "\n",
    "        Args:\n",
    "            h: Forecast horizon\n",
    "            X_df: Future exogenous features DataFrame\n",
    "            n_windows: Number of CV windows for conformal calibration\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: unique_id, ds, LGBMRegressor,\n",
    "            LGBMRegressor-lo-{level}, LGBMRegressor-hi-{level}\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.mlf is None:\n",
    "            raise RuntimeError(\"Must call fit() before predict_with_intervals()\")\n",
    "\n",
    "        try:\n",
    "            from mlforecast.utils import PredictionIntervals\n",
    "        except ImportError:\n",
    "            # Fallback: return point predictions without intervals\n",
    "            print(\"[MLForecast] PredictionIntervals not available, returning point forecast\")\n",
    "            return self.predict(h=h, X_df=X_df)\n",
    "\n",
    "        # Generate predictions with conformal intervals\n",
    "        levels = list(self.confidence_levels)\n",
    "        predictions = self.mlf.predict(\n",
    "            h=h,\n",
    "            X_df=X_df,\n",
    "            level=levels,\n",
    "            prediction_intervals=PredictionIntervals(n_windows=n_windows, h=h),\n",
    "        )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_windows: int = 3,\n",
    "        step_size: int = 168,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform cross-validation.\n",
    "\n",
    "        Args:\n",
    "            df: Training DataFrame\n",
    "            n_windows: Number of CV windows\n",
    "            step_size: Step size between windows\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with CV results\n",
    "        \"\"\"\n",
    "        if not self.fitted or self.mlf is None:\n",
    "            raise RuntimeError(\"Must call fit() before cross_validate()\")\n",
    "\n",
    "        cv_results = self.mlf.cross_validation(\n",
    "            df=df,\n",
    "            h=self.horizon,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,\n",
    "        )\n",
    "\n",
    "        return cv_results\n",
    "\n",
    "    @property\n",
    "    def feature_importance(self) -> pd.DataFrame:\n",
    "        \"\"\"Get feature importance from the LightGBM model.\"\"\"\n",
    "        if not self.fitted or self.mlf is None:\n",
    "            raise RuntimeError(\"Must call fit() before accessing feature_importance\")\n",
    "\n",
    "        model = self.mlf.models_['LGBMRegressor']\n",
    "        importance = model.feature_importances_\n",
    "        features = self.mlf.ts.features_order_\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importance,\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # REAL EXAMPLE: multi-series WND with strict gates and CV\n",
    "\n",
    "    from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "    from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "\n",
    "    regions = [\"CALI\", \"ERCO\", \"MISO\"]\n",
    "    fuel = \"WND\"\n",
    "    start_date = \"2024-11-01\"\n",
    "    end_date = \"2024-12-15\"\n",
    "\n",
    "    fetcher = EIARenewableFetcher(debug_env=True)\n",
    "    gen = fetcher.fetch_all_regions(fuel, start_date, end_date, regions=regions)\n",
    "    _log_series_summary(gen, label=\"generation_raw\")\n",
    "\n",
    "    weather_api = OpenMeteoRenewable(strict=True)\n",
    "    wx_hist = weather_api.fetch_all_regions_historical(regions, start_date, end_date, debug=True)\n",
    "\n",
    "    model = RenewableForecastModel(horizon=24, confidence_levels=(80, 95))\n",
    "\n",
    "    # CV (historical): regressors live in df, no filling allowed\n",
    "    cv = model.cross_validate(gen, weather_df=wx_hist, n_windows=3, step_size=168)\n",
    "    print(cv.head().to_string(index=False))\n",
    "\n",
    "    # Optional: fit + forecast next 24h using forecast weather (no leakage)\n",
    "    # wx_future = weather_api.fetch_all_regions_forecast(regions, horizon_hours=48, debug=True)\n",
    "    # model.fit(gen, weather_df=wx_hist)\n",
    "    # fcst = model.predict(future_weather=wx_future)\n",
    "    # print(fcst.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Module: Pipeline Tasks\n",
    "\n",
    "**File:** `src/renewable/tasks.py`\n",
    "\n",
    "This module orchestrates the complete pipeline:\n",
    "\n",
    "1. **Fetch generation data** from EIA\n",
    "2. **Fetch weather data** from Open-Meteo\n",
    "3. **Train models** with cross-validation\n",
    "4. **Generate forecasts** with prediction intervals\n",
    "5. **Compute drift metrics** vs baseline\n",
    "\n",
    "## Key Feature: Adaptive CV\n",
    "\n",
    "Cross-validation requires sufficient data:\n",
    "```\n",
    "Minimum rows = horizon + (n_windows × step_size)\n",
    "```\n",
    "\n",
    "For short series, we **adapt** the CV settings automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/tasks.py\n",
    "# file: src/renewable/tasks.py\n",
    "\"\"\"Renewable energy forecasting pipeline tasks.\n",
    "\n",
    "Idempotent tasks for:\n",
    "- Fetching EIA renewable generation data\n",
    "- Fetching weather data from Open-Meteo\n",
    "- Training probabilistic models\n",
    "- Generating forecasts with intervals\n",
    "- Computing drift metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.renewable.eia_renewable import EIARenewableFetcher\n",
    "from src.renewable.modeling import (\n",
    "    RenewableForecastModel,\n",
    "    RenewableLGBMForecaster,\n",
    "    _log_series_summary,\n",
    "    _add_time_features,\n",
    "    compute_baseline_metrics,\n",
    "    WEATHER_VARS,\n",
    ")\n",
    "from src.renewable.model_interpretability import (\n",
    "    InterpretabilityReport,\n",
    "    generate_full_interpretability_report,\n",
    ")\n",
    "from src.renewable.open_meteo import OpenMeteoRenewable\n",
    "from src.renewable.regions import REGIONS, list_regions\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RenewablePipelineConfig:\n",
    "    \"\"\"Configuration for renewable forecasting pipeline.\"\"\"\n",
    "\n",
    "    # Data parameters\n",
    "    regions: list[str] = field(default_factory=lambda: [\"CALI\", \"ERCO\", \"MISO\", \"PJM\", \"SWPP\"])\n",
    "    fuel_types: list[str] = field(default_factory=lambda: [\"WND\", \"SUN\"])\n",
    "    start_date: str = \"\"  # Set dynamically\n",
    "    end_date: str = \"\"  # Set dynamically\n",
    "    lookback_days: int = 30\n",
    "\n",
    "    # Forecast parameters\n",
    "    horizon: int = 24\n",
    "    confidence_levels: tuple[int, int] = (80, 95)\n",
    "    horizon_preset: Optional[str] = None  # \"24h\" | \"48h\" | \"72h\"\n",
    "\n",
    "    # CV parameters\n",
    "    cv_windows: int = 5\n",
    "    cv_step_size: int = 168  # 1 week\n",
    "\n",
    "    # Output paths\n",
    "    data_dir: str = \"data/renewable\"\n",
    "    overwrite: bool = False\n",
    "\n",
    "    # Horizon preset definitions (class-level constant)\n",
    "    _PRESETS = {\n",
    "        \"24h\": {\"horizon\": 24, \"cv_windows\": 2, \"lookback_days\": 15},\n",
    "        \"48h\": {\"horizon\": 48, \"cv_windows\": 3, \"lookback_days\": 21},\n",
    "        \"72h\": {\"horizon\": 72, \"cv_windows\": 3, \"lookback_days\": 28},\n",
    "    }\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Apply horizon preset if specified\n",
    "        if self.horizon_preset and self.horizon_preset in self._PRESETS:\n",
    "            preset = self._PRESETS[self.horizon_preset]\n",
    "            # Use object.__setattr__ since this is a dataclass\n",
    "            object.__setattr__(self, \"horizon\", preset[\"horizon\"])\n",
    "            object.__setattr__(self, \"cv_windows\", preset[\"cv_windows\"])\n",
    "            object.__setattr__(self, \"lookback_days\", preset[\"lookback_days\"])\n",
    "            logger.info(f\"[config] Applied preset '{self.horizon_preset}': horizon={preset['horizon']}h\")\n",
    "\n",
    "        # Set default dates if not provided\n",
    "        if not self.end_date:\n",
    "            self.end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        if not self.start_date:\n",
    "            end = datetime.strptime(self.end_date, \"%Y-%m-%d\")\n",
    "            start = end - timedelta(days=self.lookback_days)\n",
    "            self.start_date = start.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Validate configuration\n",
    "        warnings = self._validate()\n",
    "        for warning in warnings:\n",
    "            logger.warning(f\"[config] {warning}\")\n",
    "\n",
    "    def _validate(self) -> list[str]:\n",
    "        \"\"\"Validate configuration and return warnings.\"\"\"\n",
    "        warnings = []\n",
    "\n",
    "        # Check minimum data requirement\n",
    "        available_hours = self.lookback_days * 24\n",
    "        required_hours = self.horizon + (self.cv_windows * self.cv_step_size)\n",
    "        if available_hours < required_hours:\n",
    "            warnings.append(\n",
    "                f\"Insufficient data: need {required_hours}h, have {available_hours}h. \"\n",
    "                f\"Increase lookback_days to {(required_hours // 24) + 1} or reduce cv_windows.\"\n",
    "            )\n",
    "\n",
    "        # Warn about accuracy degradation\n",
    "        if self.horizon > 72:\n",
    "            warnings.append(\n",
    "                f\"Horizon {self.horizon}h exceeds recommended max (72h). \"\n",
    "                f\"Weather forecast accuracy degrades significantly beyond 3 days.\"\n",
    "            )\n",
    "\n",
    "        return warnings\n",
    "\n",
    "    def generation_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"generation.parquet\"\n",
    "\n",
    "    def weather_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"weather.parquet\"\n",
    "\n",
    "    def forecasts_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"forecasts.parquet\"\n",
    "\n",
    "    def baseline_path(self) -> Path:\n",
    "        return Path(self.data_dir) / \"baseline.json\"\n",
    "\n",
    "    def interpretability_dir(self) -> Path:\n",
    "        return Path(self.data_dir) / \"interpretability\"\n",
    "\n",
    "\n",
    "def fetch_renewable_data(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 1: Fetch EIA generation data for all regions and fuel types.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [unique_id, ds, y]\n",
    "    \"\"\"\n",
    "    output_path = config.generation_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_generation_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        _log_series_summary(df, value_col=\"y\", label=f\"generation_data_{source}\")\n",
    "\n",
    "        expected_series = {\n",
    "            f\"{region}_{fuel}\" for region in config.regions for fuel in config.fuel_types\n",
    "        }\n",
    "        present_series = set(df[\"unique_id\"]) if \"unique_id\" in df.columns else set()\n",
    "        missing_series = sorted(expected_series - present_series)\n",
    "        if missing_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Missing expected series (%s): %s\",\n",
    "                source,\n",
    "                missing_series,\n",
    "            )\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_generation] No generation data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"unique_id\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"unique_id\")\n",
    "        )\n",
    "        max_series_log = 25\n",
    "        if len(coverage) > max_series_log:\n",
    "            logger.info(\n",
    "                \"[fetch_generation] Coverage (%s, first %s series):\\n%s\",\n",
    "                source,\n",
    "                max_series_log,\n",
    "                coverage.head(max_series_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_generation] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_generation] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached coverage to surface missing series without refetching.\n",
    "        _log_generation_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_generation] Fetching {config.fuel_types} for {config.regions}\")\n",
    "\n",
    "    fetcher = EIARenewableFetcher()\n",
    "    all_dfs = []\n",
    "\n",
    "    for fuel_type in config.fuel_types:\n",
    "        df = fetcher.fetch_all_regions(\n",
    "            fuel_type=fuel_type,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            regions=config.regions,\n",
    "            diagnostics=fetch_diagnostics,\n",
    "        )\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined = combined.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh coverage to highlight gaps or unexpected negatives.\n",
    "    _log_generation_summary(combined, source=\"fresh\")\n",
    "\n",
    "    if fetch_diagnostics:\n",
    "        empty_series = [\n",
    "            entry\n",
    "            for entry in fetch_diagnostics\n",
    "            if entry.get(\"empty\")\n",
    "        ]\n",
    "        for entry in empty_series:\n",
    "            logger.warning(\n",
    "                \"[fetch_generation] Empty series detail: region=%s fuel=%s total=%s pages=%s\",\n",
    "                entry.get(\"region\"),\n",
    "                entry.get(\"fuel_type\"),\n",
    "                entry.get(\"total_records\"),\n",
    "                entry.get(\"pages\"),\n",
    "            )\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_generation] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def fetch_renewable_weather(\n",
    "    config: RenewablePipelineConfig,\n",
    "    include_forecast: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 2: Fetch weather data for all regions.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        include_forecast: Include forecast weather for predictions\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [ds, region, weather_vars...]\n",
    "    \"\"\"\n",
    "    output_path = config.weather_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _log_weather_summary(df: pd.DataFrame, source: str) -> None:\n",
    "        if df.empty:\n",
    "            logger.warning(\"[fetch_weather] No weather data rows (%s).\", source)\n",
    "            return\n",
    "\n",
    "        coverage = (\n",
    "            df.groupby(\"region\")[\"ds\"]\n",
    "            .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"region\")\n",
    "        )\n",
    "        max_region_log = 25\n",
    "        if len(coverage) > max_region_log:\n",
    "            logger.info(\n",
    "                \"[fetch_weather] Coverage (%s, first %s regions):\\n%s\",\n",
    "                source,\n",
    "                max_region_log,\n",
    "                coverage.head(max_region_log).to_string(index=False),\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"[fetch_weather] Coverage (%s):\\n%s\", source, coverage.to_string(index=False))\n",
    "\n",
    "        missing_cols = [\n",
    "            col for col in OpenMeteoRenewable.WEATHER_VARS if col not in df.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing expected weather columns (%s): %s\",\n",
    "                source,\n",
    "                missing_cols,\n",
    "            )\n",
    "\n",
    "        missing_values = {\n",
    "            col: int(df[col].isna().sum())\n",
    "            for col in OpenMeteoRenewable.WEATHER_VARS\n",
    "            if col in df.columns and df[col].isna().any()\n",
    "        }\n",
    "        if missing_values:\n",
    "            logger.warning(\n",
    "                \"[fetch_weather] Missing weather values (%s): %s\",\n",
    "                source,\n",
    "                missing_values,\n",
    "            )\n",
    "\n",
    "    if output_path.exists() and not config.overwrite:\n",
    "        logger.info(f\"[fetch_weather] exists, loading: {output_path}\")\n",
    "        cached = pd.read_parquet(output_path)\n",
    "        # Log cached weather coverage to surface missing regions/columns.\n",
    "        _log_weather_summary(cached, source=\"cache\")\n",
    "        return cached\n",
    "\n",
    "    logger.info(f\"[fetch_weather] Fetching weather for {config.regions}\")\n",
    "\n",
    "    weather = OpenMeteoRenewable()\n",
    "\n",
    "    # Historical weather\n",
    "    hist_df = weather.fetch_all_regions_historical(\n",
    "        regions=config.regions,\n",
    "        start_date=config.start_date,\n",
    "        end_date=config.end_date,\n",
    "    )\n",
    "\n",
    "    # Forecast weather (for prediction, prevents leakage)\n",
    "    if include_forecast:\n",
    "        fcst_df = weather.fetch_all_regions_forecast(\n",
    "            regions=config.regions,\n",
    "            horizon_hours=config.horizon + 24,  # Buffer\n",
    "        )\n",
    "\n",
    "        # Combine, preferring forecast for overlapping times\n",
    "        combined = pd.concat([hist_df, fcst_df], ignore_index=True)\n",
    "        combined = combined.drop_duplicates(subset=[\"ds\", \"region\"], keep=\"last\")\n",
    "    else:\n",
    "        combined = hist_df\n",
    "\n",
    "    combined = combined.sort_values([\"region\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Log fresh weather coverage and missing values before saving.\n",
    "    _log_weather_summary(combined, source=\"fresh\")\n",
    "\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[fetch_weather] Saved: {output_path} ({len(combined)} rows)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def train_renewable_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Task 3: Train models and compute baseline metrics via cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cv_results, leaderboard, baseline_metrics)\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_models] Training on {len(generation_df)} rows\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Compute adaptive CV settings based on shortest series\n",
    "    min_series_len = generation_df.groupby(\"unique_id\").size().min()\n",
    "\n",
    "    # CV needs: horizon + (n_windows * step_size) rows minimum\n",
    "    # Solve for n_windows: n_windows = (min_series_len - horizon) / step_size\n",
    "    available_for_cv = min_series_len - config.horizon\n",
    "\n",
    "    # Adjust step_size and n_windows to fit data\n",
    "    step_size = min(config.cv_step_size, max(24, available_for_cv // 3))\n",
    "    n_windows = min(config.cv_windows, max(2, available_for_cv // step_size))\n",
    "\n",
    "    logger.info(\n",
    "        f\"[train_models] Adaptive CV: {n_windows} windows, \"\n",
    "        f\"step={step_size}h (min_series={min_series_len} rows)\"\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results, leaderboard = model.cross_validate(\n",
    "        df=generation_df,\n",
    "        weather_df=weather_df,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    baseline = compute_baseline_metrics(cv_results, model_name=best_model)\n",
    "\n",
    "\n",
    "    logger.info(f\"[train_models] Best model: {best_model}, RMSE: {baseline['rmse_mean']:.1f}\")\n",
    "\n",
    "    return cv_results, leaderboard, baseline\n",
    "\n",
    "\n",
    "def train_interpretability_models(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    ") -> dict[str, InterpretabilityReport]:\n",
    "    \"\"\"Train LightGBM models and generate interpretability reports per series.\n",
    "\n",
    "    This trains a separate LightGBM model for each series (region × fuel type)\n",
    "    and generates SHAP, partial dependence, and feature importance artifacts.\n",
    "\n",
    "    Note: LightGBM is used for interpretability only. The primary forecasts\n",
    "    come from statistical models (MSTL/ARIMA) which provide better uncertainty\n",
    "    quantification.\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        generation_df: Generation data (loads from file if None)\n",
    "        weather_df: Weather data (loads from file if None)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping series_id -> InterpretabilityReport\n",
    "    \"\"\"\n",
    "    # Load data if not provided\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Training LightGBM for {generation_df['unique_id'].nunique()} series\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    reports: dict[str, InterpretabilityReport] = {}\n",
    "    output_dir = config.interpretability_dir()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for uid in sorted(generation_df[\"unique_id\"].unique()):\n",
    "        logger.info(f\"[train_interpretability] Processing {uid}...\")\n",
    "\n",
    "        # Extract series data\n",
    "        series_data = generation_df[generation_df[\"unique_id\"] == uid].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Prepare target series with proper frequency\n",
    "        y = series_data.set_index(\"ds\")[\"y\"]\n",
    "        y.index = pd.DatetimeIndex(y.index, freq=\"h\")  # Set hourly frequency\n",
    "\n",
    "        # Prepare exogenous features\n",
    "        region = uid.split(\"_\")[0]\n",
    "        series_weather = weather_df[weather_df[\"region\"] == region].copy()\n",
    "\n",
    "        if series_weather.empty:\n",
    "            logger.warning(f\"[train_interpretability] No weather data for region {region}, skipping {uid}\")\n",
    "            continue\n",
    "\n",
    "        # Merge weather to series timestamps\n",
    "        series_data = series_data.merge(\n",
    "            series_weather[[\"ds\"] + [c for c in WEATHER_VARS if c in series_weather.columns]],\n",
    "            on=\"ds\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Add time features\n",
    "        series_data = _add_time_features(series_data)\n",
    "\n",
    "        # Build exog DataFrame aligned with y\n",
    "        exog_cols = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\"]\n",
    "        exog_cols += [c for c in WEATHER_VARS if c in series_data.columns]\n",
    "        exog = series_data.set_index(\"ds\")[exog_cols]\n",
    "\n",
    "        # Check for missing weather\n",
    "        missing_weather = exog.isna().any(axis=1).sum()\n",
    "        if missing_weather > 0:\n",
    "            logger.warning(f\"[train_interpretability] {uid}: {missing_weather} rows with missing weather, filling with ffill/bfill\")\n",
    "            exog = exog.ffill().bfill()\n",
    "\n",
    "        # Fit LightGBM forecaster\n",
    "        try:\n",
    "            lgbm = RenewableLGBMForecaster(\n",
    "                horizon=config.horizon,\n",
    "                lags=168,  # 7 days of lags\n",
    "                rolling_window_sizes=[24, 168],  # 1 day, 1 week\n",
    "            )\n",
    "            lgbm.fit(y=y, exog=exog)\n",
    "\n",
    "            # Create training matrices for SHAP analysis\n",
    "            X_train, y_train = lgbm.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "            # Generate interpretability report\n",
    "            series_output_dir = output_dir / uid\n",
    "            report = generate_full_interpretability_report(\n",
    "                forecaster=lgbm.forecaster,\n",
    "                X_train=X_train,\n",
    "                series_id=uid,\n",
    "                output_dir=series_output_dir,\n",
    "                top_n_features=5,\n",
    "                shap_sample_frac=0.5,\n",
    "                shap_max_samples=1000,\n",
    "            )\n",
    "            reports[uid] = report\n",
    "\n",
    "            logger.info(\n",
    "                f\"[train_interpretability] {uid}: top_features={report.top_features[:3]}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[train_interpretability] {uid}: Failed to train - {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"[train_interpretability] Generated {len(reports)} interpretability reports\")\n",
    "    return reports\n",
    "\n",
    "\n",
    "def generate_renewable_forecasts(\n",
    "    config: RenewablePipelineConfig,\n",
    "    generation_df: Optional[pd.DataFrame] = None,\n",
    "    weather_df: Optional[pd.DataFrame] = None,\n",
    "    best_model: str = \"MSTL_ARIMA\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Task 4: Generate forecasts with prediction intervals.\"\"\"\n",
    "    output_path = config.forecasts_path()\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if generation_df is None:\n",
    "        generation_df = pd.read_parquet(config.generation_path())\n",
    "    if weather_df is None:\n",
    "        weather_df = pd.read_parquet(config.weather_path())\n",
    "\n",
    "    logger.info(f\"[generate_forecasts] Generating {config.horizon}h forecasts using model={best_model}\")\n",
    "\n",
    "    # Ensure datetime types\n",
    "    generation_df = generation_df.copy()\n",
    "    generation_df[\"ds\"] = pd.to_datetime(generation_df[\"ds\"], errors=\"raise\")\n",
    "    weather_df = weather_df.copy()\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"raise\")\n",
    "\n",
    "    model = RenewableForecastModel(\n",
    "        horizon=config.horizon,\n",
    "        confidence_levels=config.confidence_levels,\n",
    "    )\n",
    "\n",
    "    # Fit uses only historical generation timestamps, weather merge will fail-loud if missing.\n",
    "    model.fit(generation_df, weather_df)\n",
    "\n",
    "    # Future weather must cover the horizon after the EARLIEST series' last timestamp\n",
    "    # (different regions may have different publishing lags)\n",
    "    per_series_max = generation_df.groupby(\"unique_id\")[\"ds\"].max()\n",
    "    logger.info(f\"[generate_forecasts] Per-series max timestamps:\\n{per_series_max.to_dict()}\")\n",
    "\n",
    "    min_of_max = per_series_max.min()\n",
    "    global_max = generation_df[\"ds\"].max()\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Min of series maxes: {min_of_max}, \"\n",
    "        f\"Global max: {global_max}, \"\n",
    "        f\"Delta: {(global_max - min_of_max).total_seconds() / 3600:.1f}h\"\n",
    "    )\n",
    "\n",
    "    # Use min of max timestamps to ensure all series have weather for their forecasts\n",
    "    future_weather = weather_df[weather_df[\"ds\"] > min_of_max].copy()\n",
    "\n",
    "    if future_weather.empty:\n",
    "        raise RuntimeError(\n",
    "            \"[generate_forecasts] No future weather rows found after earliest series max. \"\n",
    "            f\"min_of_max={min_of_max}\"\n",
    "        )\n",
    "\n",
    "    # Generate forecasts using best model from CV\n",
    "    # The predict() method now supports best_model parameter to filter output\n",
    "    logger.info(f\"[generate_forecasts] Generating predictions using model: {best_model}\")\n",
    "    forecasts = model.predict(future_weather=future_weather, best_model=best_model)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[generate_forecasts] Generated {len(forecasts)} forecast rows \"\n",
    "        f\"for {forecasts['unique_id'].nunique()} series\"\n",
    "    )\n",
    "\n",
    "    forecasts.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"[generate_forecasts] Saved: {output_path} ({len(forecasts)} rows)\")\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "\n",
    "def compute_renewable_drift(\n",
    "    predictions: pd.DataFrame,\n",
    "    actuals: pd.DataFrame,\n",
    "    baseline_metrics: dict,\n",
    ") -> dict:\n",
    "    \"\"\"Task 5: Detect drift by comparing current metrics to baseline.\n",
    "\n",
    "    Drift is flagged when current RMSE > baseline_mean + 2*baseline_std\n",
    "\n",
    "    Args:\n",
    "        predictions: Forecast DataFrame with [unique_id, ds, yhat]\n",
    "        actuals: Actual values DataFrame with [unique_id, ds, y]\n",
    "        baseline_metrics: Baseline from cross-validation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with drift status and details\n",
    "    \"\"\"\n",
    "    from src.chapter2.evaluation import ForecastMetrics\n",
    "\n",
    "    # Merge predictions with actuals\n",
    "    merged = predictions.merge(\n",
    "        actuals[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        return {\n",
    "            \"status\": \"no_data\",\n",
    "            \"message\": \"No overlapping data between predictions and actuals\",\n",
    "        }\n",
    "\n",
    "    # Compute current metrics\n",
    "    y_true = merged[\"y\"].values\n",
    "    y_pred = merged[\"yhat\"].values\n",
    "\n",
    "    current_rmse = ForecastMetrics.rmse(y_true, y_pred)\n",
    "    current_mae = ForecastMetrics.mae(y_true, y_pred)\n",
    "\n",
    "    # Check against threshold\n",
    "    threshold = baseline_metrics.get(\"drift_threshold_rmse\", float(\"inf\"))\n",
    "    is_drifting = current_rmse > threshold\n",
    "\n",
    "    result = {\n",
    "        \"status\": \"drift_detected\" if is_drifting else \"stable\",\n",
    "        \"current_rmse\": float(current_rmse),\n",
    "        \"current_mae\": float(current_mae),\n",
    "        \"baseline_rmse\": float(baseline_metrics.get(\"rmse_mean\", 0)),\n",
    "        \"drift_threshold\": float(threshold),\n",
    "        \"threshold_exceeded_by\": float(max(0, current_rmse - threshold)),\n",
    "        \"n_predictions\": len(merged),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    if is_drifting:\n",
    "        logger.warning(\n",
    "            f\"[drift] DRIFT DETECTED: RMSE={current_rmse:.1f} > threshold={threshold:.1f}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f\"[drift] Stable: RMSE={current_rmse:.1f} <= threshold={threshold:.1f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    config: RenewablePipelineConfig,\n",
    "    fetch_diagnostics: Optional[list[dict]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Run the complete renewable forecasting pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetch generation data\n",
    "    2. Fetch weather data\n",
    "    3. Train models (CV)\n",
    "    4. Generate forecasts\n",
    "\n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        fetch_diagnostics: Optional list to capture per-region fetch metadata\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pipeline results\n",
    "    \"\"\"\n",
    "    logger.info(f\"[pipeline] Starting: {config.start_date} to {config.end_date}\")\n",
    "    logger.info(f\"[pipeline] Regions: {config.regions}\")\n",
    "    logger.info(f\"[pipeline] Fuel types: {config.fuel_types}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Fetch generation\n",
    "    generation_df = fetch_renewable_data(config, fetch_diagnostics=fetch_diagnostics)\n",
    "    results[\"generation_rows\"] = len(generation_df)\n",
    "    results[\"series_count\"] = generation_df[\"unique_id\"].nunique()\n",
    "\n",
    "    from src.renewable.validation import validate_generation_df\n",
    "\n",
    "    expected_series = [f\"{r}_{f}\" for r in config.regions for f in config.fuel_types]\n",
    "    rep = validate_generation_df(\n",
    "        generation_df,\n",
    "        expected_series=expected_series,\n",
    "        max_missing_ratio=0.02,\n",
    "        max_lag_hours=48,  # choose a value consistent with EIA publishing lag\n",
    "    )\n",
    "    if not rep.ok:\n",
    "        raise RuntimeError(f\"[pipeline][generation_validation] {rep.message} details={rep.details}\")\n",
    "\n",
    "    # Step 2: Fetch weather\n",
    "    weather_df = fetch_renewable_weather(config)\n",
    "    results[\"weather_rows\"] = len(weather_df)\n",
    "\n",
    "    # Step 3: Train and validate\n",
    "    cv_results, leaderboard, baseline = train_renewable_models(\n",
    "        config, generation_df, weather_df\n",
    "    )\n",
    "    best_model = leaderboard.iloc[0][\"model\"]\n",
    "    results[\"best_model\"] = best_model\n",
    "    results[\"best_rmse\"] = float(leaderboard.iloc[0][\"rmse\"])\n",
    "    results[\"baseline\"] = baseline\n",
    "    # Save full leaderboard for dashboard display\n",
    "    results[\"leaderboard\"] = leaderboard.to_dict(orient=\"records\")\n",
    "\n",
    "    # Step 4: Generate forecasts (use the best model from CV)\n",
    "    forecasts = generate_renewable_forecasts(\n",
    "        config, generation_df, weather_df, best_model=best_model\n",
    "    )\n",
    "    results[\"forecast_rows\"] = len(forecasts)\n",
    "\n",
    "    # Step 5: Train LightGBM models and generate interpretability reports\n",
    "    # (LightGBM is for interpretability only - MSTL/ARIMA provide primary forecasts)\n",
    "    try:\n",
    "        interpretability_reports = train_interpretability_models(\n",
    "            config, generation_df, weather_df\n",
    "        )\n",
    "        results[\"interpretability\"] = {\n",
    "            \"series_count\": len(interpretability_reports),\n",
    "            \"series\": list(interpretability_reports.keys()),\n",
    "            \"output_dir\": str(config.interpretability_dir()),\n",
    "        }\n",
    "\n",
    "        # Add top features summary per series\n",
    "        for uid, report in interpretability_reports.items():\n",
    "            results[\"interpretability\"][f\"{uid}_top_features\"] = report.top_features[:3]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[pipeline] Interpretability training failed (non-fatal): {e}\")\n",
    "        results[\"interpretability\"] = {\"error\": str(e)}\n",
    "\n",
    "    if fetch_diagnostics is not None:\n",
    "        results[\"fetch_diagnostics\"] = fetch_diagnostics\n",
    "\n",
    "    logger.info(f\"[pipeline] Complete. Best model: {results['best_model']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"CLI entry point for renewable pipeline.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Renewable Energy Forecasting Pipeline\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--regions\",\n",
    "        type=str,\n",
    "        default=\"CALI,ERCO,MISO\",\n",
    "        help=\"Comma-separated region codes (default: CALI,ERCO,MISO)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fuel\",\n",
    "        type=str,\n",
    "        default=\"WND,SUN\",\n",
    "        help=\"Comma-separated fuel types (default: WND,SUN)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--days\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"Lookback days (default: 30)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--horizon\",\n",
    "        type=int,\n",
    "        default=24,\n",
    "        help=\"Forecast horizon in hours (default: 24)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overwrite existing data files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"data/renewable\",\n",
    "        help=\"Output directory (default: data/renewable)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # Build config\n",
    "    config = RenewablePipelineConfig(\n",
    "        regions=args.regions.split(\",\"),\n",
    "        fuel_types=args.fuel.split(\",\"),\n",
    "        lookback_days=args.days,\n",
    "        horizon=args.horizon,\n",
    "        overwrite=args.overwrite,\n",
    "        data_dir=args.data_dir,\n",
    "    )\n",
    "\n",
    "    # Run pipeline\n",
    "    results = run_full_pipeline(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Series count: {results['series_count']}\")\n",
    "    print(f\"  Generation rows: {results['generation_rows']}\")\n",
    "    print(f\"  Weather rows: {results['weather_rows']}\")\n",
    "    print(f\"  Forecast rows: {results['forecast_rows']}\")\n",
    "    print(f\"  Best model: {results['best_model']}\")\n",
    "    print(f\"  Best RMSE: {results['best_rmse']:.1f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite persistence layer\n",
    "\n",
    "Extends the monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/db.py\n",
    "# file: src/renewable/db.py\n",
    "\"\"\"Database schema and operations for renewable forecasting.\n",
    "\n",
    "Extends the Chapter 4 monitoring database with:\n",
    "- Prediction intervals (80%, 95%)\n",
    "- Weather features table\n",
    "- Renewable-specific columns (fuel_type, region)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    \"\"\"Connect to SQLite database with optimized settings.\"\"\"\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def init_renewable_db(db_path: str) -> None:\n",
    "    \"\"\"Initialize renewable forecasting database schema.\n",
    "\n",
    "    Creates tables:\n",
    "    - renewable_forecasts: Forecasts with dual intervals\n",
    "    - renewable_scores: Evaluation metrics with coverage\n",
    "    - weather_features: Weather data by region\n",
    "    - drift_alerts: Drift detection history\n",
    "    - baseline_metrics: Backtest baselines for drift thresholds\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Forecasts with dual prediction intervals\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_forecasts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        run_id TEXT NOT NULL,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        yhat REAL,\n",
    "        yhat_lo_80 REAL,\n",
    "        yhat_hi_80 REAL,\n",
    "        yhat_lo_95 REAL,\n",
    "        yhat_hi_95 REAL,\n",
    "        UNIQUE (run_id, model, unique_id, ds)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Index for efficient queries\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_region_ds\n",
    "    ON renewable_forecasts (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_forecasts_fuel_ds\n",
    "    ON renewable_forecasts (fuel_type, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Evaluation scores with dual coverage\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS renewable_scores (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        scored_at TEXT NOT NULL,\n",
    "        run_id TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        region TEXT NOT NULL,\n",
    "        fuel_type TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        horizon_hours INTEGER NOT NULL,\n",
    "        rmse REAL,\n",
    "        mae REAL,\n",
    "        coverage_80 REAL,\n",
    "        coverage_95 REAL,\n",
    "        valid_rows INTEGER,\n",
    "        UNIQUE (run_id, model, unique_id, horizon_hours)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Weather features by region\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_features (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        region TEXT NOT NULL,\n",
    "        ds TEXT NOT NULL,\n",
    "        temperature_2m REAL,\n",
    "        wind_speed_10m REAL,\n",
    "        wind_speed_100m REAL,\n",
    "        wind_direction_10m REAL,\n",
    "        direct_radiation REAL,\n",
    "        diffuse_radiation REAL,\n",
    "        cloud_cover REAL,\n",
    "        is_forecast INTEGER DEFAULT 0,\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
    "        UNIQUE (region, ds, is_forecast)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_weather_region_ds\n",
    "    ON weather_features (region, ds);\n",
    "    \"\"\")\n",
    "\n",
    "    # Drift detection alerts\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS drift_alerts (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        alert_at TEXT NOT NULL,\n",
    "        run_id TEXT,\n",
    "        unique_id TEXT,\n",
    "        region TEXT,\n",
    "        fuel_type TEXT,\n",
    "        alert_type TEXT NOT NULL,\n",
    "        severity TEXT NOT NULL,\n",
    "        current_rmse REAL,\n",
    "        threshold_rmse REAL,\n",
    "        message TEXT,\n",
    "        metadata_json TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_drift_alerts_time\n",
    "    ON drift_alerts (alert_at);\n",
    "    \"\"\")\n",
    "\n",
    "    # Baseline metrics for drift detection\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS baseline_metrics (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created_at TEXT NOT NULL,\n",
    "        unique_id TEXT NOT NULL,\n",
    "        model TEXT NOT NULL,\n",
    "        rmse_mean REAL NOT NULL,\n",
    "        rmse_std REAL NOT NULL,\n",
    "        mae_mean REAL,\n",
    "        mae_std REAL,\n",
    "        drift_threshold_rmse REAL NOT NULL,\n",
    "        drift_threshold_mae REAL,\n",
    "        n_windows INTEGER,\n",
    "        metadata_json TEXT,\n",
    "        UNIQUE (unique_id, model)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_forecasts(\n",
    "    db_path: str,\n",
    "    forecasts_df: pd.DataFrame,\n",
    "    run_id: str,\n",
    "    model: str = \"MSTL_ARIMA\",\n",
    ") -> int:\n",
    "    \"\"\"Save forecasts to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        forecasts_df: DataFrame with [unique_id, ds, yhat, yhat_lo_80, ...]\n",
    "        run_id: Pipeline run identifier\n",
    "        model: Model name\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    created_at = datetime.utcnow().isoformat()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in forecasts_df.iterrows():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        parts = unique_id.split(\"_\")\n",
    "        region = parts[0] if len(parts) > 0 else \"\"\n",
    "        fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        rows.append((\n",
    "            run_id,\n",
    "            created_at,\n",
    "            unique_id,\n",
    "            region,\n",
    "            fuel_type,\n",
    "            str(row[\"ds\"]),\n",
    "            model,\n",
    "            row.get(\"yhat\"),\n",
    "            row.get(\"yhat_lo_80\"),\n",
    "            row.get(\"yhat_hi_80\"),\n",
    "            row.get(\"yhat_lo_95\"),\n",
    "            row.get(\"yhat_hi_95\"),\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(\"\"\"\n",
    "        INSERT OR REPLACE INTO renewable_forecasts\n",
    "        (run_id, created_at, unique_id, region, fuel_type, ds, model,\n",
    "         yhat, yhat_lo_80, yhat_hi_80, yhat_lo_95, yhat_hi_95)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_weather(\n",
    "    db_path: str,\n",
    "    weather_df: pd.DataFrame,\n",
    "    is_forecast: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"Save weather features to database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        weather_df: DataFrame with [ds, region, weather_vars...]\n",
    "        is_forecast: True if this is forecast weather data\n",
    "\n",
    "    Returns:\n",
    "        Number of rows inserted\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temperature_2m\", \"wind_speed_10m\", \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\", \"direct_radiation\", \"diffuse_radiation\", \"cloud_cover\"\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in weather_df.iterrows():\n",
    "        values = [row.get(col) for col in weather_cols]\n",
    "        rows.append((\n",
    "            row[\"region\"],\n",
    "            str(row[\"ds\"]),\n",
    "            *values,\n",
    "            1 if is_forecast else 0,\n",
    "        ))\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.executemany(f\"\"\"\n",
    "        INSERT OR REPLACE INTO weather_features\n",
    "        (region, ds, {', '.join(weather_cols)}, is_forecast)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "    return len(rows)\n",
    "\n",
    "\n",
    "def save_drift_alert(\n",
    "    db_path: str,\n",
    "    run_id: str,\n",
    "    unique_id: str,\n",
    "    current_rmse: float,\n",
    "    threshold_rmse: float,\n",
    "    severity: str = \"warning\",\n",
    "    metadata: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Save drift detection alert.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        run_id: Pipeline run identifier\n",
    "        unique_id: Series identifier\n",
    "        current_rmse: Current RMSE value\n",
    "        threshold_rmse: Drift threshold\n",
    "        severity: Alert severity (info, warning, critical)\n",
    "        metadata: Additional metadata\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    parts = unique_id.split(\"_\")\n",
    "    region = parts[0] if len(parts) > 0 else \"\"\n",
    "    fuel_type = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    alert_type = \"drift_detected\" if current_rmse > threshold_rmse else \"drift_check\"\n",
    "    message = (\n",
    "        f\"RMSE {current_rmse:.1f} {'>' if current_rmse > threshold_rmse else '<='} \"\n",
    "        f\"threshold {threshold_rmse:.1f}\"\n",
    "    )\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO drift_alerts\n",
    "        (alert_at, run_id, unique_id, region, fuel_type, alert_type, severity,\n",
    "         current_rmse, threshold_rmse, message, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        run_id,\n",
    "        unique_id,\n",
    "        region,\n",
    "        fuel_type,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        current_rmse,\n",
    "        threshold_rmse,\n",
    "        message,\n",
    "        json.dumps(metadata) if metadata else None,\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_baseline(\n",
    "    db_path: str,\n",
    "    unique_id: str,\n",
    "    model: str,\n",
    "    baseline: dict,\n",
    ") -> None:\n",
    "    \"\"\"Save baseline metrics for drift detection.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        unique_id: Series identifier\n",
    "        model: Model name\n",
    "        baseline: Baseline metrics dictionary\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO baseline_metrics\n",
    "        (created_at, unique_id, model, rmse_mean, rmse_std, mae_mean, mae_std,\n",
    "         drift_threshold_rmse, drift_threshold_mae, n_windows, metadata_json)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        datetime.utcnow().isoformat(),\n",
    "        unique_id,\n",
    "        model,\n",
    "        baseline.get(\"rmse_mean\"),\n",
    "        baseline.get(\"rmse_std\"),\n",
    "        baseline.get(\"mae_mean\"),\n",
    "        baseline.get(\"mae_std\"),\n",
    "        baseline.get(\"drift_threshold_rmse\"),\n",
    "        baseline.get(\"drift_threshold_mae\"),\n",
    "        baseline.get(\"n_windows\"),\n",
    "        json.dumps(baseline),\n",
    "    ))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def get_recent_forecasts(\n",
    "    db_path: str,\n",
    "    region: Optional[str] = None,\n",
    "    fuel_type: Optional[str] = None,\n",
    "    hours: int = 48,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent forecasts from database.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        region: Filter by region (optional)\n",
    "        fuel_type: Filter by fuel type (optional)\n",
    "        hours: Hours of history to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with forecasts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM renewable_forecasts\n",
    "        WHERE datetime(created_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if region:\n",
    "        query += \" AND region = ?\"\n",
    "        params.append(region)\n",
    "\n",
    "    if fuel_type:\n",
    "        query += \" AND fuel_type = ?\"\n",
    "        params.append(fuel_type)\n",
    "\n",
    "    query += \" ORDER BY ds DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_drift_alerts(\n",
    "    db_path: str,\n",
    "    hours: int = 24,\n",
    "    severity: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get recent drift alerts.\n",
    "\n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        hours: Hours of history\n",
    "        severity: Filter by severity (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with alerts\n",
    "    \"\"\"\n",
    "    con = connect(db_path)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM drift_alerts\n",
    "        WHERE datetime(alert_at) > datetime('now', ?)\n",
    "    \"\"\"\n",
    "    params = [f\"-{hours} hours\"]\n",
    "\n",
    "    if severity:\n",
    "        query += \" AND severity = ?\"\n",
    "        params.append(severity)\n",
    "\n",
    "    query += \" ORDER BY alert_at DESC\"\n",
    "\n",
    "    df = pd.read_sql_query(query, con, params=params)\n",
    "    con.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test database initialization\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        db_path = f\"{tmpdir}/test_renewable.db\"\n",
    "\n",
    "        print(\"Initializing database...\")\n",
    "        init_renewable_db(db_path)\n",
    "\n",
    "        print(\"Database initialized successfully!\")\n",
    "\n",
    "        # Test connection\n",
    "        con = connect(db_path)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = cur.fetchall()\n",
    "        print(f\"Tables created: {[t[0] for t in tables]}\")\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8: Dashboard\n",
    "\n",
    "**File:** `src/renewable/dashboard.py`\n",
    "\n",
    "The Streamlit dashboard provides:\n",
    "- **Forecast visualization** with prediction intervals\n",
    "- **Drift monitoring** and alerts\n",
    "- **Coverage analysis** (nominal vs empirical)\n",
    "- **Weather features** by region\n",
    "\n",
    "## Running the Dashboard\n",
    "\n",
    "```bash\n",
    "streamlit run src/renewable/dashboard.py\n",
    "```\n",
    "\n",
    "The dashboard will:\n",
    "1. Load forecasts from `data/renewable/forecasts.parquet`\n",
    "2. Display interactive charts with Plotly\n",
    "3. Show drift alerts from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dashboard.py\n",
    "# file: src/renewable/dashboard.py\n",
    "\"\"\"Streamlit dashboard for renewable energy forecasting.\n",
    "\n",
    "Provides:\n",
    "- Forecast visualization with prediction intervals\n",
    "- Drift monitoring and alerts\n",
    "- Coverage analysis (nominal vs empirical)\n",
    "- Weather features by region\n",
    "\n",
    "Run with:\n",
    "    streamlit run src/renewable/dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from src.renewable.db import (\n",
    "    connect,\n",
    "    get_drift_alerts,\n",
    "    get_recent_forecasts,\n",
    "    init_renewable_db,\n",
    ")\n",
    "from src.renewable.regions import FUEL_TYPES, REGIONS\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Renewable Forecast Dashboard\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main dashboard application.\"\"\"\n",
    "    st.title(\"⚡ Renewable Energy Forecast Dashboard\")\n",
    "    st.markdown(\"Next-24h wind/solar generation forecasts with drift monitoring\")\n",
    "\n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "\n",
    "        db_path = st.text_input(\n",
    "            \"Database Path\",\n",
    "            value=\"data/renewable/renewable.db\",\n",
    "        )\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not Path(db_path).exists():\n",
    "            init_renewable_db(db_path)\n",
    "            st.info(\"Database initialized\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Region filter\n",
    "        all_regions = list(REGIONS.keys())\n",
    "        selected_regions = st.multiselect(\n",
    "            \"Regions\",\n",
    "            options=all_regions,\n",
    "            default=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        )\n",
    "\n",
    "        # Fuel type filter\n",
    "        fuel_type = st.selectbox(\n",
    "            \"Fuel Type\",\n",
    "            options=[\"WND\", \"SUN\", \"Both\"],\n",
    "            index=0,\n",
    "        )\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # Actions\n",
    "        show_debug = st.checkbox(\"Show Debug\", value=False)\n",
    "        if st.button(\"🔄 Refresh Data\", width=\"stretch\"):\n",
    "            st.rerun()\n",
    "\n",
    "        if st.button(\"📊 Run Pipeline\", width=\"stretch\"):\n",
    "            run_pipeline_from_dashboard(db_path, selected_regions, fuel_type)\n",
    "\n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
    "        \"📈 Forecasts\",\n",
    "        \"⚠️ Drift Monitor\",\n",
    "        \"📊 Coverage\",\n",
    "        \"🌤️ Weather\",\n",
    "        \"🔍 Interpretability\",\n",
    "    ])\n",
    "\n",
    "    with tab1:\n",
    "        render_forecasts_tab(db_path, selected_regions, fuel_type, show_debug=show_debug)\n",
    "\n",
    "    with tab2:\n",
    "        render_drift_tab(db_path)\n",
    "\n",
    "    with tab3:\n",
    "        render_coverage_tab(db_path)\n",
    "\n",
    "    with tab4:\n",
    "        render_weather_tab(db_path, selected_regions)\n",
    "\n",
    "    with tab5:\n",
    "        render_interpretability_tab(selected_regions, fuel_type)\n",
    "\n",
    "\n",
    "def render_forecasts_tab(db_path: str, regions: list, fuel_type: str, *, show_debug: bool = False):\n",
    "    \"\"\"Render forecast visualization with prediction intervals.\"\"\"\n",
    "    st.subheader(\"Generation Forecasts\")\n",
    "\n",
    "    forecasts_df = pd.DataFrame()\n",
    "    data_source = \"none\"\n",
    "    derived_columns: list[str] = []\n",
    "\n",
    "    # Try to load from parquet file first (pipeline output)\n",
    "    parquet_path = Path(\"data/renewable/forecasts.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            forecasts_df = pd.read_parquet(parquet_path)\n",
    "            data_source = f\"parquet:{parquet_path}\"\n",
    "            # Add region/fuel_type columns if missing\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                parts = forecasts_df[\"unique_id\"].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "                if \"region\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"region\"] = parts[0]\n",
    "                    derived_columns.append(\"region\")\n",
    "                if \"fuel_type\" not in forecasts_df.columns:\n",
    "                    forecasts_df[\"fuel_type\"] = parts[1] if parts.shape[1] > 1 else pd.NA\n",
    "                    derived_columns.append(\"fuel_type\")\n",
    "            st.success(f\"Loaded {len(forecasts_df)} forecasts from pipeline\")\n",
    "\n",
    "            # Calculate and display data freshness\n",
    "            if not forecasts_df.empty and \"ds\" in forecasts_df.columns:\n",
    "                earliest_forecast_ts = forecasts_df[\"ds\"].min()\n",
    "                now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
    "\n",
    "                # Forecasts start from last_data + 1h, so last_data = earliest_forecast - 1h\n",
    "                last_data_ts = earliest_forecast_ts - pd.Timedelta(hours=1)\n",
    "\n",
    "                # Ensure both timestamps are timezone-aware for comparison\n",
    "                if not hasattr(last_data_ts, 'tz') or last_data_ts.tz is None:\n",
    "                    last_data_ts = pd.Timestamp(last_data_ts, tz=\"UTC\")\n",
    "\n",
    "                data_age_hours = (now_utc - last_data_ts).total_seconds() / 3600\n",
    "\n",
    "                # Show warning if data is > 6 hours old\n",
    "                if data_age_hours > 6:\n",
    "                    st.warning(\n",
    "                        f\"⚠️ Forecasts are based on **{data_age_hours:.1f} hour old** data \"\n",
    "                        f\"(last EIA data: {last_data_ts.strftime('%b %d %H:%M')} UTC). \"\n",
    "                        f\"Click 'Refresh Forecasts' button in sidebar to update.\"\n",
    "                    )\n",
    "                else:\n",
    "                    st.info(\n",
    "                        f\"✅ Forecasts from {last_data_ts.strftime('%b %d %H:%M')} UTC data \"\n",
    "                        f\"({data_age_hours:.1f}h old)\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load parquet: {e}\")\n",
    "\n",
    "    # Fall back to database\n",
    "    if forecasts_df.empty:\n",
    "        try:\n",
    "            forecasts_df = get_recent_forecasts(db_path, hours=72)\n",
    "            data_source = f\"db:{db_path}\"\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load from database: {e}\")\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        # Show demo data\n",
    "        st.info(\"No forecasts found. Showing demo data.\")\n",
    "        forecasts_df = generate_demo_forecasts(regions, fuel_type)\n",
    "        data_source = \"demo\"\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Forecast Data\", expanded=False):\n",
    "            st.markdown(\"**Source**\")\n",
    "            st.code(data_source)\n",
    "            st.markdown(\"**Columns**\")\n",
    "            st.code(\", \".join(forecasts_df.columns.tolist()))\n",
    "\n",
    "            st.markdown(\"**Counts (pre-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "\n",
    "            if derived_columns:\n",
    "                st.markdown(\"**Derived Columns**\")\n",
    "                st.write(derived_columns)\n",
    "\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id sample**\")\n",
    "                st.write(forecasts_df[\"unique_id\"].dropna().astype(str).head(10).tolist())\n",
    "\n",
    "            if \"fuel_type\" in forecasts_df.columns:\n",
    "                st.markdown(\"**fuel_type counts**\")\n",
    "                st.dataframe(forecasts_df[\"fuel_type\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "                unknown = sorted(\n",
    "                    {str(v) for v in forecasts_df[\"fuel_type\"].dropna().unique()}\n",
    "                    - set(FUEL_TYPES.keys())\n",
    "                )\n",
    "                if unknown:\n",
    "                    st.warning(f\"Unknown fuel_type values: {unknown}\")\n",
    "\n",
    "            if \"region\" in forecasts_df.columns:\n",
    "                st.markdown(\"**region counts**\")\n",
    "                st.dataframe(forecasts_df[\"region\"].value_counts(dropna=False).to_frame())\n",
    "\n",
    "    # Filter by selections\n",
    "    if fuel_type != \"Both\":\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"fuel_type\"] == fuel_type]\n",
    "\n",
    "    if regions:\n",
    "        forecasts_df = forecasts_df[forecasts_df[\"region\"].isin(regions)]\n",
    "\n",
    "    if show_debug:\n",
    "        with st.expander(\"Debug: Filter Result\", expanded=False):\n",
    "            st.markdown(\"**Applied Filters**\")\n",
    "            st.write({\"fuel_type\": fuel_type, \"regions\": regions})\n",
    "            st.markdown(\"**Counts (post-filter)**\")\n",
    "            st.write({\"rows\": int(len(forecasts_df))})\n",
    "            if \"unique_id\" in forecasts_df.columns:\n",
    "                st.markdown(\"**unique_id after filter**\")\n",
    "                st.write(sorted(forecasts_df[\"unique_id\"].dropna().astype(str).unique().tolist()))\n",
    "\n",
    "    if forecasts_df.empty:\n",
    "        st.warning(\"No data matching filters\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    series_options = forecasts_df[\"unique_id\"].unique().tolist()\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=series_options,\n",
    "        index=0 if series_options else None,\n",
    "        key=\"forecast_series_select\",\n",
    "    )\n",
    "\n",
    "    if selected_series:\n",
    "        series_data = forecasts_df[forecasts_df[\"unique_id\"] == selected_series].copy()\n",
    "        series_data = series_data.sort_values(\"ds\")\n",
    "\n",
    "        # Convert to local timezone for display\n",
    "        region_code = series_data[\"unique_id\"].iloc[0].split(\"_\")[0]\n",
    "        region_info = REGIONS.get(region_code)\n",
    "        timezone_name = region_info.timezone if region_info else \"UTC\"\n",
    "\n",
    "        # Create forecast plot with intervals\n",
    "        fig = create_forecast_plot(series_data, selected_series, timezone_name)\n",
    "        st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "        # Show data table\n",
    "        with st.expander(\"View Data\"):\n",
    "            st.dataframe(\n",
    "                series_data[[\"ds\", \"yhat\", \"yhat_lo_80\", \"yhat_hi_80\", \"yhat_lo_95\", \"yhat_hi_95\"]],\n",
    "                width=\"stretch\",\n",
    "            )\n",
    "\n",
    "\n",
    "def create_forecast_plot(df: pd.DataFrame, title: str, timezone_name: str = \"UTC\") -> go.Figure:\n",
    "    \"\"\"Create Plotly figure with forecast and prediction intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Forecast dataframe with ds (timestamp), yhat, and interval columns\n",
    "        title: Series name for chart title\n",
    "        timezone_name: IANA timezone name for display (e.g., \"America/Chicago\")\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert timestamps to local timezone for display\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "\n",
    "    # Convert UTC to local timezone\n",
    "    if timezone_name != \"UTC\":\n",
    "        df[\"ds\"] = df[\"ds\"].dt.tz_localize(\"UTC\").dt.tz_convert(timezone_name)\n",
    "\n",
    "    # Get timezone abbreviation for display (e.g., \"CST\", \"PST\")\n",
    "    if timezone_name != \"UTC\" and len(df) > 0:\n",
    "        tz_abbr = df[\"ds\"].iloc[0].strftime(\"%Z\")\n",
    "    else:\n",
    "        tz_abbr = \"UTC\"\n",
    "\n",
    "    # 95% interval (outer, lighter)\n",
    "    if \"yhat_lo_95\" in df.columns and \"yhat_hi_95\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_95\"], df[\"yhat_lo_95\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"95% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # 80% interval (inner, darker)\n",
    "    if \"yhat_lo_80\" in df.columns and \"yhat_hi_80\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([df[\"ds\"], df[\"ds\"][::-1]]),\n",
    "            y=pd.concat([df[\"yhat_hi_80\"], df[\"yhat_lo_80\"][::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(68, 138, 255, 0.4)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            name=\"80% Interval\",\n",
    "            hoverinfo=\"skip\",\n",
    "        ))\n",
    "\n",
    "    # Point forecast\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"ds\"],\n",
    "        y=df[\"yhat\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Forecast\",\n",
    "        line=dict(color=\"#1f77b4\", width=2),\n",
    "    ))\n",
    "\n",
    "    # Actuals if available\n",
    "    if \"y\" in df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Actual\",\n",
    "            marker=dict(color=\"#2ca02c\", size=6),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast: {title}\",\n",
    "        xaxis_title=f\"Time ({tz_abbr})\",\n",
    "        yaxis_title=\"Generation (MWh)\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "        height=450,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def render_drift_tab(db_path: str):\n",
    "    \"\"\"Render drift monitoring and alerts.\"\"\"\n",
    "    st.subheader(\"Drift Detection\")\n",
    "\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Try to load alerts\n",
    "    try:\n",
    "        alerts_df = get_drift_alerts(db_path, hours=48)\n",
    "    except Exception:\n",
    "        alerts_df = pd.DataFrame()\n",
    "\n",
    "    # Summary metrics\n",
    "    with col1:\n",
    "        critical = len(alerts_df[alerts_df[\"severity\"] == \"critical\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\n",
    "            \"Critical Alerts\",\n",
    "            critical,\n",
    "            delta=None,\n",
    "            delta_color=\"inverse\" if critical > 0 else \"off\",\n",
    "        )\n",
    "\n",
    "    with col2:\n",
    "        warning = len(alerts_df[alerts_df[\"severity\"] == \"warning\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Warnings\", warning)\n",
    "\n",
    "    with col3:\n",
    "        stable = len(alerts_df[alerts_df[\"alert_type\"] == \"drift_check\"]) if not alerts_df.empty else 0\n",
    "        st.metric(\"Stable Checks\", stable)\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    if alerts_df.empty:\n",
    "        st.info(\"No drift alerts in the last 48 hours. System is stable.\")\n",
    "\n",
    "        # Show demo drift status\n",
    "        st.markdown(\"### Demo Drift Status\")\n",
    "        demo_drift = pd.DataFrame({\n",
    "            \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "            \"Current RMSE\": [125.3, 98.7, 156.2, 45.1, 67.8],\n",
    "            \"Threshold\": [150.0, 120.0, 180.0, 60.0, 80.0],\n",
    "            \"Status\": [\"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\", \"✅ Stable\"],\n",
    "        })\n",
    "        st.dataframe(demo_drift, width=\"stretch\")\n",
    "    else:\n",
    "        # Show alerts table\n",
    "        st.dataframe(\n",
    "            alerts_df[[\"alert_at\", \"unique_id\", \"severity\", \"current_rmse\", \"threshold_rmse\", \"message\"]],\n",
    "            width=\"stretch\",\n",
    "        )\n",
    "\n",
    "        # Drift timeline\n",
    "        if len(alerts_df) > 1:\n",
    "            alerts_df[\"alert_at\"] = pd.to_datetime(alerts_df[\"alert_at\"])\n",
    "            fig = px.scatter(\n",
    "                alerts_df,\n",
    "                x=\"alert_at\",\n",
    "                y=\"current_rmse\",\n",
    "                color=\"severity\",\n",
    "                size=\"current_rmse\",\n",
    "                hover_data=[\"unique_id\", \"message\"],\n",
    "                title=\"Drift Timeline\",\n",
    "            )\n",
    "            fig.add_hline(\n",
    "                y=alerts_df[\"threshold_rmse\"].mean(),\n",
    "                line_dash=\"dash\",\n",
    "                annotation_text=\"Avg Threshold\",\n",
    "            )\n",
    "            st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_coverage_tab(db_path: str):\n",
    "    \"\"\"Render coverage analysis comparing nominal vs empirical.\"\"\"\n",
    "    st.subheader(\"Prediction Interval Coverage\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    **Coverage** measures how often actual values fall within prediction intervals.\n",
    "    - **Nominal**: The expected coverage (80% or 95%)\n",
    "    - **Empirical**: The actual observed coverage\n",
    "    - **Gap**: Difference indicates calibration quality\n",
    "    \"\"\")\n",
    "\n",
    "    # Demo coverage data\n",
    "    coverage_data = pd.DataFrame({\n",
    "        \"Series\": [\"CALI_WND\", \"ERCO_WND\", \"MISO_WND\", \"SWPP_WND\", \"CALI_SUN\", \"ERCO_SUN\"],\n",
    "        \"Nominal 80%\": [80, 80, 80, 80, 80, 80],\n",
    "        \"Empirical 80%\": [78.5, 82.1, 76.3, 79.8, 81.2, 77.9],\n",
    "        \"Nominal 95%\": [95, 95, 95, 95, 95, 95],\n",
    "        \"Empirical 95%\": [93.2, 96.1, 91.5, 94.8, 95.7, 92.3],\n",
    "    })\n",
    "\n",
    "    coverage_data[\"Gap 80%\"] = coverage_data[\"Empirical 80%\"] - coverage_data[\"Nominal 80%\"]\n",
    "    coverage_data[\"Gap 95%\"] = coverage_data[\"Empirical 95%\"] - coverage_data[\"Nominal 95%\"]\n",
    "\n",
    "    # Summary\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        avg_80 = coverage_data[\"Empirical 80%\"].mean()\n",
    "        st.metric(\"Avg 80% Coverage\", f\"{avg_80:.1f}%\", f\"{avg_80 - 80:.1f}%\")\n",
    "\n",
    "    with col2:\n",
    "        avg_95 = coverage_data[\"Empirical 95%\"].mean()\n",
    "        st.metric(\"Avg 95% Coverage\", f\"{avg_95:.1f}%\", f\"{avg_95 - 95:.1f}%\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Coverage comparison chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"80% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 80%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.7)\",\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=\"95% Empirical\",\n",
    "        x=coverage_data[\"Series\"],\n",
    "        y=coverage_data[\"Empirical 95%\"],\n",
    "        marker_color=\"rgba(68, 138, 255, 0.4)\",\n",
    "    ))\n",
    "\n",
    "    # Nominal lines\n",
    "    fig.add_hline(y=80, line_dash=\"dash\", line_color=\"red\", annotation_text=\"80% Nominal\")\n",
    "    fig.add_hline(y=95, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"95% Nominal\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Coverage by Series\",\n",
    "        xaxis_title=\"Series\",\n",
    "        yaxis_title=\"Coverage (%)\",\n",
    "        barmode=\"group\",\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Detailed table\n",
    "    with st.expander(\"View Coverage Data\"):\n",
    "        st.dataframe(coverage_data, width=\"stretch\")\n",
    "\n",
    "\n",
    "def render_weather_tab(db_path: str, regions: list):\n",
    "    \"\"\"Render weather features visualization.\"\"\"\n",
    "    st.subheader(\"Weather Features\")\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "    # Prefer real pipeline output; no demo fallback.\n",
    "    parquet_path = Path(\"data/renewable/weather.parquet\")\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            weather_df = pd.read_parquet(parquet_path)\n",
    "            st.success(f\"Loaded {len(weather_df)} weather rows from pipeline\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather parquet: {exc}\")\n",
    "\n",
    "    if weather_df.empty and Path(db_path).exists():\n",
    "        try:\n",
    "            with connect(db_path) as con:\n",
    "                weather_df = pd.read_sql_query(\n",
    "                    \"SELECT * FROM weather_features ORDER BY ds ASC\",\n",
    "                    con,\n",
    "                )\n",
    "            if not weather_df.empty:\n",
    "                st.success(f\"Loaded {len(weather_df)} weather rows from database\")\n",
    "        except Exception as exc:\n",
    "            st.warning(f\"Could not load weather data from database: {exc}\")\n",
    "\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data available. Run the pipeline to populate weather features.\")\n",
    "        return\n",
    "\n",
    "    weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"], errors=\"coerce\")\n",
    "    if regions:\n",
    "        weather_df = weather_df[weather_df[\"region\"].isin(regions)]\n",
    "    if weather_df.empty:\n",
    "        st.warning(\"No weather data matching selected regions.\")\n",
    "        return\n",
    "\n",
    "    # Variable selector\n",
    "    weather_vars = [\n",
    "        col for col in [\"wind_speed_10m\", \"wind_speed_100m\", \"direct_radiation\", \"cloud_cover\"]\n",
    "        if col in weather_df.columns\n",
    "    ]\n",
    "    if not weather_vars:\n",
    "        st.warning(\"Weather data missing expected variables.\")\n",
    "        return\n",
    "    selected_var = st.selectbox(\"Weather Variable\", options=weather_vars)\n",
    "\n",
    "    # Plot\n",
    "    fig = px.line(\n",
    "        weather_df,\n",
    "        x=\"ds\",\n",
    "        y=selected_var,\n",
    "        color=\"region\",\n",
    "        title=f\"{selected_var} by Region\",\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "    # Summary stats\n",
    "    st.markdown(\"### Current Conditions\")\n",
    "\n",
    "    cols = st.columns(len(regions[:4]))\n",
    "    for i, region in enumerate(regions[:4]):\n",
    "        if i < len(cols):\n",
    "            with cols[i]:\n",
    "                region_data = weather_df[weather_df[\"region\"] == region].iloc[-1] if len(weather_df[weather_df[\"region\"] == region]) > 0 else {}\n",
    "                st.metric(\n",
    "                    region,\n",
    "                    f\"{region_data.get('wind_speed_10m', 0):.1f} m/s\",\n",
    "                    help=\"Wind speed at 10m\",\n",
    "                )\n",
    "\n",
    "\n",
    "def render_interpretability_tab(regions: list, fuel_type: str):\n",
    "    \"\"\"Render model interpretability visualizations (SHAP, feature importance, PDP).\"\"\"\n",
    "    st.subheader(\"Model Interpretability\")\n",
    "\n",
    "    # Model Leaderboard Section\n",
    "    st.markdown(\"### 🏆 Model Leaderboard (Cross-Validation)\")\n",
    "\n",
    "    # Model descriptions for education\n",
    "    MODEL_INFO = {\n",
    "        \"AutoARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Auto-tuned ARIMA with automatic p,d,q selection. Good for univariate series with trend/seasonality.\",\n",
    "            \"strengths\": \"Robust, well-understood, good prediction intervals\",\n",
    "        },\n",
    "        \"MSTL_ARIMA\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Multiple Seasonal-Trend decomposition + ARIMA. Handles daily (24h) and weekly (168h) seasonality.\",\n",
    "            \"strengths\": \"Best for multi-seasonal patterns like energy data\",\n",
    "        },\n",
    "        \"AutoETS\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Exponential smoothing with automatic error/trend/season selection.\",\n",
    "            \"strengths\": \"Simple, fast, works well for smooth series\",\n",
    "        },\n",
    "        \"AutoTheta\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Theta method with automatic decomposition. Robust to outliers.\",\n",
    "            \"strengths\": \"Competition winner (M3), handles level shifts\",\n",
    "        },\n",
    "        \"CES\": {\n",
    "            \"type\": \"Statistical\",\n",
    "            \"description\": \"Complex Exponential Smoothing. Captures complex seasonal patterns.\",\n",
    "            \"strengths\": \"Good for complex seasonality\",\n",
    "        },\n",
    "        \"SeasonalNaive\": {\n",
    "            \"type\": \"Baseline\",\n",
    "            \"description\": \"Uses value from same hour last week. Baseline benchmark.\",\n",
    "            \"strengths\": \"Simple benchmark - if beaten, models add value\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "    if run_log_path.exists():\n",
    "        try:\n",
    "            import json\n",
    "            run_log = json.loads(run_log_path.read_text())\n",
    "            pipeline_results = run_log.get(\"pipeline_results\", {})\n",
    "            leaderboard_data = pipeline_results.get(\"leaderboard\", [])\n",
    "\n",
    "            if leaderboard_data:\n",
    "                leaderboard_df = pd.DataFrame(leaderboard_data)\n",
    "                best_model = pipeline_results.get(\"best_model\", \"\")\n",
    "                best_rmse = pipeline_results.get(\"best_rmse\", 0)\n",
    "\n",
    "                # Key metrics row\n",
    "                col1, col2, col3, col4 = st.columns(4)\n",
    "                with col1:\n",
    "                    st.metric(\"Best Model\", best_model)\n",
    "                with col2:\n",
    "                    st.metric(\"Best RMSE\", f\"{best_rmse:.3f}\")\n",
    "                with col3:\n",
    "                    st.metric(\"Models Evaluated\", len(leaderboard_data))\n",
    "                with col4:\n",
    "                    # Calculate improvement over baseline\n",
    "                    baseline_rmse = leaderboard_df[leaderboard_df[\"model\"] == \"SeasonalNaive\"][\"rmse\"].values\n",
    "                    if len(baseline_rmse) > 0 and best_rmse > 0:\n",
    "                        improvement = ((baseline_rmse[0] - best_rmse) / baseline_rmse[0]) * 100\n",
    "                        st.metric(\"vs Baseline\", f\"{improvement:+.1f}%\", help=\"Improvement over SeasonalNaive\")\n",
    "                    else:\n",
    "                        st.metric(\"vs Baseline\", \"N/A\")\n",
    "\n",
    "                # Selection rationale\n",
    "                st.markdown(\"#### Why This Model?\")\n",
    "                st.info(f\"\"\"\n",
    "                **{best_model}** was selected because it has the **lowest RMSE** on cross-validation.\n",
    "\n",
    "                - **RMSE (Root Mean Square Error)**: Penalizes large errors more heavily. Best for energy forecasting where big misses are costly.\n",
    "                - **Selection method**: Time-series CV with {run_log.get('config', {}).get('cv_windows', 2)} windows, step size {run_log.get('config', {}).get('cv_step_size', 168)}h\n",
    "                - **Horizon**: {run_log.get('config', {}).get('horizon', 24)}h ahead forecasts\n",
    "                \"\"\")\n",
    "\n",
    "                # Model description for winner\n",
    "                if best_model in MODEL_INFO:\n",
    "                    info = MODEL_INFO[best_model]\n",
    "                    st.success(f\"**{info['type']} Model**: {info['description']}\")\n",
    "\n",
    "                # Full leaderboard with visualization\n",
    "                st.markdown(\"#### All Models Ranked by RMSE\")\n",
    "\n",
    "                display_cols = [c for c in [\"model\", \"rmse\", \"mae\", \"mape\", \"coverage_80\", \"coverage_95\"]\n",
    "                               if c in leaderboard_df.columns]\n",
    "\n",
    "                # Create visualization\n",
    "                if \"rmse\" in leaderboard_df.columns:\n",
    "                    fig = px.bar(\n",
    "                        leaderboard_df.sort_values(\"rmse\"),\n",
    "                        x=\"model\",\n",
    "                        y=\"rmse\",\n",
    "                        title=\"Model Comparison (Lower RMSE = Better)\",\n",
    "                        color=\"rmse\",\n",
    "                        color_continuous_scale=\"RdYlGn_r\",\n",
    "                    )\n",
    "                    fig.add_hline(y=best_rmse, line_dash=\"dash\", line_color=\"green\",\n",
    "                                  annotation_text=f\"Best: {best_rmse:.3f}\")\n",
    "                    fig.update_layout(height=350)\n",
    "                    st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                # Format numeric columns for table\n",
    "                styled_df = leaderboard_df[display_cols].copy()\n",
    "                for col in [\"rmse\", \"mae\", \"mape\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "                for col in [\"coverage_80\", \"coverage_95\"]:\n",
    "                    if col in styled_df.columns:\n",
    "                        styled_df[col] = styled_df[col].apply(lambda x: f\"{x:.1f}%\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "                st.dataframe(styled_df, width=\"stretch\", hide_index=True)\n",
    "\n",
    "                # Coverage analysis\n",
    "                if \"coverage_80\" in leaderboard_df.columns:\n",
    "                    st.markdown(\"#### Prediction Interval Coverage\")\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Coverage** measures if prediction intervals are well-calibrated:\n",
    "                    - **80% interval** should contain ~80% of actual values\n",
    "                    - **95% interval** should contain ~95% of actual values\n",
    "                    - **Under-coverage** (<target) = intervals too narrow, overconfident\n",
    "                    - **Over-coverage** (>target) = intervals too wide, conservative\n",
    "                    \"\"\")\n",
    "\n",
    "                    coverage_df = leaderboard_df[[\"model\", \"coverage_80\", \"coverage_95\"]].copy()\n",
    "                    coverage_df[\"coverage_80_status\"] = coverage_df[\"coverage_80\"].apply(\n",
    "                        lambda x: \"Under\" if x < 75 else (\"Over\" if x > 85 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "                    coverage_df[\"coverage_95_status\"] = coverage_df[\"coverage_95\"].apply(\n",
    "                        lambda x: \"Under\" if x < 90 else (\"Over\" if x > 99 else \"Good\") if pd.notna(x) else \"N/A\"\n",
    "                    )\n",
    "\n",
    "                # Model descriptions expander\n",
    "                with st.expander(\"Model Descriptions\"):\n",
    "                    for model_name, info in MODEL_INFO.items():\n",
    "                        st.markdown(f\"**{model_name}** ({info['type']})\")\n",
    "                        st.markdown(f\"- {info['description']}\")\n",
    "                        st.markdown(f\"- *Strengths*: {info['strengths']}\")\n",
    "                        st.markdown(\"---\")\n",
    "\n",
    "                # CV configuration expander\n",
    "                config = run_log.get(\"config\", {})\n",
    "                with st.expander(\"CV Configuration\"):\n",
    "                    st.write({\n",
    "                        \"cv_windows\": config.get(\"cv_windows\"),\n",
    "                        \"cv_step_size\": config.get(\"cv_step_size\"),\n",
    "                        \"horizon\": config.get(\"horizon\"),\n",
    "                        \"regions\": config.get(\"regions\"),\n",
    "                        \"fuel_types\": config.get(\"fuel_types\"),\n",
    "                        \"run_at\": run_log.get(\"run_at_utc\", \"N/A\"),\n",
    "                    })\n",
    "            else:\n",
    "                st.info(\"Leaderboard not available. Run the pipeline with the latest code to generate.\")\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not load leaderboard: {e}\")\n",
    "    else:\n",
    "        st.info(\"No run log found. Run the pipeline to generate model comparison.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    st.markdown(\"### 🔍 Per-Series Interpretability\")\n",
    "    st.markdown(\"\"\"\n",
    "    **LightGBM** models are trained alongside statistical models (MSTL/ARIMA) to provide\n",
    "    interpretability insights. The statistical models generate the primary forecasts,\n",
    "    while LightGBM helps understand feature importance and relationships.\n",
    "    \"\"\")\n",
    "\n",
    "    interp_dir = Path(\"data/renewable/interpretability\")\n",
    "\n",
    "    if not interp_dir.exists():\n",
    "        st.info(\"No interpretability data available. Run the pipeline to generate SHAP and PDP plots.\")\n",
    "        return\n",
    "\n",
    "    # Get available series\n",
    "    series_dirs = sorted([d.name for d in interp_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "    if not series_dirs:\n",
    "        st.warning(\"Interpretability directory exists but contains no series data.\")\n",
    "        return\n",
    "\n",
    "    # Filter by selected regions and fuel type\n",
    "    filtered_series = []\n",
    "    for series_id in series_dirs:\n",
    "        parts = series_id.split(\"_\")\n",
    "        if len(parts) == 2:\n",
    "            region, ft = parts\n",
    "            if regions and region not in regions:\n",
    "                continue\n",
    "            if fuel_type != \"Both\" and ft != fuel_type:\n",
    "                continue\n",
    "            filtered_series.append(series_id)\n",
    "\n",
    "    if not filtered_series:\n",
    "        st.warning(\"No interpretability data for selected filters.\")\n",
    "        return\n",
    "\n",
    "    # Series selector\n",
    "    selected_series = st.selectbox(\n",
    "        \"Select Series\",\n",
    "        options=filtered_series,\n",
    "        index=0,\n",
    "        key=\"interpretability_series_select\",\n",
    "    )\n",
    "\n",
    "    if not selected_series:\n",
    "        return\n",
    "\n",
    "    series_dir = interp_dir / selected_series\n",
    "\n",
    "    # Layout: Feature Importance + SHAP Summary side by side\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"### Feature Importance\")\n",
    "        importance_path = series_dir / \"feature_importance.csv\"\n",
    "        if importance_path.exists():\n",
    "            try:\n",
    "                importance_df = pd.read_csv(importance_path)\n",
    "                # Show top 15 features\n",
    "                top_features = importance_df.head(15)\n",
    "\n",
    "                # Create bar chart\n",
    "                fig = px.bar(\n",
    "                    top_features,\n",
    "                    x=\"importance\",\n",
    "                    y=\"feature\",\n",
    "                    orientation=\"h\",\n",
    "                    title=f\"Top Features: {selected_series}\",\n",
    "                    labels={\"importance\": \"Importance\", \"feature\": \"Feature\"},\n",
    "                )\n",
    "                fig.update_layout(yaxis=dict(autorange=\"reversed\"), height=400)\n",
    "                st.plotly_chart(fig, width=\"stretch\")\n",
    "\n",
    "                with st.expander(\"Full Feature List\"):\n",
    "                    st.dataframe(importance_df, width=\"stretch\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error loading feature importance: {e}\")\n",
    "        else:\n",
    "            st.info(\"Feature importance not available.\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"### SHAP Summary\")\n",
    "        shap_summary_path = series_dir / \"shap_summary.png\"\n",
    "        if shap_summary_path.exists():\n",
    "            st.image(str(shap_summary_path), width=\"stretch\")\n",
    "        else:\n",
    "            # Try bar plot as fallback\n",
    "            shap_bar_path = series_dir / \"shap_bar.png\"\n",
    "            if shap_bar_path.exists():\n",
    "                st.image(str(shap_bar_path), width=\"stretch\")\n",
    "            else:\n",
    "                st.info(\"SHAP summary not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # SHAP Dependence Plots\n",
    "    st.markdown(\"### SHAP Dependence Plots\")\n",
    "    st.markdown(\"Shows how individual feature values affect predictions.\")\n",
    "\n",
    "    shap_dep_files = list(series_dir.glob(\"shap_dependence_*.png\"))\n",
    "    if shap_dep_files:\n",
    "        # Create columns for dependence plots\n",
    "        n_cols = min(3, len(shap_dep_files))\n",
    "        cols = st.columns(n_cols)\n",
    "\n",
    "        for i, dep_file in enumerate(shap_dep_files[:6]):  # Limit to 6 plots\n",
    "            feature_name = dep_file.stem.replace(\"shap_dependence_\", \"\")\n",
    "            with cols[i % n_cols]:\n",
    "                st.markdown(f\"**{feature_name}**\")\n",
    "                st.image(str(dep_file), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"SHAP dependence plots not available.\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    # Partial Dependence Plot\n",
    "    st.markdown(\"### Partial Dependence Plot\")\n",
    "    st.markdown(\"Shows the average effect of features on predictions (marginal effect).\")\n",
    "\n",
    "    pdp_path = series_dir / \"partial_dependence.png\"\n",
    "    if pdp_path.exists():\n",
    "        st.image(str(pdp_path), width=\"stretch\")\n",
    "    else:\n",
    "        st.info(\"Partial dependence plot not available.\")\n",
    "\n",
    "    # Waterfall plot for sample prediction\n",
    "    waterfall_path = series_dir / \"shap_waterfall_sample.png\"\n",
    "    if waterfall_path.exists():\n",
    "        st.markdown(\"### Sample Prediction Explanation\")\n",
    "        st.markdown(\"SHAP waterfall showing how features contributed to a single prediction.\")\n",
    "        st.image(str(waterfall_path), width=\"stretch\")\n",
    "\n",
    "\n",
    "def generate_demo_forecasts(regions: list, fuel_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate demo forecast data for display.\"\"\"\n",
    "    data = []\n",
    "    base_time = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    fuel_types = [fuel_type] if fuel_type != \"Both\" else [\"WND\", \"SUN\"]\n",
    "\n",
    "    for region in regions[:3]:\n",
    "        for ft in fuel_types:\n",
    "            unique_id = f\"{region}_{ft}\"\n",
    "            base_value = 500 if ft == \"WND\" else 300\n",
    "\n",
    "            for h in range(24):\n",
    "                ds = base_time + timedelta(hours=h)\n",
    "\n",
    "                # Add daily pattern\n",
    "                if ft == \"SUN\":\n",
    "                    hour_factor = max(0, np.sin((ds.hour - 6) * np.pi / 12)) if 6 < ds.hour < 18 else 0\n",
    "                    yhat = base_value * hour_factor + np.random.normal(0, 20)\n",
    "                else:\n",
    "                    yhat = base_value + np.sin(ds.hour * np.pi / 12) * 100 + np.random.normal(0, 30)\n",
    "\n",
    "                yhat = max(0, yhat)\n",
    "\n",
    "                data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"region\": region,\n",
    "                    \"fuel_type\": ft,\n",
    "                    \"ds\": ds,\n",
    "                    \"yhat\": yhat,\n",
    "                    \"yhat_lo_80\": yhat * 0.85,\n",
    "                    \"yhat_hi_80\": yhat * 1.15,\n",
    "                    \"yhat_lo_95\": yhat * 0.75,\n",
    "                    \"yhat_hi_95\": yhat * 1.25,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def run_pipeline_from_dashboard(db_path: str, regions: list, fuel_type: str):\n",
    "    \"\"\"Run the forecasting pipeline from the dashboard.\"\"\"\n",
    "    with st.spinner(\"Refreshing forecasts... (may take 2-3 minutes)\"):\n",
    "        try:\n",
    "            from src.renewable.jobs import run_hourly\n",
    "\n",
    "            # Run the hourly pipeline job\n",
    "            run_hourly.main()\n",
    "\n",
    "            st.success(\"Pipeline completed! Forecasts have been updated with latest EIA data.\")\n",
    "            st.info(\"Reloading page to show new forecasts...\")\n",
    "\n",
    "            # Wait a moment then reload\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            st.rerun()\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Pipeline failed: {e}\")\n",
    "            import traceback\n",
    "            with st.expander(\"Error details\"):\n",
    "                st.code(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow integration \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/data_freshness.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/data_freshness.py\n",
    "# src/renewable/data_freshness.py\n",
    "\"\"\"\n",
    "Lightweight EIA data freshness checking.\n",
    "\n",
    "This module provides functions to check if new data is available from the EIA API\n",
    "before running the full pipeline. It compares the current max timestamps with\n",
    "the previous run's max timestamps to determine if a full pipeline run is needed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from src.renewable.regions import get_eia_respondent\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FreshnessCheckResult:\n",
    "    \"\"\"Result of a data freshness check.\"\"\"\n",
    "\n",
    "    has_new_data: bool\n",
    "    checked_at_utc: str\n",
    "    series_status: dict[str, dict] = field(default_factory=dict)\n",
    "    summary: str = \"\"\n",
    "\n",
    "\n",
    "def load_previous_max_ds(run_log_path: Path) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load per-series max_ds from previous run_log.json.\n",
    "\n",
    "    Args:\n",
    "        run_log_path: Path to run_log.json\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping unique_id -> max_ds ISO string.\n",
    "        Empty dict if file doesn't exist or is malformed.\n",
    "    \"\"\"\n",
    "    if not run_log_path.exists():\n",
    "        logger.info(\"[freshness] No previous run_log.json found - first run\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        data = json.loads(run_log_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # Navigate to diagnostics.generation_coverage.coverage\n",
    "        coverage = (\n",
    "            data.get(\"diagnostics\", {})\n",
    "            .get(\"generation_coverage\", {})\n",
    "            .get(\"coverage\", [])\n",
    "        )\n",
    "\n",
    "        if not coverage:\n",
    "            logger.warning(\"[freshness] run_log.json has no coverage data\")\n",
    "            return {}\n",
    "\n",
    "        result = {}\n",
    "        for item in coverage:\n",
    "            uid = item.get(\"unique_id\")\n",
    "            max_ds = item.get(\"max_ds\")\n",
    "            if uid and max_ds:\n",
    "                result[uid] = max_ds\n",
    "\n",
    "        logger.info(f\"[freshness] Loaded {len(result)} series from previous run_log\")\n",
    "        return result\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        logger.warning(f\"[freshness] Failed to parse run_log.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def probe_eia_latest(\n",
    "    api_key: str,\n",
    "    region: str,\n",
    "    fuel_type: str,\n",
    "    *,\n",
    "    timeout: int = 15,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch only the single most recent record from EIA API.\n",
    "\n",
    "    This is a lightweight probe that uses:\n",
    "    - length=1 (only fetch 1 record)\n",
    "    - sort by period DESC (most recent first)\n",
    "\n",
    "    Args:\n",
    "        api_key: EIA API key\n",
    "        region: Region code (CALI, ERCO, MISO, etc.)\n",
    "        fuel_type: Fuel type (WND, SUN)\n",
    "        timeout: Request timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        ISO timestamp string of latest record, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        respondent = get_eia_respondent(region)\n",
    "\n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"data[]\": \"value\",\n",
    "            \"facets[respondent][]\": respondent,\n",
    "            \"facets[fueltype][]\": fuel_type,\n",
    "            \"frequency\": \"hourly\",\n",
    "            \"length\": 1,\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"desc\",\n",
    "        }\n",
    "\n",
    "        base_url = \"https://api.eia.gov/v2/electricity/rto/fuel-type-data/data/\"\n",
    "        resp = requests.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        payload = resp.json()\n",
    "        response = payload.get(\"response\", {})\n",
    "        records = response.get(\"data\", [])\n",
    "\n",
    "        if not records:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: No records returned\")\n",
    "            return None\n",
    "\n",
    "        period = records[0].get(\"period\")\n",
    "        if not period:\n",
    "            logger.warning(f\"[probe] {region}_{fuel_type}: Record missing 'period'\")\n",
    "            return None\n",
    "\n",
    "        # Parse to consistent ISO format\n",
    "        ts = pd.to_datetime(period, utc=True)\n",
    "        return ts.isoformat()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: API error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[probe] {region}_{fuel_type}: Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compare_timestamps(prev: Optional[str], current: Optional[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if current is strictly newer than prev.\n",
    "\n",
    "    Handles None values conservatively (assume new data if unknown).\n",
    "    \"\"\"\n",
    "    if not prev or not current:\n",
    "        return True  # Unknown = assume new data (conservative)\n",
    "\n",
    "    try:\n",
    "        prev_dt = pd.to_datetime(prev, utc=True)\n",
    "        curr_dt = pd.to_datetime(current, utc=True)\n",
    "        return curr_dt > prev_dt\n",
    "    except Exception:\n",
    "        return True  # Parse error = assume new data\n",
    "\n",
    "\n",
    "def check_all_series_freshness(\n",
    "    regions: list[str],\n",
    "    fuel_types: list[str],\n",
    "    run_log_path: Path,\n",
    "    api_key: str,\n",
    ") -> FreshnessCheckResult:\n",
    "    \"\"\"\n",
    "    Check all series for new data availability.\n",
    "\n",
    "    Args:\n",
    "        regions: List of region codes (e.g., [\"CALI\", \"ERCO\", \"MISO\"])\n",
    "        fuel_types: List of fuel types (e.g., [\"WND\", \"SUN\"])\n",
    "        run_log_path: Path to previous run_log.json\n",
    "        api_key: EIA API key\n",
    "\n",
    "    Returns:\n",
    "        FreshnessCheckResult with has_new_data flag and detailed status per series.\n",
    "    \"\"\"\n",
    "    checked_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # 1. Load previous max_ds values\n",
    "    prev_max_ds = load_previous_max_ds(run_log_path)\n",
    "\n",
    "    # 2. If no previous run_log, always run full pipeline (first run)\n",
    "    if not prev_max_ds:\n",
    "        return FreshnessCheckResult(\n",
    "            has_new_data=True,\n",
    "            checked_at_utc=checked_at,\n",
    "            series_status={},\n",
    "            summary=\"No previous run_log.json found - running full pipeline (first run)\",\n",
    "        )\n",
    "\n",
    "    # 3. Probe each series\n",
    "    series_status: dict[str, dict] = {}\n",
    "    has_any_new = False\n",
    "    new_series: list[str] = []\n",
    "    error_series: list[str] = []\n",
    "\n",
    "    for region in regions:\n",
    "        for fuel_type in fuel_types:\n",
    "            series_id = f\"{region}_{fuel_type}\"\n",
    "            prev = prev_max_ds.get(series_id)\n",
    "            current = probe_eia_latest(api_key, region, fuel_type)\n",
    "\n",
    "            # Determine if this series has new data\n",
    "            if current is None:\n",
    "                # API error - be conservative, assume new data\n",
    "                is_new = True\n",
    "                error_series.append(series_id)\n",
    "                logger.warning(\n",
    "                    f\"[freshness] {series_id}: probe failed, assuming new data\"\n",
    "                )\n",
    "            else:\n",
    "                is_new = _compare_timestamps(prev, current)\n",
    "\n",
    "            series_status[series_id] = {\n",
    "                \"prev_max_ds\": prev,\n",
    "                \"current_max_ds\": current,\n",
    "                \"is_new\": is_new,\n",
    "            }\n",
    "\n",
    "            if is_new:\n",
    "                has_any_new = True\n",
    "                if current is not None:\n",
    "                    new_series.append(series_id)\n",
    "\n",
    "            # Log each series check\n",
    "            status_str = \"NEW\" if is_new else \"unchanged\"\n",
    "            logger.info(\n",
    "                f\"[freshness] {series_id}: prev={prev} current={current} ({status_str})\"\n",
    "            )\n",
    "\n",
    "    # 4. Build summary\n",
    "    if error_series:\n",
    "        summary = f\"Probe errors for {error_series}, assuming new data available\"\n",
    "    elif new_series:\n",
    "        summary = f\"New data found for: {', '.join(new_series)}\"\n",
    "    else:\n",
    "        summary = \"No new data found for any series\"\n",
    "\n",
    "    return FreshnessCheckResult(\n",
    "        has_new_data=has_any_new,\n",
    "        checked_at_utc=checked_at,\n",
    "        series_status=series_status,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    api_key = os.getenv(\"EIA_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"EIA_API_KEY not set\")\n",
    "        exit(1)\n",
    "\n",
    "    run_log_path = Path(\"data/renewable/run_log.json\")\n",
    "\n",
    "    result = check_all_series_freshness(\n",
    "        regions=[\"CALI\", \"ERCO\", \"MISO\"],\n",
    "        fuel_types=[\"WND\", \"SUN\"],\n",
    "        run_log_path=run_log_path,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFreshness Check Result:\")\n",
    "    print(f\"  has_new_data: {result.has_new_data}\")\n",
    "    print(f\"  checked_at: {result.checked_at_utc}\")\n",
    "    print(f\"  summary: {result.summary}\")\n",
    "    print(f\"\\nPer-series status:\")\n",
    "    for series_id, status in result.series_status.items():\n",
    "        print(f\"  {series_id}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/jobs/run_hourly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/jobs/run_hourly.py\n",
    "# file: src/renewable/jobs/run_hourly.py\n",
    "\"\"\"Hourly renewable pipeline entry point with validation.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.renewable.tasks import RenewablePipelineConfig, run_full_pipeline\n",
    "from src.renewable.validation import validate_generation_df\n",
    "from src.renewable.data_freshness import check_all_series_freshness, FreshnessCheckResult\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _env_list(name: str, default_csv: str) -> list[str]:\n",
    "    raw = os.getenv(name, default_csv)\n",
    "    return [item.strip() for item in raw.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int) -> int:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return int(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name, str(default))\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _expected_series(regions: list[str], fuel_types: list[str]) -> list[str]:\n",
    "    return [f\"{region}_{fuel}\" for region in regions for fuel in fuel_types]\n",
    "\n",
    "\n",
    "def _json_default(value: object) -> str:\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.isoformat()\n",
    "    if isinstance(value, datetime):\n",
    "        return value.isoformat()\n",
    "    if hasattr(value, \"item\"):\n",
    "        try:\n",
    "            return value.item()\n",
    "        except Exception:\n",
    "            return str(value)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _summarize_generation_coverage(df: pd.DataFrame) -> dict:\n",
    "    if df.empty:\n",
    "        return {\"row_count\": 0, \"series_count\": 0, \"coverage\": []}\n",
    "\n",
    "    coverage = (\n",
    "        df.groupby(\"unique_id\")[\"ds\"]\n",
    "        .agg(min_ds=\"min\", max_ds=\"max\", rows=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"series_count\": int(df[\"unique_id\"].nunique()),\n",
    "        \"coverage\": coverage.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _read_previous_run_summary(data_dir: str) -> dict | None:\n",
    "    \"\"\"Read previous run_log.json for rowcount comparison.\"\"\"\n",
    "    path = Path(data_dir) / \"run_log.json\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _summarize_negative_forecasts(\n",
    "    df: pd.DataFrame,\n",
    "    sample_rows: int = 5,\n",
    ") -> dict:\n",
    "    if df.empty or \"yhat\" not in df.columns:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    neg = df[df[\"yhat\"] < 0]\n",
    "    if neg.empty:\n",
    "        return {\n",
    "            \"row_count\": int(len(df)),\n",
    "            \"negative_rows\": 0,\n",
    "            \"series\": [],\n",
    "            \"sample\": [],\n",
    "        }\n",
    "\n",
    "    series_summary = (\n",
    "        neg.groupby(\"unique_id\")[\"yhat\"]\n",
    "        .agg(count=\"count\", min_value=\"min\", max_value=\"max\", mean_value=\"mean\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"unique_id\")\n",
    "    )\n",
    "    sample = neg[[\"unique_id\", \"ds\", \"yhat\"]].head(sample_rows)\n",
    "    return {\n",
    "        \"row_count\": int(len(df)),\n",
    "        \"negative_rows\": int(len(neg)),\n",
    "        \"series\": series_summary.to_dict(orient=\"records\"),\n",
    "        \"sample\": sample.to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_hourly_pipeline() -> dict:\n",
    "    data_dir = os.getenv(\"RENEWABLE_DATA_DIR\", \"data/renewable\")\n",
    "    regions = _env_list(\"RENEWABLE_REGIONS\", \"CALI,ERCO,MISO\")\n",
    "    fuel_types = _env_list(\"RENEWABLE_FUELS\", \"WND,SUN\")\n",
    "    lookback_days = _env_int(\"LOOKBACK_DAYS\", 30)\n",
    "\n",
    "    # Horizon configuration: support both preset and direct override\n",
    "    horizon_preset = os.getenv(\"RENEWABLE_HORIZON_PRESET\", None)  # \"24h\" | \"48h\" | \"72h\"\n",
    "    horizon_override = _env_int(\"RENEWABLE_HORIZON\", 0)  # Legacy direct override\n",
    "\n",
    "    # If direct override is set, use it; otherwise use preset (or None for default)\n",
    "    if horizon_override > 0:\n",
    "        horizon = horizon_override\n",
    "        horizon_preset = None  # Ignore preset if direct override is set\n",
    "    else:\n",
    "        horizon = 24  # Default, may be overridden by preset\n",
    "\n",
    "    cv_windows = _env_int(\"RENEWABLE_CV_WINDOWS\", 2)\n",
    "    cv_step_size = _env_int(\"RENEWABLE_CV_STEP_SIZE\", 168)\n",
    "\n",
    "    start_date = os.getenv(\"RENEWABLE_START_DATE\", \"\")\n",
    "    end_date = os.getenv(\"RENEWABLE_END_DATE\", \"\")\n",
    "\n",
    "    # Check if we should force run (e.g., manual dispatch)\n",
    "    force_run = os.getenv(\"FORCE_RUN\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # Data freshness check - skip full pipeline if no new data\n",
    "    if not force_run:\n",
    "        api_key = os.getenv(\"EIA_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            print(\"WARNING: EIA_API_KEY not set, skipping freshness check\")\n",
    "        else:\n",
    "            run_log_path = Path(data_dir) / \"run_log.json\"\n",
    "            freshness = check_all_series_freshness(\n",
    "                regions=regions,\n",
    "                fuel_types=fuel_types,\n",
    "                run_log_path=run_log_path,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "\n",
    "            if not freshness.has_new_data:\n",
    "                # No new data - return early with skip status\n",
    "                skip_log = {\n",
    "                    \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"reason\": \"no_new_data\",\n",
    "                    \"freshness_check\": {\n",
    "                        \"checked_at_utc\": freshness.checked_at_utc,\n",
    "                        \"summary\": freshness.summary,\n",
    "                        \"series_status\": freshness.series_status,\n",
    "                    },\n",
    "                    \"config\": {\n",
    "                        \"regions\": regions,\n",
    "                        \"fuel_types\": fuel_types,\n",
    "                        \"data_dir\": data_dir,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                # Write skip log (append to run_log.json)\n",
    "                Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "                skip_log_path = Path(data_dir) / \"skip_log.json\"\n",
    "                skip_log_path.write_text(\n",
    "                    json.dumps(skip_log, indent=2, default=_json_default)\n",
    "                )\n",
    "\n",
    "                print(f\"SKIPPED: {freshness.summary}\")\n",
    "                print(f\"Skip log written to: {skip_log_path}\")\n",
    "\n",
    "                # Set output for GitHub Actions\n",
    "                github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "                if github_output:\n",
    "                    with open(github_output, \"a\") as f:\n",
    "                        f.write(\"status=skipped\\n\")\n",
    "\n",
    "                return skip_log\n",
    "\n",
    "            print(f\"Freshness check: {freshness.summary}\")\n",
    "    else:\n",
    "        print(\"FORCE_RUN=true - skipping freshness check\")\n",
    "\n",
    "    cfg = RenewablePipelineConfig(\n",
    "        regions=regions,\n",
    "        fuel_types=fuel_types,\n",
    "        lookback_days=lookback_days,\n",
    "        horizon=horizon,\n",
    "        horizon_preset=horizon_preset,  # Apply preset if specified\n",
    "        data_dir=data_dir,\n",
    "        overwrite=True,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "    )\n",
    "    cfg.cv_windows = cv_windows\n",
    "    cfg.cv_step_size = cv_step_size\n",
    "\n",
    "    fetch_diagnostics: list[dict] = []\n",
    "    results = run_full_pipeline(cfg, fetch_diagnostics=fetch_diagnostics)\n",
    "\n",
    "    gen_path = cfg.generation_path()\n",
    "    gen_df = pd.read_parquet(gen_path)\n",
    "    generation_coverage = _summarize_generation_coverage(gen_df)\n",
    "\n",
    "    max_lag_hours = _env_int(\"MAX_LAG_HOURS\", 48)  # EIA publishes with 12-24h delay\n",
    "    max_missing_ratio = _env_float(\"MAX_MISSING_RATIO\", 0.02)\n",
    "    report = validate_generation_df(\n",
    "        gen_df,\n",
    "        max_lag_hours=max_lag_hours,\n",
    "        max_missing_ratio=max_missing_ratio,\n",
    "        expected_series=_expected_series(regions, fuel_types),\n",
    "    )\n",
    "\n",
    "    forecasts_df = pd.read_parquet(cfg.forecasts_path())\n",
    "    negative_forecasts = _summarize_negative_forecasts(forecasts_df)\n",
    "\n",
    "    # Quality gates\n",
    "    max_rowdrop_pct = _env_float(\"MAX_ROWDROP_PCT\", 0.30)\n",
    "    max_neg_forecast_ratio = _env_float(\"MAX_NEG_FORECAST_RATIO\", 0.10)\n",
    "\n",
    "    prev_run = _read_previous_run_summary(data_dir)\n",
    "    prev_gen_rows = 0\n",
    "    if prev_run:\n",
    "        prev_gen_rows = prev_run.get(\"pipeline_results\", {}).get(\"generation_rows\", 0)\n",
    "\n",
    "    curr_gen_rows = results.get(\"generation_rows\", 0)\n",
    "    rowdrop_ok = True\n",
    "    if prev_gen_rows > 0:\n",
    "        floor_ok = int(prev_gen_rows * (1.0 - max_rowdrop_pct))\n",
    "        rowdrop_ok = curr_gen_rows >= floor_ok\n",
    "\n",
    "    neg_forecast_ratio = 0.0\n",
    "    if negative_forecasts[\"row_count\"] > 0:\n",
    "        neg_forecast_ratio = (\n",
    "            negative_forecasts[\"negative_rows\"] / negative_forecasts[\"row_count\"]\n",
    "        )\n",
    "    neg_forecast_ok = neg_forecast_ratio <= max_neg_forecast_ratio\n",
    "\n",
    "    quality_gates = {\n",
    "        \"rowdrop\": {\n",
    "            \"ok\": rowdrop_ok,\n",
    "            \"prev_rows\": prev_gen_rows,\n",
    "            \"curr_rows\": curr_gen_rows,\n",
    "            \"max_rowdrop_pct\": max_rowdrop_pct,\n",
    "        },\n",
    "        \"neg_forecast\": {\n",
    "            \"ok\": neg_forecast_ok,\n",
    "            \"ratio\": neg_forecast_ratio,\n",
    "            \"max_ratio\": max_neg_forecast_ratio,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    run_log = {\n",
    "        \"run_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"config\": {\n",
    "            \"regions\": regions,\n",
    "            \"fuel_types\": fuel_types,\n",
    "            \"lookback_days\": lookback_days,\n",
    "            \"horizon\": horizon,\n",
    "            \"cv_windows\": cv_windows,\n",
    "            \"cv_step_size\": cv_step_size,\n",
    "            \"data_dir\": data_dir,\n",
    "            \"start_date\": cfg.start_date,\n",
    "            \"end_date\": cfg.end_date,\n",
    "        },\n",
    "        \"pipeline_results\": results,\n",
    "        \"validation\": {\n",
    "            \"ok\": report.ok,\n",
    "            \"message\": report.message,\n",
    "            \"details\": report.details,\n",
    "        },\n",
    "        \"diagnostics\": {\n",
    "            \"fetch\": fetch_diagnostics,\n",
    "            \"generation_coverage\": generation_coverage,\n",
    "            \"negative_forecasts\": negative_forecasts,\n",
    "        },\n",
    "        \"quality_gates\": quality_gates,\n",
    "    }\n",
    "\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    (Path(data_dir) / \"run_log.json\").write_text(\n",
    "        json.dumps(run_log, indent=2, default=_json_default)\n",
    "    )\n",
    "\n",
    "    # Check validation\n",
    "    if not report.ok:\n",
    "        raise SystemExit(f\"VALIDATION_FAILED: {report.message} | {report.details}\")\n",
    "\n",
    "    # Check quality gates\n",
    "    if not rowdrop_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: rowdrop | \"\n",
    "            f\"curr={curr_gen_rows} prev={prev_gen_rows} max_drop={max_rowdrop_pct:.0%}\"\n",
    "        )\n",
    "    if not neg_forecast_ok:\n",
    "        raise SystemExit(\n",
    "            f\"QUALITY_GATE_FAILED: neg_forecast | \"\n",
    "            f\"ratio={neg_forecast_ratio:.1%} max={max_neg_forecast_ratio:.0%}\"\n",
    "        )\n",
    "\n",
    "    # Set output for GitHub Actions (successful run)\n",
    "    github_output = os.getenv(\"GITHUB_OUTPUT\")\n",
    "    if github_output:\n",
    "        with open(github_output, \"a\") as f:\n",
    "            f.write(\"status=success\\n\")\n",
    "\n",
    "    return run_log\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    run_hourly_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/renewable/dag_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/renewable/dag_builder.py\n",
    "# file: src/renewable/dag_builder.py\n",
    "\"\"\"Renewable pipeline DAG builder for Airflow.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "AIRFLOW_AVAILABLE = True\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "def build_hourly_dag(\n",
    "    dag_id: str = \"renewable_hourly_pipeline\",\n",
    "    schedule: str = \"17 * * * *\",\n",
    "    start_date: Optional[datetime] = None,\n",
    "    default_args: Optional[Dict[str, Any]] = None,\n",
    ") -> \"DAG\":\n",
    "    if not AIRFLOW_AVAILABLE:\n",
    "        raise ImportError(\"Airflow is not installed. Install apache-airflow to use build_hourly_dag().\")\n",
    "\n",
    "    from src.renewable.jobs.run_hourly import run_hourly_pipeline\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = datetime.utcnow() - timedelta(days=1)\n",
    "    if default_args is None:\n",
    "        default_args = DEFAULT_ARGS.copy()\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=\"Renewable hourly pipeline\",\n",
    "        schedule_interval=schedule,\n",
    "        start_date=start_date,\n",
    "        catchup=False,\n",
    "        max_active_runs=1,\n",
    "        tags=[\"renewable\", \"eia\", \"forecasting\"],\n",
    "    ) as dag:\n",
    "        PythonOperator(\n",
    "            task_id=\"run_hourly\",\n",
    "            python_callable=run_hourly_pipeline,\n",
    "        )\n",
    "\n",
    "    return dag\n",
    "\n",
    "\n",
    "def build_dag_dot() -> str:\n",
    "    return \"\"\"digraph RENEWABLE_PIPELINE {\n",
    "  rankdir=LR;\n",
    "  node [shape=box, style=\"rounded,filled\", fillcolor=\"#e8f5e9\"];\n",
    "\n",
    "  run_hourly;\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# git actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/pre-commit.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/pre-commit.yml\n",
    "# file: .github/workflows/pre-commit.yml\n",
    "name: pre-commit\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  run:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - uses: pre-commit/action@v3.0.1\n",
    "\n",
    "      - name: Install test dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install pytest pandas numpy requests python-dotenv\n",
    "\n",
    "      - name: Run smoke tests\n",
    "        env:\n",
    "          PYTHONPATH: ${{ github.workspace }}\n",
    "        run: pytest tests/ -v -k \"not slow\" --tb=short || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .github/workflows/renewable-hourly.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/renewable-hourly.yml\n",
    "# file: .github/workflows/renewable-hourly.yml\n",
    "name: renewable-hourly\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      force_run:\n",
    "        description: 'Force full pipeline run (skip freshness check)'\n",
    "        type: boolean\n",
    "        default: false\n",
    "  schedule:\n",
    "    - cron: \"17 * * * *\"\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "concurrency:\n",
    "  group: renewable-hourly\n",
    "  cancel-in-progress: true\n",
    "\n",
    "jobs:\n",
    "  update:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 25\n",
    "    env:\n",
    "      EIA_API_KEY: ${{ secrets.EIA_API_KEY }}\n",
    "      FORCE_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.force_run && 'true' || 'false' }}\n",
    "      RENEWABLE_REGIONS: \"CALI,ERCO,MISO\"\n",
    "      RENEWABLE_FUELS: \"WND,SUN\"\n",
    "      LOOKBACK_DAYS: \"30\"\n",
    "      RENEWABLE_HORIZON: \"24\"\n",
    "      RENEWABLE_CV_WINDOWS: \"2\"\n",
    "      RENEWABLE_CV_STEP_SIZE: \"168\"\n",
    "      MAX_LAG_HOURS: \"48\"  # EIA publishes hourly data with 12-24h delay\n",
    "      MAX_MISSING_RATIO: \"0.02\"\n",
    "      RENEWABLE_DATA_DIR: \"data/renewable\"\n",
    "      RENEWABLE_N_JOBS: \"1\"\n",
    "      OMP_NUM_THREADS: \"1\"\n",
    "      MKL_NUM_THREADS: \"1\"\n",
    "      OPENBLAS_NUM_THREADS: \"1\"\n",
    "      NUMBA_NUM_THREADS: \"1\"\n",
    "      VECLIB_MAXIMUM_THREADS: \"1\"\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - name: Check EIA API key\n",
    "        run: |\n",
    "          if [ -z \"$EIA_API_KEY\" ]; then\n",
    "            echo \"EIA_API_KEY is not set. Add it to repo secrets.\" >&2\n",
    "            exit 1\n",
    "          fi\n",
    "\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          # Install from pyproject.toml for single source of truth\n",
    "          # Use -e for editable install (allows imports to work correctly)\n",
    "          pip install -e .\n",
    "\n",
    "      - name: Run hourly pipeline\n",
    "        id: pipeline\n",
    "        run: |\n",
    "          python -m src.renewable.jobs.run_hourly\n",
    "\n",
    "      - name: Quality gate check\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          python -c \"\n",
    "          import json, sys\n",
    "          from pathlib import Path\n",
    "          log_path = Path('data/renewable/run_log.json')\n",
    "          if not log_path.exists():\n",
    "              print('No run_log.json found')\n",
    "              sys.exit(1)\n",
    "          log = json.loads(log_path.read_text())\n",
    "          val = log.get('validation', {})\n",
    "          if not val.get('ok'):\n",
    "              print(f'VALIDATION FAILED: {val.get(\\\"message\\\")}')\n",
    "              print(f'Details: {val.get(\\\"details\\\")}')\n",
    "              sys.exit(1)\n",
    "          gates = log.get('quality_gates', {})\n",
    "          if not gates.get('rowdrop', {}).get('ok', True):\n",
    "              print(f'ROWDROP GATE FAILED: {gates.get(\\\"rowdrop\\\")}')\n",
    "              sys.exit(1)\n",
    "          if not gates.get('neg_forecast', {}).get('ok', True):\n",
    "              print(f'NEG_FORECAST GATE FAILED: {gates.get(\\\"neg_forecast\\\")}')\n",
    "              sys.exit(1)\n",
    "          print('QUALITY GATES PASSED')\n",
    "          \"\n",
    "\n",
    "      - name: Skip notification\n",
    "        if: steps.pipeline.outputs.status == 'skipped'\n",
    "        run: |\n",
    "          echo \"### Pipeline skipped - no new EIA data\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          if [ -f data/renewable/skip_log.json ]; then\n",
    "            python -c \"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "          data = json.loads(Path('data/renewable/skip_log.json').read_text())\n",
    "          freshness = data.get('freshness_check', {})\n",
    "          print(f'- Checked at: {freshness.get(\\\"checked_at_utc\\\")}')\n",
    "          print(f'- Summary: {freshness.get(\\\"summary\\\")}')\n",
    "          \" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Summarize run\n",
    "        if: always() && steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          if [ -f data/renewable/run_log.json ]; then\n",
    "          python - <<'PY' | tee -a \"$GITHUB_STEP_SUMMARY\"\n",
    "          import json\n",
    "          from pathlib import Path\n",
    "\n",
    "          data = json.loads(Path(\"data/renewable/run_log.json\").read_text())\n",
    "          validation = data.get(\"validation\", {})\n",
    "          details = validation.get(\"details\", {})\n",
    "          pipeline = data.get(\"pipeline_results\", {})\n",
    "          interp = pipeline.get(\"interpretability\", {})\n",
    "\n",
    "          lines = [\n",
    "              \"### Renewable hourly run\",\n",
    "              f\"- run_at_utc: {data.get('run_at_utc')}\",\n",
    "              f\"- validation_ok: {validation.get('ok')}\",\n",
    "              f\"- message: {validation.get('message')}\",\n",
    "              f\"- max_ds: {details.get('max_ds')}\",\n",
    "              f\"- lag_hours: {details.get('lag_hours')}\",\n",
    "              f\"- best_model: {pipeline.get('best_model')}\",\n",
    "              f\"- best_rmse: {pipeline.get('best_rmse', 0):.1f}\",\n",
    "              \"\",\n",
    "              \"#### Interpretability\",\n",
    "              f\"- series_count: {interp.get('series_count', 0)}\",\n",
    "              f\"- output_dir: {interp.get('output_dir', 'N/A')}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          PY\n",
    "          else\n",
    "          echo \"No run_log.json found.\" >> \"$GITHUB_STEP_SUMMARY\"\n",
    "          fi\n",
    "\n",
    "      - name: Commit updated artifacts\n",
    "        if: steps.pipeline.outputs.status != 'skipped'\n",
    "        run: |\n",
    "          git config user.name \"github-actions[bot]\"\n",
    "          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n",
    "          git add data/renewable/generation.parquet \\\n",
    "            data/renewable/weather.parquet \\\n",
    "            data/renewable/forecasts.parquet \\\n",
    "            data/renewable/run_log.json\n",
    "          # Add interpretability artifacts if they exist\n",
    "          if [ -d data/renewable/interpretability ]; then\n",
    "            git add data/renewable/interpretability/\n",
    "          fi\n",
    "          git commit -m \"renewable: hourly data update (UTC)\" || echo \"No changes to commit\"\n",
    "          git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
