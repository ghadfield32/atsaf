{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8acc0d",
   "metadata": {},
   "source": [
    "# Production A/B Testing Framework\n",
    "\n",
    "## ðŸŽ¯ Purpose\n",
    "\n",
    "This is a **comprehensive, resume-worthy A/B testing framework** demonstrating mastery of modern experimentation techniques used at Netflix, Airbnb, Meta, Microsoft, and DoorDash.\n",
    "\n",
    "**NOT** another Cookie Cats tutorial. This showcases:\n",
    "- **Novel use case** (subscription onboarding, not overused public datasets)\n",
    "- **All modern techniques** from 2024-2025 industry practices\n",
    "- **Production-ready patterns** with SQL-first data engineering\n",
    "- **Both statistical depth AND business communication**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Technique Coverage\n",
    "\n",
    "### âœ… Fully Implemented\n",
    "\n",
    "| Category | Techniques |\n",
    "|----------|------------|\n",
    "| **Core Statistics** | Z-test, Welch's t-test, Mann-Whitney U |\n",
    "| **Bayesian Analysis** | Beta-Binomial, Thompson Sampling preview |\n",
    "| **Variance Reduction** | CUPED, **CUPAC (ML-enhanced)**, ANCOVA |\n",
    "| **Multiple Testing** | Bonferroni, Benjamini-Hochberg (FDR), Holm |\n",
    "| **Heterogeneous Treatment Effects** | Stratified CATE, Interaction tests, **X-Learner** |\n",
    "| **Robust Methods** | Bootstrap CI (percentile, BCa), Quantile Treatment Effects, Winsorization |\n",
    "| **Sequential Testing** | O'Brien-Fleming, Pocock boundaries |\n",
    "| **Practical Analysis** | ITT vs Per-Protocol, SRM detection, Power analysis |\n",
    "| **Business Translation** | Revenue impact, annualized projections |\n",
    "\n",
    "### ðŸŽ“ Key Industry Techniques Demonstrated\n",
    "\n",
    "1. **CUPAC (Controlled-experiment Using Predictions As Covariates)**\n",
    "   - ML model (GBM) predicts outcome from pre-experiment data\n",
    "   - Cross-validation prevents overfitting\n",
    "   - Can achieve 30-50%+ variance reduction\n",
    "   - *Used at DoorDash, Airbnb*\n",
    "\n",
    "2. **X-Learner for CATE Estimation**\n",
    "   - Better than simple stratified analysis\n",
    "   - Handles treatment/control imbalance\n",
    "   - Produces individual-level treatment effect estimates\n",
    "   - *Based on KÃ¼nzel et al. (2019)*\n",
    "\n",
    "3. **Sequential Testing with Alpha Spending**\n",
    "   - O'Brien-Fleming: conservative early, aggressive late\n",
    "   - Enables valid early stopping without Type I error inflation\n",
    "   - *Standard at Netflix, Airbnb*\n",
    "\n",
    "4. **Multiple Testing Correction**\n",
    "   - Benjamini-Hochberg for exploratory (controls FDR)\n",
    "   - Bonferroni/Holm for confirmatory (controls FWER)\n",
    "   - *Required at all major tech companies*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ File Structure\n",
    "\n",
    "```\n",
    "ab_testing_advanced/\n",
    "â”œâ”€â”€ ab_testing_framework_v4_final.py   # Complete framework (recommended)\n",
    "â”œâ”€â”€ ab_testing_framework_v3.py         # Enhanced framework\n",
    "â”œâ”€â”€ ab_testing_framework_v2.py         # Original version (reference)\n",
    "â”œâ”€â”€ duckdb_sql_patterns.py             # SQL-first analysis patterns\n",
    "â”œâ”€â”€ CRITICAL_ANALYSIS.md               # Gap analysis & improvements\n",
    "â””â”€â”€ README.md                          # This file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Quick Start\n",
    "\n",
    "```python\n",
    "from ab_testing_framework_v4_final import (\n",
    "    ExperimentConfig,\n",
    "    ExperimentDataGenerator,\n",
    "    ABTestAnalyzer,\n",
    "    CUPAC,\n",
    "    XLearner\n",
    ")\n",
    "\n",
    "# Generate data (or load your own)\n",
    "df = ExperimentDataGenerator.subscription_onboarding_experiment(n=100000)\n",
    "\n",
    "# Configure experiment\n",
    "config = ExperimentConfig(\n",
    "    experiment_id=\"onboarding_v2\",\n",
    "    hypothesis=\"New flow increases conversion\",\n",
    "    primary_metric=\"converted\",\n",
    "    metric_type=\"binary\",\n",
    "    use_cupac=True,\n",
    "    cuped_covariates=[\"pre_sessions\", \"pre_features_used\"]\n",
    ")\n",
    "\n",
    "# Run full analysis\n",
    "analyzer = ABTestAnalyzer(config)\n",
    "results = analyzer.run_full_analysis(df)\n",
    "analyzer.print_summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Novel Use Case: Subscription Onboarding\n",
    "\n",
    "Instead of overused datasets (Cookie Cats, Hillstrom), we simulate a **realistic subscription product experiment**:\n",
    "\n",
    "**Scenario:** Testing a new onboarding flow for a SaaS product\n",
    "\n",
    "**Metrics:**\n",
    "- `converted`: Trial â†’ Paid conversion (primary)\n",
    "- `ltv`: Lifetime value (secondary)\n",
    "- `sessions_week1`: Engagement (secondary)\n",
    "- `made_referral`: Virality (secondary)\n",
    "- `nps_score`: Satisfaction (guardrail)\n",
    "\n",
    "**Features:**\n",
    "- Pre-experiment data (for CUPAC demo): `pre_sessions`, `pre_features_used`, `pre_time_spent`\n",
    "- Segments: `channel` (Organic/Paid/Social/Referral), `device` (iOS/Android/Web)\n",
    "- Triggered flag (for ITT vs Per-Protocol analysis)\n",
    "\n",
    "**Heterogeneous Effects:**\n",
    "- Treatment works better for engaged users\n",
    "- Referral channel sees larger effect\n",
    "- Mobile slightly lower effect than web\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¬ Statistical Methods Reference\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Situation | Recommended Method |\n",
    "|-----------|-------------------|\n",
    "| Binary metric, quick decision | Z-test with Wilson CI |\n",
    "| Continuous metric, skewed | Bootstrap CI or Winsorized mean |\n",
    "| Multiple metrics | Benjamini-Hochberg correction |\n",
    "| Need variance reduction | CUPAC > CUPED > ANCOVA |\n",
    "| Detect segment differences | Stratified CATE + interaction tests |\n",
    "| Want probability statements | Bayesian Beta-Binomial |\n",
    "| Need to peek at results | Sequential testing (O'Brien-Fleming) |\n",
    "| Users don't see treatment | Triggered analysis (ITT vs PP) |\n",
    "\n",
    "### Effect Size Interpretation\n",
    "\n",
    "| Cohen's d | Binary Lift | Interpretation |\n",
    "|-----------|-------------|----------------|\n",
    "| < 0.2 | < 2% relative | Negligible |\n",
    "| 0.2 - 0.5 | 2-5% relative | Small |\n",
    "| 0.5 - 0.8 | 5-10% relative | Medium |\n",
    "| > 0.8 | > 10% relative | Large |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What Makes This Resume-Worthy\n",
    "\n",
    "### Technical Depth\n",
    "- âœ… Correct statistical methods (Welch's, not Student's t-test)\n",
    "- âœ… Proper CI methods (Wilson, not Wald)\n",
    "- âœ… Modern variance reduction (CUPAC, not just CUPED)\n",
    "- âœ… ML-enhanced causal inference (X-Learner)\n",
    "- âœ… Multiple testing correction (FDR control)\n",
    "\n",
    "### Production Patterns\n",
    "- âœ… SQL-first with DuckDB (data engineering skills)\n",
    "- âœ… Config-driven (reproducible experiments)\n",
    "- âœ… Comprehensive error handling\n",
    "- âœ… Business impact translation\n",
    "\n",
    "### Industry Alignment\n",
    "- âœ… Netflix: HTE, Sequential testing, CUPED\n",
    "- âœ… Airbnb: Stratified analysis, Bootstrap\n",
    "- âœ… Meta: Multiple testing correction\n",
    "- âœ… DoorDash: CUPAC\n",
    "- âœ… Microsoft: CUPED, Power analysis\n",
    "\n",
    "### Communication Skills\n",
    "- âœ… Clear decision framework (SHIP/INVESTIGATE/ABANDON)\n",
    "- âœ… Business impact quantification\n",
    "- âœ… Non-technical summary generation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– References\n",
    "\n",
    "### Papers\n",
    "- Deng et al. (2013) - \"Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data\" (CUPED)\n",
    "- KÃ¼nzel et al. (2019) - \"Metalearners for estimating heterogeneous treatment effects using machine learning\" (X-Learner)\n",
    "- Wager & Athey (2018) - \"Estimation and Inference of Heterogeneous Treatment Effects using Random Forests\"\n",
    "- Benjamini & Hochberg (1995) - \"Controlling the False Discovery Rate\"\n",
    "\n",
    "### Industry Posts\n",
    "- Netflix Tech Blog: \"Heterogeneous Treatment Effects at Netflix\" (2025)\n",
    "- DoorDash: \"CUPAC\" (2020)\n",
    "- Microsoft: \"Trustworthy Online Controlled Experiments\"\n",
    "- Airbnb: \"Experimentation & Causal Inference\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤ Author\n",
    "\n",
    "**Geoff** - Analytics Engineer / Data Scientist\n",
    "- MS Data Science, University of West Florida\n",
    "- PhD Computer Science (starting Jan 2026), Florida Atlantic University\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Version History\n",
    "\n",
    "| Version | Changes |\n",
    "|---------|---------|\n",
    "| v2.0 | Initial framework with CUPED, Sequential, Bayesian |\n",
    "| v3.0 | Added Multiple Testing, Bootstrap, QTE, HTE |\n",
    "| v4.0 | Added CUPAC, X-Learner, Novel use case, Business translation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30aeddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ COMPLETE A/B TESTING CURRICULUM - WITH FULL THOUGHT PROCESS\n",
      "â–ˆ Everything you need to ace experimentation interviews\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š DATA GENERATION: Subscription Onboarding Experiment\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Scenario: Testing redesigned onboarding flow for subscription product\n",
      "- Control (A): Existing onboarding  \n",
      "- Treatment (B): New personalized onboarding\n",
      "\n",
      "Sample: 100,000 users (50,045 control, 49,955 treatment)\n",
      "Conversion: A = 24.84%, B = 28.03%\n",
      "Observed Lift: 12.8% relative (3.19 percentage points)\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ LEVEL 1: FUNDAMENTALS (Must-know for any DS role)\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“ 1.1 SAMPLE SIZE CALCULATION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ THE QUESTION: \"How many users do we need?\"\n",
      "   This is the #1 most common experimentation interview question.\n",
      "\n",
      "ðŸ“š KEY CONCEPTS:\n",
      "   â€¢ Power (1-Î²): Probability of detecting a TRUE effect. Typically 80%.\n",
      "   â€¢ Alpha (Î±): False positive rate. Typically 5%.\n",
      "   â€¢ MDE: Minimum Detectable Effect - smallest lift worth detecting.\n",
      "\n",
      "ðŸ“ FORMULA (binary metrics):\n",
      "   n = 2 Ã— (z_Î± + z_Î²)Â² Ã— pÌ„(1-pÌ„) / Î´Â²\n",
      "\n",
      "   where z_Î±=1.96 (Î±=0.05), z_Î²=0.84 (80% power), pÌ„=pooled rate, Î´=effect size\n",
      "\n",
      "ðŸ“Š EXAMPLE - Binary Metric (Conversion):\n",
      "   Baseline: 25%\n",
      "   MDE: 10% relative lift (25% â†’ 27.5%)\n",
      "   Cohen's h: 0.0568\n",
      "\n",
      "   âœ… RESULT: Need 4,860 users/group (9,720 total)\n",
      "\n",
      "ðŸ“Š EXAMPLE - Continuous Metric (Revenue):\n",
      "   Baseline: $175 (std=$80)\n",
      "   MDE: $15 absolute lift\n",
      "   Cohen's d: 0.1875\n",
      "\n",
      "   âœ… RESULT: Need 448 users/group (896 total)\n",
      "\n",
      "ðŸ’¡ INTERVIEW TIP: Always ask:\n",
      "   - One-sided or two-sided test?\n",
      "   - What's the business-relevant MDE?\n",
      "   - How long to reach this sample size?\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š 1.2 Z-TEST FOR PROPORTIONS (Binary Metrics)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ USE WHEN: Comparing conversion rates, CTR, or any 0/1 outcome.\n",
      "\n",
      "ðŸ“ THE TEST:\n",
      "   Hâ‚€: p_A = p_B (no difference)\n",
      "   Hâ‚: p_A â‰  p_B (two-sided)\n",
      "\n",
      "   Z = (p_B - p_A) / SE_pooled\n",
      "   where SE_pooled = âˆš[pÌ„(1-pÌ„)(1/n_A + 1/n_B)]\n",
      "\n",
      "âš ï¸ KEY INSIGHT: We use POOLED SE for the test (assumes Hâ‚€ true),\n",
      "   but NON-POOLED SE for the confidence interval.\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Control:   12,429 / 50,045 = 0.2484 (24.84%)\n",
      "   Treatment: 14,000 / 49,955 = 0.2803 (28.03%)\n",
      "\n",
      "   Absolute lift: 3.19 percentage points\n",
      "   Relative lift: 12.8%\n",
      "\n",
      "   Z-statistic: 11.4369\n",
      "   P-value: 0.00e+00 âœ… Significant!\n",
      "   95% CI: (2.64pp, 3.74pp)\n",
      "\n",
      "   Effect Size (Cohen's h): 0.0724 â†’ Negligible\n",
      "\n",
      "ðŸ“– P-VALUE INTERPRETATION:\n",
      "   â€¢ P-value = 0.00e+00 means: IF there were no real effect (Hâ‚€ true),\n",
      "     we'd see a difference this large only 0.0000% of the time.\n",
      "   â€¢ P-value is NOT the probability treatment works!\n",
      "   â€¢ P-value is NOT the probability of a false positive on THIS experiment!\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š 1.3 WELCH'S T-TEST (Continuous Metrics)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ USE WHEN: Comparing means of continuous outcomes (revenue, time, sessions).\n",
      "\n",
      "ðŸ“ WHY WELCH'S (not Student's)?\n",
      "   â€¢ Does NOT assume equal variances between groups\n",
      "   â€¢ More robust in practice - always use Welch's unless specific reason not to\n",
      "   â€¢ scipy.stats.ttest_ind(equal_var=False) gives Welch's\n",
      "\n",
      "ðŸ“Š RESULTS (Revenue, converters only):\n",
      "   Control:   n=12,429, mean=$173.17, std=$89.20\n",
      "   Treatment: n=14,000, mean=$190.43, std=$88.98\n",
      "\n",
      "   Difference: $17.26 (10.0% relative)\n",
      "   95% CI: ($15.11, $19.41)\n",
      "\n",
      "   T-statistic: 15.7159\n",
      "   P-value: 2.11e-55 âœ… Significant!\n",
      "\n",
      "   Effect Size (Cohen's d): 0.1937 â†’ Negligible\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ” 1.4 SRM (Sample Ratio Mismatch) CHECK\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY CHECK SRM?\n",
      "   If randomization is broken, ALL your results are INVALID.\n",
      "   Always check BEFORE looking at outcome metrics!\n",
      "\n",
      "âš ï¸ COMMON CAUSES:\n",
      "   â€¢ Bot filtering affecting groups differently\n",
      "   â€¢ Page load failures in treatment\n",
      "   â€¢ Browser/device compatibility issues\n",
      "   â€¢ Redirect failures\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Expected: A=50,000, B=50,000 (50/50)\n",
      "   Observed: A=50,045, B=49,955 (50.04%/49.95%)\n",
      "\n",
      "   Chi-square: 0.0810\n",
      "   P-value: 0.7759\n",
      "\n",
      "   âœ… No SRM - randomization healthy\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ LEVEL 2: INTERMEDIATE (Expected for DS roles)\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸŽ² 2.1 BAYESIAN A/B TESTING\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY BAYESIAN?\n",
      "   â€¢ Gives PROBABILITY statements: \"P(B > A) = 95%\" (intuitive!)\n",
      "   â€¢ No p-hacking concerns with continuous monitoring\n",
      "   â€¢ Can compute expected loss for decision-making\n",
      "\n",
      "ðŸ“ MODEL: Beta-Binomial\n",
      "   Prior: Beta(1, 1) = Uniform\n",
      "   Posterior: Beta(1 + successes, 1 + failures)\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Posterior A: Beta(12430, 37617)\n",
      "   Posterior B: Beta(14001, 35956)\n",
      "\n",
      "   P(B > A) = 1.0000 (100.00%)\n",
      "\n",
      "   Expected lift: 12.84%\n",
      "   95% Credible Interval: (10.51%, 15.21%)\n",
      "\n",
      "   Expected loss if choose A: 0.031881 (in conversion rate)\n",
      "   Expected loss if choose B: 0.000000\n",
      "   â†’ Recommendation: Choose B\n",
      "\n",
      "ðŸ’¡ NOTE: P(B > A) â‰ˆ 100% with large samples and real effects.\n",
      "   This is CORRECT - there's essentially no posterior mass where A > B.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âš–ï¸ 2.2 MULTIPLE TESTING CORRECTION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ THE PROBLEM:\n",
      "   Testing 5 metrics at Î±=0.05: P(â‰¥1 FP) = 1 - 0.95âµ = 22.6%!\n",
      "\n",
      "ðŸ“ SOLUTIONS:\n",
      "   â€¢ Bonferroni: Î±_adj = Î±/n (strict, controls FWER)\n",
      "   â€¢ Benjamini-Hochberg: Controls FDR (less strict, more power)\n",
      "\n",
      "ðŸ” WHEN TO USE:\n",
      "   â€¢ Bonferroni: Safety metrics, confirmatory analysis\n",
      "   â€¢ BH: Exploratory analysis, many metrics\n",
      "\n",
      "ðŸ“Š RESULTS (Using ACTUAL experiment metrics):\n",
      "   False positive risk (no correction): 22.6%\n",
      "\n",
      "   Metric              Effect        Raw p       Bonf p         BH p   Raw  Bonf    BH\n",
      "   ----------------------------------------------------------------------------------\n",
      "   converted           0.0319     0.000000     0.000000     0.000000     âœ…     âœ…     âœ…\n",
      "   sessions_week1        1.06     0.000000     0.000000     0.000000     âœ…     âœ…     âœ…\n",
      "   pages_viewed          2.10     0.000000     0.000000     0.000000     âœ…     âœ…     âœ…\n",
      "   retention_7d        0.0488     0.000000     0.000000     0.000000     âœ…     âœ…     âœ…\n",
      "   nps_score           0.5027     0.000000     0.000000     0.000000     âœ…     âœ…     âœ…\n",
      "\n",
      "   Summary: Raw=5/5, Bonferroni=5/5, BH=5/5\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“‰ 2.3 CUPED (Variance Reduction)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ THE IDEA:\n",
      "   Pre-experiment data is correlated with outcome.\n",
      "   By \"adjusting out\" this correlation, we reduce variance!\n",
      "\n",
      "ðŸ“ FORMULA:\n",
      "   Y_adj = Y - Î¸(X - XÌ„)\n",
      "   where Î¸ = Cov(Y,X) / Var(X)\n",
      "\n",
      "   Variance reduction â‰ˆ ÏÂ² (correlation squared)\n",
      "   If Ï = 0.5, we reduce variance by 25%!\n",
      "\n",
      "ðŸ”§ PRACTICAL BENEFIT:\n",
      "   20% variance reduction â†’ ~20% fewer users needed\n",
      "\n",
      "ðŸ“Š RESULTS (Revenue, converters):\n",
      "   Correlation (pre_revenue â†” revenue): 0.4395\n",
      "   Î¸ (adjustment coefficient): 0.4541\n",
      "\n",
      "   Theoretical variance reduction (ÏÂ²): 19.3%\n",
      "   Actual variance reduction: 19.3%\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚             â”‚ Raw              â”‚ CUPED            â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚ Effect      â”‚ $        17.26   â”‚ $        17.45   â”‚\n",
      "   â”‚ SE          â”‚ $         1.10   â”‚ $         0.99   â”‚\n",
      "   â”‚ CI Width    â”‚ $         4.30   â”‚ $         3.86   â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   SE reduction: 10.3%\n",
      "   â†’ We could run with ~10% fewer users for same power!\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”„ 2.4 BOOTSTRAP CONFIDENCE INTERVALS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY BOOTSTRAP?\n",
      "   â€¢ Non-parametric: no normality assumption\n",
      "   â€¢ Great for skewed data (revenue!)\n",
      "   â€¢ Works for any statistic (medians, ratios, percentiles)\n",
      "\n",
      "ðŸ“ HOW IT WORKS:\n",
      "   1. Resample with replacement\n",
      "   2. Compute statistic difference\n",
      "   3. Repeat 10,000 times\n",
      "   4. Use percentiles for CI\n",
      "\n",
      "ðŸ“Š RESULTS (Revenue, 10,000 bootstrap samples):\n",
      "   Point estimate: $17.26\n",
      "   Bootstrap SE: $1.10\n",
      "   95% CI (percentile): ($15.08, $19.44)\n",
      "\n",
      "   Significant: âœ… Yes\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š 2.5 MANN-WHITNEY U TEST (Non-parametric)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY USE IT?\n",
      "   â€¢ Non-parametric: no normality assumption\n",
      "   â€¢ Compares distributions, not just means\n",
      "   â€¢ Robust to outliers\n",
      "\n",
      "ðŸ” WHEN TO USE:\n",
      "   â€¢ Heavily skewed data\n",
      "   â€¢ Ordinal data\n",
      "   â€¢ When t-test assumptions violated\n",
      "\n",
      "ðŸ“Š RESULTS (Revenue):\n",
      "   Control median: $153.80\n",
      "   Treatment median: $171.29\n",
      "\n",
      "   U-statistic: 98,965,455\n",
      "   P-value: 0.000000 âœ… Significant\n",
      "\n",
      "   Effect Size (rank-biserial r): -0.1375 â†’ Small\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“‹ 2.6 ITT vs PER-PROTOCOL ANALYSIS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ THE DISTINCTION:\n",
      "   â€¢ ITT: Analyze EVERYONE as randomized (gold standard)\n",
      "   â€¢ Per-Protocol: Only those who SAW the treatment\n",
      "\n",
      "ðŸ“š WHY BOTH?\n",
      "   â€¢ ITT preserves randomization, gives unbiased estimate\n",
      "   â€¢ PP shows effect on those actually exposed\n",
      "   â€¢ Big gap = implementation problems\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Trigger rates: A = 100.0%, B = 90.0%\n",
      "   â†’ 10% of B users didn't see treatment!\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚                 â”‚ ITT             â”‚ Per-Protocol    â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚ N (Control)     â”‚          50,045 â”‚          50,045 â”‚\n",
      "   â”‚ N (Treatment)   â”‚          49,955 â”‚          44,957 â”‚\n",
      "   â”‚ Effect          â”‚          0.0319 â”‚          0.0363 â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   PP effect is 14% higher than ITT\n",
      "   â†’ Makes sense: PP measures effect on those who actually saw treatment\n",
      "\n",
      "ðŸ’¡ RECOMMENDATION: Report ITT as primary (unbiased), PP as secondary\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ LEVEL 3: ADVANCED (Differentiating for Senior roles)\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸš€ 3.1 CUPAC (ML-Enhanced Variance Reduction)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ IMPROVEMENT OVER CUPED:\n",
      "   â€¢ Uses ML to predict from MULTIPLE features\n",
      "   â€¢ Captures non-linear relationships\n",
      "   â€¢ Often achieves 25-40%+ variance reduction\n",
      "\n",
      "ðŸ“ METHOD (DoorDash, 2020):\n",
      "   1. Train ML model to predict Y from pre-experiment features\n",
      "   2. Use cross-validated predictions (avoid overfitting)\n",
      "   3. Apply CUPED adjustment with predictions as covariate\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Model RÂ²: 0.2706\n",
      "   Prediction correlation: 0.5203\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚                 â”‚ Raw          â”‚ CUPED        â”‚ CUPAC        â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚ Var. Reduction  â”‚ --           â”‚       19.3%  â”‚       27.1%  â”‚\n",
      "   â”‚ SE Reduction    â”‚ --           â”‚       10.3%  â”‚       14.7%  â”‚\n",
      "   â”‚ SE              â”‚ $      1.10  â”‚ $      0.99  â”‚ $      0.94  â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   CUPAC achieves 1.4x the variance reduction of CUPED!\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â±ï¸ 3.2 SEQUENTIAL TESTING\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ THE PROBLEM:\n",
      "   Looking at results multiple times inflates Type I error.\n",
      "   5 looks at Î±=0.05 â†’ actual error â‰ˆ 14%!\n",
      "\n",
      "ðŸ“ SOLUTION:\n",
      "   Use adjusted boundaries at each look.\n",
      "\n",
      "   Methods:\n",
      "   â€¢ O'Brien-Fleming: Conservative early, full power at end\n",
      "   â€¢ Pocock: Constant boundaries, easier to stop early\n",
      "\n",
      "ðŸ“Š RESULTS (Using actual experiment z-statistic):\n",
      "   Actual z-statistic: 11.4369\n",
      "   Current look: 3 of 5\n",
      "   Information fraction: 60%\n",
      "\n",
      "   O'Brien-Fleming boundary: 2.5303\n",
      "   |z| > boundary? 11.4369 > 2.5303 â†’ Yes\n",
      "\n",
      "   Decision: âœ… STOP - Significant at interim\n",
      "\n",
      "ðŸ’¡ WHY THIS MATTERS:\n",
      "   Without correction, checking 5 times gives ~14% false positive rate.\n",
      "   Sequential testing maintains valid Î± = 5%.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸŽ¯ 3.3 X-LEARNER (Heterogeneous Treatment Effects)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY X-LEARNER?\n",
      "   â€¢ Estimates individual-level treatment effects (CATE)\n",
      "   â€¢ Better than stratified analysis for complex heterogeneity\n",
      "   â€¢ Enables personalization decisions\n",
      "\n",
      "ðŸ“ THE 4 STEPS:\n",
      "   1. Fit Î¼â‚€(x) on control, Î¼â‚(x) on treatment\n",
      "   2. Impute effects: Ï„â‚ = Yâ‚ - Î¼â‚€(Xâ‚), Ï„â‚€ = Î¼â‚(Xâ‚€) - Yâ‚€\n",
      "   3. Fit Ï„ models on imputed effects\n",
      "   4. Combine with propensity weighting\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "   Average Treatment Effect (ATE): 0.0322 (3.22pp)\n",
      "   ATT (effect on treated): 0.0321\n",
      "   ATC (effect on control): 0.0322\n",
      "\n",
      "   CATE Distribution:\n",
      "   â€¢ Std dev: 0.0210\n",
      "   â€¢ Range: (-0.6464, 0.4854)\n",
      "   â€¢ p10: 0.0164, p50: 0.0333, p90: 0.0484\n",
      "\n",
      "   Heterogeneity detected: Yes\n",
      "\n",
      "ðŸ’¡ INTERPRETATION:\n",
      "   High heterogeneity means treatment works differently for different users.\n",
      "   â†’ Consider personalization or targeting high-CATE segments.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š 3.4 QUANTILE TREATMENT EFFECTS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY QUANTILES?\n",
      "   â€¢ Mean can HIDE heterogeneity\n",
      "   â€¢ Treatment might affect distribution differently\n",
      "   â€¢ Example: Helps low spenders but not high spenders\n",
      "\n",
      "\n",
      "   Quantile        Control    Treatment       Effect\n",
      "   --------------------------------------------------\n",
      "   p10       $     83.11  $     99.59  $     16.47\n",
      "   p25       $    110.26  $    127.49  $     17.23\n",
      "   p50       $    153.80  $    171.29  $     17.50\n",
      "   p75       $    213.42  $    231.47  $     18.05\n",
      "   p90       $    288.26  $    304.39  $     16.12\n",
      "\n",
      "   Effect range: $16.12 to $18.05\n",
      "   Heterogeneous: No\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š 3.5 DELTA METHOD (Ratio Metrics)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ RATIO METRICS:\n",
      "   â€¢ Revenue per user = Total Revenue / Users\n",
      "   â€¢ CTR = Clicks / Impressions\n",
      "   â€¢ Pages per session = Pages / Sessions\n",
      "\n",
      "ðŸ“ THE PROBLEM:\n",
      "   Ratio of means â‰  Mean of ratios\n",
      "   Need proper variance estimation!\n",
      "\n",
      "ðŸ“Š RESULTS (Revenue per User):\n",
      "   Control: $43.01/user\n",
      "   Treatment: $53.37/user\n",
      "\n",
      "   Difference: $10.36 (24.1% relative)\n",
      "   SE (Delta method): $0.58\n",
      "   95% CI: ($9.21, $11.51)\n",
      "   P-value: 0.000000\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ‚ï¸ 3.6 WINSORIZATION (Outlier Handling)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ WHY WINSORIZE?\n",
      "   â€¢ Outliers can dominate revenue metrics\n",
      "   â€¢ More robust than trimming (keeps sample size)\n",
      "   â€¢ Common for revenue, time-on-site\n",
      "\n",
      "ðŸ“Š RESULTS (1%/99% Winsorization):\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚             â”‚ Raw          â”‚ Winsorized   â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚ Mean A      â”‚ $    173.17  â”‚ $    172.49  â”‚\n",
      "   â”‚ Mean B      â”‚ $    190.43  â”‚ $    189.68  â”‚\n",
      "   â”‚ Effect      â”‚ $     17.26  â”‚ $     17.20  â”‚\n",
      "   â”‚ SE          â”‚ $      1.10  â”‚ $      1.05  â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   SE reduction: 4.0%\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ LEVEL 4: PRODUCTION (Real-world deployment)\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… 4.1 DECISION FRAMEWORK\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸ“ DECISION MATRIX:\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚ Significant?    â”‚ Positive?    â”‚ Decision                    â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚ âœ… Yes          â”‚ âœ… Yes       â”‚ âœ… SHIP (if guardrails OK)  â”‚\n",
      "   â”‚ âœ… Yes          â”‚ âŒ No        â”‚ âŒ DO NOT SHIP              â”‚\n",
      "   â”‚ âŒ No           â”‚ âœ… Yes       â”‚ ðŸ”„ CONTINUE or ABANDON      â”‚\n",
      "   â”‚ âŒ No           â”‚ âŒ No        â”‚ âŒ ABANDON                  â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ðŸ“Š THIS EXPERIMENT:\n",
      "   Significant: âœ… Yes (p = 0.00e+00)\n",
      "   Positive: âœ… Yes (lift = 3.19pp)\n",
      "\n",
      "   â†’ DECISION: âœ… SHIP\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ’° 4.2 BUSINESS IMPACT TRANSLATION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š AT SCALE:\n",
      "   Annual users: 10,000,000\n",
      "   Baseline conversion: 24.84%\n",
      "   Treatment conversion: 28.03%\n",
      "\n",
      "   Additional conversions: 318,957/year\n",
      "   Revenue impact: $47,843,622/year\n",
      "\n",
      "   ðŸ’¼ EXECUTIVE SUMMARY:\n",
      "   \"The new onboarding flow is expected to generate 318,957 additional\n",
      "   conversions per year, worth $47.8M in annual revenue.\"\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "â–ˆ COMPLETE CURRICULUM FINISHED!\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Production A/B Testing Framework v7.0 - COMPLETE LEARNING CURRICULUM\n",
    "=====================================================================\n",
    "\n",
    "IMPROVEMENTS OVER v6:\n",
    "- ALL techniques now demonstrated in output (not just in code)\n",
    "- Uses ACTUAL experiment data (no hardcoded values)\n",
    "- Full explanations with WHY, WHEN TO USE, and KEY INSIGHTS\n",
    "- Comparison tables showing technique improvements\n",
    "- Proper effect size interpretation throughout\n",
    "- Complete thought process for learning\n",
    "\n",
    "Author: Geoff (Analytics Engineer / Data Scientist)\n",
    "Last Updated: January 2026\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import mstats\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import zt_ind_solve_power, tt_ind_solve_power\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DATA GENERATOR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"Generate realistic experiment data with proper correlations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def subscription_experiment(n: int = 100000, seed: int = 42) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Novel use case: Subscription product onboarding redesign.\n",
    "        \n",
    "        Creates data with:\n",
    "        - Pre-experiment metrics correlated with outcomes (for CUPED/CUPAC)\n",
    "        - Heterogeneous treatment effects by segment\n",
    "        - Triggered flag for ITT vs Per-Protocol\n",
    "        - Multiple metrics for multiple testing demo\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Latent user quality drives everything\n",
    "        user_quality = np.random.beta(2, 3, size=n)\n",
    "        \n",
    "        # Pre-experiment data (correlated with outcomes)\n",
    "        pre_engagement = 0.7 * user_quality + 0.3 * np.random.beta(2, 3, size=n)\n",
    "        pre_revenue = np.where(\n",
    "            np.random.random(n) < (0.3 + 0.5 * pre_engagement),\n",
    "            np.random.lognormal(4 + 1.5 * user_quality, 0.5, size=n), 0)\n",
    "        pre_sessions = np.random.poisson(5 + 15 * pre_engagement, size=n)\n",
    "        \n",
    "        # Segments for HTE\n",
    "        channel = np.random.choice(['Organic', 'Paid', 'Social', 'Referral'], n, p=[0.3, 0.25, 0.25, 0.2])\n",
    "        device = np.random.choice(['iOS', 'Android', 'Web'], n, p=[0.4, 0.35, 0.25])\n",
    "        \n",
    "        # Treatment assignment\n",
    "        variant = np.random.choice(['A', 'B'], n, p=[0.5, 0.5])\n",
    "        is_treatment = variant == 'B'\n",
    "        \n",
    "        # Triggered (90% see treatment in B, 100% in A)\n",
    "        triggered = np.where(is_treatment & (np.random.random(n) < 0.9), 1,\n",
    "                           np.where(~is_treatment, 1, 0))\n",
    "        \n",
    "        # Conversion with heterogeneous treatment effect\n",
    "        base_conv = 0.15 + 0.25 * pre_engagement\n",
    "        te = 0.03 * (0.7 + 0.6 * pre_engagement) * is_treatment * triggered\n",
    "        te = np.where(channel == 'Referral', te * 1.3, te)\n",
    "        conv_prob = np.clip(base_conv + te, 0, 1)\n",
    "        converted = np.random.binomial(1, conv_prob)\n",
    "        \n",
    "        # Revenue (only converters)\n",
    "        revenue = np.zeros(n)\n",
    "        conv_mask = converted == 1\n",
    "        n_conv = conv_mask.sum()\n",
    "        base_rev = np.random.lognormal(\n",
    "            4.5 + 0.8 * user_quality[conv_mask] + 0.4 * np.log1p(pre_revenue[conv_mask])/5, \n",
    "            0.4, n_conv)\n",
    "        te_rev = 15 * (1 + 0.5 * user_quality[conv_mask]) * is_treatment[conv_mask]\n",
    "        revenue[conv_mask] = np.maximum(0, base_rev + te_rev)\n",
    "        \n",
    "        # Additional metrics for multiple testing\n",
    "        sessions_week1 = np.random.poisson(3 + 8 * pre_engagement + 2 * converted + is_treatment, size=n)\n",
    "        pages_viewed = np.random.poisson(5 + 10 * pre_engagement + 3 * converted + 2 * is_treatment, size=n)\n",
    "        retention_7d = np.random.binomial(1, 0.4 + 0.3 * pre_engagement + 0.05 * is_treatment)\n",
    "        nps_score = np.clip(5 + 3 * user_quality + 0.5 * is_treatment + np.random.normal(0, 1.5, n), 1, 10)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'user_id': np.arange(1, n + 1),\n",
    "            'variant': variant,\n",
    "            'channel': channel,\n",
    "            'device': device,\n",
    "            'triggered': triggered,\n",
    "            'pre_engagement': pre_engagement,\n",
    "            'pre_revenue': pre_revenue,\n",
    "            'pre_sessions': pre_sessions,\n",
    "            'converted': converted,\n",
    "            'revenue': revenue,\n",
    "            'sessions_week1': sessions_week1,\n",
    "            'pages_viewed': pages_viewed,\n",
    "            'retention_7d': retention_7d,\n",
    "            'nps_score': nps_score,\n",
    "            '_user_quality': user_quality\n",
    "        })\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# COMPREHENSIVE DEMO WITH FULL EXPLANATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def run_complete_curriculum():\n",
    "    \"\"\"\n",
    "    Complete A/B Testing curriculum with full explanations.\n",
    "    Every technique is demonstrated with actual data and explained in detail.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"â•\" * 90)\n",
    "    print(\"â–ˆ COMPLETE A/B TESTING CURRICULUM - WITH FULL THOUGHT PROCESS\")\n",
    "    print(\"â–ˆ Everything you need to ace experimentation interviews\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # DATA GENERATION\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š DATA GENERATION: Subscription Onboarding Experiment\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    df = DataGenerator.subscription_experiment(n=100000)\n",
    "    df_a = df[df['variant'] == 'A']\n",
    "    df_b = df[df['variant'] == 'B']\n",
    "    \n",
    "    conv_a, conv_b = df_a['converted'].mean(), df_b['converted'].mean()\n",
    "    n_a, n_b = len(df_a), len(df_b)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Scenario: Testing redesigned onboarding flow for subscription product\n",
    "- Control (A): Existing onboarding  \n",
    "- Treatment (B): New personalized onboarding\n",
    "\n",
    "Sample: {n_a + n_b:,} users ({n_a:,} control, {n_b:,} treatment)\n",
    "Conversion: A = {conv_a:.2%}, B = {conv_b:.2%}\n",
    "Observed Lift: {(conv_b/conv_a - 1)*100:.1f}% relative ({(conv_b - conv_a)*100:.2f} percentage points)\n",
    "\"\"\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LEVEL 1: FUNDAMENTALS\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n\" + \"â•\" * 90)\n",
    "    print(\"â–ˆ LEVEL 1: FUNDAMENTALS (Must-know for any DS role)\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1.1 SAMPLE SIZE / POWER ANALYSIS\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“ 1.1 SAMPLE SIZE CALCULATION\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ THE QUESTION: \"How many users do we need?\"\n",
    "   This is the #1 most common experimentation interview question.\n",
    "\n",
    "ðŸ“š KEY CONCEPTS:\n",
    "   â€¢ Power (1-Î²): Probability of detecting a TRUE effect. Typically 80%.\n",
    "   â€¢ Alpha (Î±): False positive rate. Typically 5%.\n",
    "   â€¢ MDE: Minimum Detectable Effect - smallest lift worth detecting.\n",
    "   \n",
    "ðŸ“ FORMULA (binary metrics):\n",
    "   n = 2 Ã— (z_Î± + z_Î²)Â² Ã— pÌ„(1-pÌ„) / Î´Â²\n",
    "   \n",
    "   where z_Î±=1.96 (Î±=0.05), z_Î²=0.84 (80% power), pÌ„=pooled rate, Î´=effect size\n",
    "\"\"\")\n",
    "    \n",
    "    # Calculate for binary\n",
    "    baseline = 0.25\n",
    "    mde_rel = 0.10\n",
    "    p1, p2 = baseline, baseline * (1 + mde_rel)\n",
    "    h = 2 * (np.arcsin(np.sqrt(p2)) - np.arcsin(np.sqrt(p1)))\n",
    "    n_binary = int(np.ceil(zt_ind_solve_power(effect_size=h, alpha=0.05, power=0.80)))\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š EXAMPLE - Binary Metric (Conversion):\n",
    "   Baseline: {p1:.0%}\n",
    "   MDE: {mde_rel:.0%} relative lift ({p1:.0%} â†’ {p2:.1%})\n",
    "   Cohen's h: {h:.4f}\n",
    "   \n",
    "   âœ… RESULT: Need {n_binary:,} users/group ({n_binary*2:,} total)\n",
    "\"\"\")\n",
    "    \n",
    "    # Calculate for continuous\n",
    "    baseline_rev, std_rev = 175, 80\n",
    "    mde_abs = 15\n",
    "    d = mde_abs / std_rev\n",
    "    n_cont = int(np.ceil(tt_ind_solve_power(effect_size=d, alpha=0.05, power=0.80)))\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š EXAMPLE - Continuous Metric (Revenue):\n",
    "   Baseline: ${baseline_rev} (std=${std_rev})\n",
    "   MDE: ${mde_abs} absolute lift\n",
    "   Cohen's d: {d:.4f}\n",
    "   \n",
    "   âœ… RESULT: Need {n_cont:,} users/group ({n_cont*2:,} total)\n",
    "\n",
    "ðŸ’¡ INTERVIEW TIP: Always ask:\n",
    "   - One-sided or two-sided test?\n",
    "   - What's the business-relevant MDE?\n",
    "   - How long to reach this sample size?\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1.2 Z-TEST FOR PROPORTIONS\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š 1.2 Z-TEST FOR PROPORTIONS (Binary Metrics)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ USE WHEN: Comparing conversion rates, CTR, or any 0/1 outcome.\n",
    "\n",
    "ðŸ“ THE TEST:\n",
    "   Hâ‚€: p_A = p_B (no difference)\n",
    "   Hâ‚: p_A â‰  p_B (two-sided)\n",
    "   \n",
    "   Z = (p_B - p_A) / SE_pooled\n",
    "   where SE_pooled = âˆš[pÌ„(1-pÌ„)(1/n_A + 1/n_B)]\n",
    "   \n",
    "âš ï¸ KEY INSIGHT: We use POOLED SE for the test (assumes Hâ‚€ true),\n",
    "   but NON-POOLED SE for the confidence interval.\n",
    "\"\"\")\n",
    "    \n",
    "    x_a, x_b = int(df_a['converted'].sum()), int(df_b['converted'].sum())\n",
    "    p_a, p_b = x_a/n_a, x_b/n_b\n",
    "    \n",
    "    # Pooled for test\n",
    "    p_pooled = (x_a + x_b) / (n_a + n_b)\n",
    "    se_pooled = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_a + 1/n_b))\n",
    "    z_stat = (p_b - p_a) / se_pooled\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    # Non-pooled for CI\n",
    "    se_diff = np.sqrt(p_a*(1-p_a)/n_a + p_b*(1-p_b)/n_b)\n",
    "    ci = (p_b - p_a - 1.96*se_diff, p_b - p_a + 1.96*se_diff)\n",
    "    \n",
    "    # Effect size\n",
    "    cohens_h = 2 * (np.arcsin(np.sqrt(p_b)) - np.arcsin(np.sqrt(p_a)))\n",
    "    h_interp = \"Large\" if abs(cohens_h) > 0.8 else \"Medium\" if abs(cohens_h) > 0.5 else \"Small\" if abs(cohens_h) > 0.2 else \"Negligible\"\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Control:   {x_a:,} / {n_a:,} = {p_a:.4f} ({p_a:.2%})\n",
    "   Treatment: {x_b:,} / {n_b:,} = {p_b:.4f} ({p_b:.2%})\n",
    "   \n",
    "   Absolute lift: {(p_b - p_a)*100:.2f} percentage points\n",
    "   Relative lift: {(p_b/p_a - 1)*100:.1f}%\n",
    "   \n",
    "   Z-statistic: {z_stat:.4f}\n",
    "   P-value: {p_value:.2e} {'âœ… Significant!' if p_value < 0.05 else 'âŒ Not significant'}\n",
    "   95% CI: ({ci[0]*100:.2f}pp, {ci[1]*100:.2f}pp)\n",
    "   \n",
    "   Effect Size (Cohen's h): {cohens_h:.4f} â†’ {h_interp}\n",
    "\n",
    "ðŸ“– P-VALUE INTERPRETATION:\n",
    "   â€¢ P-value = {p_value:.2e} means: IF there were no real effect (Hâ‚€ true),\n",
    "     we'd see a difference this large only {p_value*100:.4f}% of the time.\n",
    "   â€¢ P-value is NOT the probability treatment works!\n",
    "   â€¢ P-value is NOT the probability of a false positive on THIS experiment!\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1.3 WELCH'S T-TEST (Continuous)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š 1.3 WELCH'S T-TEST (Continuous Metrics)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ USE WHEN: Comparing means of continuous outcomes (revenue, time, sessions).\n",
    "\n",
    "ðŸ“ WHY WELCH'S (not Student's)?\n",
    "   â€¢ Does NOT assume equal variances between groups\n",
    "   â€¢ More robust in practice - always use Welch's unless specific reason not to\n",
    "   â€¢ scipy.stats.ttest_ind(equal_var=False) gives Welch's\n",
    "\"\"\")\n",
    "    \n",
    "    # Revenue for converters\n",
    "    df_conv = df[df['converted'] == 1]\n",
    "    rev_a = df_conv[df_conv['variant'] == 'A']['revenue'].values\n",
    "    rev_b = df_conv[df_conv['variant'] == 'B']['revenue'].values\n",
    "    \n",
    "    t_stat, t_pval = stats.ttest_ind(rev_b, rev_a, equal_var=False)\n",
    "    diff = rev_b.mean() - rev_a.mean()\n",
    "    se = np.sqrt(rev_a.var()/len(rev_a) + rev_b.var()/len(rev_b))\n",
    "    ci_t = (diff - 1.96*se, diff + 1.96*se)\n",
    "    \n",
    "    pooled_std = np.sqrt(((len(rev_a)-1)*rev_a.var() + (len(rev_b)-1)*rev_b.var()) / (len(rev_a)+len(rev_b)-2))\n",
    "    cohens_d = diff / pooled_std\n",
    "    d_interp = \"Large\" if abs(cohens_d) > 0.8 else \"Medium\" if abs(cohens_d) > 0.5 else \"Small\" if abs(cohens_d) > 0.2 else \"Negligible\"\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Revenue, converters only):\n",
    "   Control:   n={len(rev_a):,}, mean=${rev_a.mean():.2f}, std=${rev_a.std():.2f}\n",
    "   Treatment: n={len(rev_b):,}, mean=${rev_b.mean():.2f}, std=${rev_b.std():.2f}\n",
    "   \n",
    "   Difference: ${diff:.2f} ({diff/rev_a.mean()*100:.1f}% relative)\n",
    "   95% CI: (${ci_t[0]:.2f}, ${ci_t[1]:.2f})\n",
    "   \n",
    "   T-statistic: {t_stat:.4f}\n",
    "   P-value: {t_pval:.2e} {'âœ… Significant!' if t_pval < 0.05 else 'âŒ Not significant'}\n",
    "   \n",
    "   Effect Size (Cohen's d): {cohens_d:.4f} â†’ {d_interp}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1.4 SRM CHECK\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ” 1.4 SRM (Sample Ratio Mismatch) CHECK\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY CHECK SRM?\n",
    "   If randomization is broken, ALL your results are INVALID.\n",
    "   Always check BEFORE looking at outcome metrics!\n",
    "\n",
    "âš ï¸ COMMON CAUSES:\n",
    "   â€¢ Bot filtering affecting groups differently\n",
    "   â€¢ Page load failures in treatment\n",
    "   â€¢ Browser/device compatibility issues\n",
    "   â€¢ Redirect failures\n",
    "\"\"\")\n",
    "    \n",
    "    expected = np.array([0.5, 0.5]) * (n_a + n_b)\n",
    "    observed = np.array([n_a, n_b])\n",
    "    chi2 = np.sum((observed - expected)**2 / expected)\n",
    "    srm_p = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Expected: A={int(expected[0]):,}, B={int(expected[1]):,} (50/50)\n",
    "   Observed: A={n_a:,}, B={n_b:,} ({n_a/(n_a+n_b):.2%}/{n_b/(n_a+n_b):.2%})\n",
    "   \n",
    "   Chi-square: {chi2:.4f}\n",
    "   P-value: {srm_p:.4f}\n",
    "   \n",
    "   {\"âš ï¸ SRM DETECTED! Investigate before trusting results.\" if srm_p < 0.01 else \"âœ… No SRM - randomization healthy\"}\n",
    "\"\"\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LEVEL 2: INTERMEDIATE\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n\" + \"â•\" * 90)\n",
    "    print(\"â–ˆ LEVEL 2: INTERMEDIATE (Expected for DS roles)\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.1 BAYESIAN A/B TESTING\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸŽ² 2.1 BAYESIAN A/B TESTING\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY BAYESIAN?\n",
    "   â€¢ Gives PROBABILITY statements: \"P(B > A) = 95%\" (intuitive!)\n",
    "   â€¢ No p-hacking concerns with continuous monitoring\n",
    "   â€¢ Can compute expected loss for decision-making\n",
    "\n",
    "ðŸ“ MODEL: Beta-Binomial\n",
    "   Prior: Beta(1, 1) = Uniform\n",
    "   Posterior: Beta(1 + successes, 1 + failures)\n",
    "\"\"\")\n",
    "    \n",
    "    # Posterior\n",
    "    post_a = (1 + x_a, 1 + n_a - x_a)\n",
    "    post_b = (1 + x_b, 1 + n_b - x_b)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    samples_a = np.random.beta(*post_a, 100000)\n",
    "    samples_b = np.random.beta(*post_b, 100000)\n",
    "    prob_b_better = (samples_b > samples_a).mean()\n",
    "    lift_samples = (samples_b / samples_a - 1) * 100\n",
    "    \n",
    "    loss_a = np.maximum(0, samples_b - samples_a).mean()\n",
    "    loss_b = np.maximum(0, samples_a - samples_b).mean()\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Posterior A: Beta({post_a[0]}, {post_a[1]})\n",
    "   Posterior B: Beta({post_b[0]}, {post_b[1]})\n",
    "   \n",
    "   P(B > A) = {prob_b_better:.4f} ({prob_b_better:.2%})\n",
    "   \n",
    "   Expected lift: {lift_samples.mean():.2f}%\n",
    "   95% Credible Interval: ({np.percentile(lift_samples, 2.5):.2f}%, {np.percentile(lift_samples, 97.5):.2f}%)\n",
    "   \n",
    "   Expected loss if choose A: {loss_a:.6f} (in conversion rate)\n",
    "   Expected loss if choose B: {loss_b:.6f}\n",
    "   â†’ Recommendation: {'Choose B' if loss_b < loss_a else 'Choose A'}\n",
    "\n",
    "ðŸ’¡ NOTE: P(B > A) â‰ˆ 100% with large samples and real effects.\n",
    "   This is CORRECT - there's essentially no posterior mass where A > B.\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.2 MULTIPLE TESTING CORRECTION\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"âš–ï¸ 2.2 MULTIPLE TESTING CORRECTION\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ THE PROBLEM:\n",
    "   Testing 5 metrics at Î±=0.05: P(â‰¥1 FP) = 1 - 0.95âµ = 22.6%!\n",
    "\n",
    "ðŸ“ SOLUTIONS:\n",
    "   â€¢ Bonferroni: Î±_adj = Î±/n (strict, controls FWER)\n",
    "   â€¢ Benjamini-Hochberg: Controls FDR (less strict, more power)\n",
    "   \n",
    "ðŸ” WHEN TO USE:\n",
    "   â€¢ Bonferroni: Safety metrics, confirmatory analysis\n",
    "   â€¢ BH: Exploratory analysis, many metrics\n",
    "\"\"\")\n",
    "    \n",
    "    # Calculate ACTUAL p-values from real metrics\n",
    "    metrics = ['converted', 'sessions_week1', 'pages_viewed', 'retention_7d', 'nps_score']\n",
    "    pvalues = []\n",
    "    effects = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if df[metric].nunique() <= 2:  # Binary\n",
    "            _, pval = proportions_ztest(\n",
    "                [df_a[metric].sum(), df_b[metric].sum()],\n",
    "                [len(df_a), len(df_b)]\n",
    "            )\n",
    "            effect = df_b[metric].mean() - df_a[metric].mean()\n",
    "        else:  # Continuous\n",
    "            _, pval = stats.ttest_ind(df_b[metric].dropna(), df_a[metric].dropna(), equal_var=False)\n",
    "            effect = df_b[metric].mean() - df_a[metric].mean()\n",
    "        pvalues.append(pval)\n",
    "        effects.append(effect)\n",
    "    \n",
    "    _, bonf_p, _, _ = multipletests(pvalues, method='bonferroni')\n",
    "    _, bh_p, _, _ = multipletests(pvalues, method='fdr_bh')\n",
    "    \n",
    "    fp_risk = 1 - (1 - 0.05) ** len(metrics)\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Using ACTUAL experiment metrics):\n",
    "   False positive risk (no correction): {fp_risk:.1%}\n",
    "   \n",
    "   {'Metric':<15} {'Effect':>10} {'Raw p':>12} {'Bonf p':>12} {'BH p':>12} {'Raw':>5} {'Bonf':>5} {'BH':>5}\n",
    "   {'-'*82}\"\"\")\n",
    "    \n",
    "    for i, m in enumerate(metrics):\n",
    "        raw_sig = 'âœ…' if pvalues[i] < 0.05 else 'âŒ'\n",
    "        bonf_sig = 'âœ…' if bonf_p[i] < 0.05 else 'âŒ'\n",
    "        bh_sig = 'âœ…' if bh_p[i] < 0.05 else 'âŒ'\n",
    "        eff_str = f\"{effects[i]:.4f}\" if abs(effects[i]) < 1 else f\"{effects[i]:.2f}\"\n",
    "        print(f\"   {m:<15} {eff_str:>10} {pvalues[i]:>12.6f} {bonf_p[i]:>12.6f} {bh_p[i]:>12.6f} {raw_sig:>5} {bonf_sig:>5} {bh_sig:>5}\")\n",
    "    \n",
    "    print(f\"\"\"\n",
    "   Summary: Raw={sum(p < 0.05 for p in pvalues)}/5, Bonferroni={sum(p < 0.05 for p in bonf_p)}/5, BH={sum(p < 0.05 for p in bh_p)}/5\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.3 CUPED\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“‰ 2.3 CUPED (Variance Reduction)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ THE IDEA:\n",
    "   Pre-experiment data is correlated with outcome.\n",
    "   By \"adjusting out\" this correlation, we reduce variance!\n",
    "\n",
    "ðŸ“ FORMULA:\n",
    "   Y_adj = Y - Î¸(X - XÌ„)\n",
    "   where Î¸ = Cov(Y,X) / Var(X)\n",
    "   \n",
    "   Variance reduction â‰ˆ ÏÂ² (correlation squared)\n",
    "   If Ï = 0.5, we reduce variance by 25%!\n",
    "\n",
    "ðŸ”§ PRACTICAL BENEFIT:\n",
    "   20% variance reduction â†’ ~20% fewer users needed\n",
    "\"\"\")\n",
    "    \n",
    "    # CUPED on revenue\n",
    "    y = df_conv['revenue'].values\n",
    "    x = df_conv['pre_revenue'].values\n",
    "    treatment = (df_conv['variant'] == 'B').astype(int).values\n",
    "    \n",
    "    corr = np.corrcoef(y, x)[0, 1]\n",
    "    theta = np.cov(y, x)[0, 1] / x.var(ddof=1)\n",
    "    y_adj = y - theta * (x - x.mean())\n",
    "    \n",
    "    # Raw\n",
    "    y_c, y_t = y[treatment == 0], y[treatment == 1]\n",
    "    effect_raw = y_t.mean() - y_c.mean()\n",
    "    se_raw = np.sqrt(y_c.var()/len(y_c) + y_t.var()/len(y_t))\n",
    "    ci_raw = (effect_raw - 1.96*se_raw, effect_raw + 1.96*se_raw)\n",
    "    \n",
    "    # CUPED\n",
    "    y_c_adj, y_t_adj = y_adj[treatment == 0], y_adj[treatment == 1]\n",
    "    effect_adj = y_t_adj.mean() - y_c_adj.mean()\n",
    "    se_adj = np.sqrt(y_c_adj.var()/len(y_c_adj) + y_t_adj.var()/len(y_t_adj))\n",
    "    ci_adj = (effect_adj - 1.96*se_adj, effect_adj + 1.96*se_adj)\n",
    "    \n",
    "    var_reduction = 1 - y_adj.var() / y.var()\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Revenue, converters):\n",
    "   Correlation (pre_revenue â†” revenue): {corr:.4f}\n",
    "   Î¸ (adjustment coefficient): {theta:.4f}\n",
    "   \n",
    "   Theoretical variance reduction (ÏÂ²): {corr**2 * 100:.1f}%\n",
    "   Actual variance reduction: {var_reduction * 100:.1f}%\n",
    "   \n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚             â”‚ Raw              â”‚ CUPED            â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   â”‚ Effect      â”‚ ${effect_raw:>13.2f}   â”‚ ${effect_adj:>13.2f}   â”‚\n",
    "   â”‚ SE          â”‚ ${se_raw:>13.2f}   â”‚ ${se_adj:>13.2f}   â”‚\n",
    "   â”‚ CI Width    â”‚ ${ci_raw[1]-ci_raw[0]:>13.2f}   â”‚ ${ci_adj[1]-ci_adj[0]:>13.2f}   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   \n",
    "   SE reduction: {(1 - se_adj/se_raw)*100:.1f}%\n",
    "   â†’ We could run with ~{(1 - se_adj/se_raw)*100:.0f}% fewer users for same power!\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.4 BOOTSTRAP CI\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ”„ 2.4 BOOTSTRAP CONFIDENCE INTERVALS\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY BOOTSTRAP?\n",
    "   â€¢ Non-parametric: no normality assumption\n",
    "   â€¢ Great for skewed data (revenue!)\n",
    "   â€¢ Works for any statistic (medians, ratios, percentiles)\n",
    "\n",
    "ðŸ“ HOW IT WORKS:\n",
    "   1. Resample with replacement\n",
    "   2. Compute statistic difference\n",
    "   3. Repeat 10,000 times\n",
    "   4. Use percentiles for CI\n",
    "\"\"\")\n",
    "    \n",
    "    boot_diffs = []\n",
    "    for _ in range(10000):\n",
    "        boot_a = np.random.choice(rev_a, len(rev_a), replace=True)\n",
    "        boot_b = np.random.choice(rev_b, len(rev_b), replace=True)\n",
    "        boot_diffs.append(boot_b.mean() - boot_a.mean())\n",
    "    boot_diffs = np.array(boot_diffs)\n",
    "    \n",
    "    boot_ci = (np.percentile(boot_diffs, 2.5), np.percentile(boot_diffs, 97.5))\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Revenue, 10,000 bootstrap samples):\n",
    "   Point estimate: ${diff:.2f}\n",
    "   Bootstrap SE: ${boot_diffs.std():.2f}\n",
    "   95% CI (percentile): (${boot_ci[0]:.2f}, ${boot_ci[1]:.2f})\n",
    "   \n",
    "   Significant: {'âœ… Yes' if not (boot_ci[0] <= 0 <= boot_ci[1]) else 'âŒ No'}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.5 MANN-WHITNEY U\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š 2.5 MANN-WHITNEY U TEST (Non-parametric)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY USE IT?\n",
    "   â€¢ Non-parametric: no normality assumption\n",
    "   â€¢ Compares distributions, not just means\n",
    "   â€¢ Robust to outliers\n",
    "\n",
    "ðŸ” WHEN TO USE:\n",
    "   â€¢ Heavily skewed data\n",
    "   â€¢ Ordinal data\n",
    "   â€¢ When t-test assumptions violated\n",
    "\"\"\")\n",
    "    \n",
    "    u_stat, u_pval = stats.mannwhitneyu(rev_b, rev_a, alternative='two-sided')\n",
    "    r = 1 - (2 * u_stat) / (len(rev_a) * len(rev_b))\n",
    "    r_interp = \"Large\" if abs(r) > 0.5 else \"Medium\" if abs(r) > 0.3 else \"Small\" if abs(r) > 0.1 else \"Negligible\"\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Revenue):\n",
    "   Control median: ${np.median(rev_a):.2f}\n",
    "   Treatment median: ${np.median(rev_b):.2f}\n",
    "   \n",
    "   U-statistic: {u_stat:,.0f}\n",
    "   P-value: {u_pval:.6f} {'âœ… Significant' if u_pval < 0.05 else 'âŒ Not significant'}\n",
    "   \n",
    "   Effect Size (rank-biserial r): {r:.4f} â†’ {r_interp}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2.6 ITT vs PER-PROTOCOL\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“‹ 2.6 ITT vs PER-PROTOCOL ANALYSIS\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ THE DISTINCTION:\n",
    "   â€¢ ITT: Analyze EVERYONE as randomized (gold standard)\n",
    "   â€¢ Per-Protocol: Only those who SAW the treatment\n",
    "\n",
    "ðŸ“š WHY BOTH?\n",
    "   â€¢ ITT preserves randomization, gives unbiased estimate\n",
    "   â€¢ PP shows effect on those actually exposed\n",
    "   â€¢ Big gap = implementation problems\n",
    "\"\"\")\n",
    "    \n",
    "    # ITT\n",
    "    effect_itt = df_b['converted'].mean() - df_a['converted'].mean()\n",
    "    \n",
    "    # Per-Protocol\n",
    "    df_a_pp = df[(df['variant'] == 'A') & (df['triggered'] == 1)]\n",
    "    df_b_pp = df[(df['variant'] == 'B') & (df['triggered'] == 1)]\n",
    "    effect_pp = df_b_pp['converted'].mean() - df_a_pp['converted'].mean()\n",
    "    \n",
    "    trigger_a = df_a['triggered'].mean()\n",
    "    trigger_b = df_b['triggered'].mean()\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Trigger rates: A = {trigger_a:.1%}, B = {trigger_b:.1%}\n",
    "   â†’ {(1-trigger_b)*100:.0f}% of B users didn't see treatment!\n",
    "   \n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚                 â”‚ ITT             â”‚ Per-Protocol    â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   â”‚ N (Control)     â”‚ {len(df_a):>15,} â”‚ {len(df_a_pp):>15,} â”‚\n",
    "   â”‚ N (Treatment)   â”‚ {len(df_b):>15,} â”‚ {len(df_b_pp):>15,} â”‚\n",
    "   â”‚ Effect          â”‚ {effect_itt:>15.4f} â”‚ {effect_pp:>15.4f} â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   \n",
    "   PP effect is {(effect_pp/effect_itt - 1)*100:.0f}% higher than ITT\n",
    "   â†’ Makes sense: PP measures effect on those who actually saw treatment\n",
    "\n",
    "ðŸ’¡ RECOMMENDATION: Report ITT as primary (unbiased), PP as secondary\n",
    "\"\"\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LEVEL 3: ADVANCED\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n\" + \"â•\" * 90)\n",
    "    print(\"â–ˆ LEVEL 3: ADVANCED (Differentiating for Senior roles)\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.1 CUPAC\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸš€ 3.1 CUPAC (ML-Enhanced Variance Reduction)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ IMPROVEMENT OVER CUPED:\n",
    "   â€¢ Uses ML to predict from MULTIPLE features\n",
    "   â€¢ Captures non-linear relationships\n",
    "   â€¢ Often achieves 25-40%+ variance reduction\n",
    "\n",
    "ðŸ“ METHOD (DoorDash, 2020):\n",
    "   1. Train ML model to predict Y from pre-experiment features\n",
    "   2. Use cross-validated predictions (avoid overfitting)\n",
    "   3. Apply CUPED adjustment with predictions as covariate\n",
    "\"\"\")\n",
    "    \n",
    "    # CUPAC\n",
    "    X = df_conv[['pre_revenue', 'pre_engagement', 'pre_sessions']].values\n",
    "    model = GradientBoostingRegressor(n_estimators=100, max_depth=4, min_samples_leaf=100, random_state=42)\n",
    "    cv_preds = cross_val_predict(model, X, y, cv=5)\n",
    "    \n",
    "    r2 = 1 - np.sum((y - cv_preds)**2) / np.sum((y - y.mean())**2)\n",
    "    pred_corr = np.corrcoef(y, cv_preds)[0, 1]\n",
    "    \n",
    "    theta_cupac = np.cov(y, cv_preds)[0, 1] / cv_preds.var()\n",
    "    y_cupac = y - theta_cupac * (cv_preds - cv_preds.mean())\n",
    "    \n",
    "    y_c_cupac, y_t_cupac = y_cupac[treatment == 0], y_cupac[treatment == 1]\n",
    "    effect_cupac = y_t_cupac.mean() - y_c_cupac.mean()\n",
    "    se_cupac = np.sqrt(y_c_cupac.var()/len(y_c_cupac) + y_t_cupac.var()/len(y_t_cupac))\n",
    "    var_reduction_cupac = 1 - y_cupac.var() / y.var()\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Model RÂ²: {r2:.4f}\n",
    "   Prediction correlation: {pred_corr:.4f}\n",
    "   \n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚                 â”‚ Raw          â”‚ CUPED        â”‚ CUPAC        â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   â”‚ Var. Reduction  â”‚ --           â”‚ {var_reduction*100:>10.1f}%  â”‚ {var_reduction_cupac*100:>10.1f}%  â”‚\n",
    "   â”‚ SE Reduction    â”‚ --           â”‚ {(1-se_adj/se_raw)*100:>10.1f}%  â”‚ {(1-se_cupac/se_raw)*100:>10.1f}%  â”‚\n",
    "   â”‚ SE              â”‚ ${se_raw:>10.2f}  â”‚ ${se_adj:>10.2f}  â”‚ ${se_cupac:>10.2f}  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   \n",
    "   CUPAC achieves {var_reduction_cupac/var_reduction:.1f}x the variance reduction of CUPED!\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.2 SEQUENTIAL TESTING\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"â±ï¸ 3.2 SEQUENTIAL TESTING\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ THE PROBLEM:\n",
    "   Looking at results multiple times inflates Type I error.\n",
    "   5 looks at Î±=0.05 â†’ actual error â‰ˆ 14%!\n",
    "\n",
    "ðŸ“ SOLUTION:\n",
    "   Use adjusted boundaries at each look.\n",
    "   \n",
    "   Methods:\n",
    "   â€¢ O'Brien-Fleming: Conservative early, full power at end\n",
    "   â€¢ Pocock: Constant boundaries, easier to stop early\n",
    "\"\"\")\n",
    "    \n",
    "    # Use actual z-statistic from experiment\n",
    "    actual_z = z_stat  # From our z-test earlier\n",
    "    current_look = 3\n",
    "    total_looks = 5\n",
    "    info_frac = current_look / total_looks\n",
    "    \n",
    "    # O'Brien-Fleming boundary\n",
    "    boundary = stats.norm.ppf(1 - 0.05/2) / np.sqrt(info_frac)\n",
    "    can_stop = abs(actual_z) > boundary\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Using actual experiment z-statistic):\n",
    "   Actual z-statistic: {actual_z:.4f}\n",
    "   Current look: {current_look} of {total_looks}\n",
    "   Information fraction: {info_frac:.0%}\n",
    "   \n",
    "   O'Brien-Fleming boundary: {boundary:.4f}\n",
    "   |z| > boundary? {abs(actual_z):.4f} > {boundary:.4f} â†’ {'Yes' if can_stop else 'No'}\n",
    "   \n",
    "   Decision: {\"âœ… STOP - Significant at interim\" if can_stop else f\"ðŸ”„ CONTINUE - Wait for look {current_look + 1}\"}\n",
    "\n",
    "ðŸ’¡ WHY THIS MATTERS:\n",
    "   Without correction, checking 5 times gives ~14% false positive rate.\n",
    "   Sequential testing maintains valid Î± = 5%.\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.3 X-LEARNER (HTE)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸŽ¯ 3.3 X-LEARNER (Heterogeneous Treatment Effects)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY X-LEARNER?\n",
    "   â€¢ Estimates individual-level treatment effects (CATE)\n",
    "   â€¢ Better than stratified analysis for complex heterogeneity\n",
    "   â€¢ Enables personalization decisions\n",
    "\n",
    "ðŸ“ THE 4 STEPS:\n",
    "   1. Fit Î¼â‚€(x) on control, Î¼â‚(x) on treatment\n",
    "   2. Impute effects: Ï„â‚ = Yâ‚ - Î¼â‚€(Xâ‚), Ï„â‚€ = Î¼â‚(Xâ‚€) - Yâ‚€\n",
    "   3. Fit Ï„ models on imputed effects\n",
    "   4. Combine with propensity weighting\n",
    "\"\"\")\n",
    "    \n",
    "    # X-Learner\n",
    "    T = (df['variant'] == 'B').astype(int).values\n",
    "    X_full = df[['pre_engagement', 'pre_sessions']].values\n",
    "    Y_full = df['converted'].values\n",
    "    \n",
    "    control_mask = T == 0\n",
    "    treated_mask = T == 1\n",
    "    \n",
    "    mu_0 = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "    mu_1 = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "    mu_0.fit(X_full[control_mask], Y_full[control_mask])\n",
    "    mu_1.fit(X_full[treated_mask], Y_full[treated_mask])\n",
    "    \n",
    "    tau_1 = Y_full[treated_mask] - mu_0.predict(X_full[treated_mask])\n",
    "    tau_0 = mu_1.predict(X_full[control_mask]) - Y_full[control_mask]\n",
    "    \n",
    "    tau_model_0 = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "    tau_model_1 = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "    tau_model_0.fit(X_full[control_mask], tau_0)\n",
    "    tau_model_1.fit(X_full[treated_mask], tau_1)\n",
    "    \n",
    "    g = T.mean()\n",
    "    cate = g * tau_model_0.predict(X_full) + (1 - g) * tau_model_1.predict(X_full)\n",
    "    ate = cate.mean()\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS:\n",
    "   Average Treatment Effect (ATE): {ate:.4f} ({ate*100:.2f}pp)\n",
    "   ATT (effect on treated): {cate[treated_mask].mean():.4f}\n",
    "   ATC (effect on control): {cate[control_mask].mean():.4f}\n",
    "   \n",
    "   CATE Distribution:\n",
    "   â€¢ Std dev: {cate.std():.4f}\n",
    "   â€¢ Range: ({cate.min():.4f}, {cate.max():.4f})\n",
    "   â€¢ p10: {np.percentile(cate, 10):.4f}, p50: {np.percentile(cate, 50):.4f}, p90: {np.percentile(cate, 90):.4f}\n",
    "   \n",
    "   Heterogeneity detected: {'Yes' if cate.std() > abs(ate) * 0.5 else 'No'}\n",
    "   \n",
    "ðŸ’¡ INTERPRETATION:\n",
    "   High heterogeneity means treatment works differently for different users.\n",
    "   â†’ Consider personalization or targeting high-CATE segments.\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.4 QUANTILE TREATMENT EFFECTS\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š 3.4 QUANTILE TREATMENT EFFECTS\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY QUANTILES?\n",
    "   â€¢ Mean can HIDE heterogeneity\n",
    "   â€¢ Treatment might affect distribution differently\n",
    "   â€¢ Example: Helps low spenders but not high spenders\n",
    "\"\"\")\n",
    "    \n",
    "    quantiles = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "    print(f\"\\n   {'Quantile':<10} {'Control':>12} {'Treatment':>12} {'Effect':>12}\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    \n",
    "    for q in quantiles:\n",
    "        q_a = np.percentile(rev_a, q*100)\n",
    "        q_b = np.percentile(rev_b, q*100)\n",
    "        print(f\"   p{int(q*100):<8} ${q_a:>10.2f}  ${q_b:>10.2f}  ${q_b-q_a:>10.2f}\")\n",
    "    \n",
    "    effects_q = [np.percentile(rev_b, q*100) - np.percentile(rev_a, q*100) for q in quantiles]\n",
    "    print(f\"\"\"\n",
    "   Effect range: ${min(effects_q):.2f} to ${max(effects_q):.2f}\n",
    "   Heterogeneous: {'Yes' if max(effects_q) - min(effects_q) > np.abs(np.mean(effects_q)) * 0.5 else 'No'}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.5 DELTA METHOD\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ“Š 3.5 DELTA METHOD (Ratio Metrics)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ RATIO METRICS:\n",
    "   â€¢ Revenue per user = Total Revenue / Users\n",
    "   â€¢ CTR = Clicks / Impressions\n",
    "   â€¢ Pages per session = Pages / Sessions\n",
    "\n",
    "ðŸ“ THE PROBLEM:\n",
    "   Ratio of means â‰  Mean of ratios\n",
    "   Need proper variance estimation!\n",
    "\"\"\")\n",
    "    \n",
    "    # Revenue per user (including non-converters)\n",
    "    num_a = df_a['revenue'].values\n",
    "    denom_a = np.ones(len(df_a))  # Per user\n",
    "    num_b = df_b['revenue'].values\n",
    "    denom_b = np.ones(len(df_b))\n",
    "    \n",
    "    ratio_a = num_a.sum() / len(df_a)\n",
    "    ratio_b = num_b.sum() / len(df_b)\n",
    "    \n",
    "    def delta_variance(num, denom):\n",
    "        n = len(num)\n",
    "        mu_n, mu_d = num.mean(), denom.mean()\n",
    "        var_n = num.var(ddof=1)\n",
    "        var_d = denom.var(ddof=1)\n",
    "        cov_nd = np.cov(num, denom, ddof=1)[0, 1]\n",
    "        var_ratio = (1/mu_d**2) * var_n - (2*mu_n/mu_d**3) * cov_nd + (mu_n**2/mu_d**4) * var_d\n",
    "        return var_ratio / n\n",
    "    \n",
    "    var_a = delta_variance(num_a, denom_a)\n",
    "    var_b = delta_variance(num_b, denom_b)\n",
    "    se_ratio = np.sqrt(var_a + var_b)\n",
    "    diff_ratio = ratio_b - ratio_a\n",
    "    ci_ratio = (diff_ratio - 1.96*se_ratio, diff_ratio + 1.96*se_ratio)\n",
    "    z_ratio = diff_ratio / se_ratio\n",
    "    p_ratio = 2 * (1 - stats.norm.cdf(abs(z_ratio)))\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (Revenue per User):\n",
    "   Control: ${ratio_a:.2f}/user\n",
    "   Treatment: ${ratio_b:.2f}/user\n",
    "   \n",
    "   Difference: ${diff_ratio:.2f} ({diff_ratio/ratio_a*100:.1f}% relative)\n",
    "   SE (Delta method): ${se_ratio:.2f}\n",
    "   95% CI: (${ci_ratio[0]:.2f}, ${ci_ratio[1]:.2f})\n",
    "   P-value: {p_ratio:.6f}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3.6 WINSORIZATION\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"âœ‚ï¸ 3.6 WINSORIZATION (Outlier Handling)\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸŽ¯ WHY WINSORIZE?\n",
    "   â€¢ Outliers can dominate revenue metrics\n",
    "   â€¢ More robust than trimming (keeps sample size)\n",
    "   â€¢ Common for revenue, time-on-site\n",
    "\"\"\")\n",
    "    \n",
    "    rev_a_wins = mstats.winsorize(rev_a, limits=[0.01, 0.01])\n",
    "    rev_b_wins = mstats.winsorize(rev_b, limits=[0.01, 0.01])\n",
    "    \n",
    "    effect_wins = float(rev_b_wins.mean() - rev_a_wins.mean())\n",
    "    se_wins = np.sqrt(float(rev_a_wins.var())/len(rev_a) + float(rev_b_wins.var())/len(rev_b))\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š RESULTS (1%/99% Winsorization):\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚             â”‚ Raw          â”‚ Winsorized   â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   â”‚ Mean A      â”‚ ${rev_a.mean():>10.2f}  â”‚ ${float(rev_a_wins.mean()):>10.2f}  â”‚\n",
    "   â”‚ Mean B      â”‚ ${rev_b.mean():>10.2f}  â”‚ ${float(rev_b_wins.mean()):>10.2f}  â”‚\n",
    "   â”‚ Effect      â”‚ ${diff:>10.2f}  â”‚ ${effect_wins:>10.2f}  â”‚\n",
    "   â”‚ SE          â”‚ ${se:>10.2f}  â”‚ ${se_wins:>10.2f}  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   \n",
    "   SE reduction: {(1 - se_wins/se)*100:.1f}%\n",
    "\"\"\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LEVEL 4: PRODUCTION\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n\" + \"â•\" * 90)\n",
    "    print(\"â–ˆ LEVEL 4: PRODUCTION (Real-world deployment)\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 4.1 DECISION FRAMEWORK\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"âœ… 4.1 DECISION FRAMEWORK\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸ“ DECISION MATRIX:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Significant?    â”‚ Positive?    â”‚ Decision                    â”‚\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   â”‚ âœ… Yes          â”‚ âœ… Yes       â”‚ âœ… SHIP (if guardrails OK)  â”‚\n",
    "   â”‚ âœ… Yes          â”‚ âŒ No        â”‚ âŒ DO NOT SHIP              â”‚\n",
    "   â”‚ âŒ No           â”‚ âœ… Yes       â”‚ ðŸ”„ CONTINUE or ABANDON      â”‚\n",
    "   â”‚ âŒ No           â”‚ âŒ No        â”‚ âŒ ABANDON                  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "    \n",
    "    is_sig = p_value < 0.05\n",
    "    is_positive = (p_b - p_a) > 0\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š THIS EXPERIMENT:\n",
    "   Significant: {'âœ… Yes' if is_sig else 'âŒ No'} (p = {p_value:.2e})\n",
    "   Positive: {'âœ… Yes' if is_positive else 'âŒ No'} (lift = {(p_b - p_a)*100:.2f}pp)\n",
    "   \n",
    "   â†’ DECISION: {'âœ… SHIP' if is_sig and is_positive else 'âŒ DO NOT SHIP'}\n",
    "\"\"\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 4.2 BUSINESS IMPACT\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"â”€\" * 90)\n",
    "    print(\"ðŸ’° 4.2 BUSINESS IMPACT TRANSLATION\")\n",
    "    print(\"â”€\" * 90)\n",
    "    \n",
    "    effect_conv = p_b - p_a\n",
    "    annual_users = 10_000_000\n",
    "    ltv = 150\n",
    "    additional_conversions = effect_conv * annual_users\n",
    "    revenue_impact = additional_conversions * ltv\n",
    "    \n",
    "    print(f\"\"\"ðŸ“Š AT SCALE:\n",
    "   Annual users: {annual_users:,}\n",
    "   Baseline conversion: {p_a:.2%}\n",
    "   Treatment conversion: {p_b:.2%}\n",
    "   \n",
    "   Additional conversions: {additional_conversions:,.0f}/year\n",
    "   Revenue impact: ${revenue_impact:,.0f}/year\n",
    "   \n",
    "   ðŸ’¼ EXECUTIVE SUMMARY:\n",
    "   \"The new onboarding flow is expected to generate {additional_conversions:,.0f} additional\n",
    "   conversions per year, worth ${revenue_impact/1e6:.1f}M in annual revenue.\"\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"â•\" * 90)\n",
    "    print(\"â–ˆ COMPLETE CURRICULUM FINISHED!\")\n",
    "    print(\"â•\" * 90)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = run_complete_curriculum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef16c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atsaf (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
